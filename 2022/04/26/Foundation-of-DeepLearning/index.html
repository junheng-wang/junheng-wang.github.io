<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="���͡�ѧϰ�����˼�������С�����" />
       
      <meta name="description" content="���˺�����" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>深度学习_吴恩达_Part_1 |  Wang Junheng</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Foundation-of-DeepLearning"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  深度学习_吴恩达_Part_1
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/04/26/Foundation-of-DeepLearning/" class="article-date">
  <time datetime="2022-04-26T04:11:17.000Z" itemprop="datePublished">2022-04-26</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87/">学习提升</a> / <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87/%E7%BC%96%E7%A8%8B%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">编程与深度学习</a> / <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87/%E7%BC%96%E7%A8%8B%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">深度学习基础</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">4.9k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">19 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h3><h4 id="1-0-深度学习发展历史"><a href="#1-0-深度学习发展历史" class="headerlink" title="1.0 深度学习发展历史"></a>1.0 深度学习发展历史</h4><p><img src="https://pic.imgdb.cn/item/6267f189239250f7c59f6a42.png"></p>
<p>本小节参考链接：<br>参考链接1.1：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29096536">深度学习(deep learning)发展史 - 极海·GeoHey的文章 - 知乎</a><br>参考链接1.2：深度学习发展历程(MindSpore)-哔哩哔哩, <a target="_blank" rel="noopener" href="https://b23.tv/t4osJ6I">https://b23.tv/t4osJ6I</a></p>
<h4 id="1-1-欢迎"><a href="#1-1-欢迎" class="headerlink" title="1.1 欢迎"></a>1.1 欢迎</h4><p>从本节课我们将学到：</p>
<ul>
<li>学习神经网络的基础——神经网络与深度学习；</li>
<li>深度学习方面的实践；</li>
<li>如何结构化机器学习工程；</li>
<li>卷积神经网络(经常用于图像)；</li>
<li>序列模型以及如何应用到自然语言处理(NLP)，常见的序列模型有：循环神经网络(RNN)，长短期记忆网络(LSTM)模型。 </li>
</ul>
<h4 id="1-2-什么是神经网络"><a href="#1-2-什么是神经网络" class="headerlink" title="1.2 什么是神经网络"></a>1.2 什么是神经网络</h4><p>深度学习指的是“训练神经网络”</p>
<p>下面以房屋价格预测为例。首先，将已知的六间房子的价格和面积的关系绘制在二维平面上，如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267f293239250f7c5a1d58d.png" style="zoom:40%"></p>
<p>一般地，会用一条直线来拟合图中这些离散点，即建立房价与面积的线性模型。但是从实际考虑，价格永远不会是负数。所以对该直线做一点点修正，让它变成折线的形状，当面积小于某个值时，价格始终为零。如下图蓝色折线所示，就是建立的房价预测模型。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f3cf239250f7c5a4cb1c.png" style="zoom:40%"></p>
<p>其实这个简单的模型（蓝色折线）就可以看成是一个神经网络，而且几乎是一个最简单的神经网络。我们把该房价预测用一个最简单的神经网络模型来表示，如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267f419239250f7c5a57391.png" style="zoom:40%"></p>
<p>上图中的小圆圈就可以视为一个独立的神经元，这个简单网络实现了左边函数的功能值得一提的是，上图神经元的预测函数（蓝色折线）在神经网络应用中比较常见。把这个函数称为<strong>线性整流函数(Rectified Linear Unit, ReLU)</strong>，形如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267f506239250f7c5a7c622.png" style="zoom:70%"></p>
<p>上面是一个最为简单的神经网络，更深的神经网络可以视为：将这些单个的神经元看作乐高积木，通过搭建积木来构建更大更深的网络。把上面举的房价预测的例子变得复杂一些，而不是仅仅使用房屋面积一个判断因素。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f560239250f7c5a8b1e3.png" style="zoom:50%"></p>
<p>在给定这四个输入后，神经网络所做的就是输出房屋的预测价格y。上图中三个神经元所在的位置称之为<strong>中间层或者隐藏层</strong>(x所在的称之为输入层，y所在的称之为输出层)，每个神经元与所有的输入x都有关联(直线相连)。 </p>
<h4 id="1-3-使用神经网络进行监督学习"><a href="#1-3-使用神经网络进行监督学习" class="headerlink" title="1.3 使用神经网络进行监督学习"></a>1.3 使用神经网络进行监督学习</h4><p>由神经网络模型创造的价值基本上都是基于<strong>监督式学习(Supervised Learning)</strong>的。监督式学习与非监督式学习本质区别就是<strong>是否已知训练样本的输出y</strong>。在实际应用中，机器学习解决的大部分问题都属于监督式学习，神经网络模型也大都属于监督式学习。下面我们来看几个监督式学习在神经网络中应用的例子。</p>
<ul>
<li>房屋价格预测。根据训练样本的输入x和输出y，训练神经网络模型，预测房价。</li>
<li>线上广告。输入x是广告和用户个人信息，输出y是用户是否对广告进行点击。神经网络模型经过训练，能够根据广告类型和用户信息对用户的点击行为进行预测，从而向用户提供用户自己可能感兴趣的广告。</li>
<li>电脑视觉(computer vision)。电脑视觉是近些年来越来越火的课题，而电脑视觉发展迅速的原因很大程度上是得益于深度学习。其中，输入x是图片像素值，输出是图片所属的不同类别。</li>
<li>语音识别(speech recognition)。深度学习可以将一段语音信号辨识为相应的文字信息。</li>
<li>智能翻译。例如通过神经网络输入英文，然后直接输出中文。</li>
<li>自动驾驶。通过输入一张图片或者汽车雷达信息，神经网络通过训练来告诉你相应的路况信息并作出相应的决策。 </li>
</ul>
<p><img src="https://pic.imgdb.cn/item/6267f5d9239250f7c5a9dfcd.png" style="zoom:50%"></p>
<p>根据不同的问题和应用场合，应该使用不同类型的神经网络模型。CNN和RNN是比较常用的神经网络模型。下图给出了Standard NN，Convolutional NN和Recurrent NN的神经网络结构图。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f6c4239250f7c5ac1c6c.png"></p>
<p>数据类型一般分为两种：<strong>结构化数据(Structured Data)</strong>和<strong>非结构化数据(Unstructured Data)</strong> </p>
<p><img src="https://pic.imgdb.cn/item/6267f734239250f7c5ad2a26.png" style="zoom:40%"></p>
<h4 id="1-4-为什么深度学习流行起来了"><a href="#1-4-为什么深度学习流行起来了" class="headerlink" title="1.4 为什么深度学习流行起来了"></a>1.4 为什么深度学习流行起来了</h4><p>略</p>
<h3 id="第二章-神经网络基础之逻辑回归"><a href="#第二章-神经网络基础之逻辑回归" class="headerlink" title="第二章 神经网络基础之逻辑回归"></a>第二章 神经网络基础之逻辑回归</h3><p>下面开始介绍神经网络的基础：逻辑回归（Logistic Regression）。通过对逻辑回归模型结构的分析，为后面学习神经网络模型打下基础。 </p>
<h4 id="2-1-二分类-Binary-Classification"><a href="#2-1-二分类-Binary-Classification" class="headerlink" title="2.1 二分类(Binary Classification)"></a>2.1 二分类(Binary Classification)</h4><p>逻辑回归模型一般用来解决二分类(Binary Classification)问题。二分类就是输出只有{0,1\}两个离散值(也有{-1,1}的情况)。以一个图像识别问题为例，判断图片中是否有猫存在，0代表not cat，1代表cat。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f7e7239250f7c5aece58.png" style="zoom:50%"></p>
<p>如上图所示，这是一个典型的二分类问题。一般来说，彩色图片包含RGB三个通道。例如该cat图片的尺寸为<script type="math/tex">(64, 64, 3)</script>。在神经网络模型中，我们首先要将图片输入<script type="math/tex">x</script>(维度是<script type="math/tex">(64, 64, 3)</script>)转化为一维的特征向量(feature vector)。方法是每个通道一行一行取，再连接起来。由于<script type="math/tex">64\times 64\times 3=12288</script>，则转化后的输入特征向量维度为<script type="math/tex">(12288, 1)</script>。此特征向量<script type="math/tex">x</script>是列向量，维度一般记为<script type="math/tex">n_x</script>。</p>
<p>如果训练样本共有<script type="math/tex">m</script>张图片，那么整个训练样本<script type="math/tex">X</script>组成了矩阵，维度是<script type="math/tex">(n_x,m)</script>。注意，这里矩阵<script type="math/tex">X</script>的行<script type="math/tex">n_x</script>代表了每个样本<script type="math/tex">x^{(i)}</script>特征个数，列<script type="math/tex">m</script>代表了样本个数。这里，Andrew解释了<script type="math/tex">X</script>的维度之所以是<script type="math/tex">(n_x, m)</script>而不是<script type="math/tex">(m, n_x)</script>的原因是为了之后矩阵运算的方便。算是Andrew给我们的一个小小的经验吧。而所有训练样本的输出<script type="math/tex">Y</script>也组成了一维的行向量，写成矩阵的形式后，它的维度就是<script type="math/tex">(1, m)</script>。</p>
<blockquote>
<p>后面课程会用到的一些符号<br>用一对<script type="math/tex">(x, y)</script>来表示一个单独的样本，其中<script type="math/tex">x</script>是<script type="math/tex">n_x</script>维特征向量(可记为<script type="math/tex">x \in \mathbb{R}^{n_x}</script>)，<script type="math/tex">y \in \{0, 1\}</script>，训练集由<script type="math/tex">m</script>个训练样本组成，<script type="math/tex">(x^{(i)}, y^{(i)})</script>表示样本<script type="math/tex">n</script>的输入输出。为了便于表示和区分，有时训练集表示为<script type="math/tex">m = m_{\text{train}}</script>，测试集表示为<script type="math/tex">m = m_{\text{test}}</script>，可进一步将训练集表示为更紧凑的形式，用矩阵<script type="math/tex">X</script>表示：<script type="math/tex">X=\left(\begin{array}{cccc}\vdots & \vdots & & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\\vdots & \vdots & & \vdots \\\end{array}\right)</script>，<script type="math/tex">X</script>的大小为<script type="math/tex">(n_x, m)</script>，<script type="math/tex">X \in \mathbb{R}^{n_x \times m}</script>，输出用<script type="math/tex">Y</script>表示：<script type="math/tex">Y = [y^{(1)}, y^{(1)}, \cdots, y^{(m)}]</script>，<script type="math/tex">Y</script>的大小为<script type="math/tex">(1, m)</script>，<script type="math/tex">Y \in \mathbb{R}^{1\times m}</script></p>
</blockquote>
<h4 id="2-2-logistic回归"><a href="#2-2-logistic回归" class="headerlink" title="2.2 logistic回归"></a>2.2 logistic回归</h4><p>这是一个学习算法，用于监督学习中输出<script type="math/tex">y​</script>是<script type="math/tex">0​</script>或<script type="math/tex">1​</script>的二元分类问题。</p>
<p>逻辑回归中，预测值<script type="math/tex">\hat{h} = \text{P}(y=1|x)</script>表示为输入<script type="math/tex">x</script>输出为<script type="math/tex">y = 1</script>的概率，取值范围在<script type="math/tex">[0,1]</script>之间，这是其与二分类模型不同的地方。使用线性模型，引入参数<script type="math/tex">w</script>和<script type="math/tex">b</script>。权重<script type="math/tex">w</script>的维度是<script type="math/tex">(n_x, 1)</script>，<script type="math/tex">b</script>是一个常数项，即<script type="math/tex">w \in \mathbb{R}^{n_x \times 1}, b \in \mathbb{R}</script>。这样，逻辑回归的线性预测输出可以写成：</p>
<script type="math/tex; mode=display">
\hat{y} = w^{\text{T}}x+b</script><p>值得注意的是，很多其它机器学习资料中，可能把常数<script type="math/tex">b</script>当做<script type="math/tex">w_0</script>处理，并引入<script type="math/tex">x_0=1</script>。这样从维度上来看，<script type="math/tex">x</script>和<script type="math/tex">w</script>都会增加一维。但在本课程中，为了简化计算和便于理解，Andrew建议还是使用上式这种形式将<script type="math/tex">w</script>和<script type="math/tex">b</script>分开比较好。</p>
<p>上式的线性输出区间为整个实数范围，而<strong>逻辑回归要求输出范围在</strong><script type="math/tex">[0,1]</script><strong>之间</strong>，所以还需要对上式的线性函数输出进行处理。方法是引入sigmoid函数，让输出限定在<script type="math/tex">[0,1]</script>之间。这样，逻辑回归的预测输出就可以完整写成：</p>
<script type="math/tex; mode=display">
\hat{y} = \text{sigmoid}(w^{\text T}x+b) = \sigma(w^{\text T}x+b)</script><p>sigmoid函数是一种非线性的S型函数，输出被限定在<script type="math/tex">[0,1]</script>之间，通常被用在神经网络中当作<strong>激活函数(Activation function)</strong>使用。Sigmoid函数的表达式：</p>
<script type="math/tex; mode=display">
\text{sigmoid}(z) = \frac{1}{1+e^{-z}}</script><p> 通过Sigmoid函数，就能够将逻辑回归的输出限定在<script type="math/tex">[0,1]</script>之间了。</p>
<h4 id="2-3-logistic回归损失函数"><a href="#2-3-logistic回归损失函数" class="headerlink" title="2.3 logistic回归损失函数"></a>2.3 logistic回归损失函数</h4><p>逻辑回归中，<script type="math/tex">w</script>和<script type="math/tex">b</script>都是未知参数，需要反复训练优化得到。因此，我们需要定义一个成本函数(cost function)，包含了参数<script type="math/tex">w</script>和<script type="math/tex">b</script>。通过优化cost function，当cost function取值最小时，得到对应的<script type="math/tex">w</script>和<script type="math/tex">b</script>。</p>
<p>如何定义所有<script type="math/tex">m</script>个样本的cost function呢？先从单个样本出发，我们希望该样本的预测值<script type="math/tex">\hat y</script>与真实值越相似越好。我们把单个样本的cost function用<strong>Loss function</strong>来表示，根据以往经验，如果使用平方错误(squared error)来衡量，如下所示：</p>
<script type="math/tex; mode=display">
L(\hat y, y) = \frac{1}{2}(\hat y-y)^2</script><p>但是，对于逻辑回归，我们一般不使用平方错误来作为Loss function。原因是这种Loss function一般是<strong>非凸(non-convex)</strong>的。non-convex函数在使用梯度下降算法时，容易得到局部最小值(local minumum)，即<strong>局部最优化</strong>。而我们最优化的目标是计算得到全局最优化(Global optimization)。因此，我们一般选择的Loss function应该是convex的。因此，我们可以构建另外一种Loss function，且是convex的，如下所示：</p>
<script type="math/tex; mode=display">
L(\hat y, y) = -(y\log \hat y + (1-y)\log(1-\hat y))</script><p>我们来分析一下这个Loss function，它是衡量错误大小的，Loss function越小越好。</p>
<p>当<script type="math/tex">y = 1</script>时，我们带入上式容易得知<script type="math/tex">\hat y\rightarrow1</script>时<script type="math/tex">L(\hat y, y)\rightarrow0</script>，预测效果越好；同理，当<script type="math/tex">y=0</script>时，<script type="math/tex">\hat y \rightarrow 0</script>则<script type="math/tex">L(\hat y,y)\rightarrow0</script>，预测效果越好。后续将会提到这个损失函数是如何推导出来的。</p>
<p>上面介绍的Loss function是针对单个样本的。那对于<script type="math/tex">m</script>个样本，我们定义Cost function，Cost function是<script type="math/tex">m</script>个样本的Loss function的平均值，反映了<script type="math/tex">m</script>个样本的预测输出<script type="math/tex">\hat y</script>与真实样本输出<script type="math/tex">y</script>的平均接近程度。Cost function可表示为：</p>
<script type="math/tex; mode=display">
J(w,b) = \frac1m \sum_{i=0}^{m}L(\hat y^{(i)}, y^{(i)})</script><p>Cost function已经推导出来了，Cost function是关于待求系数w和b的函数。我们的目标就是迭代计算出最佳的w和b值，<strong>最小化Cost function</strong>，让Cost function尽可能地接近于零。</p>
<p>其实逻辑回归问题可以看成是一个简单的神经网络，只包含<strong>一个神经元</strong>。这也是我们这里先介绍逻辑回归的原因。</p>
<h4 id="2-4-梯度下降法"><a href="#2-4-梯度下降法" class="headerlink" title="2.4 梯度下降法"></a>2.4 梯度下降法</h4><p>使用<strong>梯度下降(Gradient Descent)</strong>算法来计算出合适的<script type="math/tex">w</script>和<script type="math/tex">b</script>值，从而最小化<script type="math/tex">m</script>个训练样本的Cost function，即<script type="math/tex">J(w,b)</script>。</p>
<p>由于<script type="math/tex">J(w,b)</script>是convex  function，梯度下降算法是先随机选择一组参数<script type="math/tex">w</script>和<script type="math/tex">b</script>值，然后每次迭代的过程中分别沿着<script type="math/tex">w</script>和<script type="math/tex">b</script>的梯度(偏导数)的反方向前进一小步，不断修正<script type="math/tex">w</script>和<script type="math/tex">b</script>。每次迭代更新后，都能让<script type="math/tex">J(w,b)</script>更接近全局最小值。梯度下降的过程如下图所示。</p>
<p><img src="https://pic.imgdb.cn/item/648a82371ddac507ccb0ec36.jpg" style="zoom:50%"></p>
<p>梯度下降算法每次迭代更新，<script type="math/tex">w</script>和<script type="math/tex">b</script>的修正表达式为：</p>
<script type="math/tex; mode=display">
w:=w-\alpha \frac{\partial J(w, b)}{\partial w}</script><script type="math/tex; mode=display">
b:=b-\alpha \frac{\partial J(w, b)}{\partial b}</script><p>上式中，<script type="math/tex">\alpha</script>是学习因子(learning rate)，表示梯度下降的不仅长度。梯度下降算法能够保证每次迭代w和b都能向着J(w,b)全局最小化的方向进行。其<a target="_blank" rel="noopener" href="http://blog.csdn.net/red_stone1/article/details/72229903">数学原理</a>主要是运用泰勒一阶展开来证明的。</p>
<h4 id="2-5-2-6-导数复习"><a href="#2-5-2-6-导数复习" class="headerlink" title="2.5-2.6 导数复习"></a>2.5-2.6 导数复习</h4><p>这一部分的内容相对简单，Andrew主要是给对微积分、求导数不太清楚的同学介绍的。梯度或者导数一定程度上可以看成是斜率。关于求导数的方法这里就不再赘述了。</p>
<h4 id="2-7-计算图"><a href="#2-7-计算图" class="headerlink" title="2.7 计算图"></a>2.7 计算图</h4><p>整个神经网络的训练过程实际上包含了两个过程：<strong>正向传播(Forward Propagation)</strong>和<strong>反向传播(Back Propagation)</strong>。正向传播是从输入到输出，由神经网络计算得到预测输出的过程；反向传播是从输出到输入，对参数w和b计算梯度的过程。下面，我们用<strong>计算图(Computation graph)</strong>的形式来理解这两个过程。</p>
<p>举个简单的例子，假如Cost function为<script type="math/tex">J(a,b,c)=3(a+bc)</script>，包含<script type="math/tex">a</script>，<script type="math/tex">b</script>，<script type="math/tex">c</script>三个变量。我们用<script type="math/tex">u</script>表示<script type="math/tex">bc</script>，<script type="math/tex">v</script>表示<script type="math/tex">a+u</script>，则<script type="math/tex">J=3v</script>。它的计算图可以写成如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648a82ed1ddac507ccb243d5.jpg" style="zoom:50%"></p>
<p>令<script type="math/tex">a=5,b=3,c=2</script>，则<script type="math/tex">u=bc=6,v=a+u=11,J=3v=33</script>。计算图中，这种从左到右，从输入到输出的过程就对应着神经网络或者逻辑回归中输入与权重经过运算计算得到Cost function的正向过程。</p>
<h4 id="2-8-使用计算图求导"><a href="#2-8-使用计算图求导" class="headerlink" title="2.8 使用计算图求导"></a>2.8 使用计算图求导</h4><p>下面我们来介绍反向传播(Back Propagation)，即计算输出对输入的偏导数。</p>
<p><img src="https://pic.imgdb.cn/item/648a83431ddac507ccb2e777.jpg" style="zoom:50%"></p>
<h4 id="2-9-logistic回归中的梯度下降法"><a href="#2-9-logistic回归中的梯度下降法" class="headerlink" title="2.9 logistic回归中的梯度下降法"></a>2.9 logistic回归中的梯度下降法</h4><p>对逻辑回归进行梯度计算。对单个样本而言，逻辑回归Loss function表达式如下：</p>
<script type="math/tex; mode=display">
\begin{gathered}
&z = w^\text Tx+b \\
&\hat y = a = \sigma(z) \\
&L(\hat y,y) = L(a, y) = -(y\log a+(1-y)\log(1-a))
\end{gathered}</script><p>该逻辑回归的正向传播过程非常简单。据上述公式，例如输入样本<script type="math/tex">x</script>有两个特征<script type="math/tex">(x_1,x_2)</script>，相应的权重也有两个<script type="math/tex">(w_1,w_2)</script>，则<script type="math/tex">z = w_1x_1+w_2x_2+b</script>。</p>
<p>然后，计算该逻辑回归的反向传播过程，即由Loss function计算参数<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数：</p>
<script type="math/tex; mode=display">
\begin{gathered}
&\mathrm d a=\frac{\partial L}{\partial a}=-\frac{y}{a}+\frac{1-y}{1-a}\\
&\mathrm d z=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z}=\left(-\frac{y}{a}+\frac{1-y}{1-a}\right) \cdot a(1-a)=a-y
\end{gathered}</script><p>知道了<script type="math/tex">\mathrm dz</script>之后，就可以直接对<script type="math/tex">w_1，w_2</script>和<script type="math/tex">b</script>进行求导了：</p>
<script type="math/tex; mode=display">
\begin{gathered}
&\mathrm d w_1=\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w_1}=x_1 \cdot d z=x_1(a-y)\\
&\mathrm d w_2=\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w_2}=x_2 \cdot d z=x_2(a-y)\\
&\mathrm d b=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial b}=1 \cdot d z=a-y
\end{gathered}</script><p>则梯度下降算法可表示为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
w{1}:=w{1}-\alpha ~\mathrm d w{1} \\
w{2}:=w{2}-\alpha ~\mathrm d w{2} \\
b:=b-\alpha ~\mathrm d b
\end{gathered}</script><p><img src="https://pic.imgdb.cn/item/648a84f21ddac507ccb64b43.jpg" style="zoom:50%"></p>
<h4 id="2-10-m-个样本的梯度下降"><a href="#2-10-m-个样本的梯度下降" class="headerlink" title="2.10 m 个样本的梯度下降"></a>2.10 m 个样本的梯度下降</h4><p>上一部分讲的是对单个样本求偏导和梯度下降。如果有m个样本，其Cost function表达式如下：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z^{(i)}=w^{T} x^{(i)}+b \\
\hat{y}^{(i)}=a^{(i)}=\sigma\left(z^{(i)}\right) \\
J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \hat{y}^{(i)}+\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)\right]
\end{gathered}</script><p>Cost function关于<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数可以写成和平均的形式：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathrm d w_{1}=\frac{1}{m} \sum_{i=1}^{m} x_{1}^{(i)}\left(a^{(i)}-y^{(i)}\right) \\
\mathrm d w_{2}=\frac{1}{m} \sum_{i=1}^{m} x_{2}^{(i)}\left(a^{(i)}-y^{(i)}\right) \\
\mathrm d b=\frac{1}{m} \sum_{i=1}^{m} \left(a^{(i)}-y^{(i)}\right)
\end{gathered}</script><p>这样，每次迭代中<script type="math/tex">w</script>和<script type="math/tex">b</script>的梯度有$m$个训练样本计算平均值得到。其算法伪代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">J=<span class="number">0</span>; dw1=<span class="number">0</span>; dw2=<span class="number">0</span>; db=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to m</span><br><span class="line">    z(i) = wx(i)+b;</span><br><span class="line">    a(i) = sigmoid(z(i));</span><br><span class="line">    J += -[y(i)log(a(i))+(<span class="number">1</span>-y(i)）log(<span class="number">1</span>-a(i));</span><br><span class="line">    dz(i) = a(i)-y(i);</span><br><span class="line">    dw1 += x1(i)dz(i);</span><br><span class="line">    dw2 += x2(i)dz(i);</span><br><span class="line">    db += dz(i);</span><br><span class="line">J /= m;</span><br><span class="line">dw1 /= m;</span><br><span class="line">dw2 /= m;</span><br><span class="line">db /= m;</span><br></pre></td></tr></table></figure>
<p>经过每次迭代后，根据梯度下降算法，<script type="math/tex">w</script>和<script type="math/tex">b</script>都进行更新：</p>
<script type="math/tex; mode=display">
\begin{gathered}
w_{1}:=w_{1}-\alpha ~ \mathrm d w_{1} \\
w_{2}:=w_{2}-\alpha ~ \mathrm d w_{2} \\
b:=b-\alpha ~ \mathrm d b
\end{gathered}</script><p>这样经过<script type="math/tex">n</script>次迭代后，整个梯度下降算法就完成了。</p>
<p>值得一提的是，在上述的梯度下降算法中，是利用for循环对每个样本进行dw1，dw2和db的累加计算最后再求平均数的。在深度学习中，样本数量$m$通常很大，使用for循环会让神经网络程序运行得很慢。所以，我们应该尽量避免使用for循环操作，而使用<strong>矩阵运算</strong>，能够大大提高程序运行速度。关于<strong>向量化(vectorization)</strong>的内容我们放在下次笔记中再说。</p>
<h4 id="2-11-2-12-向量化"><a href="#2-11-2-12-向量化" class="headerlink" title="2.11-2.12 向量化"></a>2.11-2.12 向量化</h4><p>深度学习算法中，数据量很大，在程序中应该尽量减少使用loop循环语句，而可以使用向量运算来提高程序运行速度。</p>
<p>向量化(Vectorization)就是利用矩阵运算的思想，大大提高运算速度。</p>
<p>上一部分我们讲了应该尽量避免使用for循环而使用向量化矩阵运算。在python的numpy库中，我们通常使用<strong>np.dot()</strong>函数来进行矩阵运算。</p>
<p>我们将向量化的思想使用在逻辑回归算法上，尽可能减少for循环，而只使用矩阵运算。值得注意的是，算法最顶层的迭代训练的for循环是不能替换的。而每次迭代过程对J，dw，b的计算是可以直接使用矩阵运算。</p>
<h4 id="2-13-向量化logistic回归"><a href="#2-13-向量化logistic回归" class="headerlink" title="2.13 向量化logistic回归"></a>2.13 向量化logistic回归</h4><p>在前面的笔记中我们提到过，整个训练样本构成的输入矩阵<script type="math/tex">X</script>的维度是<script type="math/tex">(,m)</script>，权重矩阵<script type="math/tex">w</script>的维度是<script type="math/tex">(,1)</script>，<script type="math/tex">b</script>是一个常数值，而整个训练样本构成的输出矩阵<script type="math/tex">Y</script>的维度为<script type="math/tex">(1,m)</script>。利用向量化的思想，所有<script type="math/tex">m</script>个样本的线性输出<script type="math/tex">Z</script>可以用矩阵表示：</p>
<script type="math/tex; mode=display">
Z = w^{\text T}X+b</script><p>在python的numpy库中可以表示为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b      <span class="comment"># w.T表示w的转置</span></span><br><span class="line">A = sigmoid(Z)</span><br></pre></td></tr></table></figure>
<p>这样，我们就能够使用向量化矩阵运算代替for循环，对所有<script type="math/tex">m</script>个样本同时运算，大大提高了运算速度。 </p>
<h4 id="2-14-向量化logistic回归的梯度输出"><a href="#2-14-向量化logistic回归的梯度输出" class="headerlink" title="2.14 向量化logistic回归的梯度输出"></a>2.14 向量化logistic回归的梯度输出</h4><p>逻辑回归中的梯度下降算法如何转化为向量化的矩阵形式。对于所有<script type="math/tex">m</script>个样本，<script type="math/tex">\mathrm dZ</script>的维度是<script type="math/tex">(1, m)</script>，可表示为：</p>
<script type="math/tex; mode=display">
dZ = A-Y</script><script type="math/tex; mode=display">\mathrm d b$$可以表示为：</script><p>\mathrm d b = \frac{1}{m} \sum_{i=1}^{m}dz^{(i)}</p>
<p>$$<br>对应的程序可以写成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dw = <span class="number">1</span>/m*np.dot(X,dZ.T)</span><br></pre></td></tr></table></figure>
<p>这样，我们把整个逻辑回归中的for循环尽可能用矩阵运算代替，对于单次迭代，梯度下降算法流程如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A-Y</span><br><span class="line">dw = <span class="number">1</span>/m*np.dot(X,dZ.T)</span><br><span class="line">db = <span class="number">1</span>/m*np.<span class="built_in">sum</span>(dZ)</span><br><span class="line">w = w - alpha*dw</span><br><span class="line">b = b - alpha*db</span><br></pre></td></tr></table></figure><br>其中，<strong>alpha是学习因子</strong>，决定<script type="math/tex">w</script>和<script type="math/tex">b</script>的更新速度。上述代码只是对单次训练更新而言的，外层还需要一个for循环，表示迭代次数。</p>
<h3 id="第三章-浅层神经网络"><a href="#第三章-浅层神经网络" class="headerlink" title="第三章 浅层神经网络"></a>第三章 浅层神经网络</h3><h4 id="3-1-神经网络概览"><a href="#3-1-神经网络概览" class="headerlink" title="3.1 神经网络概览"></a>3.1 神经网络概览</h4><link rel="stylesheet" href="/css/bilicard.css" type="text/css"> 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          打赏
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://junheng-wang.github.io/2022/04/26/Foundation-of-DeepLearning/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" rel="tag">深度学习基础</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2022/04/26/Basic-knowledge-of-programming-software/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            编程软件基础知识——杂记
          
        </div>
      </a>
    
    
      <a href="/2022/04/25/Python-code-tips/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Python其他小知识</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "0DieopImIy7vnuzj4jQ2wk6O-gzGzoHsz",
    app_key: "j6eRiYtlDSl8eRXVN54blF25",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2023
        <i class="ri-heart-fill heart_icon"></i> wjh
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Wang Junheng"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/Alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechatpay.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>

</html>