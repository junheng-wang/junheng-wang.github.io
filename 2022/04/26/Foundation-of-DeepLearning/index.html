<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="���͡�ѧϰ�����˼�������С�����" />
       
      <meta name="description" content="���˺�����" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>深度学习_吴恩达_Part_1 |  Wang Junheng</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Foundation-of-DeepLearning"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  深度学习_吴恩达_Part_1
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/04/26/Foundation-of-DeepLearning/" class="article-date">
  <time datetime="2022-04-26T04:11:17.000Z" itemprop="datePublished">2022-04-26</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87/">学习提升</a> / <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87/%E7%BC%96%E7%A8%8B%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">编程与深度学习</a> / <a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0%E6%8F%90%E5%8D%87/%E7%BC%96%E7%A8%8B%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/">深度学习基础</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">8.2k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">32 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h3 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h3><h4 id="1-0-深度学习发展历史"><a href="#1-0-深度学习发展历史" class="headerlink" title="1.0 深度学习发展历史"></a>1.0 深度学习发展历史</h4><p><img src="https://pic.imgdb.cn/item/6267f189239250f7c59f6a42.png"></p>
<p>本小节参考链接：<br>参考链接1.1：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29096536">深度学习(deep learning)发展史 - 极海·GeoHey的文章 - 知乎</a><br>参考链接1.2：深度学习发展历程(MindSpore)-哔哩哔哩, <a target="_blank" rel="noopener" href="https://b23.tv/t4osJ6I">https://b23.tv/t4osJ6I</a></p>
<h4 id="1-1-欢迎"><a href="#1-1-欢迎" class="headerlink" title="1.1 欢迎"></a>1.1 欢迎</h4><p>从本节课我们将学到：</p>
<ul>
<li>学习神经网络的基础——神经网络与深度学习；</li>
<li>深度学习方面的实践；</li>
<li>如何结构化机器学习工程；</li>
<li>卷积神经网络(经常用于图像)；</li>
<li>序列模型以及如何应用到自然语言处理(NLP)，常见的序列模型有：循环神经网络(RNN)，长短期记忆网络(LSTM)模型。 </li>
</ul>
<h4 id="1-2-什么是神经网络"><a href="#1-2-什么是神经网络" class="headerlink" title="1.2 什么是神经网络"></a>1.2 什么是神经网络</h4><p>深度学习指的是“训练神经网络”</p>
<p>下面以房屋价格预测为例。首先，将已知的六间房子的价格和面积的关系绘制在二维平面上，如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267f293239250f7c5a1d58d.png" style="zoom:40%"></p>
<p>一般地，会用一条直线来拟合图中这些离散点，即建立房价与面积的线性模型。但是从实际考虑，价格永远不会是负数。所以对该直线做一点点修正，让它变成折线的形状，当面积小于某个值时，价格始终为零。如下图蓝色折线所示，就是建立的房价预测模型。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f3cf239250f7c5a4cb1c.png" style="zoom:40%"></p>
<p>其实这个简单的模型（蓝色折线）就可以看成是一个神经网络，而且几乎是一个最简单的神经网络。我们把该房价预测用一个最简单的神经网络模型来表示，如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267f419239250f7c5a57391.png" style="zoom:40%"></p>
<p>上图中的小圆圈就可以视为一个独立的神经元，这个简单网络实现了左边函数的功能值得一提的是，上图神经元的预测函数（蓝色折线）在神经网络应用中比较常见。把这个函数称为<strong>线性整流函数(Rectified Linear Unit, ReLU)</strong>，形如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267f506239250f7c5a7c622.png" style="zoom:70%"></p>
<p>上面是一个最为简单的神经网络，更深的神经网络可以视为：将这些单个的神经元看作乐高积木，通过搭建积木来构建更大更深的网络。把上面举的房价预测的例子变得复杂一些，而不是仅仅使用房屋面积一个判断因素。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f560239250f7c5a8b1e3.png" style="zoom:50%"></p>
<p>在给定这四个输入后，神经网络所做的就是输出房屋的预测价格y。上图中三个神经元所在的位置称之为<strong>中间层或者隐藏层</strong>(x所在的称之为输入层，y所在的称之为输出层)，每个神经元与所有的输入x都有关联(直线相连)。 </p>
<h4 id="1-3-使用神经网络进行监督学习"><a href="#1-3-使用神经网络进行监督学习" class="headerlink" title="1.3 使用神经网络进行监督学习"></a>1.3 使用神经网络进行监督学习</h4><p>由神经网络模型创造的价值基本上都是基于<strong>监督式学习(Supervised Learning)</strong>的。监督式学习与非监督式学习本质区别就是<strong>是否已知训练样本的输出y</strong>。在实际应用中，机器学习解决的大部分问题都属于监督式学习，神经网络模型也大都属于监督式学习。下面我们来看几个监督式学习在神经网络中应用的例子。</p>
<ul>
<li>房屋价格预测。根据训练样本的输入x和输出y，训练神经网络模型，预测房价。</li>
<li>线上广告。输入x是广告和用户个人信息，输出y是用户是否对广告进行点击。神经网络模型经过训练，能够根据广告类型和用户信息对用户的点击行为进行预测，从而向用户提供用户自己可能感兴趣的广告。</li>
<li>电脑视觉(computer vision)。电脑视觉是近些年来越来越火的课题，而电脑视觉发展迅速的原因很大程度上是得益于深度学习。其中，输入x是图片像素值，输出是图片所属的不同类别。</li>
<li>语音识别(speech recognition)。深度学习可以将一段语音信号辨识为相应的文字信息。</li>
<li>智能翻译。例如通过神经网络输入英文，然后直接输出中文。</li>
<li>自动驾驶。通过输入一张图片或者汽车雷达信息，神经网络通过训练来告诉你相应的路况信息并作出相应的决策。 </li>
</ul>
<p><img src="https://pic.imgdb.cn/item/6267f5d9239250f7c5a9dfcd.png" style="zoom:50%"></p>
<p>根据不同的问题和应用场合，应该使用不同类型的神经网络模型。CNN和RNN是比较常用的神经网络模型。下图给出了Standard NN，Convolutional NN和Recurrent NN的神经网络结构图。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f6c4239250f7c5ac1c6c.png"></p>
<p>数据类型一般分为两种：<strong>结构化数据(Structured Data)</strong>和<strong>非结构化数据(Unstructured Data)</strong> </p>
<p><img src="https://pic.imgdb.cn/item/6267f734239250f7c5ad2a26.png" style="zoom:40%"></p>
<h4 id="1-4-为什么深度学习流行起来了"><a href="#1-4-为什么深度学习流行起来了" class="headerlink" title="1.4 为什么深度学习流行起来了"></a>1.4 为什么深度学习流行起来了</h4><p>略</p>
<h3 id="第二章-神经网络基础之逻辑回归"><a href="#第二章-神经网络基础之逻辑回归" class="headerlink" title="第二章 神经网络基础之逻辑回归"></a>第二章 神经网络基础之逻辑回归</h3><p>下面开始介绍神经网络的基础：逻辑回归（Logistic Regression）。通过对逻辑回归模型结构的分析，为后面学习神经网络模型打下基础。 </p>
<h4 id="2-1-二分类-Binary-Classification"><a href="#2-1-二分类-Binary-Classification" class="headerlink" title="2.1 二分类(Binary Classification)"></a>2.1 二分类(Binary Classification)</h4><p>逻辑回归模型一般用来解决二分类(Binary Classification)问题。二分类就是输出只有{0,1\}两个离散值(也有{-1,1}的情况)。以一个图像识别问题为例，判断图片中是否有猫存在，0代表not cat，1代表cat。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f7e7239250f7c5aece58.png" style="zoom:50%"></p>
<p>如上图所示，这是一个典型的二分类问题。一般来说，彩色图片包含RGB三个通道。例如该cat图片的尺寸为<script type="math/tex">(64, 64, 3)</script>。在神经网络模型中，我们首先要将图片输入<script type="math/tex">x</script>(维度是<script type="math/tex">(64, 64, 3)</script>)转化为一维的特征向量(feature vector)。方法是每个通道一行一行取，再连接起来。由于<script type="math/tex">64\times 64\times 3=12288</script>，则转化后的输入特征向量维度为<script type="math/tex">(12288, 1)</script>。此特征向量<script type="math/tex">x</script>是列向量，维度一般记为<script type="math/tex">n_x</script>。</p>
<p>如果训练样本共有<script type="math/tex">m</script>张图片，那么整个训练样本<script type="math/tex">X</script>组成了矩阵，维度是<script type="math/tex">(n_x,m)</script>。注意，这里矩阵<script type="math/tex">X</script>的行<script type="math/tex">n_x</script>代表了每个样本<script type="math/tex">x^{(i)}</script>特征个数，列<script type="math/tex">m</script>代表了样本个数。这里，Andrew解释了<script type="math/tex">X</script>的维度之所以是<script type="math/tex">(n_x, m)</script>而不是<script type="math/tex">(m, n_x)</script>的原因是为了之后矩阵运算的方便。算是Andrew给我们的一个小小的经验吧。而所有训练样本的输出<script type="math/tex">Y</script>也组成了一维的行向量，写成矩阵的形式后，它的维度就是<script type="math/tex">(1, m)</script>。</p>
<blockquote>
<p>后面课程会用到的一些符号<br>用一对<script type="math/tex">(x, y)</script>来表示一个单独的样本，其中<script type="math/tex">x</script>是<script type="math/tex">n_x</script>维特征向量(可记为<script type="math/tex">x \in \mathbb{R}^{n_x}</script>)，<script type="math/tex">y \in \{0, 1\}</script>，训练集由<script type="math/tex">m</script>个训练样本组成，<script type="math/tex">(x^{(i)}, y^{(i)})</script>表示样本<script type="math/tex">n</script>的输入输出。为了便于表示和区分，有时训练集表示为<script type="math/tex">m = m_{\text{train}}</script>，测试集表示为<script type="math/tex">m = m_{\text{test}}</script>，可进一步将训练集表示为更紧凑的形式，用矩阵<script type="math/tex">X</script>表示：<script type="math/tex">X=\left(\begin{array}{cccc}\vdots & \vdots & & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\\vdots & \vdots & & \vdots \\\end{array}\right)</script>，<script type="math/tex">X</script>的大小为<script type="math/tex">(n_x, m)</script>，<script type="math/tex">X \in \mathbb{R}^{n_x \times m}</script>，输出用<script type="math/tex">Y</script>表示：<script type="math/tex">Y = [y^{(1)}, y^{(1)}, \cdots, y^{(m)}]</script>，<script type="math/tex">Y</script>的大小为<script type="math/tex">(1, m)</script>，<script type="math/tex">Y \in \mathbb{R}^{1\times m}</script></p>
</blockquote>
<h4 id="2-2-logistic回归"><a href="#2-2-logistic回归" class="headerlink" title="2.2 logistic回归"></a>2.2 logistic回归</h4><p>这是一个学习算法，用于监督学习中输出<script type="math/tex">y​</script>是<script type="math/tex">0​</script>或<script type="math/tex">1​</script>的二元分类问题。</p>
<p>逻辑回归中，预测值<script type="math/tex">\hat{h} = \text{P}(y=1|x)</script>表示为输入<script type="math/tex">x</script>输出为<script type="math/tex">y = 1</script>的概率，取值范围在<script type="math/tex">[0,1]</script>之间，这是其与二分类模型不同的地方。使用线性模型，引入参数<script type="math/tex">w</script>和<script type="math/tex">b</script>。权重<script type="math/tex">w</script>的维度是<script type="math/tex">(n_x, 1)</script>，<script type="math/tex">b</script>是一个常数项，即<script type="math/tex">w \in \mathbb{R}^{n_x \times 1}, b \in \mathbb{R}</script>。这样，逻辑回归的线性预测输出可以写成：</p>
<script type="math/tex; mode=display">
\hat{y} = w^{\text{T}}x+b</script><p>值得注意的是，很多其它机器学习资料中，可能把常数<script type="math/tex">b</script>当做<script type="math/tex">w_0</script>处理，并引入<script type="math/tex">x_0=1</script>。这样从维度上来看，<script type="math/tex">x</script>和<script type="math/tex">w</script>都会增加一维。但在本课程中，为了简化计算和便于理解，Andrew建议还是使用上式这种形式将<script type="math/tex">w</script>和<script type="math/tex">b</script>分开比较好。</p>
<p>上式的线性输出区间为整个实数范围，而<strong>逻辑回归要求输出范围在</strong><script type="math/tex">[0,1]</script><strong>之间</strong>，所以还需要对上式的线性函数输出进行处理。方法是引入sigmoid函数，让输出限定在<script type="math/tex">[0,1]</script>之间。这样，逻辑回归的预测输出就可以完整写成：</p>
<script type="math/tex; mode=display">
\hat{y} = \text{sigmoid}(w^{\text T}x+b) = \sigma(w^{\text T}x+b)</script><p>sigmoid函数是一种非线性的S型函数，输出被限定在<script type="math/tex">[0,1]</script>之间，通常被用在神经网络中当作<strong>激活函数(Activation function)</strong>使用。Sigmoid函数的表达式：</p>
<script type="math/tex; mode=display">
\text{sigmoid}(z) = \frac{1}{1+e^{-z}}</script><p> 通过Sigmoid函数，就能够将逻辑回归的输出限定在<script type="math/tex">[0,1]</script>之间了。</p>
<h4 id="2-3-logistic回归损失函数"><a href="#2-3-logistic回归损失函数" class="headerlink" title="2.3 logistic回归损失函数"></a>2.3 logistic回归损失函数</h4><p>逻辑回归中，<script type="math/tex">w</script>和<script type="math/tex">b</script>都是未知参数，需要反复训练优化得到。因此，我们需要定义一个成本函数(cost function)，包含了参数<script type="math/tex">w</script>和<script type="math/tex">b</script>。通过优化cost function，当cost function取值最小时，得到对应的<script type="math/tex">w</script>和<script type="math/tex">b</script>。</p>
<p>如何定义所有<script type="math/tex">m</script>个样本的cost function呢？先从单个样本出发，我们希望该样本的预测值<script type="math/tex">\hat y</script>与真实值越相似越好。我们把单个样本的cost function用<strong>Loss function</strong>来表示，根据以往经验，如果使用平方错误(squared error)来衡量，如下所示：</p>
<script type="math/tex; mode=display">
L(\hat y, y) = \frac{1}{2}(\hat y-y)^2</script><p>但是，对于逻辑回归，我们一般不使用平方错误来作为Loss function。原因是这种Loss function一般是<strong>非凸(non-convex)</strong>的。non-convex函数在使用梯度下降算法时，容易得到局部最小值(local minumum)，即<strong>局部最优化</strong>。而我们最优化的目标是计算得到全局最优化(Global optimization)。因此，我们一般选择的Loss function应该是convex的。因此，我们可以构建另外一种Loss function，且是convex的，如下所示：</p>
<script type="math/tex; mode=display">
L(\hat y, y) = -(y\log \hat y + (1-y)\log(1-\hat y))</script><p>我们来分析一下这个Loss function，它是衡量错误大小的，Loss function越小越好。</p>
<p>当<script type="math/tex">y = 1</script>时，我们带入上式容易得知<script type="math/tex">\hat y\rightarrow1</script>时<script type="math/tex">L(\hat y, y)\rightarrow0</script>，预测效果越好；同理，当<script type="math/tex">y=0</script>时，<script type="math/tex">\hat y \rightarrow 0</script>则<script type="math/tex">L(\hat y,y)\rightarrow0</script>，预测效果越好。后续将会提到这个损失函数是如何推导出来的。</p>
<p>上面介绍的Loss function是针对单个样本的。那对于<script type="math/tex">m</script>个样本，我们定义Cost function，Cost function是<script type="math/tex">m</script>个样本的Loss function的平均值，反映了<script type="math/tex">m</script>个样本的预测输出<script type="math/tex">\hat y</script>与真实样本输出<script type="math/tex">y</script>的平均接近程度。Cost function可表示为：</p>
<script type="math/tex; mode=display">
J(w,b) = \frac1m \sum_{i=0}^{m}L(\hat y^{(i)}, y^{(i)})</script><p>Cost function已经推导出来了，Cost function是关于待求系数w和b的函数。我们的目标就是迭代计算出最佳的w和b值，<strong>最小化Cost function</strong>，让Cost function尽可能地接近于零。</p>
<p>其实逻辑回归问题可以看成是一个简单的神经网络，只包含<strong>一个神经元</strong>。这也是我们这里先介绍逻辑回归的原因。</p>
<h4 id="2-4-梯度下降法"><a href="#2-4-梯度下降法" class="headerlink" title="2.4 梯度下降法"></a>2.4 梯度下降法</h4><p>使用<strong>梯度下降(Gradient Descent)</strong>算法来计算出合适的<script type="math/tex">w</script>和<script type="math/tex">b</script>值，从而最小化<script type="math/tex">m</script>个训练样本的Cost function，即<script type="math/tex">J(w,b)</script>。</p>
<p>由于<script type="math/tex">J(w,b)</script>是convex  function，梯度下降算法是先随机选择一组参数<script type="math/tex">w</script>和<script type="math/tex">b</script>值，然后每次迭代的过程中分别沿着<script type="math/tex">w</script>和<script type="math/tex">b</script>的梯度(偏导数)的反方向前进一小步，不断修正<script type="math/tex">w</script>和<script type="math/tex">b</script>。每次迭代更新后，都能让<script type="math/tex">J(w,b)</script>更接近全局最小值。梯度下降的过程如下图所示。</p>
<p><img src="https://pic.imgdb.cn/item/648a82371ddac507ccb0ec36.jpg" style="zoom:50%"></p>
<p>梯度下降算法每次迭代更新，<script type="math/tex">w</script>和<script type="math/tex">b</script>的修正表达式为：</p>
<script type="math/tex; mode=display">
w:=w-\alpha \frac{\partial J(w, b)}{\partial w}</script><script type="math/tex; mode=display">
b:=b-\alpha \frac{\partial J(w, b)}{\partial b}</script><p>上式中，<script type="math/tex">\alpha</script>是学习因子(learning rate)，表示梯度下降的不仅长度。梯度下降算法能够保证每次迭代w和b都能向着J(w,b)全局最小化的方向进行。其<a target="_blank" rel="noopener" href="http://blog.csdn.net/red_stone1/article/details/72229903">数学原理</a>主要是运用泰勒一阶展开来证明的。</p>
<h4 id="2-5-2-6-导数复习"><a href="#2-5-2-6-导数复习" class="headerlink" title="2.5-2.6 导数复习"></a>2.5-2.6 导数复习</h4><p>这一部分的内容相对简单，Andrew主要是给对微积分、求导数不太清楚的同学介绍的。梯度或者导数一定程度上可以看成是斜率。关于求导数的方法这里就不再赘述了。</p>
<h4 id="2-7-计算图"><a href="#2-7-计算图" class="headerlink" title="2.7 计算图"></a>2.7 计算图</h4><p>整个神经网络的训练过程实际上包含了两个过程：<strong>正向传播(Forward Propagation)</strong>和<strong>反向传播(Back Propagation)</strong>。正向传播是从输入到输出，由神经网络计算得到预测输出的过程；反向传播是从输出到输入，对参数w和b计算梯度的过程。下面，我们用<strong>计算图(Computation graph)</strong>的形式来理解这两个过程。</p>
<p>举个简单的例子，假如Cost function为<script type="math/tex">J(a,b,c)=3(a+bc)</script>，包含<script type="math/tex">a</script>，<script type="math/tex">b</script>，<script type="math/tex">c</script>三个变量。我们用<script type="math/tex">u</script>表示<script type="math/tex">bc</script>，<script type="math/tex">v</script>表示<script type="math/tex">a+u</script>，则<script type="math/tex">J=3v</script>。它的计算图可以写成如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648a82ed1ddac507ccb243d5.jpg" style="zoom:50%"></p>
<p>令<script type="math/tex">a=5,b=3,c=2</script>，则<script type="math/tex">u=bc=6,v=a+u=11,J=3v=33</script>。计算图中，这种从左到右，从输入到输出的过程就对应着神经网络或者逻辑回归中输入与权重经过运算计算得到Cost function的正向过程。</p>
<h4 id="2-8-使用计算图求导"><a href="#2-8-使用计算图求导" class="headerlink" title="2.8 使用计算图求导"></a>2.8 使用计算图求导</h4><p>下面我们来介绍反向传播(Back Propagation)，即计算输出对输入的偏导数。</p>
<p><img src="https://pic.imgdb.cn/item/648a83431ddac507ccb2e777.jpg" style="zoom:50%"></p>
<h4 id="2-9-logistic回归中的梯度下降法"><a href="#2-9-logistic回归中的梯度下降法" class="headerlink" title="2.9 logistic回归中的梯度下降法"></a>2.9 logistic回归中的梯度下降法</h4><p>对逻辑回归进行梯度计算。对单个样本而言，逻辑回归Loss function表达式如下：</p>
<script type="math/tex; mode=display">
\begin{gathered}
&z = w^\text Tx+b \\
&\hat y = a = \sigma(z) \\
&L(\hat y,y) = L(a, y) = -(y\log a+(1-y)\log(1-a))
\end{gathered}</script><p>该逻辑回归的正向传播过程非常简单。据上述公式，例如输入样本<script type="math/tex">x</script>有两个特征<script type="math/tex">(x_1,x_2)</script>，相应的权重也有两个<script type="math/tex">(w_1,w_2)</script>，则<script type="math/tex">z = w_1x_1+w_2x_2+b</script>。</p>
<p>然后，计算该逻辑回归的反向传播过程，即由Loss function计算参数<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数：</p>
<script type="math/tex; mode=display">
\begin{gathered}
&\mathrm d a=\frac{\partial L}{\partial a}=-\frac{y}{a}+\frac{1-y}{1-a}\\
&\mathrm d z=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z}=\left(-\frac{y}{a}+\frac{1-y}{1-a}\right) \cdot a(1-a)=a-y
\end{gathered}</script><p>知道了<script type="math/tex">\mathrm dz</script>之后，就可以直接对<script type="math/tex">w_1，w_2</script>和<script type="math/tex">b</script>进行求导了：</p>
<script type="math/tex; mode=display">
\begin{gathered}
&\mathrm d w_1=\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w_1}=x_1 \cdot \mathrm  d z=x_1(a-y)\\
&\mathrm d w_2=\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w_2}=x_2 \cdot \mathrm  d z=x_2(a-y)\\
&\mathrm d b=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial b}=1 \cdot \mathrm  d z=a-y
\end{gathered}</script><p>则梯度下降算法可表示为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
w{1}:=w{1}-\alpha ~\mathrm d w{1} \\
w{2}:=w{2}-\alpha ~\mathrm d w{2} \\
b:=b-\alpha ~\mathrm d b
\end{gathered}</script><p><img src="https://pic.imgdb.cn/item/648a84f21ddac507ccb64b43.jpg" style="zoom:50%"></p>
<h4 id="2-10-m-个样本的梯度下降"><a href="#2-10-m-个样本的梯度下降" class="headerlink" title="2.10 m 个样本的梯度下降"></a>2.10 m 个样本的梯度下降</h4><p>上一部分讲的是对单个样本求偏导和梯度下降。如果有m个样本，其Cost function表达式如下：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z^{(i)}=w^{T} x^{(i)}+b \\
\hat{y}^{(i)}=a^{(i)}=\sigma\left(z^{(i)}\right) \\
J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \hat{y}^{(i)}+\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)\right]
\end{gathered}</script><p>Cost function关于<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数可以写成和平均的形式：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathrm d w_{1}=\frac{1}{m} \sum_{i=1}^{m} x_{1}^{(i)}\left(a^{(i)}-y^{(i)}\right) \\
\mathrm d w_{2}=\frac{1}{m} \sum_{i=1}^{m} x_{2}^{(i)}\left(a^{(i)}-y^{(i)}\right) \\
\mathrm d b=\frac{1}{m} \sum_{i=1}^{m} \left(a^{(i)}-y^{(i)}\right)
\end{gathered}</script><p>这样，每次迭代中<script type="math/tex">w</script>和<script type="math/tex">b</script>的梯度有$m$个训练样本计算平均值得到。其算法伪代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">J=<span class="number">0</span>; dw1=<span class="number">0</span>; dw2=<span class="number">0</span>; db=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to m</span><br><span class="line">    z(i) = wx(i)+b;</span><br><span class="line">    a(i) = sigmoid(z(i));</span><br><span class="line">    J += -[y(i)log(a(i))+(<span class="number">1</span>-y(i)）log(<span class="number">1</span>-a(i));</span><br><span class="line">    dz(i) = a(i)-y(i);</span><br><span class="line">    dw1 += x1(i)dz(i);</span><br><span class="line">    dw2 += x2(i)dz(i);</span><br><span class="line">    db += dz(i);</span><br><span class="line">J /= m;</span><br><span class="line">dw1 /= m;</span><br><span class="line">dw2 /= m;</span><br><span class="line">db /= m;</span><br></pre></td></tr></table></figure>
<p>经过每次迭代后，根据梯度下降算法，<script type="math/tex">w</script>和<script type="math/tex">b</script>都进行更新：</p>
<script type="math/tex; mode=display">
\begin{gathered}
w_{1}:=w_{1}-\alpha ~ \mathrm d w_{1} \\
w_{2}:=w_{2}-\alpha ~ \mathrm d w_{2} \\
b:=b-\alpha ~ \mathrm d b
\end{gathered}</script><p>这样经过<script type="math/tex">n</script>次迭代后，整个梯度下降算法就完成了。</p>
<p>值得一提的是，在上述的梯度下降算法中，是利用for循环对每个样本进行dw1，dw2和db的累加计算最后再求平均数的。在深度学习中，样本数量$m$通常很大，使用for循环会让神经网络程序运行得很慢。所以，我们应该尽量避免使用for循环操作，而使用<strong>矩阵运算</strong>，能够大大提高程序运行速度。关于<strong>向量化(vectorization)</strong>的内容我们放在下次笔记中再说。</p>
<h4 id="2-11-2-12-向量化"><a href="#2-11-2-12-向量化" class="headerlink" title="2.11-2.12 向量化"></a>2.11-2.12 向量化</h4><p>深度学习算法中，数据量很大，在程序中应该尽量减少使用loop循环语句，而可以使用向量运算来提高程序运行速度。</p>
<p>向量化(Vectorization)就是利用矩阵运算的思想，大大提高运算速度。</p>
<p>上一部分我们讲了应该尽量避免使用for循环而使用向量化矩阵运算。在python的numpy库中，我们通常使用<strong>np.dot()</strong>函数来进行矩阵运算。</p>
<p>我们将向量化的思想使用在逻辑回归算法上，尽可能减少for循环，而只使用矩阵运算。值得注意的是，算法最顶层的迭代训练的for循环是不能替换的。而每次迭代过程对J，dw，b的计算是可以直接使用矩阵运算。</p>
<h4 id="2-13-向量化logistic回归"><a href="#2-13-向量化logistic回归" class="headerlink" title="2.13 向量化logistic回归"></a>2.13 向量化logistic回归</h4><p>在前面的笔记中我们提到过，整个训练样本构成的输入矩阵<script type="math/tex">X</script>的维度是<script type="math/tex">(,m)</script>，权重矩阵<script type="math/tex">w</script>的维度是<script type="math/tex">(,1)</script>，<script type="math/tex">b</script>是一个常数值，而整个训练样本构成的输出矩阵<script type="math/tex">Y</script>的维度为<script type="math/tex">(1,m)</script>。利用向量化的思想，所有<script type="math/tex">m</script>个样本的线性输出<script type="math/tex">Z</script>可以用矩阵表示：</p>
<script type="math/tex; mode=display">
Z = w^{\text T}X+b</script><p>在python的numpy库中可以表示为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b      <span class="comment"># w.T表示w的转置</span></span><br><span class="line">A = sigmoid(Z)</span><br></pre></td></tr></table></figure>
<p>这样，我们就能够使用向量化矩阵运算代替for循环，对所有<script type="math/tex">m</script>个样本同时运算，大大提高了运算速度。 </p>
<h4 id="2-14-向量化logistic回归的梯度输出"><a href="#2-14-向量化logistic回归的梯度输出" class="headerlink" title="2.14 向量化logistic回归的梯度输出"></a>2.14 向量化logistic回归的梯度输出</h4><p>逻辑回归中的梯度下降算法如何转化为向量化的矩阵形式。对于所有<script type="math/tex">m</script>个样本，<script type="math/tex">\mathrm dZ</script>的维度是<script type="math/tex">(1, m)</script>，可表示为：</p>
<script type="math/tex; mode=display">
\mathrm dZ = A-Y</script><p>$\mathrm d b$可以表示为：</p>
<script type="math/tex; mode=display">
\mathrm d b = \frac{1}{m} \sum_{i=1}^{m}dz^{(i)}</script><p>对应的程序可以写成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dw = <span class="number">1</span>/m*np.dot(X,dZ.T)</span><br></pre></td></tr></table></figure>
<p>这样，我们把整个逻辑回归中的for循环尽可能用矩阵运算代替，对于单次迭代，梯度下降算法流程如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A-Y</span><br><span class="line">dw = <span class="number">1</span>/m*np.dot(X,dZ.T)</span><br><span class="line">db = <span class="number">1</span>/m*np.<span class="built_in">sum</span>(dZ)</span><br><span class="line">w = w - alpha*dw</span><br><span class="line">b = b - alpha*db</span><br></pre></td></tr></table></figure><br>其中，<strong>alpha是学习因子</strong>，决定<script type="math/tex">w</script>和<script type="math/tex">b</script>的更新速度。上述代码只是对单次训练更新而言的，外层还需要一个for循环，表示迭代次数。</p>
<h3 id="第三章-浅层神经网络"><a href="#第三章-浅层神经网络" class="headerlink" title="第三章 浅层神经网络"></a>第三章 浅层神经网络</h3><h4 id="3-1-神经网络概览"><a href="#3-1-神经网络概览" class="headerlink" title="3.1 神经网络概览"></a>3.1 神经网络概览</h4><p>首先，我们从整体结构上来大致看一下神经网络模型。</p>
<p>前面的课程中，我们已经使用<strong>计算图</strong>的方式介绍了逻辑回归<strong>梯度下降算法</strong>的<strong>正向传播</strong>和<strong>反向传播</strong>两个过程。如下图所示。神经网络的结构与逻辑回归类似，只是神经网络的层数比逻辑回归多一层，多出来的中间那层称为<strong>隐藏层或中间层</strong>。这样从计算上来说，神经网络的正向传播和反向传播过程只是比逻辑回归多了一次重复的计算。</p>
<p>正向传播过程分成两层，<strong>第一层是输入层到隐藏层</strong>，用上标[1]来表示：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z^{[1]}=W^{[1]} x+b^{[1]} \\
a^{[1]} = \sigma (z^{[1]})
\end{gathered}</script><p>第二层是<strong>隐藏层到输出层</strong>，用上标[2]来表示：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z^{[2]}=W^{[2]} x+b^{[2]} \\
a^{[2]} = \sigma (z^{[2]})
\end{gathered}</script><p>在写法上值得注意的是，方括号上标[i]表示当前所处的层数；圆括号上标(i)表示第i个样本。</p>
<p>同样，<strong>反向传播过程也分成两层</strong>。第一层是输出层到隐藏层，第二层是隐藏层到输入层。其细节部分我们之后再来讨论。</p>
<p><img src="https://pic.imgdb.cn/item/648ac1111ddac507cc54a4ee.png" style="zoom:50%"></p>
<h4 id="3-2-神经网络的表示"><a href="#3-2-神经网络的表示" class="headerlink" title="3.2 神经网络的表示"></a>3.2 神经网络的表示</h4><p>下面我们以图示的方式来介绍单隐藏层的神经网络结构。如下图所示，单隐藏层神经网络就是典型的浅层(shallow)神经网络。</p>
<p><img src="https://pic.imgdb.cn/item/648ac1ad1ddac507cc56265f.png" style="zoom:60%"></p>
<p>结构上，从左到右，可以分成三层：<strong>输入层(Input layer)</strong>，<strong>隐藏层(Hidden layer)</strong>和<strong>输出层(Output  layer)</strong>。输入层和输出层，顾名思义，对应着训练样本的输入和输出，很好理解。隐藏层是抽象的非线性的中间层，这也是其被命名为隐藏层的原因。</p>
<p>在写法上，我们通常把输入矩阵<script type="math/tex">X</script>记为<script type="math/tex">a^{[0]}</script>，把隐藏层输出记为<script type="math/tex">a^{[1]}</script>，上标从<script type="math/tex">0</script>开始。用下标表示第几个神经元，注意下标从<script type="math/tex">1</script>开始。例如<script type="math/tex">a^{[1]}_1</script>表示隐藏层第<script type="math/tex">1</script>个神经元，<script type="math/tex">a^{[1]}_2</script>表示隐藏层第<script type="math/tex">2</script>个神经元等等。这样隐藏层有<script type="math/tex">4</script>个神经元就可以将其输出<script type="math/tex">a^{[1]}</script>写成矩阵的形式：</p>
<script type="math/tex; mode=display">
a^{[1]}=\left[\begin{array}{c}a_{1}^{[1]} \\a_{2}^{[1]} \\a_{3}^{[1]} \\a_{4}^{[1]}\end{array}\right]</script><p>相应的输出层记为<script type="math/tex">a^{[2]}</script>，即<script type="math/tex">\hat y</script>。这种单隐藏层神经网络也称为两层神经网络(2 layer NN)。</p>
<blockquote>
<p>之所以叫两层神经网络是因为，通常我们只会计算隐藏层输出和输出层的输出，输入层是不用计算的。这也是我们把输入层层数上标记为<script type="math/tex">0</script>的原因(<script type="math/tex">a^{[0]}</script>)。<br>关于隐藏层对应的权重<script type="math/tex">W^{[1]}</script>和常数项<script type="math/tex">b^{[1]}</script>，<script type="math/tex">W^{[1]}</script>的维度是<script type="math/tex">(4,3)</script>。这里的<script type="math/tex">4</script>对应着隐藏层神经元个数，<script type="math/tex">3</script>对应着输入层<script type="math/tex">x</script>特征向量包含元素个数。常数项<script type="math/tex">b^{[1]}</script>的维度是<script type="math/tex">(4,1)</script>，这里的<script type="math/tex">4</script>同样对应着隐藏层神经元个数。关于输出层对应的权重<script type="math/tex">W^{[2]}</script>和常数项<script type="math/tex">b^{[2]}</script>，<script type="math/tex">W^{[2]}</script>的维度是<script type="math/tex">(1,4)</script>，这里的<script type="math/tex">1</script>对应着输出层神经元个数，<script type="math/tex">4</script>对应着隐藏层神经元个数。常数项<script type="math/tex">b^{[2]}</script>的维度是<script type="math/tex">(1,1)</script>，因为输出只有一个神经元。</p>
</blockquote>
<p>总结一下，第<script type="math/tex">i</script>层的权重<script type="math/tex">W^{[i]}</script>维度的行等于<script type="math/tex">i</script>层神经元的个数，列等于<script type="math/tex">i-1</script>层神经元的个数；第<script type="math/tex">i</script>层常数项<script type="math/tex">b^{[i]}</script>维度的行等于<script type="math/tex">i</script>层神经元的个数，列始终为<script type="math/tex">1</script>。</p>
<h4 id="3-3-计算神经网络的输出"><a href="#3-3-计算神经网络的输出" class="headerlink" title="3.3 计算神经网络的输出"></a>3.3 计算神经网络的输出</h4><p>本节详细推导神经网络的计算过程。前面讲过两层神经网络可以看成是逻辑回归再重复计算一次。如下图所示，逻辑回归的正向计算可以分解成计算<script type="math/tex">z</script>和<script type="math/tex">a</script>的两部分：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z = w^{\text T}x+b \\
a = \sigma(z)
\end{gathered}</script><p><img src="https://pic.imgdb.cn/item/648ac29f1ddac507cc58f815.png" style="zoom:50%"></p>
<p>对于两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算。每层计算时，要注意对应的上标和下标，一般我们记上标方括号表示layer，下标表示第几个神经元。例如<script type="math/tex">a^{[l]}_i</script>表示第<script type="math/tex">l</script>层的第<script type="math/tex">i</script>个神经元。<strong>注意，</strong><script type="math/tex">i</script><strong>从</strong><script type="math/tex">1</script><strong>开始，</strong><script type="math/tex">l</script><strong>从</strong><script type="math/tex">0</script><strong>开始</strong>。</p>
<p>将从输入层到输出层的计算公式列出来：</p>
<script type="math/tex; mode=display">
\begin{aligned}&z_{1}^{[1]}=w_{1}^{[1] T} x+b_{1}^{[1]}, a_{1}^{[1]}=\sigma\left(z_{1}^{[1]}\right) \\&z_{2}^{[1]}=w_{2}^{[1] T} x+b_{2}^{[1]}, a_{2}^{[1]}=\sigma\left(z_{2}^{[1]}\right) \\&z_{3}^{[1]}=w_{3}^{[1] T} x+b_{3}^{[1]}, a_{3}^{[1]}=\sigma\left(z_{3}^{[1]}\right) \\&z_{4}^{[1]}=w_{4}^{[1] T} x+b_{4}^{[1]}, a_{4}^{[1]}=\sigma\left(z_{4}^{[1]}\right)\end{aligned}</script><p>然后，从隐藏层到输出层的计算公式为：</p>
<script type="math/tex; mode=display">
z^{[2]}_1 = w^{[1] \text T}_1a^{[1]}+b^{[2]}_1,a^{[2]}_1 = \sigma(z^{[2]}_1)</script><p>其中</p>
<script type="math/tex; mode=display">
a^{[1]}=\left[\begin{array}{c}a_{1}^{[1]} \\a_{2}^{[1]} \\a_{3}^{[1]} \\a_{4}^{[1]}\end{array}\right]</script><p>上述每个节点的计算都对应着一次逻辑运算的过程，分别由计算<script type="math/tex">z</script>和<script type="math/tex">a</script>两部分组成。</p>
<p>为了提高程序运算速度，我们引入向量化和矩阵运算的思想，将上述表达式转换成矩阵运算的形式：</p>
<p><img src="https://pic.imgdb.cn/item/648ac37a1ddac507cc5b55f3.png" style="zoom:50%"></p>
<p>之前也介绍过，这里顺便提一下，<script type="math/tex">W^{[1]}</script>的维度是<script type="math/tex">(4,3)</script>，<script type="math/tex">b^{[1]}</script>的维度是<script type="math/tex">(4,1)</script>，<script type="math/tex">W^{[2]}</script>的维度是<script type="math/tex">(1,4)</script>，<script type="math/tex">b^{[2]}</script>的维度是<script type="math/tex">(1,1)</script>。这点需要特别注意。</p>
<h4 id="3-4-多个样本的向量化"><a href="#3-4-多个样本的向量化" class="headerlink" title="3.4 多个样本的向量化"></a>3.4 多个样本的向量化</h4><p>上一部分我们只是介绍了单个样本的神经网络正向传播矩阵运算过程。而对于<script type="math/tex">m</script>个训练样本，我们也可以使用矩阵相乘的形式来提高计算效率。而且它的形式与上一部分单个样本的矩阵运算十分相似，比较简单。</p>
<p>之前我们也介绍过，在书写标记上用上标<script type="math/tex">(i)</script>表示第<script type="math/tex">i</script>个样本，例如<script type="math/tex">x^{(i)}</script>，<script type="math/tex">z^{(i)}</script>，<script type="math/tex">a^{[2](i)}</script>。对于每个样本<script type="math/tex">i</script>，可以使用for循环来求解其正向输出：</p>
<script type="math/tex; mode=display">
\begin{aligned}for \  i = 1 \ to \ m \\&z^{[1](i)}=W^{[1]} x^{(i)}+b^{[1]} \\&a^{[1](i)}=\sigma\left(z^{[1](i)}\right) \\&z^{[2](i)}=W^{[2]} a^{[1](i)}+b^{[2]} \\&a^{[2](i)}=\sigma\left(z^{[2](i)}\right)\end{aligned}</script><p>不使用for循环，利用矩阵运算的思想，输入矩阵<script type="math/tex">X</script>的维度为<script type="math/tex">(n_x,m)</script>。这样，我们可以把上面的for循环写成矩阵运算的形式：</p>
<script type="math/tex; mode=display">
\begin{gathered}Z^{[1]}=W^{[1]} X+b^{[1]} \\A^{[1]}=\sigma\left(Z^{[1]}\right) \\Z^{[2]}=W^{[2]} A^{[1]}+b^{[2]} \\A^{[2]}=\sigma\left(Z^{[2]}\right)\end{gathered}</script><p>其中，<script type="math/tex">Z^{[1]}</script>的维度是<script type="math/tex">(4,m)</script>，<script type="math/tex">4</script>是隐藏层神经元的个数；<script type="math/tex">A^{[1]}</script>的维度与<script type="math/tex">Z^{[1]}</script>相同；<script type="math/tex">Z^{[2]}</script>和<script type="math/tex">A^{[2]}</script>的维度均为<script type="math/tex">(1,m)</script>。对上面这四个矩阵来说，均可以这样来理解：<strong>行表示神经元个数，列表示样本数目</strong><script type="math/tex">m</script>。</p>
<h4 id="3-5向量化实现的解释"><a href="#3-5向量化实现的解释" class="headerlink" title="3.5向量化实现的解释"></a>3.5向量化实现的解释</h4><p>这部分Andrew用图示的方式解释了<script type="math/tex">m</script>个样本的神经网络矩阵运算过程。其实内容比较简单，只要记住上述四个矩阵的行表示神经元个数，列表示样本数目<script type="math/tex">m</script>就行了。</p>
<p>值得注意的是输入矩阵<script type="math/tex">X</script>也可以写成<script type="math/tex">A^{[0]}</script>。</p>
<h4 id="3-6-激活函数"><a href="#3-6-激活函数" class="headerlink" title="3.6 激活函数"></a>3.6 激活函数</h4><p>神经网络隐藏层和输出层都需要<strong>激活函数(activation function)</strong>，在之前的课程中我们都默认使用Sigmoid函数<script type="math/tex">σ(x)</script>作为激活函数。其实，还有其它激活函数可供使用，不同的激活函数有各自的优点。下面我们就来介绍几个不同的激活函数<script type="math/tex">g(x)</script>。</p>
<p><strong>(1) sigmoid函数</strong></p>
<p><img src="https://pic.imgdb.cn/item/648ac3fb1ddac507cc5c8851.png" style="zoom:70%"></p>
<p><strong>(2) tanh函数</strong></p>
<p><img src="https://pic.imgdb.cn/item/648ac49f1ddac507cc5df268.png" style="zoom:70%"></p>
<p><strong>(3) ReLU函数</strong></p>
<p><img src="https://pic.imgdb.cn/item/648ac4d01ddac507cc5e604c.png" style="zoom:70%"></p>
<p><strong>(4) Leaky ReLU函数</strong></p>
<p><img src="https://pic.imgdb.cn/item/648ac4ed1ddac507cc5ea77a.png" style="zoom:70%"></p>
<p>如上图所示，不同激活函数形状不同，<script type="math/tex">a</script>的取值范围也有差异。</p>
<p>如何选择合适的激活函数呢？首先我们来比较sigmoid函数和tanh函数。对于隐藏层的激活函数，一般来说，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果。因此，隐藏层的激活函数，tanh比sigmoid更好一些。而对于输出层的激活函数，因为二分类问题的输出取值为{0,+1}，所以一般会选择sigmoid作为激活函数。</p>
<p>观察sigmoid函数和tanh函数，我们发现有这样一个问题，就是当<script type="math/tex">|z|</script>很大的时候，激活函数的斜率（梯度）很小。因此，在这个区域内，梯度下降算法会运行得比较慢。在实际应用中，应尽量避免使z落在这个区域，使<script type="math/tex">|z|</script>尽可能限定在零值附近，从而提高梯度下降算法运算速度。</p>
<p>为弥补sigmoid函数和tanh函数的这个缺陷，就出现了ReLU激活函数。ReLU激活函数在<script type="math/tex">z</script>大于零时梯度始终为1；在<script type="math/tex">z</script>小于零时梯度始终为0；<script type="math/tex">z</script><strong>等于零时的梯度可以当成1也可以当成0</strong>，实际应用中并不影响。对于隐藏层，选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点，实际应用中，这个缺点影响不是很大。为了弥补这个缺点，出现了Leaky ReLU激活函数，能够保证z小于零是梯度不为0。</p>
<p>最后总结一下，如果是分类问题，输出层的激活函数一般会选择sigmoid函数。但是隐藏层的激活函数通常不会选择sigmoid函数，tanh函数的表现会比sigmoid函数好一些。实际应用中，通常会会选择使用ReLU或者Leaky ReLU函数，保证梯度下降速度不会太小。其实具体选择哪个函数作为激活函数没有一个固定的准确的答案，应该要根据具体实际问题进行验证(validation)。</p>
<h4 id="3-7-为什么需要非线性激活函数"><a href="#3-7-为什么需要非线性激活函数" class="headerlink" title="3.7 为什么需要非线性激活函数"></a>3.7 为什么需要非线性激活函数</h4><p>我们知道上一部分讲的四种激活函数都是<strong>非线性(non-linear)</strong>的。那是否可以使用线性激活函数呢？答案是不行！下面我们就来进行简要的解释和说明。</p>
<p>假设所有的激活函数都是线性的，为了简化计算，我们直接令激活函数<script type="math/tex">g(z)=z</script>，即<script type="math/tex">a=z</script>。那么，浅层神经网络的各层输出为：</p>
<script type="math/tex; mode=display">
\begin{gathered}z^{[1]}=W^{[1]} x+b^{[1]} \\a^{[1]}=z^{[1]} \\z^{[2]}=W^{[2]} a^{[1]}+b^{[2]} \\a^{[2]}=z^{[2]}\end{gathered}</script><p>我们对上式中<script type="math/tex">a^{[2]}</script>进行化简计算：</p>
<script type="math/tex; mode=display">
\begin{align}a^{[2]}& =z^{[2]} \\& =W^{[2]} a^{[1]}+b^{[2]} \\& =W^{[2]}\left(W^{[1]} x+b^{[1]}\right)+b^{[2]} \\&=\left(W^{[2]} W^{[1]}\right) x+\left(W^{[2]} b^{[1]}+b^{[2]}\right) \\& =W^{\prime} x+b^{\prime}\end{align}</script><p>经过推导我们发现<script type="math/tex">a^{[2]}</script>仍是输入变量<script type="math/tex">x</script>的<strong>线性组合</strong>。这表明，使用神经网络与直接使用线性模型的效果并没有什么两样。即便是包含多层隐藏层的神经网络，<strong>如果使用线性函数作为激活函数，最终的输出仍然是输入</strong><script type="math/tex">x</script><strong>的线性模型</strong>。这样的话神经网络就没有任何作用了。因此，<strong>隐藏层的激活函数必须要是非线性的</strong>。</p>
<p>另外，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。</p>
<p>值得一提的是，如果是预测问题而不是分类问题，输出<script type="math/tex">y</script>是连续的情况下，输出层的激活函数可以使用线性函数。如果输出<script type="math/tex">y</script>恒为正值，则也可以使用ReLU激活函数，具体情况，具体分析。</p>
<h4 id="3-8-激活函数的导数"><a href="#3-8-激活函数的导数" class="headerlink" title="3.8 激活函数的导数"></a>3.8 激活函数的导数</h4><p>在梯度下降反向计算过程中少不了计算激活函数的导数即梯度。</p>
<p>(1) sigmoid函数的导数</p>
<script type="math/tex; mode=display">
\begin{gathered}g(z)=\frac{1}{1+e^{(-z)}} \\g^{\prime}(z)=\frac{\mathrm d}{\mathrm d z} g(z)=g(z)(1-g(z))=a(1-a)\end{gathered}</script><p>(2) tanh函数的导数</p>
<script type="math/tex; mode=display">
\begin{gathered}g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}} \\g^{\prime}(z)=\frac{\mathrm d}{\mathrm d z} g(z)=1-(g(z))^{2}=1-a^{2}\end{gathered}</script><p>(3) ReLU函数的导数</p>
<script type="math/tex; mode=display">
\begin{gathered}g(z)=\max (0, z) \\g^{\prime}(z)= \begin{cases}0, & z<0 \\1, & z > 0 \\0\ \text{or}\ 1,& z = 0\end{cases}\end{gathered}</script><p>(4) Leaky ReLU函数的导数</p>
<script type="math/tex; mode=display">
\begin{gathered}g(z)=\max (0.01z, z) \\g^{\prime}(z)= \begin{cases}0.01, & z<0 \\1, & z \geq 0\end{cases}\end{gathered}</script><h4 id="3-9-神经网络的梯度下降法"><a href="#3-9-神经网络的梯度下降法" class="headerlink" title="3.9 神经网络的梯度下降法"></a>3.9 神经网络的梯度下降法</h4><p>你的单隐层神经网络会有<script type="math/tex">W^{[1]}</script>，<script type="math/tex">b^{[1]}</script>，<script type="math/tex">W^{[2]}</script>，<script type="math/tex">b^{[2]}</script>这些参数，还有<script type="math/tex">n_x</script>个表示输入特征的个数，<script type="math/tex">n^{[1]}</script>表示隐藏单元个数, <script type="math/tex">n^{[2]}</script>表示输出单元个数。只介绍这种情况，那么参数:</p>
<p>矩阵<script type="math/tex">W^{[1]}</script>的维度就是<script type="math/tex">(n^{[1]}, n^{[0]})</script>，<script type="math/tex">b^{[1]}</script>就是<script type="math/tex">n^{[1]}</script>维向量，可以写成<script type="math/tex">(n^{[1]},1)</script>，就是一个的列向量。矩阵<script type="math/tex">W^{[2]}</script>的维度就是<script type="math/tex">(n^{[2]},n^{[1]} )</script>, <script type="math/tex">b^{[2]}</script>的维度就是<script type="math/tex">(n^{[2]},1 )</script>。</p>
<p>你还有一个神经网络的成本函数，假设你在做二分类任务，那么你的成本函数等于</p>
<script type="math/tex; mode=display">
J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \frac{1}{m}\sum_{i=1}^{m}{L(\hat y,y)}</script><p>训练参数需要做梯度下降，在训练神经网络的时候，随机初始化参数很重要，而<strong>不是初始化成全零</strong>。当你参数初始化成某些值后，每次梯度下降都会循环计算以下预测值<script type="math/tex">\hat y, (i= 1,2,.... m)</script></p>
<script type="math/tex; mode=display">
\begin{aligned}&\mathrm d W^{[1]}=\frac{\mathrm d J}{\mathrm d W^{[1]}}, \mathrm d b^{[1]}=\frac{\mathrm d J}{\mathrm d b^{[1]}} \\&\mathrm d W^{[2]}=\frac{\mathrm d J}{\mathrm d W^{[2]}}, \mathrm d b^{[2]}=\frac{\mathrm d J}{\mathrm d b^{[2]}}\end{aligned}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{aligned}&W^{[1]} \Rightarrow W^{[1]}-\alpha \mathrm d W^{[1]}, b^{[1]} \Rightarrow b^{[1]}-\alpha \mathrm d b^{[1]} \\&W^{[2]} \Rightarrow W^{[2]}-\alpha\mathrm  d W^{[2]}, b^{[2]} \Rightarrow b^{[2]}-\alpha \mathrm d b^{[2]}\end{aligned}</script><p>使用计算图的方式来推导神经网络反向传播过程。记得之前介绍逻辑回归时，我们就引入了计算图来推导正向传播和反向传播，其过程如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648ac6371ddac507cc6308ed.png"></p>
<p>由于多了一个隐藏层，神经网络的计算图要比逻辑回归的复杂一些，如下图所示。对于单个训练样本，正向过程很容易，反向过程可以根据梯度计算方法逐一推导。</p>
<script type="math/tex; mode=display">
\begin{gathered}\mathrm d z^{[2]}=a^{[2]}-y \\
\mathrm d W^{[2]}=\mathrm d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial W^{[2]}}=\mathrm d z^{[2]} a^{[1] T} \\
\mathrm d b^{[2]}=\mathrm d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial b^{[2]}}=\mathrm d z^{[2]} \cdot 1=\mathrm d z^{[2]} \\
\mathrm d z^{[1]}=\mathrm d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2] T}\mathrm  d z^{[2]} * g^{[1]^{\prime}}\left(z^{[1]}\right) \\
\mathrm d b^{[1]}=\mathrm d z^{[1]} \cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=\mathrm d z^{[1]} \cdot 1=\mathrm d z^{[1]} \\
\mathrm d z^{[1]}=\mathrm d z^{[1]} \cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=\mathrm d z^{[1]} x^{T}\end{gathered}</script><script type="math/tex; mode=display">
\begin{aligned}&\mathrm d z^{[2]}=A^{[2]}-Y, Y=\left[\begin{array}{lll}y^{[1]} & y^{[2]} & \ldots & \left.y^{[m]}\right]\end{array}\right. \\
&\mathrm d W^{[2]}=\frac{1}{m} \mathrm d z^{[2]} A^{[1] T} \\&\mathrm d b^{[2]}=\frac{1}{m} \text { np.sum }\left(\mathrm d z^{[2]}, \text { axis }=1, \text { keepdims }=\text { True }\right) \\
&\mathrm d z^{[1]}=\underbrace{W^{[2] T} \mathrm d z^{[2]}}_{\left(n^{[1]}, m\right)} \quad \text { activation function of hidden layer } \quad * \underbrace{\left(z^{[1]}\right)}_{\left(n^{[1]}, m\right)} \\
&\mathrm d W^{[1]}=\frac{1}{m} \mathrm d z^{[1]} x^{T} \\
&\underbrace{\mathrm d b^{[1]}}=\frac{1}{m} \text { np.sum }\left(\mathrm d z^{[1]}, \text { axis }=1, \text { keepdims }=\text { True }\right)\end{aligned}</script><p><img src="https://pic.imgdb.cn/item/648ac7ce1ddac507cc677d5e.png"></p>
<p>上述是反向传播的步骤，注：这些都是针对所有样本进行过向量化，<script type="math/tex">Y</script>是<script type="math/tex">1 ∗ m</script>的矩阵；这里np.sum是python的numpy命令，<strong>axis=1表示水平相加求和</strong>，<strong>keepdims防止python输出那些古怪的秩数</strong><script type="math/tex">(n , )</script>，加上这个确保矩阵<script type="math/tex">db^{[2]}</script>这个向量输出维度为<script type="math/tex">( n , 1 )</script>这样标准的形式。</p>
<p>总结一下，浅层神经网络(包含一个隐藏层)，<script type="math/tex">m</script>个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，其向量化矩阵形式如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648ac8041ddac507cc681841.png"></p>
<h4 id="3-10-选修-直观理解反向传播"><a href="#3-10-选修-直观理解反向传播" class="headerlink" title="3.10 (选修)直观理解反向传播"></a>3.10 (选修)直观理解反向传播</h4><p>本节记录一下我的两个疑问：</p>
<p>(1) 矩阵微积分问题</p>
<p>(2) <script type="math/tex">\mathrm dZ</script>的式子中没有<script type="math/tex">1/m</script>项</p>
<p>本小节笔记参考链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_36815313/article/details/105341107">3.10 直观理解反向传播-深度学习-Stanford吴恩达教授_赵继超的笔记-CSDN博客</a></p>
<h4 id="3-11-随机初始化"><a href="#3-11-随机初始化" class="headerlink" title="3.11 随机初始化"></a>3.11 随机初始化</h4><p>神经网络模型中的参数权重W是不能全部初始化为零的，接下来我们分析一下原因。</p>
<p>举个简单的例子，一个浅层神经网络包含两个输入，隐藏层包含两个神经元。如果权重<script type="math/tex">W[1]</script>和<script type="math/tex">W^{[2]}</script>都初始化为零，即：</p>
<script type="math/tex; mode=display">
\begin{gathered}W^{[1]}=\left[\begin{array}{ll}0 & 0 \\0 & 0\end{array}\right] \\W^{[2]}=\left[\begin{array}{ll}0 & 0\end{array}\right]\end{gathered}</script><p><img src="https://pic.imgdb.cn/item/648ac8e91ddac507cc6ae7b1.png" style="zoom:60%"></p>
<p>这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即<script type="math/tex">a^{[1]}_1=a^{[1]}_2</script>。经过推导得到<script type="math/tex">dz^{[1]}_1 = dz^{[1]}_2</script>，以及<script type="math/tex">dW^{[1]}_1 = dW^{[1]}_2</script>。因此，这样的结果是隐藏层两个神经元对应的权重行向量<script type="math/tex">W^{[1]}_1</script>和<script type="math/tex">W^{[1]}_2</script>，每次迭代更新都会得到完全相同的结果，<script type="math/tex">W^{[1]}_1</script>始终等于<script type="math/tex">W^{[1]}_2</script>，完全对称。这样隐藏层设置多个神经元就没有任何意义了。值得一提的是，<strong>参数</strong><script type="math/tex">b</script><strong>可以全部初始化为零</strong>，并不会影响神经网络训练效果；<strong>此外权重</strong><script type="math/tex">W</script><strong>不能全初始化为零，也不能全部初始化为一样的值。</strong></p>
<p>我们把这种权重W全部初始化为零带来的问题称为symmetry breaking problem。解决方法也很简单，就是将W进行随机初始化(b可初始化为零)。python里可以使用如下语句进行W和b的初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_1 = np.random.randn((<span class="number">2</span>,<span class="number">2</span>))*<span class="number">0.01</span></span><br><span class="line">b_1 = np.zero((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">W_2 = np.random.randn((<span class="number">1</span>,<span class="number">2</span>))*<span class="number">0.01</span></span><br><span class="line">b_2 = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>这里我们将<script type="math/tex">W^{[1]}_1</script>和<script type="math/tex">W^{[1]}_2</script>乘以<script type="math/tex">0.01</script>的目的是尽量使得权重<script type="math/tex">W</script>初始化比较小的值。之所以让<script type="math/tex">W</script>比较小，是因为如果使用sigmoid函数或者tanh函数作为激活函数的话，<script type="math/tex">W</script>比较小，得到的<script type="math/tex">|z|</script>也比较小(靠近零点)，而<strong>零点区域的梯度比较大</strong>，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解。如果<script type="math/tex">W</script>较大，得到的<script type="math/tex">|z|</script>也比较大，附近曲线平缓，梯度较小，训练过程会慢很多。</p>
<p>当然，如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题。但是，如果输出层是sigmoid函数，则对应的权重最好初始化到比较小的值。</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"> 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          打赏
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://junheng-wang.github.io/2022/04/26/Foundation-of-DeepLearning/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" rel="tag">深度学习基础</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2022/04/26/Basic-knowledge-of-programming-software/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            编程软件基础知识——杂记
          
        </div>
      </a>
    
    
      <a href="/2022/04/25/Python-code-tips/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Python其他小知识</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "0DieopImIy7vnuzj4jQ2wk6O-gzGzoHsz",
    app_key: "j6eRiYtlDSl8eRXVN54blF25",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2023
        <i class="ri-heart-fill heart_icon"></i> wjh
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Wang Junheng"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/Alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechatpay.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>

</html>