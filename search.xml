<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>高等数学1</title>
    <url>/2022/06/26/Advanced-mathematics1/</url>
    <content><![CDATA[<h3 id="1-极限"><a href="#1-极限" class="headerlink" title="1 极限"></a>1 极限</h3><p>微积分，既是一种工具也思想。方法虽看似繁多其实背后蕴含的道理浅显而直观莫要迷失于众多的题目中，关键在于体会方法背后的想。</p>
<h4 id="1-1-认识极限"><a href="#1-1-认识极限" class="headerlink" title="1.1 认识极限"></a>1.1 认识极限</h4><center><img src="https://pic.imgdb.cn/item/62b6d79a0947543129609682.jpg" style="zoom:50%"></center>

<p>极限式如$\lim\limits_{x \to a}f(x)$、$\lim\limits_{x \to \infty}f(x)$的含义是：当$x$向某个值(或无穷远)处靠近时，$f(x)$向那个值靠近。例如：$\lim\limits_{x \to 2}(x+3) = 5$。</p>
<h4 id="1-2-理解极限"><a href="#1-2-理解极限" class="headerlink" title="1.2 理解极限"></a>1.2 理解极限</h4><p>极限过程是一个动态的过程，不是一个“死”的数字。而至于“$\lim$”符号，是解算出接近的目标。要体会“$x \to a$”和“$x = a$”之间的区别。当$x \to a$时，$(x - a)$是无穷小的，所以我们研究极限时，大部分时候都是在和“无穷小”以及“无穷大”打交道。</p>
<h4 id="1-3-求函数极限"><a href="#1-3-求函数极限" class="headerlink" title="1.3 求函数极限"></a>1.3 求函数极限</h4><h5 id="1-3-1-求解函数极限"><a href="#1-3-1-求解函数极限" class="headerlink" title="1.3.1 求解函数极限"></a>1.3.1 求解函数极限</h5><ul>
<li>第一步，代入：将自变量极限值代入极限表达式，如果不能得到结果，继续下一步；<ul>
<li>代入时，需要注意三点问题：① 无穷大与无穷小之间呈倒数关系；② 无穷小×有界函数=无穷小；③ 需要注意极限的方向性问题，区分“$x \to +\infty$”和“$x \to -\infty$”</li>
</ul>
</li>
<li>第二步，分类：判断极限类型属于“$\dfrac{0}{0}$”、“$\dfrac{\infty}{\infty}$”、“$1^{\infty}$”、“${\infty} - {\infty}$”、“$0 \cdot \infty$”、“$\infty^0$”中的哪一种，重点是识别出式中的无穷小和无穷大成分；</li>
<li>第三步，求解：根据极限类型，选择分别适用的求解方法，进行化简或者变形。</li>
</ul>
<h5 id="1-3-2-无穷小相关的极限问题"><a href="#1-3-2-无穷小相关的极限问题" class="headerlink" title="1.3.2 无穷小相关的极限问题"></a>1.3.2 无穷小相关的极限问题</h5><p>对于无穷小相关的极限，可采用的方法有：</p>
<ul>
<li>1° 化简：消除“致0 因子”；</li>
<li>2° 等价无穷小代换；</li>
<li>3° 洛必达法则。</li>
</ul>
<p>常用的等价无穷小代换：当$x \to 0$时，下列表格中左侧的无穷小量可以用右侧的替换：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">原函数</th>
<th style="text-align:center">等价无穷小</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\sin x$</td>
<td style="text-align:center">$x$</td>
</tr>
<tr>
<td style="text-align:center">$\tan x$</td>
<td style="text-align:center">$x$</td>
</tr>
<tr>
<td style="text-align:center">$\arcsin x$</td>
<td style="text-align:center">$x$</td>
</tr>
<tr>
<td style="text-align:center">$\arctan x$</td>
<td style="text-align:center">$x$</td>
</tr>
<tr>
<td style="text-align:center">$\ln(1 + x)$</td>
<td style="text-align:center">$x$</td>
</tr>
<tr>
<td style="text-align:center">$e^x - 1$</td>
<td style="text-align:center">$x$</td>
</tr>
<tr>
<td style="text-align:center">$[(1+x)^a - 1]$</td>
<td style="text-align:center">$ax$</td>
</tr>
<tr>
<td style="text-align:center">$1-\cos x$</td>
<td style="text-align:center">$\dfrac{1}{2}x^2$</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>Tips：函数$f(x)$如果过原点，即$f(0)=0$，而且导数有$f’(0) = a$，则在$x \to a$时， 可以和$ax$等价无穷小代换(<strong>泰勒展开</strong>)。</p>
</blockquote>
<p>使用等价无穷小代换时，需要关注以下2个细节：</p>
<ul>
<li>要学会“抓住”无穷小，如果$x \to 0$，则相应的$ax$也是无穷小。如果$x \to a(a \neq 0)$，此时就要从式子中凑出$(x - a)$这个量；</li>
<li>两个无穷小量相加减时，不可将其中任意一个部分进行代换。</li>
</ul>
<h5 id="1-3-3-无穷大相关的极限问题"><a href="#1-3-3-无穷大相关的极限问题" class="headerlink" title="1.3.3 无穷大相关的极限问题"></a>1.3.3 无穷大相关的极限问题</h5><p>解决无穷大类型问题的方法：“抓大头”的思想理念，抓住主要矛盾，忽略次要成分：</p>
<script type="math/tex; mode=display">
\ln x \ll x^{a} \ll x^{b} \ll c^{x} \ll d^{x} \ll x ! \ll x^{x} \quad(x \rightarrow \infty, 0<a<b, 1<c<d)</script><p>当两个相差无穷倍的量相进行加减时，这时候我们眼里可以忽略相对较小的一个量。在具体操作上，应该将最大的成分提括号外，分子分母同时除掉即可。</p>
<h5 id="1-3-4-极限运算法则"><a href="#1-3-4-极限运算法则" class="headerlink" title="1.3.4 极限运算法则"></a>1.3.4 极限运算法则</h5><p>如果函数$u, v$极限存在，则：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\lim (u \pm v)=\lim u \pm \lim v \\
&\lim u \cdot v=\lim u \cdot \lim v \\
&\lim \frac{u}{v}=\frac{\lim u}{\lim v}(v \neq 0)
\end{aligned}</script><p>在处理极限的时候，我们可以选择将极限进行部分地运算。但要尤其注意前提条件！</p>
<h3 id="2-函数的连续性与间断点"><a href="#2-函数的连续性与间断点" class="headerlink" title="2 函数的连续性与间断点"></a>2 函数的连续性与间断点</h3><p>函数连续性应满足条件：$\lim\limits_{x \to a}f(x) = f(a)$(着重体会$x \to a$和$x = a$的区别)该条件意味着函数图像应该是一条连续的线条，不会有断点。我们接触到的函数，断点往往会出现在这两种地方：分段函数的分界点、函数定义域的边界。比如下面这种情况：</p>
<center><img src="https://pic.imgdb.cn/item/62b6e1bd09475431296ebdcb.jpg" style="zoom:50%"></center>

<p>通过计算$\lim\limits_{x \to a}f(x)$后与$f(a)$作比较，就可以得到函数在$x = a$处的连续或间断情况。函数间断点有以下四种情形：</p>
<center><img src="https://pic.imgdb.cn/item/62b6e25b09475431296f8932.jpg" style="zoom:50%"></center>

<center><img src="https://pic.imgdb.cn/item/648c16f31ddac507cc36c237.png" style="zoom:100%"></center>

<p><strong>总结：</strong></p>
<center><img src="https://pic.imgdb.cn/item/648c176f1ddac507cc382514.jpg"></center>

<h3 id="3-微分"><a href="#3-微分" class="headerlink" title="3 微分"></a>3 微分</h3><h4 id="3-1-导数的定义"><a href="#3-1-导数的定义" class="headerlink" title="3.1 导数的定义"></a>3.1 导数的定义</h4><p>直线的斜率$k$：</p>
<p>(1) $y$随$x$变化的方向与快慢：$x$每增加一个单位，$y$的变化量为$k$；<br>(2) 反映出直线的坡度：直线与$x$轴正方向夹角为$\alpha$，$k = \tan \alpha$。</p>
<center><img src="https://pic.imgdb.cn/item/648c17be1ddac507cc38fd4a.jpg"></center>

<p>如何研究曲线中$y$随$x$变化的快慢？</p>
<center><img src="https://pic.imgdb.cn/item/648c17de1ddac507cc3967e0.jpg"></center>

<p>导数反映出因变量随着自变量增长而变化的快慢。导数定义如下：</p>
<center><img src="https://pic.imgdb.cn/item/648c18001ddac507cc39dbc8.jpg"></center>

<p>导数也被叫做“微商”，可以理解为两个无穷小量之间的比值。</p>
<h4 id="3-2-导数的基本运算"><a href="#3-2-导数的基本运算" class="headerlink" title="3.2 导数的基本运算"></a>3.2 导数的基本运算</h4><h5 id="3-2-1-常用函数的导数"><a href="#3-2-1-常用函数的导数" class="headerlink" title="3.2.1 常用函数的导数"></a>3.2.1 常用函数的导数</h5><script type="math/tex; mode=display">
\begin{array}{ll}
(c)^{\prime}=0 \quad(c \text { 为常数 }) & \left(x^{\alpha}\right)^{\prime}=\alpha x^{\alpha-1} \\
(\sin x)^{\prime}=\cos x & (\cos x)^{\prime}=-\sin x \\
(\tan x)^{\prime}=\sec ^{2} x & (\cot x)^{\prime}=-\csc ^{2} x \\
(\ln x)^{\prime}=\frac{1}{x} & \left(\log _{a} x\right)^{\prime}=\frac{1}{x \ln a} \quad(a>0, a \neq 1) \\
\left(\mathrm{e}^{x}\right)^{\prime}=\mathrm{e}^{x} & \left(a^{x}\right)^{\prime}=a^{x} \ln a \quad(a>0) \\
(\arcsin x)^{\prime}=\frac{1}{\sqrt{1-x^{2}}} & (\arctan x)^{\prime}=\frac{1}{1+x^{2}}
\end{array}</script><h5 id="3-2-2-导数四则运算法则"><a href="#3-2-2-导数四则运算法则" class="headerlink" title="3.2.2 导数四则运算法则"></a>3.2.2 导数四则运算法则</h5><script type="math/tex; mode=display">
\begin{aligned}
&{[f(x) \pm g(x)]^{\prime}=f^{\prime}(x) \pm g^{\prime}(x)} \\
&{[f(x) \cdot g(x)]^{\prime}=f^{\prime}(x) \cdot g(x)+f(x) \cdot g^{\prime}(x)} \\
&{\left[\frac{f(x)}{g(x)}\right]^{\prime}=\frac{f^{\prime}(x) g(x)-f(x) g^{\prime}(x)}{g^{2}(x)} \quad(g(x) \neq 0)}
\end{aligned}</script><h5 id="3-2-3-复合函数求导"><a href="#3-2-3-复合函数求导" class="headerlink" title="3.2.3 复合函数求导"></a>3.2.3 复合函数求导</h5><script type="math/tex; mode=display">
[ f(g(x))]' = f'(g(x)) \cdot g'(x)</script><h5 id="3-2-4-求导链式法则"><a href="#3-2-4-求导链式法则" class="headerlink" title="3.2.4 求导链式法则"></a>3.2.4 求导链式法则</h5><p>即设置中间变量：</p>
<script type="math/tex; mode=display">
\dfrac{\text{d}y}{\text{d}x} = \dfrac{\text{d}y}{\text{d}u} \cdot \dfrac{\text{d}u}{\text{d}x}</script><h5 id="3-2-5-莱布尼茨公式"><a href="#3-2-5-莱布尼茨公式" class="headerlink" title="3.2.5 莱布尼茨公式"></a>3.2.5 莱布尼茨公式</h5><p>设$u = u(x)$、$v = v(x)$分别是关于$x$的函数，那么：</p>
<script type="math/tex; mode=display">
\begin{aligned}
& (uv)' = u'v + uv' \\
& (uv)'' = (u'v + uv')' = (u'v)' + (uv')' = u''v + 2u'v' + uv''
\end{aligned}</script><p>如果函数$u = u(x)$、$v = v(x)$分别具有$n$阶导数，那么两者乘积的$n$阶导数为如下形式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
{[u(x) \cdot v(x)]^{(n)} } &=C_{n}^{0} u^{(n)} v+C_{n}^{1} u^{(n-1)} v^{\prime}+C_{n}^{2} u^{(n-2)} v^{\prime \prime}+\cdots+C_{n}^{n-1} u^{\prime} v^{(n-1)}+C_{n}^{n} v^{(n)} u \\
&=\sum_{k=0}^{n} C_{n}^{k} u^{(k)} \cdot v^{(n-k)}
\end{aligned}</script><p>掌握莱布尼茨公式并不难，它类似于我们高中的“二项式定理”。</p>
<h5 id="3-2-6-隐函数求导"><a href="#3-2-6-隐函数求导" class="headerlink" title="3.2.6 隐函数求导"></a>3.2.6 隐函数求导</h5><p>隐函数，例如：$y - xe^y = 1$，可以写为$y - xe^y - 1 = 0$，但是无法改写为$y = f(x)$的形式。这时求$y’$，左右两侧同时对$x$求导。注意，有时候我们遇到需要求导的问题，给出的函数并非隐函数的形式，但是转化成隐函数求导反而更简单。</p>
<h5 id="3-2-7-参数方程求导"><a href="#3-2-7-参数方程求导" class="headerlink" title="3.2.7 参数方程求导"></a>3.2.7 参数方程求导</h5><p>形如$\left\{\begin{array}{l}y=f(t) \\ x=g(t)\end{array}\right.$，如果求$\dfrac{\text{d}y}{\text{d}x}$，则就等于$\dfrac{\text{d}y}{\text{d}x} = \dfrac{\dfrac{\text{d}y}{\text{d}t}}{\dfrac{\text{d}x}{\text{d}t}}$，也就是分别求出$\dfrac{\text{d}f}{\text{d}t}$、$\dfrac{\text{d}g}{\text{d}t}$再相除。</p>
<h4 id="3-3-泰勒公式"><a href="#3-3-泰勒公式" class="headerlink" title="3.3 泰勒公式"></a>3.3 泰勒公式</h4><h5 id="3-3-1-什么是泰勒公式"><a href="#3-3-1-什么是泰勒公式" class="headerlink" title="3.3.1 什么是泰勒公式"></a>3.3.1 什么是泰勒公式</h5><p>$x$的多项式函数：$y = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n$，$x$的多项式次数越高，对应的曲线形状就越多变。于是我们可以用高次多项式来逼近其他函数，比如三角函数、对数函数、指数函数等等，如下图所示：</p>
<center><img src="https://pic.imgdb.cn/item/648c18251ddac507cc3a4292.jpg"></center>

<p>泰勒公式就是把$(n + 1)$阶可导的函数写成下列格式：</p>
<script type="math/tex; mode=display">
f(x)=f\left(x_{0}\right)+\frac{f^{\prime}\left(x_{0}\right)}{1 !}\left(x-x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right)}{2 !}\left(x-x_{0}\right)^{2}+\cdots+\frac{f^{(n)}\left(x_{0}\right)}{n !}\left(x-x_{0}\right)^{n}+R_{n}(x)</script><p>我们需要把握下列常见函数的泰勒展开(麦克劳林展开，佩亚诺余项)形式：</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\sin x=x-\frac{1}{3 !} x^{3}+\frac{1}{5 !} x^{5}+o\left(x^{5}\right) & \mathrm{e}^{x}=1+\frac{1}{1 !} x+\frac{1}{2 !} x^{2}+\frac{1}{3 !} x^{3}+o\left(x^{3}\right) \\
\cos x=1-\frac{1}{2 !} x^{2}+\frac{1}{4 !} x^{4}+o\left(x^{4}\right) & \frac{1}{1-x}=1+x+x^{2}+x^{3}+o\left(x^{3}\right) \\
\tan x=x+\frac{1}{3} x^{3}+\frac{2}{15} x^{5}+o\left(x^{5}\right) & \ln (1+x)=x-\frac{1}{2} x^{2}+\frac{1}{3} x^{3}+o\left(x^{3}\right)
\end{array}</script><ul>
<li>泰勒公式的要点：<br>(1) 上述是在$x = 0$附近进行的近似，当$x \to 0$时，余项可以近似舍弃；<br>(2) $\dfrac{1}{1-x}=1+x+x^{2}+x^{3}+o(x^3)$可以用等比数列从右向左推导。<br>(3) 利用导数、原函数的关系，就可以辅助记忆，例如$\ln(1+x)$泰勒公式可以用$\dfrac{1}{1-x}$来获得；<br>(4) 泰勒公式可以将复杂的函数计算化简成为加减乘除；<br>(5) 泰勒公式具有拓展性，当$x \to 0$时：$\sin x$、$\ln(1+x)$、$5x$也是无穷小量，在泰勒公式左右两侧<br>对进行替换，例如：<script type="math/tex; mode=display">
\dfrac{1}{1-5 x}=1+5 x+25 x^{2}+125 x^{3}+\cdots</script></li>
</ul>
<h5 id="3-3-2-泰勒公式的应用"><a href="#3-3-2-泰勒公式的应用" class="headerlink" title="3.3.2 泰勒公式的应用"></a>3.3.2 泰勒公式的应用</h5><p>(1) 利用泰勒公式求极限</p>
<p>泰勒公式可以看作是超级版本的无穷小代换，它不会受到加减法不能换的限制。</p>
<script type="math/tex; mode=display">
\lim _{x \rightarrow 0} \frac{\tan x-\sin x}{x^{3}}=\lim _{x \rightarrow 0} \frac{\left(x+\frac{1}{3} x^{3}\right)-\left(x-\frac{1}{6} x^{3}\right)}{x^{3}}=\lim _{x \rightarrow 0} \frac{\frac{1}{2} x^{3}}{x^{3}}=\frac{1}{2}</script><p>(2) 判断无穷小的阶数</p>
<p>$x \to 0$时，判断下式的无穷小阶数(答案是3阶)：</p>
<script type="math/tex; mode=display">
\frac{1}{1-x}-\mathrm{e}^{x}+\cos x-1=\left(1+x+x^{2}+x^{3}\right)-\left(1+x+\frac{1}{2} x^{2}+\frac{1}{6} x^{3}\right)+\left(1-\frac{x^{2}}{2}+\frac{x^{4}}{24}\right)-1=\frac{5}{6} x^{3}</script><p>(3) 求高阶导数</p>
<h3 id="4-积分部分"><a href="#4-积分部分" class="headerlink" title="4 积分部分"></a>4 积分部分</h3><h4 id="4-1-积分的基本概念：定积分和不定积分"><a href="#4-1-积分的基本概念：定积分和不定积分" class="headerlink" title="4.1 积分的基本概念：定积分和不定积分"></a>4.1 积分的基本概念：定积分和不定积分</h4><center><img src="https://pic.imgdb.cn/item/648c18501ddac507cc3aad00.jpg"></center>

<p>例如求$y = e^x$在$x \in [0, 1]$区间内的函数区域面积，则有：</p>
<center><img src="https://pic.imgdb.cn/item/648c18691ddac507cc3af312.jpg" style="zoom:60%"></center>

<p>假设把$0 \sim 1$区间切割为$n$份，每个小矩形的宽度为$\dfrac{1}{n}$，则总面积有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
S &=\lim _{n \rightarrow \infty} \frac{1}{n} \mathrm{e}^{\frac{1}{n}}+\frac{1}{n} \mathrm{e}^{\frac{2}{n}}+\frac{1}{n} \mathrm{e}^{\frac{3}{n}}+\cdots+\frac{1}{n} \mathrm{e}^{\frac{n}{n}} \\
&=\lim _{n \rightarrow \infty} \frac{1}{n}\left(\mathrm{e}^{\frac{1}{n}}+\mathrm{e}^{\frac{2}{n}}+\mathrm{e}^{\frac{3}{n}}+\cdots+\mathrm{e}^{\frac{n}{n}}\right) \\
&=\lim _{n \rightarrow \infty} \frac{1}{n} \frac{\mathrm{e}^{\frac{1}{n}}\left[1-\left(\mathrm{e}^{\frac{1}{n}}\right)^{n}\right]}{1-\mathrm{e}^{\frac{1}{n}}}=\mathrm{e}-1
\end{aligned}</script><p>微积分最为核心的公式 —— <strong><font color="red">牛顿-莱布尼茨公式</font></strong>：</p>
<script type="math/tex; mode=display">
\int_a^b f(x) \text{d}x = F(b) - F(a)</script><p>其中函数$F(x)$与函数$f(x)$之间存在关系：$f(x) = F’(x)$，则称函数$F(x)$为函数$f(x)$的“原函数”，记<br>为$\int f(x) \text{d}x = F(x)$，求函数$f(x)$的不定积分就是求函数的原函数$F(x)$。</p>
<h4 id="4-2-常见函数积分"><a href="#4-2-常见函数积分" class="headerlink" title="4.2 常见函数积分"></a>4.2 常见函数积分</h4><script type="math/tex; mode=display">
\begin{array}{ll}
\int k \mathrm{~d} x=k x+C & \int \cos x \mathrm{~d} x=\sin x+C \\
\int x^{a} \mathrm{~d} x=\frac{x^{a+1}}{a+1}+C & \int \sin x \mathrm{~d} x=-\cos x+C \\
\int \frac{1}{x} \mathrm{~d} x=\ln |x|+C & \int \frac{1}{\cos ^{2} x} \mathrm{~d} x=\int \sec ^{2} x \mathrm{~d} x=\tan x+C \\
\int \frac{1}{1+x^{2}} \mathrm{~d} x=\arctan x+C & \int \frac{1}{\sin ^{2} x} \mathrm{~d} x=\int \csc ^{2} x \mathrm{~d} x=-\cot x+C \\
\int \frac{1}{\sqrt{1-x^{2}}} \mathrm{~d} x=\arcsin x+C & \int \mathrm{e}^{x} \mathrm{~d} x=\mathrm{e}^{x}+C \\
& \int a^{x} \mathrm{~d} x=\frac{a^{x}}{\ln a}+C
\end{array}</script><p>需要注意的是，求不定积分一定要在表达式结尾“$+C$”，因为同一个函数的原函数不仅有一个，它们之间相差一个常数。</p>
<h4 id="4-3-积分运算法则"><a href="#4-3-积分运算法则" class="headerlink" title="4.3 积分运算法则"></a>4.3 积分运算法则</h4><script type="math/tex; mode=display">
\int[f(x) \pm g(x)]\text{d}x = \int f(x)\text{d}x \pm \int  g(x)\text{d}x</script><script type="math/tex; mode=display">
\int kf(x) \text{d}x = k\int f(x)\text{d}x</script><h4 id="4-4-常见求积分类型"><a href="#4-4-常见求积分类型" class="headerlink" title="4.4 常见求积分类型"></a>4.4 常见求积分类型</h4><h5 id="4-4-1-积分的线性变换"><a href="#4-4-1-积分的线性变换" class="headerlink" title="4.4.1 积分的线性变换"></a>4.4.1 积分的线性变换</h5><p>被积函数是$(ax+b)$的形式时，可以采用令$(ax+b=t)$的形式，将其换成简单的函数形式。</p>
<h5 id="4-4-2-凑微分法：要求熟练掌握各类导数"><a href="#4-4-2-凑微分法：要求熟练掌握各类导数" class="headerlink" title="4.4.2 凑微分法：要求熟练掌握各类导数"></a>4.4.2 凑微分法：要求熟练掌握各类导数</h5><p>三角积分函数中的凑微分法小技巧，若积分式$R(\sin x, \cos x)$存在下列条件：</p>
<ul>
<li>$R(-\sin x, \cos x) = -R(\sin x, \cos x)$，则要凑$\sin x\text{d}x = -\text{d} \cos x$</li>
<li>$R(\sin x, -\cos x) = -R(\sin x, \cos x)$，则要凑$\cos x\text{d}x = \text{d} \sin x$</li>
<li>$R(-\sin x, -\cos x) = R(\sin x, \cos x)$，则要凑$\sec^2 x\text{d}x = \text{d} \tan x$</li>
</ul>
<h5 id="4-4-3-根式-三角变换"><a href="#4-4-3-根式-三角变换" class="headerlink" title="4.4.3 根式/三角变换"></a>4.4.3 根式/三角变换</h5><p><strong>换元目标：消除根号。</strong></p>
<ul>
<li>第一类：根号内为$x$的一次多项式$\sqrt{ax+b}$，令$\sqrt{ax+b} = t$；</li>
<li>第二类：根号内为$x$的二次多项式：</li>
</ul>
<center><img src="https://pic.imgdb.cn/item/648c18901ddac507cc3b69cf.jpg"></center>

<h5 id="4-4-4-分部积分法"><a href="#4-4-4-分部积分法" class="headerlink" title="4.4.4 分部积分法"></a>4.4.4 分部积分法</h5><p>引入例题：求下面图示阴影区域的面积：</p>
<center><img src="https://pic.imgdb.cn/item/648c19131ddac507cc3d0c80.jpg" style="zoom:30%"></center>

<p>由于$y = \ln x, x = e^y$，所以面积有：</p>
<script type="math/tex; mode=display">
\int_{\frac{1}{2}}^2 x \text{d}y = \int_{\frac{1}{2}}^2 e^y \text{d}y = e^2 - e^{\frac{1}{2}}</script><p>由此可以推导得到分部积分的原理与公式：</p>
<script type="math/tex; mode=display">
\int u \cdot v' \text{d}x = \int u \text{d}v = uv - \int v \text{d}u = uv - \int v u' \text{d}x</script><center><img src="https://pic.imgdb.cn/item/648c19551ddac507cc3de466.png"></center>

<p>被积函数为两种不同类型函数相乘时，一般用分部积分法。<br>记住<strong>口诀</strong>：反对幂指三(反三角函数、对数函数、幂函数、指数函数、三角函数)。<br>口诀中，越是靠后类型的函数，越优先与“$\text{d}x$”结合。</p>
<h5 id="4-4-5-变限积分"><a href="#4-4-5-变限积分" class="headerlink" title="4.4.5 变限积分"></a>4.4.5 变限积分</h5><p>对于变上限积分函数，我们常用到的是它的导数：</p>
<script type="math/tex; mode=display">
(\int_0^x f(t) \text{d}t)' = f(x)</script><p>而这种题目有相应的变体，我们也需要掌握：</p>
<ul>
<li>$(\int_0^{g(x)} f(t) \text{d}t)’ = f(g(x)) \cdot g’(x)$，变化的上限是$g(x)$，利用复合函数求导；</li>
<li>$(\int_{a(x)}^{b(x)} f(t) \text{d}t)’ = (\int_{0}^{b(x)} f(t) \text{d}t - \int_{0}^{a(x)} f(t) \text{d}t)’$，上下限中均含有变量；</li>
<li>$(\int_{0}^{x} x f(t) \mathrm{d} t)^{\prime}=(x \int_{0}^{x} f(t) \mathrm{d} t)^{\prime}=x f(x)+\int_{0}^{x} f(t) \mathrm{d} t$，被积表达式中有可提取的$x$；</li>
<li>$(\int_{0}^{x} f(x t) \mathrm{d} t)^{\prime}=(\int_{0}^{x^{2}} \frac{f(u)}{x} \mathrm{~d} u)^{\prime}=(\frac{1}{x} \int_{0}^{x^{2}} f(u) \mathrm{d} u)$，被积函数中$x$和$t$结合。</li>
</ul>
<h4 id="4-5-定积分运算"><a href="#4-5-定积分运算" class="headerlink" title="4.5 定积分运算"></a>4.5 定积分运算</h4><p>定积分与不定积分类似，也需要求函数的原函数。但是不同的是定积分指定了上、下限，在求得原函数后需要代入、作差，例如$\int_{a}^{b} f(x) \text{d}x = F(b) - F(a) $。</p>
<p>相比于不定积分，定积分还需要格外注意在引用新字母进行换元方法时，<strong>上下限也需要更换</strong>。</p>
<h3 id="5-中值定理"><a href="#5-中值定理" class="headerlink" title="5 中值定理"></a>5 中值定理</h3><center><img src="https://pic.imgdb.cn/item/648c19721ddac507cc3e4672.png"></center>

<h4 id="5-1-罗尔中值定理证明等式"><a href="#5-1-罗尔中值定理证明等式" class="headerlink" title="5.1 罗尔中值定理证明等式"></a>5.1 罗尔中值定理证明等式</h4><p>从图像上理解罗尔/拉格朗日中值定理：</p>
<center><img src="https://pic.imgdb.cn/item/648c19c31ddac507cc3f3722.png"></center>

<p><strong>第一步</strong>：将需要证明的等式中的“$\xi$”换为“x”，将右侧项移至左侧，设左侧的内容为$g(x)$，则原题需要证明的结论转化为：需要证明$g(x) = 0$在$(0,a)$内有实根；<br><strong>第二步</strong>：需要构造辅助函数$G(x)$，满足$G’(x) = g(x)$，或者$G’(x)$中含有$g(x)$；<br><strong>第三步</strong>：结合题目给出的其他条件，在区间$[0, a]$上找到两点，使辅助函数$G(x)$在这两点处函数值相等，结合罗尔定理，证明结论。</p>
<p>此类题目的难点无非两点：构造辅助函数，证明两处值相等。</p>
<h4 id="5-2-数列极限"><a href="#5-2-数列极限" class="headerlink" title="5.2 数列极限"></a>5.2 数列极限</h4><h5 id="5-2-1-直接化简法"><a href="#5-2-1-直接化简法" class="headerlink" title="5.2.1 直接化简法"></a>5.2.1 直接化简法</h5><h5 id="5-2-2-利用夹逼准则-缩放"><a href="#5-2-2-利用夹逼准则-缩放" class="headerlink" title="5.2.2 利用夹逼准则(缩放)"></a>5.2.2 利用夹逼准则(缩放)</h5><h5 id="5-2-3-转化为定积分"><a href="#5-2-3-转化为定积分" class="headerlink" title="5.2.3 转化为定积分"></a>5.2.3 转化为定积分</h5><p>导数dy/dx理解(除法？)<br>泰勒展开<br>高阶无穷小削去</p>
<hr>
<h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p>课程链接1：<a href="https://www.bilibili.com/video/BV1j44y1E7ot">【高等数学（上）】6小时从0基础直追满绩！- 李天意 - bilibili</a></p>
<p>课程链接2：<a href="https://www.bilibili.com/video/BV1ZK4y1u7WU">【高等数学（下）】概念理解+解题方法 7小时 精讲速学！- 李天意 - bilibili</a></p>
<p>PDF资料链接：<a href="https://pan.baidu.com/s/1ZD6Qj9R0VycOF4S0SMYY1w">https://pan.baidu.com/s/1ZD6Qj9R0VycOF4S0SMYY1w</a> 提取码：gb6s</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>数学基础</category>
        <category>高等数学</category>
      </categories>
      <tags>
        <tag>高数</tag>
        <tag>本质问题</tag>
      </tags>
  </entry>
  <entry>
    <title>概率基础概念本质的辨析理解</title>
    <url>/2022/01/24/Basic-concept-of-probability/</url>
    <content><![CDATA[<h3 id="〇、如何学好概率论与数理统计"><a href="#〇、如何学好概率论与数理统计" class="headerlink" title="〇、如何学好概率论与数理统计"></a>〇、如何学好概率论与数理统计</h3><h4 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a><strong>参考链接：</strong></h4><p>参考链接0.1：<a href="http://www.hanhai.org/xuexijiaoliu/26.htm">如何学好概率论与数理统计 - 邹群 - 浙大个人博客</a></p>
<p>参考链接0.2：<a href="https://www.zhihu.com/question/36833473/answer/862307409">怎样学习大学概率论与数理统计？ - 石溪的回答 - 知乎</a></p>
<h3 id="一、什么是概率"><a href="#一、什么是概率" class="headerlink" title="一、什么是概率"></a>一、什么是概率</h3><p><strong><font color="red">用两个字来表述概率的本质——函数</font></strong></p>
<p><strong><font color="red">用四个字来表述概率的本质——集合函数，集合到$[0, 1]$区间的映射</font></strong></p>
<h4 id="1-概率公理化定义"><a href="#1-概率公理化定义" class="headerlink" title="1. 概率公理化定义"></a>1. 概率公理化定义</h4><p>定义在事件域$ \mathscr{F} $上的集合函数$P$称为概率，它需要满足下面这三个要求：</p>
<p>​    (1) 非负性：$P(A) \geq 0, ~ \forall A \subset   \mathscr{F}$</p>
<p>​    (2) 规范性/正则性：$P(\Omega) = 1$</p>
<p>​    (3) 可列可加性：若$ A_i \subset  \mathscr{F}, ~ i = 1,2,…$，且两两互不相容，则</p>
<script type="math/tex; mode=display">
P(\sum_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}P(A_i)</script><p>​        或者写作</p>
<script type="math/tex; mode=display">
P(\mathop{\bigcup}\limits_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}P(A_i)</script><h4 id="2-名词解释"><a href="#2-名词解释" class="headerlink" title="2. 名词解释"></a>2. 名词解释</h4><h5 id="2-1-样本空间"><a href="#2-1-样本空间" class="headerlink" title="2.1 样本空间"></a>2.1 样本空间</h5><p>$\Omega = $ { $  \omega_1, \omega_2,…, \omega_n $ }，其中$\omega_n$称为样本点，比如掷硬币则$\Omega =$ { $  “正面”, “反面” $}，掷骰子则$\Omega =${ $  “1点”, “2点”, “3点”, “4点”, “5点”, “6点” $} </p>
<h5 id="2-2-事件"><a href="#2-2-事件" class="headerlink" title="2.2 事件"></a>2.2 事件</h5><p>样本点的某个集合，比如掷骰子中{$1$}、{$2$} 或者{$2, 3, 5$}等所有可能的组合，事件可以看成$\Omega$的某个子集。</p>
<p>根据以上，我们可以称$\Omega$为必然事件，$\varnothing $是不可能事件。</p>
<h5 id="2-3-事件域"><a href="#2-3-事件域" class="headerlink" title="2.3 事件域"></a>2.3 事件域</h5><p>为了方便展示例子，我们这里将掷骰子的正六面体改为正四面体，也就是此时$\Omega =$ { $1, 2, 3, 4 $}，此时它所有的子集(可能的事件)为：</p>
<center>$A_1 = \varnothing$, $ A_2 = $ { $1$ }, $A_3 = $ { $2$ }, $A_4 = $ {$3$}, $A_5 =$ {$4$}, $ A_6 = ${$1, 2$}</center>

<center>$A_7 = ${$1, 3$}, $A_8 = ${$1,4$}, $ A_9 = ${$2, 3$}, $A _ {10} =$ {$2, 4$}, $A _ {11} =$ {$3, 4$}</center>

<center>$A _ {12} = ${$1, 2, 3$}, $A _ {13} = ${$1, 2, 4$}, $A _ {14} = ${$2, 3, 4$}, $A _ {15} = ${$1, 3, 4$}, $ A _ {16} = \Omega = ${$1, 2, 3 ,4$} </center>

<p>事件域就是<script type="math/tex">\mathscr{F} = \begin{Bmatrix} A_1, A_2, ..., A _ {16} \end{Bmatrix}</script>是由样本空间<script type="math/tex">\Omega</script>的一些子集构成的一个<script type="math/tex">\sigma</script>域。<script type="math/tex">\mathscr{F}</script>中的元素被称为事件。</p>
<h5 id="2-4-σ域"><a href="#2-4-σ域" class="headerlink" title="2.4 σ域"></a>2.4 σ域</h5><p>事件域$\mathscr{F}$必须是$\sigma$域，那么什么样的$\mathscr{F}$在$\sigma$域中呢：</p>
<ul>
<li>(1) <script type="math/tex">\Omega \subset \mathscr{F}</script>；</li>
<li>(2) 若<script type="math/tex">A \subset \mathscr{F}</script>，则<script type="math/tex">\bar A \in \mathscr{F}</script>；</li>
<li>(3) 若<script type="math/tex">A_i \subset \mathscr{F},(i = 1,2,...)</script> ，则<script type="math/tex">\mathop{\bigcup}\limits_{i=1}^{\infty}A_i \subset \mathscr{F}</script>。</li>
</ul>
<p>由上面3条我们可以进一步推导得到：</p>
<p>​    1° <script type="math/tex">\varnothing \subset \mathscr{F}</script></p>
<p>​    2° 交集属于事件域——<script type="math/tex">\mathop{\bigcap}\limits_{i=1}^{\infty}A_i = \overline {\mathop{\bigcup}\limits_{i = 1}^{\infty}\bar{A_i}}</script></p>
<p>​    3° 有限并属于事件域</p>
<p>​    4° 有限交属于事件域</p>
<p>例如<script type="math/tex">\mathscr{F}_1 = \begin{Bmatrix} A_1, A_2, ..., A _ {16} \end{Bmatrix}</script>，<script type="math/tex">\mathscr{F}_2 = \begin{Bmatrix} A_1, A _ {16}, A_2, A _ {14} \end{Bmatrix}</script>，<script type="math/tex">\mathscr{F}_3 = \begin{Bmatrix} A_1,  A _ {16} \end{Bmatrix}</script>等都是事件域(都满足上面3条)，其中<script type="math/tex">\mathscr{F}_3</script>是最简单的事件域(一个事件域最少由空集和样本空间这两个集合组成)，又称平凡事件域。</p>
<h5 id="2-5-事件域和样本空间辨析"><a href="#2-5-事件域和样本空间辨析" class="headerlink" title="2.5 事件域和样本空间辨析"></a>2.5 事件域和样本空间辨析</h5><p><strong>事件域是样本空间幂集的子集。也就是说，事件域中的每个元素是样本空间的一个子集。</strong>例如，掷骰子，样本空间取<script type="math/tex">A = \begin{Bmatrix}1,2,3,4,5,6 \end{Bmatrix}</script>，事件域可以取上述集合的全部子集，即$F=2^A$。此时F中的元素称为事件。例如，“掷出偶数”指的是F中的{2,4,6}这一元素。顺便一提：</p>
<p><strong><font color="red">概率本质是定义在事件域上的函数，而随机变量本质是定义在样本空间上的函数。</font></strong> </p>
<h4 id="3-重新梳理逻辑"><a href="#3-重新梳理逻辑" class="headerlink" title="3. 重新梳理逻辑"></a>3. 重新梳理逻辑</h4><p>上面我们先是给出了定义，再进一步解释的定义中各部分的意义，有点”由果寻因“的意味，逻辑上逆推。现在我们简单通过逻辑正推梳理一下：</p>
<p>样本点 { $\omega_1, \omega_2,…, \omega_n$}   $\Rightarrow$   样本空间$\Omega =$ {$ \omega_1, \omega_2,…, \omega_n $ }   $\Rightarrow$   某个事件$A_i =$ { $\omega_1, \omega_2$ }, $(A \subset \Omega)$   $\Rightarrow$   事件域$\mathscr{F} =$ { $A_1,A_2,…$}(事件域里面的事件$A_i$不是随意的，必须满足$\sigma$域条件)，那么概率就是一个函数，将事件域映射到$[0,1]$，计算概率的示例如下：</p>
<p>对于之前的<script type="math/tex">\mathscr{F}_1</script>，有：<script type="math/tex">P(A_1) = 0, P(A_2) = \frac{1}{4}, P(A _ {10}) = \frac{1}{2}</script></p>
<p>对于之前的<script type="math/tex">\mathscr{F}_2</script>，有：<script type="math/tex">P(A_1) = 0, P(A _ {16}) = 1, P(A_2) = \frac{1}{4}, P(A _ {14}) = \frac{1}{4}</script></p>
<p>对于之前的<script type="math/tex">\mathscr{F}_3</script>，有：<script type="math/tex">P(A_1) = 0, P(A _ {16}) = 1</script></p>
<h4 id="参考链接：-1"><a href="#参考链接：-1" class="headerlink" title="参考链接："></a><strong>参考链接：</strong></h4><p>参考链接1.1：<a href="https://www.bilibili.com/video/BV1fE411M7ra">到底什么是概率？- 概率统计小迷哥 - 哔哩哔哩</a>  </p>
<p>参考链接1.2：<a href="https://zhuanlan.zhihu.com/p/58995841">什么是概率？ - 马同学的文章 - 知乎</a></p>
<p>参考链接1.3：<a href="https://blog.csdn.net/zhoujunr1/article/details/77466826?spm=1001.2101.3001.6650.18&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-18.pc_relevant_aa&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-18.pc_relevant_aa&amp;utm_relevant_index=21">sigma代数、Borel 集、测度概念borel集 - zhoujunr1的博客 - CSDN</a> </p>
<p>参考链接1.4：<a href="https://blog.csdn.net/kac0c440/article/details/54928122?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7EHighlightScore-2.queryctrv2&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7EHighlightScore-2.queryctrv2&amp;utm_relevant_index=4">【初等概率论】 01 - kac0c440的专栏 - CSDN</a> </p>
<p>参考链接1.5：<a href="https://zhuanlan.zhihu.com/p/61729415">概率论复习笔记(3)——概率的公理化定义 - Fiddie的文章 - 知乎</a></p>
<p>参考链接1.6：<a href="https://www.enmsb.com/article/%E4%BA%8B%E4%BB%B6%E5%9F%9F%E5%92%8C%E4%BA%8B%E4%BB%B6%E7%A9%BA%E9%97%B4.html">事件域和样本空间有什么区别？- 恩牛网</a> </p>
<h3 id="二、什么是随机变量"><a href="#二、什么是随机变量" class="headerlink" title="二、什么是随机变量"></a>二、什么是随机变量</h3><h4 id="1-随机变量的本质"><a href="#1-随机变量的本质" class="headerlink" title="1. 随机变量的本质"></a>1. 随机变量的本质</h4><p><strong><font color="red">随机变量的本质是函数——样本点的函数</font></strong></p>
<p>定义：设$X(w)$是定义在概率空间$(\Omega, \mathscr{F}, P)$上的单值实函数(也就是说它的自变量就是一个个的样本点，因变量是实函数)，如果对直线上任意一博雷尔点集$B$，有$\begin{Bmatrix} \omega, X(\omega) \in B \end{Bmatrix} \subset \mathscr{F}$则称$X(\omega)$为随机变量(r.v.)。</p>
<blockquote>
<p>思考题目(某年北京考研题目)：</p>
<p>已知：$\Omega =$ { $\omega_1, \omega_2, \omega_3$ }, $P(\omega_1) = \frac{1}{3}, P(\omega_2) = \frac{1}{6}, P(\omega_3) = \frac{1}{2},  X(\omega_1) = 2, X(\omega_2) = X(\omega_3) = 0$</p>
<p>求：(1) $X$的分布；(2) 已知$E[Y] = 2$，$P(Y=X|X&gt;0) = 1,P(Y = X|X = 0)=\frac{3}{4}$，求$Y$的概率分布。</p>
<p>答案参考链接[2-1]</p>
</blockquote>
<h4 id="2-多维随机变量的本质"><a href="#2-多维随机变量的本质" class="headerlink" title="2. 多维随机变量的本质"></a>2. 多维随机变量的本质</h4><p><strong><font color="red">多维随机变量的本质是参数方程</font></strong></p>
<p>二维随机变量定义：设$E$是一个随机试验，它的样本空间是$\Omega$，设$X=X(e)$和$Y=Y(e)$是定义在$\Omega$上的随机变量，由它们构成的一个向量$(X, Y)$，叫做二维随机变量。</p>
<p>多为随机变量定义：一般，设$E$是一个随机试验，它的样本空间是$\Omega = $ {$e$}，设$X_1=X_1(e),…,X_n=X_n(e)$是定义在$\Omega$上的随机变量，由它们构成的一个$n$维向量$(X_1,X_2,…,X_n)$叫做$n$维随机向量或$n$维随机变量 </p>
<p><strong>特别要主要两个随机变量的自变量是来自一个样本空间。</strong></p>
<p>例子：假设要了解一下学生的身体指标，这个班里有3个人，我们做一次实验，随机抽出一个人测一下身高$X$，体重$Y$。则我们想一下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>张三</th>
<th>李四</th>
<th>王五</th>
</tr>
</thead>
<tbody>
<tr>
<td>身高</td>
<td>180</td>
<td>175</td>
<td>168</td>
</tr>
<tr>
<td>体重</td>
<td>71</td>
<td>65</td>
<td>60</td>
</tr>
</tbody>
</table>
</div>
<p>二维随机变量$(X,Y)$的样本空间是$\Omega$等于什么，这时如果我们回答$X$的样本空间是{$180, 175, 168$}，$Y$的样本空间是{$71, 65, 60$}就错了，时刻要记得多维随机变量的自变量是来自一个样本空间，所以正确的结果是：</p>
<center>$\Omega =$ {$"抽到张三", "抽到李四", "抽到王五"$}</center>

<h4 id="3-随机变量的大小写表示"><a href="#3-随机变量的大小写表示" class="headerlink" title="3. 随机变量的大小写表示"></a>3. 随机变量的大小写表示</h4><p>大写字母表示随机变量，是概率论里面特有的变量，如$X, Y, Z, …$，但这其实是随机变量的简化形式，实际完整的形式如上文提及应该是$X(\omega), Y(\omega), Z(\omega)$，当拓展到$N$维随机变量时，一般使用$X_1, X_2,…,X_n$。</p>
<p>小写的表示函数的自变量，与高等数学里面的函数自变量意义相同。 在概率论里可以代指随机变量的具体取值，例如：$X(\omega) = x$。</p>
<p>表示随机变量之间的关系时，应该用大写，表示随机变量具体的分布函数或者概率密度时，应该用小写。 </p>
<blockquote>
<p><strong>我的疑问——对于多维随机变量定义的理解还不是很清楚：</strong></p>
<p>第一种情况：例如上面表格的情况，我们做一次随机实验$E$，抽出一名同学，则根据人的两个属性/指标可以得到二位随机变量：$X$—身高(属性)，$Y$—体重(属性)，这种情况我明白是正确的。</p>
<p>第二种情况：再例如我们测量某一电路的电流，我们测量了$N$次(相当于做了$N$次实验)，那么$N$次实验的随机变量是否可以组合起来变成$N$维随机变量呢？也就是$(X_1, X_2,…,X_n)$是否可以算是$N$维随机变量吗？？？</p>
</blockquote>
<h4 id="参考链接：-2"><a href="#参考链接：-2" class="headerlink" title="参考链接："></a><strong>参考链接：</strong></h4><p>参考链接2.1：<a href="https://www.bilibili.com/video/BV1AE411u7Et">什么是随机变量？- 概率统计小迷哥 - 哔哩哔哩</a>  </p>
<p>参考链接2.2：<a href="https://www.bilibili.com/video/BV1KJ411C7Pa">随机变量究竟是什么 - 数学救火队长马丁 - 哔哩哔哩</a></p>
<p>参考链接2.3：<a href="https://www.bilibili.com/video/BV1rJ411C7rJ">多维随机变量究竟是什么 - 数学救火队长马丁 - 哔哩哔哩</a> </p>
<p>参考链接2.4：<a href="https://zhuanlan.zhihu.com/p/47113623">随机变量和随机过程的个人理解 - UPPER的文章 - 知乎</a></p>
<p>参考链接2.5：<a href="https://zhidao.baidu.com/question/501709573263510124.html">概率统计中，小x和大X有什么区别 - 百度知道</a> </p>
<p>参考链接2.6：<a href="https://www.zhihu.com/question/404719167/answer/1344528800">请问在概率论中的随机变量X，与统计学中总体随机变量X，极其样本X_1，X_2，X_3有怎么样的关系？ - 顾念一人的回答 - 知乎</a></p>
<h3 id="三、概率函数P-x-、概率分布函数F-x-、概率密度函数f-x"><a href="#三、概率函数P-x-、概率分布函数F-x-、概率密度函数f-x" class="headerlink" title="三、概率函数P(x)、概率分布函数F(x)、概率密度函数f(x)"></a>三、概率函数P(x)、概率分布函数F(x)、概率密度函数f(x)</h3><h4 id="1-写在前面"><a href="#1-写在前面" class="headerlink" title="1. 写在前面"></a>1. 写在前面</h4><p>进入主题前，先明确几个概念：<br><strong>离散型变量（或取值个数有限的变量）</strong>：取值可一一列举，且总数是确定的，如投骰子出现的点数（1点、2点、3点、4点、5点、6点）。<br><strong>连续型变量（或取值个数无限的变量）</strong>：取值无法一一列举，且总数是不确定的，如所有的自然数（0、1、2、3……）。</p>
<p><strong>离散型变量取某个值$x_i$的概率$P(x_i)$是个确定的值（虽然很多时候我们不知道这个值是多少），即$P(x_i)≠0$</strong>：例如，投一次骰子出现2点的概率是$P(2)=\dfrac{1}{6}$。</p>
<p><strong><font color="red">连续型变量取某个值$x_i$的概率$P(x_i)=0$</font></strong>：对于连续型变量而言，<strong><font color="blue">“取某个具体值的概率”的说法是无意义的</font>，因为取任何单个值的概率都等于0</strong>，只能说<strong>“取值落在某个区间内的概率”</strong>，或<strong>“取值落在某个值邻域内的概率”</strong>，即<strong>只能说$P(a&lt;xi≤b)$，而不能说$P(x_i)$。</strong>  为什么是这样？且看下例：</p>
<ul>
<li>例如，从所有自然数中任取一个数，问这个数等于5的概率是多少？从所有的自然数中取一个，当然是有可能取到5的，但是自然数有无穷多个，因此取到5的概率是$\dfrac{1}{\infty}$，也就是0。</li>
<li>又如扔飞镖，虽然是有可能落在靶心的，但其概率也是0（不考虑熟练程度等其他因素），因为靶盘上有无数个点，每个点的概率是一样的，因此落在某一个具体的点上的概率为$\dfrac{1}{\infty} = 0$。</li>
</ul>
<p>根据前面的例子可知：<strong><font color="red">在连续型变量中：概率为0的事件是有可能发生的，概率为1的事件不一定必然发生。</font> </strong></p>
<h4 id="2-概率分布和概率函数P-X"><a href="#2-概率分布和概率函数P-X" class="headerlink" title="2. 概率分布和概率函数P(X)"></a>2. 概率分布和概率函数P(X)</h4><p><strong>概率分布</strong>：给出了所有取值及其对应的概率（少一个也不行），<strong>只对离散型变量有意义</strong>。例如： </p>
<p><img src="https://pic.imgdb.cn/item/621457fd2ab3f51d913ee894.jpg" style="zoom:40%"></p>
<p><strong>概率函数</strong>：用函数形式给出每个取值发生的概率，<script type="math/tex">P(x)(x = x_1, x_2, x_3, \cdots)</script>，<strong>只对离散型变量有意义</strong>，实际上是对概率分布的数学描述。</p>
<p><strong>概率分布和概率函数只对离散型变量有意义，那如何描述连续型变量呢？</strong></p>
<p>答案就是<strong><font color="red">“概率分布函数F(x)”和“概率密度函数f(x)”</font></strong>， 当然这两者也是可以描述离散型变量的。</p>
<h4 id="3-概率分布函数F-X-与概率密度函数f-x"><a href="#3-概率分布函数F-X-与概率密度函数f-x" class="headerlink" title="3. 概率分布函数F(X)与概率密度函数f(x)"></a>3. 概率分布函数F(X)与概率密度函数f(x)</h4><p>1、<strong>概率分布函数$F(x)$：给出取值小于某个值的概率，是概率的累加形式</strong>，即：</p>
<script type="math/tex; mode=display">
F(x_i)=P(x<x_i)= \sum(P(x_1),P(x_2), \cdots ,P(x_i))</script><p>对于离散型变量是求和，对于连续型变量是求积分，见后图。</p>
<p>2、<strong>概率分布函数F(x)的性质</strong></p>
<ul>
<li>单调非减性：$\forall a&lt;b$，总有$F(a) \leq F(b)$；</li>
<li>$F(x)$是一个右连续函数；</li>
<li>有界性：<script type="math/tex">\forall x \in \rm R</script>，总有<script type="math/tex">0 \leq F(x) \leq 1</script>，且<script type="math/tex">\lim\limits_{x \to -\infty} F(x) = 0, \lim\limits_{x \to \infty} F(x) = 1</script>；</li>
</ul>
<p>3、<strong>概率分布函数$F(x)$的作用</strong></p>
<p> （1）<strong>给出$x$落在某区间$(a,b]$内的概率</strong>：$P(a&lt;x≤b)=F(b)-F(a)$</p>
<p>（2）<strong>根据$F(x)$的斜率判断“区间概率”$P(A&lt;x≤B)$的变化（实际上就是后面要说的概率密度函数$f(x)$）（特别注意：是判断“区间概率”，即$x$落在$(A,B]$中的概率，而不是$x$取某个确定值的概率，这是连续型变量和离散型变量的本质区别）</strong></p>
<p>   <strong>某区间$(A,B]$内，$F(x)$越倾斜，表示$x$落在该区间内的概率$P(A&lt;x≤B)$ 越大</strong>。如图中$(a,b]$区间内$F(x)$的斜率最大，如果将整个取值区间以$δ_x=b-a$的间隔等距分开，则$x$落在$(a,b]$内的概率最大。这是因为：</p>
<script type="math/tex; mode=display">
P(A<x≤B) )=F(B)-F(A)</script><p>所有区间中只有在$(a,b]$这个区间上（即$A=a$，$B=b$）$F(B)-F(A)$达到最大值，也就是图中竖向红色线段最长。</p>
<p><img src="https://pic.imgdb.cn/item/62148eb52ab3f51d91b44170.jpg" style="zoom:40%"></p>
<p>4、<strong>概率密度函数$f(x)$</strong></p>
<p>给出了变量落在某值$x_i$邻域内（或者某个区间内）的<strong><font color="red">概率变化快慢，概率密度函数的值不是概率，而是概率的变化率，概率密度函数下面的面积才是概率</font></strong>。   </p>
<p>定义1：若存在非负可积函数$f(x)$，使随机变量X取值于任一区间$(a,b]$的概率可表示成</p>
<script type="math/tex; mode=display">
P(a \leq X \leq b) = \int_a^b f(x)dx</script><p>则称X为连续型随机变量，$f(x)$为X的概率密度函数，简称概率密度或密度。</p>
<p> 5、概率分布函数和概率密度函数之间的关系</p>
<script type="math/tex; mode=display">
F(x) = \int_{-\infty}^{x}f(t)dt \\
f(x) = \frac{d}{dx}F(x)</script><blockquote>
<p>注意：概率密度函数$f(x)$在点a处取值，不是事件${X=a}$的概率。但是，该值越大，$X$在$a$点附近取值的概率越大。</p>
</blockquote>
<p><img src="https://pic.imgdb.cn/item/62148ee92ab3f51d91b4c173.jpg" style="zoom:50%"></p>
<p><strong>连续型变量的概率、概率分布函数、概率密度函数之间的关系（以正态分布为例）</strong></p>
<p>如下图：<strong>对于正态分布而言，$x$落在$u$附近的概率最大，而$F(x)$是概率的累加和，因此在$u$附近$F(x)$的递增变化最快，即$F(x)$曲线在$(u，F(u))$这一点的切线的斜率最大，这个斜率就等于$f(u)$。</strong>$x$落在$a$和$b$之间的概率为$F(b)-F(a)$（图中的红色小线段），而在概率密度曲线中则是$f(x)$与$ab$围成的面积$S$。如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/62148f2e2ab3f51d91b58251.jpg" style="zoom:60%"></p>
<h4 id="4-概率密度函数在某点-a-的值-f-a-的物理意义到底是什么？"><a href="#4-概率密度函数在某点-a-的值-f-a-的物理意义到底是什么？" class="headerlink" title="4. 概率密度函数在某点$a$的值$f(a)$的物理意义到底是什么？"></a>4. 概率密度函数在某点$a$的值$f(a)$的物理意义到底是什么？</h4><p><strong>我们知道$f(a)$表示，概率分布函数$F(x)$在$a$点的变化率(或导数)；其物理意义实际上就是$x$落在$a$点附近的无穷小邻域内的概率，但不是落在$a$点的概率（前已述及，连续变量单点概率=0），用数学语言描述就是：</strong></p>
<script type="math/tex; mode=display">
f(a) = F'(a) = \lim_{\delta x >0 且\delta x \to 0} P(a-\delta x < x \leq a+\delta x) ≠ P(a)</script><h4 id="参考链接：-3"><a href="#参考链接：-3" class="headerlink" title="参考链接："></a><strong>参考链接：</strong></h4><p>参考链接3.1：<a href="https://www.zhihu.com/question/25367654">概率分布就很好，为什么还要提出概率密度的概念？ - 知乎</a></p>
<p>参考链接3.2：<a href="https://zhuanlan.zhihu.com/p/413360980">概率基础：概率密度函数并不是概率 - coasxu的文章 - 知乎</a></p>
<p>参考链接3.3：<a href="https://www.zhihu.com/question/23237834">概率密度函数在某一点的值有什么意义？ - 知乎</a></p>
<p>参考链接3.4：<a href="https://www.zhihu.com/question/263467674">如何通俗的理解概率密度函数？ - 知乎</a></p>
<p>参考链接3.5：<a href="https://www.jianshu.com/p/0cfc3204af77">概率函数P(x)、概率分布函数F(x)、概率密度函数f(x)</a></p>
<p>参考链接3.6：<a href="https://zhuanlan.zhihu.com/p/119453101">一文读懂什么是联合概率，条件概率，贝叶斯 - 知乎</a></p>
<p>参考链接3.7：<a href="https://blog.csdn.net/SIGAI_CSDN/article/details/83586458?utm_source=app&amp;app_version=5.0.0">理解概率密度函数 - CSDN</a></p>
<h3 id="四、概率中的矩的理解"><a href="#四、概率中的矩的理解" class="headerlink" title="四、概率中的矩的理解"></a>四、概率中的矩的理解</h3><h4 id="参考链接：-4"><a href="#参考链接：-4" class="headerlink" title="参考链接："></a><strong>参考链接：</strong></h4><p>参考链接4.1：<a href="https://www.zhihu.com/question/19915565">统计学中「矩」这个概念是怎么引入的？它为什么被称为矩？它与物理意义上的矩有什么相同与不同？ - 知乎</a></p>
<p>参考链接4.2：<a href="https://www.zhihu.com/question/23236070">概率论中「矩」（moment）的实际含义是什么，高阶矩表示数据的哪些状态？ - 知乎</a></p>
<p>参考链接4.3：<a href="https://zhuanlan.zhihu.com/p/57802400">概率论中的“矩”是什么意思 - Mr.看海的文章 - 知乎</a></p>
<p>参考链接4.4：<a href="https://zhuanlan.zhihu.com/p/78067348">矩（moment）、鞅（martingale）、似然（Likelihood）的「奇葩翻译」是怎么想的 - 范函子Skyline的文章 - 知乎</a></p>
<h3 id="五、随机变量的特征函数"><a href="#五、随机变量的特征函数" class="headerlink" title="五、随机变量的特征函数"></a>五、随机变量的特征函数</h3><h4 id="参考链接：-5"><a href="#参考链接：-5" class="headerlink" title="参考链接："></a><strong>参考链接：</strong></h4><p>参考链接5.1：<a href="https://blog.csdn.net/ccnt_2012/article/details/109776736">如何理解统计中的特征函数？- CSDN</a></p>
<p>参考链接5.2：<a href="https://blog.csdn.net/JaneandDaria/article/details/83722395">随机变量的特征函数 - CSDN</a></p>
<p>参考链接5.3：<a href="https://blog.csdn.net/The_Time_Runner/article/details/90107979">特征函数解释了世界的哪个环节？- CSDN</a></p>
<p>参考链接5.4：<a href="https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0_(%E6%A6%82%E7%8E%87%E8%AE%BA">特征函数 (概率论) - 维基百科</a>) </p>
<p>参考链接5.5：<a href="https://www.zhihu.com/question/23686709">如何理解统计中的特征函数？ - 知乎</a></p>
<p>参考链接5.6：<a href="http://zhidao.baidu.com/question/922864273710991979?sharesource=weibo">随机函数的“特征函数”，是什么意思 - 百度知道</a></p>
<p>参考链接5.7：<a href="https://zhuanlan.zhihu.com/p/358618882">随机变量的特征函数及应用 - Sunsnow的文章 - 知乎</a></p>
<hr>
<h3 id="总参考链接："><a href="#总参考链接：" class="headerlink" title="总参考链接："></a>总参考链接：</h3><p>参考链接1：<a href="https://zhangzhenhu.github.io/blog/glm/source/index.html">广义线性模型 - 张振虎 - 个人博客</a></p>
<p>参考链接2：<a href="https://codeantenna.com/a/T0ryFFnuIZ">概率论的基本概念 - CodeAntenna</a></p>
<p>参考链接3：<a href="http://www.360doc.com/content/18/0118/23/33209086_723218111.shtml">零基础概率论入门:基本概念</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>数学基础</category>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率基础</tag>
        <tag>概念辨析</tag>
      </tags>
  </entry>
  <entry>
    <title>编程软件基础知识——杂记</title>
    <url>/2022/04/26/Basic-knowledge-of-programming-software/</url>
    <content><![CDATA[<h3 id="1-电脑知识"><a href="#1-电脑知识" class="headerlink" title="1 电脑知识"></a>1 电脑知识</h3><h3 id="2-编程语言——编译型和解释型"><a href="#2-编程语言——编译型和解释型" class="headerlink" title="2 编程语言——编译型和解释型"></a>2 编程语言——编译型和解释型</h3><h4 id="2-1-编译型语言"><a href="#2-1-编译型语言" class="headerlink" title="2.1 编译型语言"></a>2.1 编译型语言</h4><p>C和C++这两种语言是编译型语言，编译型语言的特点是执行速度快，缺点是什么呢？编译型语言需要编译器处理，主要工作流程如下：</p>
<p><strong>源代码 (source code) → 预处理器 (preprocessor) → 编译器 (compiler) → 目标代码 (object code) → 链接器 (Linker) → 可执行程序 (executables)</strong></p>
<p>编译器调用预处理器进行相关处理，将源代码进行优化转换（包括清除注释、宏定义、包含文件和条件编译），然后，通过将经过预处理的源代码编译成目标代码（二进制机器语言），再通过调用链接器外加库文件（例如操作系统提供的API），从而形成可执行程序，让机器能够执行。在这个工作流程中，目标代码要和机器的CPU架构相匹配，库文件要和操作系统相匹配。如果想在不同CPU的机器或者系统上运行C语言的源代码，就需要针对不同的CPU架构和操作系统进行编译，这样才能够在机器上运行程序。所以，编译型语言的缺点我们就看到了，它不适合跨平台。而且，到这里大家应该能知道，为什么CPU一样，但是exe程序只能Windows中运行，而不能在Mac中运行了。 </p>
<p><img src="https://pic.imgdb.cn/item/6267dfd1239250f7c578ce42.png"></p>
<h4 id="2-2-解释型语言"><a href="#2-2-解释型语言" class="headerlink" title="2.2 解释型语言"></a>2.2 解释型语言</h4><p>解释型语言源代码无需预先编译成可执行程序，在程序执行时，解释器读取一句源代码之后，先进行词法分析和语法分析，再将源代码转换为解释器能够执行的中间代码（字节码），最后，由解释器将中间代码解释为可执行的机器指令。所以，编译型语言的可执行程序产生的是直接执行机器指令，而解释型语言的每一句源代码都要经过解释器解释为可以执行的机器指令，相比之下解释型语言的执行效率会低一些。 </p>
<p><img src="https://pic.imgdb.cn/item/6267e0ba239250f7c57ac7d8.png"></p>
<p>例如：Python程序运行时，先将源代码完整的进行转换，编译成更有效率的字节码，保存成后缀为“.pyc”的字节码文件，然后，翻译器再通过这个文件一句一句的翻译为机器语言去执行。 </p>
<p><img src="https://pic.imgdb.cn/item/6267e0e4239250f7c57b2434.png"></p>
<h5 id="2-2-1-JIT即时编译器（Just-In-Time-Compiler）"><a href="#2-2-1-JIT即时编译器（Just-In-Time-Compiler）" class="headerlink" title="2.2.1 JIT即时编译器（Just-In-Time Compiler）"></a>2.2.1 JIT即时编译器（Just-In-Time Compiler）</h5><p>无论是使用解释器进行解释执行，还是使用编译器进行编译后执行，最终源代码都需要被转换为对应平台的本地机器指令。那么，一些重复出现的代码，就可以将其编译为本地机器指令，重复使用，从而提高效率。这些重复出现的代码包括多次调用的方法和多次执行的循环体。 </p>
<p>JIT即时编译器比较典型的例子是在JVM（Java虚拟机）中。Java程序最初是通过解释器进行解释执行的，当Java虚拟机发现某个方法或代码块运行特别频繁的时候，就会认为这是“热点代码”（Hot Spot Code)。JIT即时编译器会将这些“热点代码”编译成与本地机器相关的机器指令，进行各个层次的优化。 </p>
<p><img src="https://pic.imgdb.cn/item/6267e128239250f7c57bc2ab.png"></p>
<h3 id="3-操作系统、环境变量、服务器"><a href="#3-操作系统、环境变量、服务器" class="headerlink" title="3 操作系统、环境变量、服务器"></a>3 操作系统、环境变量、服务器</h3><h4 id="3-1-为什么你需要操作系统"><a href="#3-1-为什么你需要操作系统" class="headerlink" title="3.1 为什么你需要操作系统"></a>3.1 为什么你需要操作系统</h4><p>假设你是一个程序员, 你写了一个C程序，如下所示： </p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> b = <span class="number">2</span>; </span><br><span class="line">    <span class="keyword">int</span> c = a + b;</span><br><span class="line">    <span class="built_in">printf</span>(%d, c);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>你的程序经过编译大概变成了这样的CPU指令 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 初始化a和b , mov指令把前一个操作数的值赋值给后一个操作数</span><br><span class="line">Mov 1, a</span><br><span class="line">Mov 2, b</span><br><span class="line"># %开头的是CPU寄存器, 你可能还记得它是最快最小的存储设备, 我们要把a和b从内存里拿出来, 放进寄存器里</span><br><span class="line"># 接下来Add指令才能工作 </span><br><span class="line">Mov a, %eax</span><br><span class="line">Mov b, %ecx</span><br><span class="line"># 加两个寄存器的值, 和写入后一个寄存器中, 在这里是写入eax寄存器</span><br><span class="line">Add %ecx, %eax</span><br><span class="line"># 把这个值写回内存</span><br><span class="line">Mov %eax, c</span><br><span class="line">#打印, 即把内存里的值送到显示器,这里涉及一系列系统调用，现在先不管</span><br></pre></td></tr></table></figure>
<p>指令送进内存，CPU从内存里读取指令(读取指令和执行指令是CPU硬件实现的)，执行访问内存，执行计算。 一起看起来都很美好，看起来我们不需要一个操作系统。</p>
<p>没错！恭喜你发现一个惊天大秘密！我们就是不需要操作系统！</p>
<p>如果我们的程序就一直这么简单的运行，那可能的确不需要。可这不符合我们对现代计算机的认知。浏览器和音乐播放器是两个不同的程序，为什么你能一边刷网页一边听歌？一定是这两个程序一起运行了，可是按说你只有一个CPU， 一套内存，只能跑一个程序，为什么你能同时运行好几个程序呢？是的, 这就是操作系统最基本的功能，<strong>操作系统负责管理CPU和内存，让每一个程序都觉得自己是独立运行的。</strong></p>
<p><img src="https://pic.imgdb.cn/item/6267e209239250f7c57dba4d.png" style="zoom:70%"></p>
<h4 id="3-2-环境变量"><a href="#3-2-环境变量" class="headerlink" title="3.2 环境变量"></a>3.2 环境变量</h4><p>环境变量的作用就是指明操作系统的重要具录在哪里 </p>
<p>参考链接：<a href="https://www.bilibili.com/video/BV1w741147G9?from=search&amp;seid=17903121616999271166&amp;spm_id_from=333.337.0.0">『教程』什么是环境变量 哔哩哔哩 bilibili</a>  </p>
<h4 id="3-3-服务器"><a href="#3-3-服务器" class="headerlink" title="3.3 服务器"></a>3.3 服务器</h4><h5 id="3-3-1-什么是服务器"><a href="#3-3-1-什么是服务器" class="headerlink" title="3.3.1 什么是服务器"></a>3.3.1 什么是服务器</h5><div class="bvideo">
    <a href="//www.bilibili.com/video/BV1bk4y1m79S" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i0.hdslb.com/bfs/archive/0a16f645b008f3e5654185407ef3b135d778dcc7.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:07:03</span>
            </div>
            <div class="bvideo-info">
                <p class="title">【白话科普】服务器是什么 ｜能做什么  | 和电脑有什么区别 | 什么是云服务器 | 网站上线系列分享</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>5.0万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>89</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">好奇代码的三木</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p>首先可以很明确地说服务器也是电脑，<strong>服务器是为电脑提供服务的电脑</strong>，既然是电脑，那么它也一样是由CPU，主板，内存条，硬盘，机箱，电源等硬件组成。 </p>
<blockquote>
<p>还是拿人类来举例子，如果说电脑是人类的话，那么，服务器就是人类中的医生或者公务员或者其他不同的角色，也就是说，都是人类，只是不同人通过学习一些专业技能然后赋予了不同的角色。</p>
<p>现在理解一些了么？也就是说一台电脑，如果它安装某些特殊的软件用于某种专业的用途，那么它就可以叫做是某种服务器。这是软件层面的，硬件层面的后面再说。  </p>
</blockquote>
<p>比如最常见的网站服务器，当你在浏览器里敲入<a href="https://link.zhihu.com/?target=http%3A//www.baidu.com">http://www.baidu.com</a>的时候，最终你的电脑是访问到的另外一台电脑，这台电脑会被安装网站服务的软件，并且会有网站方面的网站程序，最终起作用的是这些软件跟程序，这台电脑啥事情都不干，专门用来响应大家的访问请求，于是乎，他就叫做网站服务器，也叫web服务器，现实中并不是一台电脑，而是很多很多台电脑专门干这个事情，因为全球这么大的用户量，如果都访问一台电脑，不光是网络带宽问题，就这一台电脑早就累死掉了，这背后有很深层次的技术问题。 </p>
<p>再说两种服务器，一种是文件服务器，一种是数据库服务器，直观一些的你可以把百度网盘理解为文件服务器，虽然背后其实更加的综合，但是这种专门用来做文件存储的电脑就可以称为文件服务器了。 </p>
<p>跟另外两种服务器一样，数据库服务器也是专门只干一件事情，就是存储数据，数据库服务器一般不对外，它再为网站服务器或者其他的程序来提供服务，而且它会安装专门的数据库管理软件，来进行数据管理，这些数据的组织形式有点像excel表格里面的数据，其他电脑对他进行操作的时候，就好像跟他吼一声： “hey，帮我查查那个谁的什么什么信息。”，数据库服务器的软件就会把需要的数据给它返回回去。 </p>
<p>所以当你访问百度搜素某个关键词的时候，首先你访问到的是百度的某台网站服务器，网站服务器只是负责请求的处理跟结果的反馈，具体的数据它得更数据库服务器要去，数据库服务器把数据反馈给网站服务器，网站服务器再把结果组织输出成为你看到的模样。 </p>
<h5 id="3-3-2-为什么要为深度学习配置专门的服务器"><a href="#3-3-2-为什么要为深度学习配置专门的服务器" class="headerlink" title="3.3.2 为什么要为深度学习配置专门的服务器"></a>3.3.2 为什么要为深度学习配置专门的服务器</h5><p>首先解释一下深度学习为什么需要显卡计算，GPU是为大规模的并行运算而优化，GPU上则更多的是运算单元，GPU往往拥有更大宽带的显存，因此在大吞吐量的应用中也会有很好的性能，所以显卡选择最重要。专门的深度学习服务器可以方便实现实验室计算资源共享，多用户可以在个人电脑编写程序，远程访问到深度学习服务器上排队使用计算资源，减少购买设备的开支。因此准备基于CUDA计算是NVIDIA开发的GPU并行计算环境，英伟达产品系列是个不错的选择，或者选择英伟达国内代理商。</p>
<ul>
<li>深度学习需要大量的并行计算资源，而且动辄计算几天甚至数周，而显卡（GPU)恰好适合这种工作，提供几十上百倍的加速，性能强劲的GPU能在几个小时内完成原本CPU需要数月完成的任务，所以目前深度学习乃至于机器学习领域已经全面转向GPU架构，使用GPU完成训练任务。</li>
<li>如今即使使用GPU的深度学习任务也要持续数天乃至数月（取决于数据规模和深度学习网络模型），需要使用单独的设备保障保证训练任务能够7x24小时长期稳定运行。 </li>
<li>独立的深度学习工作站(服务器)可以方便实现实验室计算资源共享，多用户可以在个人电脑写程序，远程访问到深度学习服务器上排队使用计算资源，减少购买设备的开支且避免了在本地计算机配置复杂的软件环境。 </li>
</ul>
<h5 id="3-3-3-如何免费使用Google-colab服务器训练网络"><a href="#3-3-3-如何免费使用Google-colab服务器训练网络" class="headerlink" title="3.3.3 如何免费使用Google colab服务器训练网络"></a>3.3.3 如何免费使用Google colab服务器训练网络</h5><div class="bvideo">
    <a href="//www.bilibili.com/video/BV13K4y1P7dx" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i1.hdslb.com/bfs/archive/84653d2d5e398cda5317954a991dc2d0d2bd0149.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:10:09</span>
            </div>
            <div class="bvideo-info">
                <p class="title">Google colab使用分享/小白上手Google colab</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>2.7万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>23</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">一只乌龟大王</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<h3 id="4-Pycharm的基本使用"><a href="#4-Pycharm的基本使用" class="headerlink" title="4 Pycharm的基本使用"></a>4 Pycharm的基本使用</h3><h4 id="4-1-Python安装前必须知道的三个概念"><a href="#4-1-Python安装前必须知道的三个概念" class="headerlink" title="4.1 Python安装前必须知道的三个概念"></a>4.1 Python安装前必须知道的三个概念</h4><h5 id="4-1-1-Python解释器"><a href="#4-1-1-Python解释器" class="headerlink" title="4.1.1 Python解释器"></a>4.1.1 Python解释器</h5><p>Python解释器是将Python源码高级语言解析(<strong>翻译</strong>)为二进制机器语言的工具。通常说安装Python就是指安装Python解释器。目前最新的Python解释器版本是Python 3.8x。另外，Python 2.x和Python 3.x<strong>不兼容</strong>。 </p>
<p><img src="https://pic.imgdb.cn/item/6267e304239250f7c57fcec3.png" style="zoom:60%"></p>
<h5 id="4-1-2-Python编辑器"><a href="#4-1-2-Python编辑器" class="headerlink" title="4.1.2 Python编辑器"></a>4.1.2 Python编辑器</h5><p>正如在电脑上编辑文档需要用Word、处理数据需要用Excel、做演示文稿需要用PPT、修图需要用PS一样，<strong>编写代码也需要特定的工具，而这个用来编写代码的工具就叫做编辑器</strong>。Python的编辑器有很多，有Python解释器自带的<strong>IDLE</strong>、基于iPython的<strong>Jupyter Notebook</strong>、也有如<strong>PyCharm</strong>、<strong>Spyder</strong>、<strong>WinglDE</strong>等主要针对Python代码编辑的编辑器；还有很多编辑器，如Sublime Text、Vim、<strong>VSCode等适合各种编程语言的编辑器</strong>。 </p>
<h5 id="4-1-3-包管理工具"><a href="#4-1-3-包管理工具" class="headerlink" title="4.1.3 包管理工具"></a>4.1.3 包管理工具</h5><p>Python最大的优点之一就在于其丰富的库，<strong>pip ( package installer for Python)</strong>就是库管理工具，通过pip就可以安装、卸载、更新众多的库。 </p>
<p>Python3.4以后的版本的解释器自带pip功能 </p>
<h5 id="4-1-4-小结"><a href="#4-1-4-小结" class="headerlink" title="4.1.4 小结"></a>4.1.4 小结</h5><ul>
<li>一般说的安装python就是指安装python解释器，因此python解释器必须安装。</li>
<li>出于编写代码效率的考虑，有必要安装专业的代码编辑工具，最常用的包括Python解释器自带的IDLE、Anaconda中的Jupyter Notebook以及专门针对Python的PyCharm。</li>
<li>Python 3.4以后版本的解释器自带pip工具，因此不用自行安装。 </li>
</ul>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>编程与深度学习</category>
        <category>编程语言学习</category>
      </categories>
      <tags>
        <tag>编程软件基础</tag>
      </tags>
  </entry>
  <entry>
    <title>性格与为人处事</title>
    <url>/2022/05/06/Character-and-conduct/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="792c533be91d17f35d9da6a79d77d3f02a9b918a406570514f34ccbe69fdbe96">270cb3bbed971930ec4ebcba9f8cc1380ece294f8f2a7e7ee3a18ece7aac93473c0ee2b0c4c1f4481ca275ad6eb739682c3ac37a8376afcaac5b162a9ed31c612bc0397ee6e502e6743dbe0e27c9caaf30e999230722fa2ff71a7cfd86f5c0a80b93cb30eb847792a9c9dadfdc253366b6f3e11b70997ad1021e723690b3268f39f93371de53b37b329541f1ab5a15f8bd9634c92f9b4afc0633a1b5994c760b3576959970ccbcdc42e3745c53428eeb0ce17c72c00c89bdf8617ec12a6025571dce63ba5da60a555a749ffa7c3d915083f890c73722b0833130acba3c635295dab301248f25d7575ca523db53569a730641751a887077ad4bf351ea77b67c71b2b70c919d1801b7a9924ae38b60f573</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>生活休闲</category>
        <category>心性与改善</category>
      </categories>
      <tags>
        <tag>性格</tag>
        <tag>做事</tag>
      </tags>
  </entry>
  <entry>
    <title>压缩感知和稀疏恢复基础</title>
    <url>/2023/08/08/Compressed-Perception-Foundation/</url>
    <content><![CDATA[<h4 id="1-压缩感知基础"><a href="#1-压缩感知基础" class="headerlink" title="1. 压缩感知基础"></a>1. 压缩感知基础</h4><p>压缩感知(Compressed Sensing, CS)是由陶哲轩等人提出的一种用于信息获取的突破性理论。该理论指出：<strong>对于稀疏信号或可压缩信号，可采用低于奈奎斯特采样频率的方式对数据采样，降低数据传输量，并能以高概率精确地重建该信号</strong>。其原始文献为：<em>D. L. Donoh, Compressed sensing, IEEE Transactions on Information Theory, 2006, 52(4): 1289-1306.</em>  </p>
<h5 id="1-1-压缩感知的流程"><a href="#1-1-压缩感知的流程" class="headerlink" title="1.1 压缩感知的流程"></a>1.1 压缩感知的流程</h5><p>压缩感知主要分为三步：</p>
<ul>
<li>1、信号的稀疏化表示，实现信号的压缩；</li>
<li>2、观测矩阵的设计，得到观测数值；</li>
<li>3、信号的重构，得到恢复信号。</li>
</ul>
<h5 id="1-2-信号稀疏化表示"><a href="#1-2-信号稀疏化表示" class="headerlink" title="1.2 信号稀疏化表示"></a>1.2 信号稀疏化表示</h5><p>稀疏信号定义：设一维离散信号$\boldsymbol x$，长度为$N$，可看作为$N$维空间$N\times1$的列向量，若此列向量中含有$K$个不为0元素，且$K \ll N$，则称该信号$\boldsymbol x$是$K$-稀疏信号，具有稀疏性。$K$称为信号$\boldsymbol x$的稀疏度。<font color="red">牢记，信号具有稀疏性（在本域或其他变换域皆可）是可以使用压缩感知的前提</font>！</p>
<p>如果信号稀疏，则信号$\boldsymbol x$可以表示为：</p>
<script type="math/tex; mode=display">
\boldsymbol x = \sum_{k=0}^{N-1} \boldsymbol \psi_k s_k = \boldsymbol {\Psi s}</script><p>其中，$\boldsymbol s$为稀疏系数，也是我们使用算法恢复出来的重建新号，其尺寸与信号$\boldsymbol x$相同；$\boldsymbol \Psi$是我们所熟知的稀疏矩阵，其尺寸为$N\times N$，这就是信号的稀疏过程。</p>
<h5 id="1-3-观测矩阵设计"><a href="#1-3-观测矩阵设计" class="headerlink" title="1.3 观测矩阵设计"></a>1.3 观测矩阵设计</h5><p>观测矩阵的作用主要是使人们可以看到由仪器所获得的观测值$ \boldsymbol y$，其中，你想要看到多少，这就是我们所熟知的采样率了。对观测矩阵的要求是，可从观测值$\boldsymbol y$中高精度的重构出长度为$N$的原始信号$\boldsymbol s$，或者重构出在稀疏矩阵下的等价信号。其具体表达式为：</p>
<script type="math/tex; mode=display">
\boldsymbol y = \boldsymbol{\Phi x}</script><p>式中，$\boldsymbol \Phi$为观测矩阵(也称测量矩阵)，其维度为$M\times N$，$\boldsymbol x$为原始信号，其维度为$N\times 1$；$\boldsymbol y$为观测信号，其维度为$M\times 1$。从原始信号中可以观察到多少信号，就是由测量矩阵决定的，因此其会涉及到采样率的概念。通常的采样率概念定义为测量矩阵的行数/列数，即$\dfrac{M}{N}$。</p>
<p>将这个式子带入1.2节中的式子，则可以得到大家经常看到的式子了：</p>
<script type="math/tex; mode=display">
\boldsymbol y = \boldsymbol{\Phi x} =  \boldsymbol{\Phi \Psi s} = \boldsymbol{\Theta s}</script><p>其中，$\boldsymbol {\Theta=\Phi \Psi}$为$M\times N$阶矩阵，又称为传感矩阵。</p>
<p>在此步中，最重要的是构造出合适的观测矩阵，使得可通过仪器采集到从稀疏信号中获得的观测值，并在反向求解时，由观测值重构稀疏信号，即构造出有解的$M \times K$线性方程组。牢记，在进行上述过程时，<strong>一定要对观测矩阵进行RIP性质分析</strong>，说白了就是观测矩阵和稀疏矩阵的相关性很小很小很小！</p>
<h5 id="1-4-信号重构"><a href="#1-4-信号重构" class="headerlink" title="1.4 信号重构"></a>1.4 信号重构</h5><p>信号重构是对$\boldsymbol{y =\Phi x = \Phi \Psi s = \Theta s}$式求最优解，是压缩感知理论中的求解问题，如何得到最优解是研究的主要内容，也是最后一个关键的步骤。压缩感知目前的重构算法主要分为两类：<strong>贪婪算法与凸优化算法</strong>。</p>
<ul>
<li>贪婪算法主要是选择合适的列向量经过多次的逐步加和以实现信号的逼近，其中匹配追踪算法、正交匹配追踪等算法均属于贪婪算法；</li>
<li>凸优化算法则是将范数的求解置于范数进行线性规划求解，此算法包括基追踪算法、梯度投影算法等。</li>
</ul>
<p>最后总结一下：<strong>对信号进行压缩感知恢复重建时，信号要映射到稀疏域，即信号在一定的变换域中是稀疏的。在进行测量矩阵压缩采样时，测量矩阵需满足RIP条件，或者应满足观测矩阵与稀疏基不相关的条件。在两个条件的基础上，可运用压缩感知算法进行恢复重建。</strong> </p>
<h4 id="2-压缩感知的常用稀疏基"><a href="#2-压缩感知的常用稀疏基" class="headerlink" title="2 压缩感知的常用稀疏基"></a>2 压缩感知的常用稀疏基</h4><h5 id="2-1-稀疏基的概念"><a href="#2-1-稀疏基的概念" class="headerlink" title="2.1 稀疏基的概念"></a>2.1 稀疏基的概念</h5><p>稀疏基是将可以信号进行稀疏表示的一个矩阵。设一维离散信号$\boldsymbol x$，长度为$N$，可看作为$N$维空间$N \times1$的列向量，若此列向量中含有$K$个不为0元素，且$K \ll N$，则称该信号$\boldsymbol x$是$K$-稀疏信号，具有稀疏性。</p>
<p>如果信号稀疏，则信号$\boldsymbol x$可以表示为：</p>
<script type="math/tex; mode=display">
\boldsymbol x = \boldsymbol{\Psi s}</script><p>其中，$\boldsymbol \Psi$就是稀疏基(稀疏矩阵)。 </p>
<h5 id="2-2-一些常用的稀疏基"><a href="#2-2-一些常用的稀疏基" class="headerlink" title="2.2 一些常用的稀疏基"></a>2.2 一些常用的稀疏基</h5><ul>
<li>DFT稀疏基</li>
<li>DCT(离散余弦)稀疏基</li>
<li>DWT稀疏基</li>
<li>0-1随机稀疏基</li>
</ul>
<h4 id="3-贪婪算法"><a href="#3-贪婪算法" class="headerlink" title="3 贪婪算法"></a>3 贪婪算法</h4><p>压缩感知第三步是进行信号的重构，需要用到恢复重构算法。前面的文章提到过，压缩感知的恢复算法主要分为贪婪算法和凸优化算法两种，这里主要介绍贪婪算法的两种基础算法：MP算法和OMP算法 </p>
<h5 id="3-1-匹配追踪算法-MP"><a href="#3-1-匹配追踪算法-MP" class="headerlink" title="3.1 匹配追踪算法(MP)"></a>3.1 匹配追踪算法(MP)</h5><p>算法假定输入信号与字典库中的原子在结构上具有一定的相关性，这种相关性通过信号与原子库中原子的内积表示，即内积越大，表示信号与字典库中的这个原子的相关性越大，因此可以使用这个原子来近似表示这个信号。</p>
<p>当然这种表示会有误差，将表示误差称为信号残差，用原信号减去这个原子，得到残差，再通过计算相关性的方式从字典库中选出一个原子表示这个残差。迭代进行上述步骤，随着迭代次数的增加，信号残差将越来越小，当满足停止条件时终止迭代，得到一组原子，及残差，将这组原子进行线性组合就能重构输入信号。</p>
<h5 id="3-2-正交匹配追踪算法-OMP"><a href="#3-2-正交匹配追踪算法-OMP" class="headerlink" title="3.2 正交匹配追踪算法(OMP)"></a>3.2 正交匹配追踪算法(OMP)</h5><p>正交匹配追踪算法迭代的基本思想就是每次迭代过程中从全息矩阵$\boldsymbol T$中选出与测量信号$\boldsymbol s$相关度(内积)最大的那一列，然后从$\boldsymbol T$中去掉该列并加入到扩充矩阵$\boldsymbol {\mathrm{Aug}}_t$中，然后利用最小二乘法原理求出使残差$\boldsymbol r_n =\boldsymbol s-\boldsymbol {\mathrm{Aug}}_t*\boldsymbol {\mathrm{Aug}}_y$最小的一个估计$\boldsymbol {\mathrm{Aug}}_y$，然后不断的从$\boldsymbol T$中减去相关列重复以上过程，直到达到迭代次数结束。</p>
<h4 id="4-凸优化算法"><a href="#4-凸优化算法" class="headerlink" title="4 凸优化算法"></a>4 凸优化算法</h4><p>除了贪婪算法以外，压缩感知重构算法的另一大类就是凸优化算法，这类方法通过将非凸问题转化为凸问题求解找到信号的逼近。</p>
<h5 id="4-1-基追踪算法（BP）"><a href="#4-1-基追踪算法（BP）" class="headerlink" title="4.1 基追踪算法（BP）"></a>4.1 基追踪算法（BP）</h5><p>该算法全称为Basis Pursuit，其提出使用$l_1$范数替代$l_0$范数来解决最优化问题，以便使用线性规划方法来求解。即将求解 $\min \limits_{\boldsymbol \alpha} ||\boldsymbol \alpha||_{l_{0}}$ 的问题转化为求解 $\min\limits_{\boldsymbol \alpha} ||\boldsymbol \alpha||_{l_{1}}$ 的问题，其中$\boldsymbol \alpha$是需要构建出来的原始信号(重构信号)。</p>
<p>参考链接4.1.1：<a href="https://www.zhihu.com/question/290679001/answer/954062882">如何理解“L1范数是L0范数的最优凸近似”？ - 牛狮的回答 - 知乎</a><br>参考链接4.1.2：<a href="https://zhuanlan.zhihu.com/p/390844225">Basis Pursuit, LASSO, TV三种压缩感知优化求解算法 - SINK IN的文章 - 知乎</a><br>参考链接4.1.3：<a href="https://jz.docin.com/p-2310356158.html">压缩感知中l1正则化最小二乘问题的算法研究 - 豆丁建筑</a> </p>
<h4 id="4-2-梯度投影法"><a href="#4-2-梯度投影法" class="headerlink" title="4.2 梯度投影法"></a>4.2 梯度投影法</h4><p>梯度投影法(gradient projection method)利用梯度的投影技巧求约束非线性规划问题最优解的一种方法，求带线性约束的非线性规划问题更为有效。它是从一个基本可行解开始，由约束条件确定出凸约束集边界上梯度的投影，以便求出下次的搜索方向和步长。每次搜索后，都要进行检验，直到满足精度要求为止。具体原理可参考<a href="https://baike.baidu.com/item/%E6%A2%AF%E5%BA%A6%E6%8A%95%E5%BD%B1%E6%B3%95/19134105?fr=aladdin">梯度投影法 - 百度百科</a>。</p>
<p>本文转自：<br>版权声明：本文为CSDN博主「爱学习的一一」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/weixin_42105848/article/details/124280903">https://blog.csdn.net/weixin_42105848/article/details/124280903</a></p>
<p>参考链接1：<a href="https://zhuanlan.zhihu.com/p/22445302">形象易懂讲解算法II——压缩感知 - 咚懂咚懂咚的文章 - 知乎</a>：压缩感知的简要概述。<br>参考链接2：<a href="https://www.zhihu.com/question/28552876">如何理解压缩感知(compressive sensing)？ - 知乎</a>：压缩感知的简要概述。<br>参考链接3：<a href="https://blog.csdn.net/FDS99999/article/details/129784658">压缩感知基本理论-飞大圣的博客-CSDN</a>：压缩感知的简要概述。<br>参考链接4：<a href="https://www.cnblogs.com/AndyJee/category/579870.html">随笔分类 - 压缩感知compressive sensing-AndyJee的博客-博客园</a>：<strong>强推！！！</strong>博客园博主AndyJee关于压缩感知的31篇文章。<br>参考链接5：<a href="https://www.docin.com/p-777226899.html">Bessel序列与框架的若干刻画 - 豆丁网 (docin.com)</a><br>参考链接6：<a href="https://zhuanlan.zhihu.com/p/507765573">最优化理论——线性规划的标准形式 - 不喝牛奶的文章 - 知乎</a>、<a href="https://zhuanlan.zhihu.com/p/375526356">优化理论——线性规划 - 星空爱好者的文章 - 知乎</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>信号处理与雷达</category>
        <category>信号与系统</category>
      </categories>
      <tags>
        <tag>数字信号</tag>
        <tag>压缩感知</tag>
        <tag>稀疏恢复</tag>
      </tags>
  </entry>
  <entry>
    <title>文献数据库基础知识</title>
    <url>/2022/06/26/Database-knowledge/</url>
    <content><![CDATA[<h3 id="1-国际知名的权威学术出版社-会议组织者"><a href="#1-国际知名的权威学术出版社-会议组织者" class="headerlink" title="1 国际知名的权威学术出版社/会议组织者"></a>1 国际知名的权威学术出版社/会议组织者</h3><h4 id="1-1-IEEE-Institute-of-Electrical-and-Electronics-Engineering"><a href="#1-1-IEEE-Institute-of-Electrical-and-Electronics-Engineering" class="headerlink" title="1.1 IEEE(Institute of Electrical and Electronics Engineering)"></a>1.1 IEEE(Institute of Electrical and Electronics Engineering)</h4><ul>
<li>简介：IEEE是出版社和会议组织者，既组织学术会议，也出版学术期刊，IEEE出版社有自己出版的文献数据库，而网站IEEE Xplore就是查询该出版社文献数据库的入口。对应的(全文)文献数据库(查文献网址)：<a href="https://ieeexplore.ieee.org/Xplore/home.jsp">IEEE Xplore</a>。<h4 id="1-2-Springer-Springer-Verlag"><a href="#1-2-Springer-Springer-Verlag" class="headerlink" title="1.2 Springer(Springer-Verlag)"></a>1.2 Springer(Springer-Verlag)</h4></li>
<li>简介：德国斯普林格(Springer-Verlag)通过SpringerLink系统提供其学术期刊及电子图书的在线服务，该数据库包括了各类期刊、丛书、图书、参考工具书以及回溯文档。    对应的(全文)文献数据库(查文献网址)：<a href="https://link.springer.com/">SpringerLink</a>。<h4 id="1-3-Elsevier"><a href="#1-3-Elsevier" class="headerlink" title="1.3 Elsevier"></a>1.3 Elsevier</h4></li>
<li>简介：作为世界上最大的医学与其他科学文献出版商，爱思唯尔旗下出版2500余种期刊，包括具有高影响因子的《细胞》(《Cell》)和《柳叶刀》(《The Lancet》)这两大国际顶尖学术期刊。每年，爱思唯尔公司出版的期刊共发表约35万篇论文，同时爱思唯尔还提供世界最大的摘要和引文数据库Scopus。对应的(全文)文献数据库(查文献网址)：<a href="https://www.elsevier.com/solutions/sciencedirect">Elsevier</a>。<h4 id="1-4-Taylor-amp-Francis"><a href="#1-4-Taylor-amp-Francis" class="headerlink" title="1.4 Taylor &amp; Francis"></a>1.4 Taylor &amp; Francis</h4></li>
<li>简介：以出版科技书籍与参考书著称，在人类工程学、地理信息系统、建筑、土木工程、物理和生物技术等领域尤为突出。对应的(全文)文献数据库(查文献网址)：<a href="https://www.tandfonline.com/">Tandfonline</a>。</li>
</ul>
<h3 id="2-文摘-索引数据库"><a href="#2-文摘-索引数据库" class="headerlink" title="2 文摘/索引数据库"></a>2 文摘/索引数据库</h3><p>世界著名的三大期刊文献检索工具，SCI、EI、CPCI-S，它们也是国际公认的进行科学统计与科学评价的主要检索工具，其中 SCI 最为重要。<br>上面介绍了出版社和它们的文献数据库，现在说的 SCI、EI、CPCI-S 也是数据库，它们是引文数据库。文献数据库有该出版社的论文的全文，而引文数据库只有收录的文献的一些公开信息(题目、作者、摘要、参考文献等)。你在引文数据库只能找到论文的一些公开信息，想下载全文还得到出版社的文献数据库。<br><strong><font color="blue">引文数据库有点儿像豆瓣，我们能在豆瓣上看到某电影的相关信息，但要看电影还得跳到具体的视频网站(文献数据库)。</font></strong></p>
<blockquote>
<p>各出版社有了自己的文献数据库，为什么还要有引文数据库呢？<br>简单理解，这些出版社的文献的数据库是相互独立的，这时候用一种方法把它们整合到一起，在一个数据库就能搜到某一方向有哪些文献（各个出版社的）。但不是所有的出版社的都收录，不是每篇文献都有一样的权重，怎么整合需要采用一定的规则。比如SCI通过论文的被引用频次等的统计，对学术期刊和科研成果进行多方位的评价研究，从而评判一个国家或地区、科研单位、个人的科研产出绩效，来反映其在国际上的学术水平。被SCI收录的论文就称为SCI论文，被EI收录的论文称为EI论文。SCI一般会去收录高质量的期刊的文章，高质量的会议的文章，EI 会收录的更多一些，因为水平低一点的文章他也收录了。</p>
<h4 id="2-1-SCI-Science-Citation-Index"><a href="#2-1-SCI-Science-Citation-Index" class="headerlink" title="2.1 SCI(Science Citation Index)"></a>2.1 SCI(Science Citation Index)</h4><ul>
<li>简介：由美国科学信息研究所(ISI)1961年创办出版的引文数据库；SCI 既是检索工具，也是引文数据库。对应的引文数据库(查文献网址)：<a href="http://webofknowledge.com">Web Of Science</a>。</li>
<li>注意：SCIE 是 SCI 扩展版，二者在选刊上并没有什么本质的区别，只是收录的期刊数不同。随着网络版的普及，SCIE就是SCI，慢慢趋于通用，它们都是指Web of Science平台上的SCI-Expanded数据库，大学图书馆买的SCI数据库名称一般就为 SCI-E。<h4 id="2-2-EI-The-Engineering-Index"><a href="#2-2-EI-The-Engineering-Index" class="headerlink" title="2.2 EI(The Engineering Index)"></a>2.2 EI(The Engineering Index)</h4></li>
<li>简介：由美国工程信息公司出版，报道工程技术各学科的期刊、会议论文、科技报告等文献。对应的引文数据库(查文献网址)：<a href="https://www.engineeringvillage.com/">EngineeringVillage</a>。<h4 id="2-3-CPCI-Conference-Proceedings-Citation-Index"><a href="#2-3-CPCI-Conference-Proceedings-Citation-Index" class="headerlink" title="2.3 CPCI(Conference Proceedings Citation Index)"></a>2.3 CPCI(Conference Proceedings Citation Index)</h4></li>
<li>简介：汇集了世界上最新出版的会议录资料，包括专著、丛书、预印本以及来源于期刊的会议论文，提供了综合全面、多学科的会议论文资料，对应的引文数据库(查文献网址)：<a href="http://webofknowledge.com">Web Of Science</a>。<h4 id="2-4-国内外常见数据库"><a href="#2-4-国内外常见数据库" class="headerlink" title="2.4 国内外常见数据库"></a>2.4 国内外常见数据库</h4></li>
<li>外文引文数据库<ul>
<li>Science Citation Index, SCI</li>
<li>Social Sciences Citation Index, SSCI</li>
<li>Arts &amp; HumanitiesCitation Index, A&amp;HCI</li>
<li>Emerging Sources Citation Index</li>
<li>Conference Proceedings Citation Index, CPCI</li>
<li>The Engineering Index, EI</li>
<li>Chemical Abstracts, CA</li>
<li>JST(日本)、AJ(俄国)、ISR</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>中文引文数据库<ul>
<li>中国科学引文数据库, CSCD</li>
<li>中文社会科学引文索引, CSSCI, (南大核心)</li>
<li>中文核心期刊要目总览, (北大核心)</li>
<li>中国科技论文统计源期刊, CSTPCD</li>
<li>中国人文社会科学核心期刊, CHSSCD</li>
<li>中国人文社会科学核心期刊</li>
<li>中国核心期刊遴选数据库</li>
</ul>
</li>
</ul>
<h3 id="3-基础知识辨析"><a href="#3-基础知识辨析" class="headerlink" title="3 基础知识辨析"></a>3 基础知识辨析</h3><h4 id="3-1-会议组织者、出版商、科研工作者的关系"><a href="#3-1-会议组织者、出版商、科研工作者的关系" class="headerlink" title="3.1 会议组织者、出版商、科研工作者的关系"></a>3.1 会议组织者、出版商、科研工作者的关系</h4><p><strong>会议组织者：</strong>可能是某个学校；也可能是某个组织；他们为学术/经济利益举办会议，从科研工作者处收文章，收注册费；然后将论文集扔给出版商；<br><strong>出版商：</strong>具备一定声望，比如Elsevier/IEEE/Springer，其旗下有一些会议论文的项目，如Elsevier的Procedia(新推出的)，IEEE的Conference Program，Springer的Lecture Notes系列；<br>这些会议项目的共同特点是：<br>1.拿论文的版权；<br>2.向会议组织者收取出版费用(比如每100篇多少钱)；<br>3.会把论文放在自己的数据库里面，比如ScienceDirect，IEEExplorer等等；</p>
<blockquote>
<p><strong>收录：</strong>比如我建立起来了一个数据库叫 RabbitPaper，我可以自己抓取一些公开的数据，比如 IEEExplorer / Lecture Notes上面能找到的所有关于Rabbit的文章，我把所有的题目+摘要+参考文献，全扣下来放在我的数据库里面，那么我可以宣传说：我这个 Rabbit数据库里面收录了重要关于Rabbit的文献；<br>我不持有IEEExplorer / Lecture Notes的全文内容，只是抓取了公开的信息而已；</p>
<p><strong>检索：</strong>“检索”本身的意思是“在某个数据库里面发现某种内容”；检索工具有很多，Google/Google Scholar/Scirus/ScienceDirect/IEEExplorer/ISI这些都是检索工具；</p>
<ol>
<li>检索工具可能能帮你拿到论文的全文(比如Elsevier的Procedia的全文就是免费公开的，很方便下载)； </li>
<li>也可能不能帮你拿到全文(比如IEEExplorer是收费的，LNCS也是收费的)，需要有权限才能下载全文；<h4 id="3-2-SCI和EI是什么"><a href="#3-2-SCI和EI是什么" class="headerlink" title="3.2 SCI和EI是什么"></a>3.2 SCI和EI是什么</h4><h5 id="3-2-1-SCI是什么"><a href="#3-2-1-SCI是什么" class="headerlink" title="3.2.1 SCI是什么"></a>3.2.1 SCI是什么</h5>SCI(Science Citation Index)是ISI(Institute Scientific Information)做的数据库，其特点为：</li>
</ol>
<ul>
<li>SCI不是出版商，只是数据库，不是具体某篇文章内容版权的拥有者；所以在SCI里面，能看到只是题目+摘要+参考文献；</li>
<li>SCI自己有一个自己确定的期刊列表，SCI只是从出版商处抓取某些期刊的 论文里面的信息（题目/作者/摘要/参考文献），然后可能添加一些自己的信息，比如分类号一类的东西；</li>
<li>SCI的内容不是原始文献全文，卖点是每年推出JCR，里面给出影响因子。</li>
<li>“SCI全文收录”这个短语就是扯淡。。。因为SCI里面哪有全文啊。<h5 id="3-2-3-EI是什么"><a href="#3-2-3-EI是什么" class="headerlink" title="3.2.3 EI是什么"></a>3.2.3 EI是什么</h5>Engineering information Inc.，现在隶属于Elsevier旗下；大家常说的EI Compendex 收录论文的题录、摘要，并加个主题词、分类号什么的。</li>
<li>EI只是从 出版商处抓取某些期刊的论文里面的 信息(题目/作者/摘要/参考文献)。</li>
<li>“EI全文收录”这个短语就是扯淡。。。因为EI里面哪有全文啊。</li>
</ul>
</blockquote>
<h4 id="3-3-ISI、SCI-、汤森路透、路透社的关系"><a href="#3-3-ISI、SCI-、汤森路透、路透社的关系" class="headerlink" title="3.3 ISI、SCI 、汤森路透、路透社的关系"></a>3.3 ISI、SCI 、汤森路透、路透社的关系</h4><p>SCI是由美国科学信息研究所(ISI)1961年创办出版的一部世界著名的期刊文献检索工具，其出版形式包括印刷版期刊和光盘版及联机数据库，现在还发行了互联网上Web版数据库。</p>
<p>ISI通过它严格的选刊标准和评估程序挑选刊源，而且每年略有增减，从而做到SCI收录的文献能全面覆盖全世界最重要和最有影响力的研究成果。40多年来，SCI数据库不断发展，已经成为当代世界最为重要的大型数据库，被列在国际六大著名检索系统(SCI，EI，CA，SA，AJ/РЖ，JST)和三大检索工具(SCI，EI，ISTP)之首。它不仅是一部重要的检索工具书，而且也是科学研究成果评价一项重要依据。在我国，经国家科学技术部有关部门和领导研究，决定从2000年的统计工作起，SCI论文统计用检索系统改为用SCIE。也就是说：如果某位作者的论文被2000年版以后的SCIE收录，就算是被SCI收录了。</p>
<p>美国科学信息研究所(Institute for Scientific Information, 简称ISI)于1960年由Eugene Garfield创立。1992被加拿大汤姆森公司(The Thomson Corporation)的分公司Thomson Scientific &amp; Healthcare收购，于是就成了现在有名的Thomson ISI。后来这个分公司在2006年又分为Thomson Scientific and Thomson Healthcare两个子公司。</p>
<p>至于现在的汤森路透(Thomson Reuters)，网址<a href="http://thomsonreuters.com/，成立于2008年4月17日，是由Thomson公司与英国路透集团(Reuters">http://thomsonreuters.com/，成立于2008年4月17日，是由Thomson公司与英国路透集团(Reuters</a> Group PLC)合并组成的商务和专业智能信息提供商。现在的路透社只是其新闻业务的一部分。以下就介绍一下Thomson Scientific的SCI相关的产品。</p>
<p><strong>(1) ISI web of knowledge</strong></p>
<p>Thomson Scientific网络平台。收费的，通过<a href="http://apps.webofknowledge.com登录进入，或通过你所在学校图书馆或机构网站提供的入口进入。该产品是汤森路透公司的online学术服务产品，它包含了子产品">http://apps.webofknowledge.com登录进入，或通过你所在学校图书馆或机构网站提供的入口进入。该产品是汤森路透公司的online学术服务产品，它包含了子产品</a> “web of science”数据库以及其他资源。Web of science数据库就是我们平时检索SCI数据时所使用的网页，除SCI数据库外，Web of science数据库还包含另外的6个数据库：SSCI，AHCI，IC，CCR，CPCI:Science和CPCI:Social science and Humanities。</p>
<p><strong>(2) Master Journal List</strong></p>
<p>是免费的，汤森路透公司提供的ISI收录的期刊总列表。包括SCI，SCI-E，AHCI，SSCI，CC等众多索引数据库收录的期刊的信息。现在ISI收录的期刊总数是16415个。其中SCI 3772个，SCI-E 8198个。</p>
<ul>
<li>SCI从来源期刊数量来看可分为SCI和SCI-E。<br>SCI是指来源期刊为3772种左右的SCI核心区期刊，<br>SCI-E的全称是SCI-Expanded，是SCI扩展版，其来源期刊8198种左右。<br>SCI-E包括SCI中的期刊。</li>
</ul>
<p><strong>(3)Journal Citation Reports</strong></p>
<p>美国科学情报研究所（ISI）大概每年6月份公布上一年的期刊引用报告（Journal Citation Reports on the Web, JCR Web），JCR对包括SCI收录的SCI-Expanded（包括SCI核心区期刊）的期刊之间引用和被引用数据进行统计、运算，并针对每种期刊定义了影响因子(Impact Factor)等指数加以报道。</p>
<p>JCR可以告知用户的信息包括：每种期刊在当前年被引用的总次数(Total Cites)、每种期刊的影响因子(即该刊前两年发表的文章在当前年的平均被引次数(Impact Factor))、每种期刊当前年发表的文章在当前年的平均被引次数(Immediacy Index)、每种期刊当前年的文章总数(Articles)、每种期刊论文研究课题的延续时间(Cited Half-life)、每种期刊的引用期刊列表(Citing Journal)、每种期刊的被引用期刊列表(Cited Journal)、每种期刊的影响因子在近几年的变化情况(Trends)、每种期刊的来源数据情况(Source Data)等。</p>
<p>Thomson Scientific 还有许多著名的产品。如EndNote 和 Science Watch等。</p>
<h4 id="3-4-SCI分区"><a href="#3-4-SCI分区" class="headerlink" title="3.4 SCI分区"></a>3.4 SCI分区</h4><p>在中国，SCI有两种分区：一种是JCR分区；另一种是中科院分区。在国外，基本不存在分区概念，这只是一个参考，他们一般只投本学科权威期刊，不管分区的，但这些权威期刊一般影响因子都很高。</p>
<h5 id="3-4-1-两种分区方法"><a href="#3-4-1-两种分区方法" class="headerlink" title="3.4.1 两种分区方法"></a>3.4.1 两种分区方法</h5><p><strong>(1) JCR分区</strong><br>JCR将收录期刊分为176个不同学科类别，每个学科分类按照期刊的影响因子高低，平均分为4个区：</p>
<ul>
<li>影响因子前25%（含25%）的期刊为Q1区；</li>
<li>影响因子位于25-50%（含50%）为Q2区；</li>
<li>影响因子位于50-75%（含75%）为Q3区；</li>
<li>影响因子位于75%之后的期刊为Q4区。</li>
</ul>
<p><strong>(2) 中科院分区</strong><br>中科院首先将JCR中所有期刊分为数学、物理、化学、生物、地学、天文、工程技术、医学、环境科学、农林科学、社会科学、管理科学及综合性期刊13大类。然后，将13大类期刊分各自为4个等级，按照各类期刊影响因子划分，4个区:</p>
<ul>
<li>前5%为该类1区；</li>
<li>6%～20%为2区；</li>
<li>21%～50%为3区；</li>
<li>其余的为4区。 可以发现中科院完全使用JCR的影响因子数据，只是进行了不同的学科划分。今年中科院进行了改革，分区表分为基础班和升级版，比往年更加合理一点。<h5 id="3-4-2-两种分区比较"><a href="#3-4-2-两种分区比较" class="headerlink" title="3.4.2  两种分区比较"></a>3.4.2  两种分区比较</h5><strong>(1) 学科划分不同</strong><br>科睿唯安JCR的Journal Ranking没有设置大类学科，只分为176个具体学科，也就是中科院分区表中所指的小类学科。<br><img src="https://pic.imgdb.cn/item/62a896c70947543129ccbfb6.jpg" style="zoom:70%"></li>
</ul>
<p><strong>(2) 分区方法不同</strong><br>JCR是按照“平均主义”思想，根据刊物IF的高至低平均划分4个区，每个区含有该领域总量25%的期刊；中科院的分区如同社会阶层的金字塔结构，1区只有5%的顶级期刊，2~4区期刊数量也逐层增加。</p>
<p><img src="https://pic.imgdb.cn/item/62a897430947543129cd7cf5.jpg" style="zoom:70%"></p>
<p>显然中科院的1区和2区杂志很少，杂志质量相对也高，基本都是本领域的顶级期刊。 所以有的人发表的论文按JCR分区标准算一区，而按中科院分区算2区甚至三区。</p>
<p><strong>(3)指标取值不同 </strong><br>JCR是按本年度公布的IF进行分区，中科院是按期刊前3年IF的平均值进行分区。</p>
<h5 id="3-4-3-我们到底该看哪个分区"><a href="#3-4-3-我们到底该看哪个分区" class="headerlink" title="3.4.3 我们到底该看哪个分区"></a>3.4.3 我们到底该看哪个分区</h5><p>由于中科院分区备受争议，现在很多高校已经不用中科院分区了，虽然在今年中科院进行了改革，分区表分为基础班和升级版，但依然无济于事。 我认为JCR分区可以作为参考，不可SCI至上，因为数据是可以操纵的。</p>
<h3 id="4-资料链接"><a href="#4-资料链接" class="headerlink" title="4 资料链接"></a>4 资料链接</h3><p><a href="https://lib.tsinghua.edu.cn/__local/4/D6/C7/A4DC654D5B4D658CF131335DE9C_9EECFA6B_1E48B8.pdf?e=.pdf">https://lib.tsinghua.edu.cn/__local/4/D6/C7/A4DC654D5B4D658CF131335DE9C_9EECFA6B_1E48B8.pdf?e=.pdf</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>学术科研</category>
      </categories>
      <tags>
        <tag>科研技能</tag>
      </tags>
  </entry>
  <entry>
    <title>数字信号频域变换分析</title>
    <url>/2022/02/02/Digital-signal-spectrum-analysis/</url>
    <content><![CDATA[<h3 id="一、数字信号处理中常用矩函数计算和其物理意义"><a href="#一、数字信号处理中常用矩函数计算和其物理意义" class="headerlink" title="一、数字信号处理中常用矩函数计算和其物理意义"></a>一、数字信号处理中常用矩函数计算和其物理意义</h3><h4 id="一-常用矩函数"><a href="#一-常用矩函数" class="headerlink" title="(一) 常用矩函数"></a>(一) 常用矩函数</h4><h5 id="1-均值"><a href="#1-均值" class="headerlink" title="1. 均值"></a>1. 均值</h5><p>均值表示信号中<strong>直流分量</strong>的大小，用$E(x)$表示。对于高斯白噪声信号而言，它的均值为0，所以它只有交流分量。 </p>
<script type="math/tex; mode=display">
E(x) = \bar x = \frac{x_1 + x_2 + \cdots +x_n}{n} = \frac{\sum_{i=1}^{n}x_i}{n}</script><h5 id="2-均值的平方"><a href="#2-均值的平方" class="headerlink" title="2. 均值的平方"></a>2. 均值的平方</h5><p>均值的平方，用${E^2(x)}$表示，它表示的是信号中<strong>直流分量的功率</strong>。</p>
<h5 id="3-均方值"><a href="#3-均方值" class="headerlink" title="3. 均方值"></a>3. 均方值</h5><p>均方值表示信号平方后的均值，用$E(x^2)$表示。<strong>均方值表示信号的平均功率</strong>。信号的平均功率 = 信号交流分量功率信号直流分量功率。</p>
<h5 id="4-均方根值"><a href="#4-均方根值" class="headerlink" title="4. 均方根值"></a>4. 均方根值</h5><p>用RMS（root meansquare），即均方值的开根号。</p>
<script type="math/tex; mode=display">
X_{rms} = \sqrt{\frac{\sum_{i=1}^{n}X_i^2}{N}}</script><h5 id="5-均方差"><a href="#5-均方差" class="headerlink" title="5. 均方差"></a>5. 均方差</h5><p>均方差（mean squareerror），用MSE表示。均方差是各数据偏离真实值的距离平方和的平均数，也即误差平方和的平均数，计算公式形式上接近方差，它的开方叫均方根误差，均方根误差才和标准差形式上接近。均方差有时候被认为等同于方差。</p>
<script type="math/tex; mode=display">
X_{MSE} = \frac{1}{n}</script><h5 id="6-均方根误差"><a href="#6-均方根误差" class="headerlink" title="6. 均方根误差"></a>6. 均方根误差</h5><p>均方根误差用RMSE（root mean  squareerror）表示。它是观测值与真值偏差的平方和观测次数n比值的平方根，在实际测量中，观测次数n总是有限的，真值只能用最可信赖（最佳）值来代替。方根误差对一组测量中的特大或特小误差反映非常敏感，所以均方根误差能够很好地反映出测量的精密度。均方根误差有时被认为是标准差。</p>
<h5 id="7-方差"><a href="#7-方差" class="headerlink" title="7. 方差"></a>7. 方差</h5><p>方差用variance或deviation 或Var表示。方差描述信号的波动范围，表示信号中<strong>交流分量</strong>的强弱，即<strong>交流信号的平均功率</strong>。</p>
<script type="math/tex; mode=display">
D(X) = E[(X-E(X))^2]</script><script type="math/tex; mode=display">
s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \bar x)^2  ——或者另一种形式——s^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i - \bar x)^2</script><blockquote>
<p>注意上面除以的是n-1，只有这样由样本值估计出的方差才是<strong>无偏的</strong>，即上面式子的期望才是X的方差。但是有的地方也有用除以n来表示方差，只不过这样求出的结果不是方差的无偏估计，计算结果的数学期望并不是X的方差，而是X方差的倍。 </p>
</blockquote>
<h5 id="8-标准差"><a href="#8-标准差" class="headerlink" title="8. 标准差"></a>8. 标准差</h5><p>标准差（Standard Deviation）用σ表示，有的时候标准差又可以被称为均方根误差RMSE。标准差是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示，标准差能反映一个数据集的离散程度。</p>
<p>标准差σ， 反映了测量数据偏离真实值的程度，σ越小，表示测量精度越高，因此可用σ作为评定这一测量过程精度的标准。</p>
<script type="math/tex; mode=display">
S = \sqrt{\frac{\sum_{i=1}^{n}(x_i-\bar x)^2}{n}}</script><blockquote>
<p>有了方差为什么要使用标准差？标准差比方差有什么优势？</p>
<p>因为方差与我们要处理的数据的量纲是不一致的，虽然能很好的描述数据与均值的偏离程度，但是处理结果是不符合我们的直观思维的。 </p>
</blockquote>
<h4 id="二-总结"><a href="#二-总结" class="headerlink" title="(二) 总结"></a>(二) 总结</h4><p>（1）总的来说，均方差，均方根误差和方差，标准差是不能够等同的，尽管它们的公式相似。我们需要从真实值和均值之间的关系来区分它们</p>
<p>（2）对于方差和标准差而言，它们反映的是数据序列与均值的关系。</p>
<p>（3）对于均方差和均方根误差而言，它们反映的是数据序列与真实值之间的关系。</p>
<h5 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h5><p><strong>文章转自：新浪博客/未注明作者，感谢原作者的辛勤付出，如涉及版权，请联系我删除！</strong> <a href="http://www.360doc.com/content/18/0715/14/908538_770556276.shtml">http://www.360doc.com/content/18/0715/14/908538_770556276.shtml</a></p>
<hr>
<h3 id="二、数字域频率与模拟频率"><a href="#二、数字域频率与模拟频率" class="headerlink" title="二、数字域频率与模拟频率"></a>二、数字域频率与模拟频率</h3><h4 id="一-二者关系"><a href="#一-二者关系" class="headerlink" title="(一) 二者关系"></a>(一) 二者关系</h4><p><strong>重点：数字域频率是模拟角频率相对于采样频率的归一化。</strong></p>
<h5 id="1-第一种理解："><a href="#1-第一种理解：" class="headerlink" title="1. 第一种理解："></a>1. 第一种理解：</h5><p>从”连续时间正弦信号与离散时间正弦信号“的关系来理解。</p>
<p><img src="https://pic.imgdb.cn/item/61f9eb132ab3f51d91a2dbbe.jpg" style="zoom:70%"></p>
<p>敲黑板，重点是</p>
<p><img src="https://pic.imgdb.cn/item/61f9ec4d2ab3f51d91a3d307.jpg" style="zoom:70%"></p>
<h5 id="2-第二种理解"><a href="#2-第二种理解" class="headerlink" title="2. 第二种理解"></a>2. 第二种理解</h5><p>从”采样信号的傅里叶变换与离散时间信号的傅里叶变换“的关系来理解。从时域采样出发，先看时域上离散时间信号$x(n)$的自变量$n$与连续时间信号$x(t)$的自变量$t$的关系，如下图。</p>
<p><img src="https://pic.imgdb.cn/item/61f9ed512ab3f51d91a4a253.jpg" style="zoom:70%"></p>
<p>在频域上模拟信号、采样信号、和采样序列(也就是离散时间信号)三者的频谱的关系，如下图所示。看看横轴，时域上，横轴由$t$变成$n$，是按照$n=t/T$的规则；而频域上，横轴由$\Omega$(模拟角频率)变成了$\omega$(数字域频率)，按照$\omega = \Omega × T$的规则。</p>
<p><img src="https://pic.imgdb.cn/item/61f9ee502ab3f51d91a55e3c.jpg" style="zoom:70%"></p>
<p>数字域频率$\omega$的最高频率是$\pi$，这一点可以结合采样定理来理解。采样间隔$T$，意味着采样角频率为$2\pi/T$，根据采样定理，最高的频率是采样频率的一半，也就是$\pi/T$，再将它转换为数字域频率（(乘以$T$)，不就是$\pi$吗?</p>
<h5 id="3-第三种理解"><a href="#3-第三种理解" class="headerlink" title="3. 第三种理解"></a>3. 第三种理解</h5><p>从z平面与s平面的映射关系上来理解。</p>
<p><img src="https://pic.imgdb.cn/item/61f9ef592ab3f51d91a62e74.jpg" style="zoom:70%"></p>
<h4 id="二-总结-1"><a href="#二-总结-1" class="headerlink" title="(二) 总结"></a>(二) 总结</h4><p>不管从哪种角度、哪种方式，殊途同归，结论是一样的，那就是——数字域频率是模拟角频率相对于采样频率的归一化。这一结论，是我们能够正确解读采样后的信号它的频谱分析结果的前提。<br>模拟角频率和数字域频率是数字信号处理中非常重要的两个概念，因为经常需要将模拟信号离散化后再进行频域分析，那么，得到的是数字域频率，必须正确转换为模拟角频率后，才能得到分析对象——模拟信号的频域信息。</p>
<h5 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h5><p><strong>文章转自：个人图书馆/未注明作者，感谢原作者的辛勤付出，如涉及版权，请联系我删除！</strong> <a href="http://www.360doc.com/content/19/0611/18/908538_841808354.shtml">http://www.360doc.com/content/19/0611/18/908538_841808354.shtml</a></p>
<hr>
<h3 id="三、离散时间傅里叶变换DTFT"><a href="#三、离散时间傅里叶变换DTFT" class="headerlink" title="三、离散时间傅里叶变换DTFT"></a>三、离散时间傅里叶变换DTFT</h3><p>首先要需要理解英文缩写DTFT的含义：离散时间DT、傅里叶变换FT，连起来就是DTFT。注意第二个T (也就是Time)，至关重要。它是“离散D”这个特性所描述的主体：也就是，时间是离散的。</p>
<h4 id="一-从z变换到DTFT"><a href="#一-从z变换到DTFT" class="headerlink" title="(一) 从z变换到DTFT"></a>(一) 从z变换到DTFT</h4><p><strong>重点1：从“单位圆上的z变换“这个角度来理解DTFT正变换的定义。</strong><br><strong>重点2：理解DTFT的周期性——离散时间信号的傅里叶变换都是以$2\pi$为周期的。</strong></p>
<p><img src="https://pic.imgdb.cn/item/61f9f1bb2ab3f51d91a81cbd.jpg"></p>
<h5 id="1-DTFT的正变换："><a href="#1-DTFT的正变换：" class="headerlink" title="1. DTFT的正变换："></a>1. DTFT的正变换：</h5><script type="math/tex; mode=display">
\text{DTFT}[x(n)] = X(e^{j\omega}) = \sum_{n = -\infty}^{\infty}x(n)e^{-j\omega n}</script><h5 id="2-DTFT的基本性质："><a href="#2-DTFT的基本性质：" class="headerlink" title="2. DTFT的基本性质："></a>2. DTFT的基本性质：</h5><p>​    $X(e^{j\omega})$是以$\omega$为自变量的<font color="red"><strong>连续</strong>函数</font>；</p>
<p>​    $X(e^{j\omega})$是以$2\pi$为周期的<font color="red"><strong>周期</strong>函数</font>；</p>
<h5 id="3-DTFT存在的充分条件："><a href="#3-DTFT存在的充分条件：" class="headerlink" title="3. DTFT存在的充分条件："></a>3. DTFT存在的充分条件：</h5><script type="math/tex; mode=display">
x(n)绝对可和：\sum_{n = -\infty}^{\infty}|x(n)| < \infty</script><h4 id="二-DTFT变换对及物理含义"><a href="#二-DTFT变换对及物理含义" class="headerlink" title="(二) DTFT变换对及物理含义"></a>(二) DTFT变换对及物理含义</h4><p><strong>重点1：理解离散信号频谱的物理概念及特点。</strong><br><strong>重点2：会求几种常用信号的DTFT(单位样值信号、矩形脉冲、指数衰减信号、离散Sa函数等)。</strong></p>
<h5 id="1-DTFT公式整理"><a href="#1-DTFT公式整理" class="headerlink" title="1. DTFT公式整理"></a>1. DTFT公式整理</h5><p><img src="https://pic.imgdb.cn/item/61f9f4c02ab3f51d91aa9178.jpg"></p>
<h5 id="2-物理含义"><a href="#2-物理含义" class="headerlink" title="2. 物理含义"></a>2. 物理含义</h5><p>(1) $x(n)$可以表示成复指数信号的线性组合(只需一个$2\pi$区间内的频率)；</p>
<p>(2) $X(e^{j\omega})$表示了$x(n)$中各个频率分量的相对大小及位置，称为$x(n)$的频谱(以$2\pi$为周期)。</p>
<script type="math/tex; mode=display">
X(e^{j \omega})=|X(e^{j \omega})| e^{j \angle X(e^{j \omega})}</script><h5 id="说明-2"><a href="#说明-2" class="headerlink" title="说明"></a>说明</h5><p><strong>文章转自：个人图书馆/未注明作者，感谢原作者的辛勤付出，如涉及版权，请联系我删除！</strong> <a href="http://www.360doc.com/content/19/0611/18/908538_841808425.shtml">http://www.360doc.com/content/19/0611/18/908538_841808425.shtml</a></p>
<hr>
<h3 id="四、离散傅里叶级数DFS"><a href="#四、离散傅里叶级数DFS" class="headerlink" title="四、离散傅里叶级数DFS"></a>四、离散傅里叶级数DFS</h3><h4 id="一-离散时间周期信号的DFS"><a href="#一-离散时间周期信号的DFS" class="headerlink" title="(一) 离散时间周期信号的DFS"></a>(一) 离散时间周期信号的DFS</h4><p><strong>重点1：对照连续时间周期信号的FS的思想，理解离散时间周期信号的FS，二者的相同(离散谱)和不同之处。</strong><br><strong>重点2：理解离散时间周期信号的FS展开式为何只有N项，理解离散时间周期信号频谱的周期性。</strong></p>
<p><img src="https://pic.imgdb.cn/item/61f9f6e12ab3f51d91ac386f.jpg"></p>
<p>需要说明的是，DFS有两种表示形式，如下面两个式子，这两种方式的区别在于，1/N的系数在正变换(FS系数求解式)中，还是反变换(级数展开式)中。没有实质区别，只是差一个常数N。这里均采用第二种表示形式，即将1/N的系数放在反变换中。</p>
<script type="math/tex; mode=display">
第一种：\tilde{X} _ k = \frac{1}{N}\sum _ {n = 0}^{N-1}\tilde{x} _ ne^{-j\frac{2\pi}{N}kn}\text{ } - - - - \text{ }\tilde{x}(n) = \sum _ {k=0}^{N-1}\tilde{X} _ ke^{j\frac{2\pi}{N}kn}</script><script type="math/tex; mode=display">
第二种：\tilde{X}(k) =\sum _ {n = 0}^{N-1}\tilde{x} _ ne^{-j\frac{2\pi}{N}kn} \text{ } - - - -\text{ }\tilde{x}(n) =  \frac{1}{N}\sum _ {k=0}^{N-1}\tilde{X}(k)e^{j\frac{2\pi}{N}kn}</script><h4 id="二-离散时间周期信号的频谱"><a href="#二-离散时间周期信号的频谱" class="headerlink" title="(二) 离散时间周期信号的频谱"></a>(二) 离散时间周期信号的频谱</h4><p><strong>重点1：掌握离散时间周期信号频谱的特点——离散性、谐波性、周期性；</strong><br><strong>重点2：会求常用离散时间周期信号的DFS(见例题1和例题2)。</strong></p>
<p><img src="https://pic.imgdb.cn/item/61f9faf22ab3f51d91af8e23.jpg" style="zoom:80%"></p>
<h5 id="1-两类典型的求解DFS的题目："><a href="#1-两类典型的求解DFS的题目：" class="headerlink" title="1. 两类典型的求解DFS的题目："></a>1. 两类典型的求解DFS的题目：</h5><p><strong>第一类：</strong>周期信号直接以正弦、余弦之和形式给出(如例1)<br>方法：与标准形式的FS对照，直接得出FS的系数（即频谱)。<br><strong>第二类：</strong>一般的周期信号(如例2)<br>方法：用FS的系数求解公式。</p>
<h5 id="2-DFS与DTFT的关系"><a href="#2-DFS与DTFT的关系" class="headerlink" title="2. DFS与DTFT的关系"></a>2. DFS与DTFT的关系</h5><p><strong>重点：理解DFS与DTFT的关系——周期序列的离散谱是其主值序列连续谱的离散抽样。</strong></p>
<p>与上一讲中离散矩形脉冲的DTFT进行比较：可见，二者在时域上的关系是周期延拓，频域上的关系是离散抽样。如下图所示。 </p>
<p><img src="https://pic.imgdb.cn/item/61f9fc072ab3f51d91b070dc.jpg"></p>
<p><img src="https://pic.imgdb.cn/item/61f9fc512ab3f51d91b0a4f4.jpg"></p>
<h5 id="3-四种傅里叶变换的关系"><a href="#3-四种傅里叶变换的关系" class="headerlink" title="3. 四种傅里叶变换的关系"></a>3. 四种傅里叶变换的关系</h5><p><strong>重点：一个域周期，对应另一个域离散。</strong></p>
<p><img src="https://pic.imgdb.cn/item/61f9fd6e2ab3f51d91b1b54d.jpg"></p>
<h5 id="说明-3"><a href="#说明-3" class="headerlink" title="说明"></a>说明</h5><p><strong>文章转自：个人图书馆/未注明作者，感谢原作者的辛勤付出，如涉及版权，请联系我删除！</strong> <a href="http://www.360doc.com/content/19/0611/18/908538_841808371.shtml">http://www.360doc.com/content/19/0611/18/908538_841808371.shtml</a></p>
<hr>
<h3 id="五、离散傅里叶变换DFT"><a href="#五、离散傅里叶变换DFT" class="headerlink" title="五、离散傅里叶变换DFT"></a>五、离散傅里叶变换DFT</h3><h4 id="一-背景介绍"><a href="#一-背景介绍" class="headerlink" title="(一) 背景介绍"></a>(一) 背景介绍</h4><p>为什么需要定义一种“新”的变换?</p>
<p>计算机处理的两个基本条件：<br><strong>第一，只能处理离散的数据(时域和频域都要离散)；</strong><br><strong>第二，要有限长。</strong></p>
<p>DTFT，时域上离散，但频域是连续的；DFS，时域频域都是离散的，但同时又都是周期的，周期序列长度为无限长。但同时我们也注意到，周期序列实际上只有有限个序列值有意义，因而它的离散傅里叶级数也适用于有限长序列，这就得到有限长序列的离散傅里叶变换(DFT)。</p>
<p><img src="https://pic.imgdb.cn/item/61f9fe8b2ab3f51d91b28d6c.jpg" style="zoom:80%"></p>
<p>所以，<strong>DFT并不是一种新的变换</strong>。它只是将DFS时域和频域上都取一个周期而已。<strong>DFT正反变换的定义式与DFS相同，只是加了一个取值范围的限定而已</strong>。换汤不换药。</p>
<h4 id="二-DFT的定义及物理含义"><a href="#二-DFT的定义及物理含义" class="headerlink" title="(二) DFT的定义及物理含义"></a>(二) DFT的定义及物理含义</h4><p><strong>重点：DFT与DFS、DTFT的关系</strong></p>
<h5 id="1-变换公式"><a href="#1-变换公式" class="headerlink" title="1. 变换公式"></a>1. 变换公式</h5><script type="math/tex; mode=display">
正变换：X(k)=\sum_{n=0}^{N-1} x(n) e^{-j \frac{2 \pi}{N} k n}, k=0,1, \ldots, N-1</script><script type="math/tex; mode=display">
逆变换：x(n)=\frac{1}{N} \sum_{k=0}^{N-1} X(k) e^{j \frac{2 \pi}{N} k n}, n=0,1, \ldots, N-1</script><p>定义<font color="red"><strong>旋转因子$W_N=e^{-j\frac{2\pi}{N}}$</strong></font>，则上式转变为：</p>
<script type="math/tex; mode=display">
正变换：X(k)=\sum_{n=0}^{N-1} x(n) W_{N}^{k n}, k=0,1, \ldots, N-1</script><script type="math/tex; mode=display">
逆变换：x(n)=\frac{1}{N} \sum_{k=0}^{N-1} X(k) W_{N}^{-k n}, n=0,1, \ldots, N-1</script><p>总上，这对变换记为：</p>
<script type="math/tex; mode=display">
x(n)\overset{N点\text{DFT}}{\longleftrightarrow}X(k)</script><p><img src="https://pic.imgdb.cn/item/61fa03b92ab3f51d91b6cb31.jpg" style="zoom:80%"></p>
<p><strong>DFT不是序列x(n)的真正的频谱。x(n)的真正的频谱是DTFT，DFT只是对其真正频谱的一个周期上的离散抽样值。</strong></p>
<h4 id="三-DFT的计算"><a href="#三-DFT的计算" class="headerlink" title="(三) DFT的计算"></a>(三) DFT的计算</h4><p><strong>重点：DFT的两种计算方法。</strong></p>
<h5 id="1-方法一：利用DFT定义式"><a href="#1-方法一：利用DFT定义式" class="headerlink" title="1. 方法一：利用DFT定义式"></a>1. 方法一：利用DFT定义式</h5><script type="math/tex; mode=display">
X(k) = \sum_{n=0}^{N-1}x(n)e^{-j\frac{2\pi}{N}kn},k=0,1,\cdots,N-1</script><h5 id="2-方法二：先求DTFT再抽样"><a href="#2-方法二：先求DTFT再抽样" class="headerlink" title="2. 方法二：先求DTFT再抽样"></a>2. 方法二：先求DTFT再抽样</h5><script type="math/tex; mode=display">
X(k)=X(e^{j \omega})| _ {\omega=\frac{2 \pi}{N} k} R _ {N}(k)</script><h5 id="3-方法三：DFT矩阵"><a href="#3-方法三：DFT矩阵" class="headerlink" title="3. 方法三：DFT矩阵"></a>3. 方法三：DFT矩阵</h5><p>设有一序列$x(n), n = 0, 1, \cdots, N-1$，求其$N$点DFT有：</p>
<script type="math/tex; mode=display">
X(k) = \mathrm{DFT}[x(n)] =  \sum_{n=0}^{N-1} x(n) \exp\left[{-\mathrm j}\dfrac{2\pi}{N}nk\right] = \sum_{n=0}^{N-1} x(n) W_N^{nk}</script><p>其中，$k = 0,1, \cdots, N-1$。</p>
<p>令 $\boldsymbol x = \left[x(0), x(1),\cdots, x(N-1)\right]^{\mathrm T}$，则DFT计算可以看做向量$\boldsymbol x$和一个矩阵的乘积，我们一般讲这个矩阵称作<strong>DFT矩阵</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{F_1} 
&= \left[\begin{array}{ccccc}
\mathrm e^{-\mathrm j \frac{2\pi}{N} 0 \cdot 0} &\mathrm e^{-\mathrm j \frac{2\pi}{N} 0 \cdot 1} & \mathrm e^{-\mathrm j \frac{2\pi}{N} 0 \cdot 2} & \cdots & \mathrm e^{-\mathrm j \frac{2\pi}{N} 0 \cdot (N-1)} \\
\mathrm e^{-\mathrm j \frac{2\pi}{N} 1 \cdot 0} &\mathrm e^{-\mathrm j \frac{2\pi}{N} 1 \cdot 1} & \mathrm e^{-\mathrm j \frac{2\pi}{N} 1 \cdot 2} & \cdots & \mathrm e^{-\mathrm j \frac{2\pi}{N} 1 \cdot (N-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\mathrm e^{-\mathrm j \frac{2\pi}{N} (N-1) \cdot 0} &\mathrm e^{-\mathrm j \frac{2\pi}{N} (N-1) \cdot 1} & \mathrm e^{-\mathrm j \frac{2\pi}{N} (N-1) \cdot 2} & \cdots & \mathrm e^{-\mathrm j \frac{2\pi}{N} (N-1) \cdot (N-1)}
\end{array}\right] \\
\\
&= \left[\begin{array}{ccccc}
W_N^{0 0} & W_N^{0 1} & W_N^{0 2} & \cdots & W_N^{0 (N-1)} \\
W_N^{1 0} & W_N^{1 1} & W_N^{1 2} & \cdots & W_N^{1 (N-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
W_N^{(N-1) 0} &W_N^{(N-1) 1} & W_N^{(N-1) 2} & \cdots & W_N^{(N-1)(N-1)}
\end{array}\right]
\end{aligned}</script><p>此时，序列$x(n)$的DFT为：</p>
<script type="math/tex; mode=display">
\boldsymbol X = \left[\begin{array}{c} X(0)  \\ X(1) \\ \vdots \\ X(k) \\ \vdots \\ X(N-1) 
\end{array}\right] = \boldsymbol{F_1 x} = \boldsymbol{F_1}\left[\begin{array}{c} x(0)  \\ x(1) \\ \vdots \\ x(n) \\ \vdots \\ x(N-1) 
\end{array}\right]</script><p>或者，若将<strong>DFT矩阵</strong>定义为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{F_2} = \boldsymbol{F_1}^{\mathrm H} 
&= \left[\begin{array}{ccccc}
\mathrm e^{\mathrm j \frac{2\pi}{N} 0 \cdot 0} &\mathrm e^{\mathrm j \frac{2\pi}{N} 0 \cdot 1} & \mathrm e^{\mathrm j \frac{2\pi}{N} 0 \cdot 2} & \cdots & \mathrm e^{\mathrm j \frac{2\pi}{N} 0 \cdot (N-1)} \\
\mathrm e^{\mathrm j \frac{2\pi}{N} 1 \cdot 0} &\mathrm e^{\mathrm j \frac{2\pi}{N} 1 \cdot 1} & \mathrm e^{\mathrm j \frac{2\pi}{N} 1 \cdot 2} & \cdots & \mathrm e^{\mathrm j \frac{2\pi}{N} 1 \cdot (N-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\mathrm e^{\mathrm j \frac{2\pi}{N} (N-1) \cdot 0} &\mathrm e^{\mathrm j \frac{2\pi}{N} (N-1) \cdot 1} & \mathrm e^{\mathrm j \frac{2\pi}{N} (N-1) \cdot 2} & \cdots & \mathrm e^{\mathrm j \frac{2\pi}{N} (N-1) \cdot (N-1)}
\end{array}\right]
\\
\\
&= \left[\begin{array}{ccccc}
W_N^{-0 0} & W_N^{-0 1} & W_N^{-0 2} & \cdots & W_N^{-0 (N-1)} \\
W_N^{-1 0} & W_N^{-1 1} & W_N^{-1 2} & \cdots & W_N^{-1 (N-1)} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
W_N^{-(N-1) 0} &W_N^{-(N-1) 1} & W_N^{-(N-1) 2} & \cdots & W_N^{-(N-1)(N-1)}
\end{array}\right]
\end{aligned}</script><p>此时，序列$x(n)$的DFT变换为：</p>
<script type="math/tex; mode=display">
\boldsymbol X = \left[\begin{array}{c} X(0)  \\ X(1) \\ \vdots \\ X(k) \\ \vdots \\ X(N-1) 
\end{array}\right] = \boldsymbol{F_2}^{\mathrm H} \boldsymbol x = \boldsymbol{F_2}^{\mathrm H}\left[\begin{array}{c} x(0)  \\ x(1) \\ \vdots \\ x(n) \\ \vdots \\ x(N-1) 
\end{array}\right]</script><p>容易发现，DFT矩阵具有如下性质：</p>
<ul>
<li>① DFT矩阵是对称矩阵，即$\boldsymbol{F_1} = \boldsymbol{F_1}^{\mathrm T} \qquad \boldsymbol{F_2} = \boldsymbol{F_2}^{\mathrm T}$；</li>
<li>② $\boldsymbol{F_2} = \boldsymbol{F_1}^{\mathrm H}$；</li>
</ul>
<p>参考链接1：<a href="https://web.xidian.edu.cn/kywang/files/20171213_190241.pdf">第三章 离散傅里叶变换（DFT） 及其快速算法（FFT）</a><br>参考链接2：<a href="https://blog.csdn.net/weixin_44144171/article/details/100083924">DFT和IDFT分析 - CSDN</a></p>
<h5 id="4-例题"><a href="#4-例题" class="headerlink" title="4. 例题"></a>4. 例题</h5><p>下面的例题，分别用这两种方法进行求解。</p>
<html>
    <table style="margin-left: auto; margin-right: auto;">
        <tr>
            <td>
                <!--左侧内容-->
                求5点矩形窗函数$x(n) = R_5(n)$的5点DFT$X_1(k)$和10点DFT$X_2(x)$
            </td>
            <td>
                <!--右侧内容-->
                <img src="https://pic.imgdb.cn/item/61fa05922ab3f51d91b85a37.jpg">
            </td>
        </tr>
    </table>
</html>

<p><img src="https://pic.imgdb.cn/item/61fa090f2ab3f51d91bb0fba.jpg"></p>
<p>对于本题来说，方法二特别直观，便于理解DFT与DTFT的关系。</p>
<p><img src="https://pic.imgdb.cn/item/61fa09a32ab3f51d91bb8efb.jpg" style="zoom:70%"></p>
<p>可见，同一个序列的不同点数的DFT，得到的结果不同。DFT的点数$N$越大，$X(k)$越能反映连续频谱的形状。<br>当DFT的点数$N$&gt;序列的长度$N_0$时，相当于在序列后面补上$N-N_0$个零，故称为<strong>”补零DFT“</strong>。</p>
<p><img src="https://pic.imgdb.cn/item/61fa0a292ab3f51d91bbff68.jpg" style="zoom:70%"></p>
<h4 id="四-DFT性质"><a href="#四-DFT性质" class="headerlink" title="(四) DFT性质"></a>(四) DFT性质</h4><h5 id="1-圆周移位-循环移位"><a href="#1-圆周移位-循环移位" class="headerlink" title="1. 圆周移位(循环移位)"></a>1. 圆周移位(循环移位)</h5><p>首先看，为什么要定义一种新的移位?这种新的移位为什么称为“圆周移位”？它与普通的移位有何异同？见下图。</p>
<p><img src="https://pic.imgdb.cn/item/61fa0ab42ab3f51d91bc7111.jpg" style="zoom:70%"></p>
<p>可见，圆周移位与普通移位的区别在于：多一个周期延拓、再取主值区间的过程。定义式如下：</p>
<script type="math/tex; mode=display">
圆周移位(循环移位)——x(n+m)_NR_N(n) = \tilde{x}(n+m)R_N(n)</script><p>圆周移位的步骤：先周期延拓、再移位、最后取主值区间；或者先移位、再周期延拓、最后取主值区间。前两步谁先谁后都可以。见下图。</p>
<p><img src="https://pic.imgdb.cn/item/61fa0bf72ab3f51d91bd7a9a.jpg" style="zoom:70%"></p>
<p>可见，这种新的移位，相当于先把序列放在一个圆上，然后转圈圈：左移，沿顺时针方向转(即上图中的情况);右移，沿逆时针方向转。<br><strong>与其他傅里叶变换一样，DFT依然满足：时域移位，频域线性相移；频域移位，时域调制。需要注意的是，这里的移位，都指的是<font color="red">圆周(循环)移位</font></strong>。</p>
<p><img src="https://pic.imgdb.cn/item/61fa0cb02ab3f51d91be2ca9.jpg" style="zoom:80%"></p>
<h5 id="2-圆周卷积-循环卷积"><a href="#2-圆周卷积-循环卷积" class="headerlink" title="2. 圆周卷积(循环卷积)"></a>2. 圆周卷积(循环卷积)</h5><p><strong>(1) 圆周卷积的定义</strong><br>仿照上面我们对移位操作做的改进——增加了“周期延拓、取主值区间”的过程，将“移位”改造成“圆周移位”一样，我们也对之前很熟悉的卷积和公式做改进，就得到一种新的卷积”圆周卷积“(或称为循环卷积)的定义。</p>
<p>$x_1(n)$的长度为$N_1$，$x_2(n)$的长度为$N_2$，$N \geq \max(N_1,N_2)$，则$N$点圆周卷积为：</p>
<script type="math/tex; mode=display">
x_1(n) \odot x_2(n) = \sum_{m=0}^{N-1}x_1(m)x_2(n-m)_NR_N(n)</script><p>为了区分，大家在《信号与系统》中所学习的卷积和，称为”线性卷积“。<br>显然易见，圆周卷积的结果与$N$有关。<br><strong>(2) DFT的圆周卷积定理</strong><br>在圆周卷积的定义下，DFT依然满足”一个域卷积，另外一个域相乘“这一更古不变的关系。</p>
<p><font color="bule"><strong>时域圆周卷积定理：</strong></font></p>
<script type="math/tex; mode=display">
x_1(n) \odot x_2(n) \overset{N点\text{DFT}}{\longleftrightarrow}X_1(k)X_2(k)</script><p><font color="bule"><strong>频域圆周卷积定理：</strong></font></p>
<script type="math/tex; mode=display">
x_1(n)  x_2(n) \overset{N点\text{DFT}}{\longleftrightarrow}\frac{1}{N}X_1(k) \odot X_2(k)</script><p><strong>(3) 圆周卷积的计算</strong><br>下面重点来看圆周卷积的计算。</p>
<p><img src="https://pic.imgdb.cn/item/61fa10de2ab3f51d91c1fa15.jpg" style="zoom:85%"></p>
<p><strong>(4) 圆周卷积与线性卷积的关系</strong><br>上面的例题中，两个序列的线性卷积是多少呢?圆周卷积与之相同吗?为什么?<br>线性卷积，是直接将序列2反转、平移，而没有”周期延拓、再取主值“这两步。可以用图解法或者竖式法得到（信号与系统中已学，此处直接给出结果)，线性卷积的结果如下：</p>
<script type="math/tex; mode=display">
两序列的5点线性卷积为:x _ 1(n) * x _ 2(n)=[1,3,6,9,12,9,5]</script><p>两种卷积为何不同呢?又有何关系呢?<br>下图从公式上推导圆周卷积与线性卷积的关系</p>
<p><img src="https://pic.imgdb.cn/item/61fa11ac2ab3f51d91c2b12a.jpg" style="zoom:80%"></p>
<p>我们可以得到以下结论：</p>
<p><strong>圆周卷积是线性卷积周期延拓后的主值序列：</strong></p>
<script type="math/tex; mode=display">
y_c(n) = \sum_{r=-\infty}^{\infty}y_l(n+rN)R_N(n)</script><p><strong>当$N\geq N_1+N_2-1$时，圆周卷积和线性卷积结果相同。</strong></p>
<p>根据以上结论，我们可以根据线性卷积的结果，直接得到N点圆周卷积的结果。如下图：</p>
<p><img src="https://pic.imgdb.cn/item/61fa12862ab3f51d91c36961.jpg" style="zoom:70%"></p>
<h5 id="3-圆周共轭对称性"><a href="#3-圆周共轭对称性" class="headerlink" title="3. 圆周共轭对称性"></a>3. 圆周共轭对称性</h5><p>这里不讲证明(教材上都有)，重点讲怎么理解教材上让人眼花缭乱的公式。我们把“公式”翻译成“人话”。<br>首先说明，文中所说的N点长序列，都指的是自变量取值范围为$[0,N-1]$，除此之外的区间，序列值为0。<br>先看第一个。<br><strong>(1) 共轭序列的DFT</strong><br>时频域有这样一个基本对应关系——时域取共轭，对应频域自变量取负然后函数取共轭。具体到DFT呢？“自变量取负”也就是“反转”，而“DFT隐含着周期性”，所以这里的“反转”要加上“周期延拓，再取主值区间”，所以，公式及证明过程如下</p>
<p><img src="https://pic.imgdb.cn/item/61fa135b2ab3f51d91c41e2a.jpg" style="zoom:70%"></p>
<p><strong>时域取共轭，对应DFT是：先周期延拓，再反转，再取主值区间，最后取共扼。</strong>当然，第一步与第二步可以交换次序，取共轮可以放在任意步骤上。关键是理解这个操作用公式的三种描述方式(上图中画红线处)：</p>
<ul>
<li>第①种：$X^*((-k))_N R_N(k)$，是最直观地展现上述过程的；</li>
<li>第②种：$X^*((N-k))_N R_N(k)$，可认为用其周期性(周期延拓得到的当然是以$N$为周期啦)，把$-k$换成$N-k$；</li>
<li>第③种：去掉了双括号，也去掉了$R_N(k)$，好像看不出“周期延拓”和“取主值区间”的操作了。大家会心存疑虑，这个等号成立吗?</li>
</ul>
<p>我们用下图的例子来说明一下这个等号成立，为了画图的方便，我们用函数值为实数的情况，图中是以$n$为自变量，换作$k$当然也是一样的。</p>
<p><img src="https://pic.imgdb.cn/item/61fa223c2ab3f51d91d25487.jpg"></p>
<p>$x(N-n)$可以看作简写形式，优点在于形式简洁明了，缺点在于掩盖了周期延拓再取主值的过程。用这种简写形式，要注意一点，$N$点长序列$x(n)$，$n$的取值范围为$0&lt;n&lt;N-1$，也即本应该$x(N)=0$，但此处当$n=0$时，$x(N-n)=x(N)$，不能认为$x(N)=0$，而要认为$x(N)=x(0)$。也就是说，要把$x(n)$的这$N$个点，认为是周期序列的主值区间，那么$x(N)$就是下一个周期的第一个点，所以$x(N)=x(0)$。</p>
<p>用这种简写形式来描述这个性质，就是：时域取共轭，对应的DFT，相当于把序号$k$与序号$N-k$做一个互换，然后取共轭。</p>
<p>下面看这个性质的两个推论：</p>
<p><strong>第一个推论：实序列的DFT是圆周共轭对称序列。</strong></p>
<ul>
<li>对于实序列$x(n)$，其DFT即$X(k)$满足：<script type="math/tex; mode=display">
X(k) = X^*(N-k)_NR_N(k) = X^*(N-k)</script><font color="red"><strong>实序列的DFT $X(k)$是圆周共轭对称序列</strong></font>。即：实部圆周偶对称，虚部圆周奇对称；模圆周偶对称，相角圆周奇对称。</li>
</ul>
<p>“圆周共轭对称”是个什么鬼？我们按照以下几步来解释一下：</p>
<ul>
<li><p>第一步：从“偶对称、奇对称”到“共轭对称/共轭反对称”</p>
<p>偶对称/奇对称地球人都知道吧。共轭对称/反对称就不是地球人都知道了，大学生才知道。</p>
<p>对于实函数$x(n)$，如果$x(-n)=x(n)$，称之为偶对称，$x(-n)=-x(n)$称之为奇对称。</p>
<p>扩展到复函数$x(n)$，如果$x^*(-n)=x(n)$，称之为共轭对称，与之相对应的还有共轭反对称。</p>
</li>
<li><p>第二步：从“共轭对称”到“圆周共轭对称”</p>
<p>圆周共轭对称定义：对于$N$点长序列$x(n)$，若$x(n)=x((N-n))_NR_N(n)$，或者用简写形式：$x(n)=x(N-n)$，那么称之为“圆周共轭对称”。</p>
<p>可以理解为：把$x(n)$放在一个圆周的$N$个等分点上，或者说把横轴掰弯成一个圆($n=N-1$与原点重合)，则这$N$个序列值关于原点对称，或者说关于$\dfrac{N}{2}$也对称，如下图所示。</p>
</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/61fa27d02ab3f51d91d78d3a.jpg" style="zoom:70%"></p>
<p>我们前面求解过的例题：5点矩形脉冲的DFT，如下图，也体现出圆周偶对称的特点。</p>
<p><img src="https://pic.imgdb.cn/item/61fa28242ab3f51d91d7e051.jpg" style="zoom:70%"></p>
<p><strong>第二个推论：实部/虚部与圆周共轭对称/反对称分量的关系</strong></p>
<p>首先解释一下什么叫圆周共轭对称分量和圆周共轭反对称分量。需要经过以下几步循序渐进的理解。</p>
<ul>
<li>第一步：实函数可以分解为偶分量+奇分量</li>
</ul>
<script type="math/tex; mode=display">
x(n) = x_e(n)+x_o(n)</script><p>其中：偶分量 —— $x_e = \dfrac{x(n)+x(-n)}{2}$，奇分量 —— $x_o = \dfrac{x(n)-x(-n)}{2}$</p>
<ul>
<li>第二步：从“偶分量/奇分量”到复函数可以分解为“共轭对称分量+共轭反对称分量”，把上式中的$x(-n)$改为$x^*(-n)$即可</li>
</ul>
<script type="math/tex; mode=display">
x(n) = x_e(n)+x_o(n)</script><p>其中：共轭对称分量 —— $x_e(n) = \dfrac{x(n)+\overline{x(-n)}}{2}$，共轭反对称分量——$x_o = \dfrac{x(n)-\overline {x^*(-n)}}{2}$</p>
<p>以上两式，无论是对无限长序列，还是有限长序列，都是适用的。如果$x(n)$为$N$点长，并且$0&lt;n&lt;N-1$，那么$x_e(n)$和$x_o(n)$是$2N-1$点长，并且$-(N-1) \leq n \leq N-1$。</p>
<ul>
<li>第三步：改造成适合DFT的。凡是涉及到自变量取负(也就是反转)的，都加上“周期延拓，再取主值区间”的操作。也就是把上式中的$x(-n)$改为$x((N-n))_NR_N(n)$，用简写形式表示就是$x(N-n)$。</li>
</ul>
<p>因此，得到圆周共轭对称分量和圆周共轭反对称分量的定义：</p>
<p>$N$点长的序列$x(n)$，可以分解为<font color="red">圆周共轭对称分量+圆周共轭反对称分量</font></p>
<script type="math/tex; mode=display">
x(n) = x_{ep}(n) + x_{op}(n)</script><p>其中：圆周共轭对称分量——$x_{ep}(n) = \dfrac{x(n)+\overline{x(N-n)}}{2}$，圆周共轭反对称分量——$x_{op}(n) = \dfrac{x(n)-\overline{x(N-n)}}{2}$</p>
<blockquote>
<p>注意，前提是$x(n)$为$N$点长序列，并且n的范围是$0≤n≤N-1$，圆周共轭对称/反对称分量的长度仍是$N$，$n$的范围也不变。而且如前所述，$n=0$时，$x(N-0)=x(N)=x(0)$。</p>
</blockquote>
<p>上面，是以$x(n)$为例，同样，对于DFT $X(k)$，也可以定义圆周共扼对称/反对称分量，不再赘述。</p>
<p>解释完这些，我们的核心公式就出来啦（证明过程省略，直接看结论)。序列$x(n)$及其DFT的实部/虚部与圆周共轭对称/反对称分量之间的关系，见下图：</p>
<p><img src="https://pic.imgdb.cn/item/61fa317c2ab3f51d91e0bda3.jpg" style="zoom:70%"></p>
<p>(此处省略若干公式)，翻译成人话(绕口令)就是：<br>序列实部的DFT是序列DFT的共轭对称分量<br>序列虚部×j的DFT是序列DFT的共轭反对称分量<br>序列共轭对称分量的DFT是序列DFT的实部<br>序列共轭反对称分量的DFT是序列DFT的虚部巧j</p>
<p>是不是像绕口令，但总比公式强多了。这—切，意义何在?</p>
<p><strong>第一，从图形上可以淋漓尽致地体现DFT隐含的周期性。</strong><br><strong>第二，为DFT的简化运算提供了思路。</strong></p>
<h5 id="4-Parseval定理"><a href="#4-Parseval定理" class="headerlink" title="4. Parseval定理"></a>4. Parseval定理</h5><p>有限长序列的能量：</p>
<script type="math/tex; mode=display">
\sum\limits_{n=0}^{N-1}|x(n)|^{2}=\dfrac{1}{N} \sum\limits_{k=0}^{N-1}|X(k)|^{2}</script><h4 id="五-频域抽样"><a href="#五-频域抽样" class="headerlink" title="(五) 频域抽样"></a>(五) 频域抽样</h4><p>实际上，<font color="red"><strong>DFT就是频域抽样</strong></font>。包括三个问题，这三个问题环环相扣、层层推进。</p>
<h5 id="1-DFT与DTFT、z变换的关系"><a href="#1-DFT与DTFT、z变换的关系" class="headerlink" title="1. DFT与DTFT、z变换的关系"></a>1. DFT与DTFT、z变换的关系</h5><p>先从公式上看三个变换的关系，再结合z平面的单位圆的概念，从图形上理解。如下图:</p>
<p><img src="https://pic.imgdb.cn/item/61fa339a2ab3f51d91e2d239.jpg" style="zoom:70%"></p>
<p><img src="https://pic.imgdb.cn/item/61fa33b52ab3f51d91e2eb74.jpg" style="zoom:70%"></p>
<p>毫无疑问，DFT的自变量$k$为离散的，而DTFT的自变量$\omega$、以及z变换的自变量$z$都是连续变量。DFT是另外两种变换的离散采样值。因为这种采样是在频域，所以称为”频域采样“。</p>
<p>那么问题来啦：能否由离散频谱值$X(k)$得到$X(z)$和$X(e^{j\omega})$？</p>
<p>不管在那个域进行抽样，其数学本质都是用一些离散的数值代替原来连续变化的函数，或者说用一些离散的点代表原来连续的曲线。能不能代表取决于两个因素：一是这些离散的点的间隔，即抽样间隔；二是原来那条连续曲线的变化起伏程度，(<font color="red"><strong>频域抽样定理</strong></font>)。</p>
<h5 id="2-频域抽样定理"><a href="#2-频域抽样定理" class="headerlink" title="2. 频域抽样定理"></a>2. 频域抽样定理</h5><p>傅里叶分析方法的好处在于，建立起时域和频域的一种重要的对应关系：一个域离散抽样，另外一个域周期延拓。所以，研究时域抽样时，把问题对应到频域上去研究；那么现在研究频域抽样时，又要把问题对应到时域上去研究。毫无疑问，时域上会周期延拓。如下图：</p>
<p><img src="https://pic.imgdb.cn/item/61fa34a62ab3f51d91e3d3f7.jpg" style="zoom:70%"></p>
<p>既然是以$N$为周期延拓，条件自然而然就出来了：</p>
<p>对于$M$点长的序列$x(n)$，频域抽样不失真的条件是：<font color="red"> <strong>一个周期内的频域抽样点数$N \geq M$</strong></font> ，此时有：</p>
<script type="math/tex; mode=display">
x_N(n) = \sum_{r = -\infty}^{\infty}x(n+rN)R_N(n) = x(n)</script><p>即满足该条件时，$N$个频率抽样值$X(k)$（即$x(n)$的$N$点DFT)能够完全代表$x(e^{j\omega})$及$X(z)$。</p>
<p>问题又来了，怎么表示？这就是第三个问题：频域的插值恢复。</p>
<h5 id="3-频域的插值恢复"><a href="#3-频域的插值恢复" class="headerlink" title="3. 频域的插值恢复"></a>3. 频域的插值恢复</h5><p>与时域抽样的恢复完全相同的思路，用离散的样本值乘以一个插值函数，得到一个连续的函数，只不过这里的插值函数是关于$\omega$或$z$的函数。下面的任务就是找这个函数$\varphi(w)$或$\varphi(z)$。</p>
<p><img src="https://pic.imgdb.cn/item/61fa365f2ab3f51d91e58cc6.jpg" style="zoom:70%"></p>
<p>z变换的形式更为简洁，因此首先解决由$X(k)$得到$X(z)$的问题。</p>
<p>以下推导过程的大致思路：把z变换定义式中的$x(n)$用IDFT的公式替换，然后交换求和次序，再利用旋转因子的性质，即可得到。如下图：</p>
<p><img src="https://pic.imgdb.cn/item/61fa36922ab3f51d91e5bf47.jpg" style="zoom:70%"></p>
<p>解决了由$X(k)$得到$X(z)$的问题，将$z$换成$e^{j\omega}$，自然就得到了$X(e^{j\omega})$。如下图：</p>
<p><img src="https://pic.imgdb.cn/item/61fa37172ab3f51d91e642e8.jpg" style="zoom:70%"></p>
<p>把内插公式和内插函数总结如下图，这个内插函数的幅度部分的图形我们可以画出来，我们发现，它在一些固定的位置($\dfrac{2\pi}{N}$的整数倍处）是零，而$\dfrac{2\pi}{N}​$恰好是频域抽样时的间隔，这是巧合吗？显然不是，这是必然的。</p>
<p><img src="https://pic.imgdb.cn/item/61fa377b2ab3f51d91e6a274.jpg" style="zoom:70%"></p>
<p>我们把内插公式展开来看，如下图所示。也就是说，把各个频域抽样值$X(k)$与做相应平移后的内插函数（平移$\dfrac{2\pi}N$的$k$倍）相乘，再相加，就得到连续的频谱函数$X(e^{j\omega})$。与第$k$个抽样值相乘的内插函数，在所有其他抽样点处刚好是零点，只有在第$k$个抽样点处的值不为零(值为1)。所以，重建后的这个连续函数，在每个抽样位置（也就是$\dfrac{2}{N}$的整数倍)上的值，就等于$X(k)$这一点的值，不需要任何其他抽样值参与;而在两个抽样点之间的值（没抽到的地方)，需要所有抽样值来参与共同构成。</p>
<p><img src="https://pic.imgdb.cn/item/61fa38172ab3f51d91e73dc1.jpg" style="zoom:70%"></p>
<p>这个问题的理解，与“时域抽样后信号的重建”问题是一样的。但有的人可能会说，时域抽样后信号的重建，我记得是通过理想低通滤波器来推导出重建的内插公式，这里怎么不是呢?<br>如果你能提出这样的疑问，要表扬，说明你“信号与系统”学的不错。其实，图7和图8中的内插公式，完全可以用“时域抽样信号重建连续时间信号的内插公式”相同的推导方法推导出来。感兴趣的同学可以试一下哦。</p>
<h5 id="说明-4"><a href="#说明-4" class="headerlink" title="说明"></a>说明</h5><p><strong>文章转自：个人图书馆/未注明作者，感谢原作者的辛勤付出，如涉及版权，请联系我删除！</strong> </p>
<p><a href="http://www.360doc.com/content/19/0611/18/908538_841808337.shtml">http://www.360doc.com/content/19/0611/18/908538_841808337.shtml</a></p>
<p><a href="http://www.360doc.com/content/19/0611/18/908538_841808319.shtml">http://www.360doc.com/content/19/0611/18/908538_841808319.shtml</a></p>
<p><a href="http://www.360doc.com/content/19/0611/18/908538_841808298.shtml">http://www.360doc.com/content/19/0611/18/908538_841808298.shtml</a></p>
<p><a href="http://www.360doc.com/content/19/0611/18/908538_841808274.shtml">http://www.360doc.com/content/19/0611/18/908538_841808274.shtml</a></p>
<hr>
<h3 id="六、快速傅里叶变换FFT"><a href="#六、快速傅里叶变换FFT" class="headerlink" title="六、快速傅里叶变换FFT"></a>六、快速傅里叶变换FFT</h3><h4 id="一-改进DFT计算的方法"><a href="#一-改进DFT计算的方法" class="headerlink" title="(一) 改进DFT计算的方法"></a>(一) 改进DFT计算的方法</h4><h5 id="1-DFT计算量分析"><a href="#1-DFT计算量分析" class="headerlink" title="1. DFT计算量分析"></a>1. DFT计算量分析</h5><p>对于$N$点长的序列$x(n)$其DFT变换为</p>
<script type="math/tex; mode=display">
正变换：X(k) = \text{DFT}[x(n)] = \sum_{n=0}^{N-1}x(n)e^{-j\frac{2\pi}{N}kn} = \sum_{n=0}^{N-1}x(n)W_N^{nk}</script><script type="math/tex; mode=display">
逆变换：x(n) = \text{IDFT}[X(k)] = \frac{1}{N}\sum_{k=0}^{N-1}X(k)e^{j\frac{2\pi}{N}kn} = \sum_{k=0}^{N-1}X(k)W_N^{-nk}</script><p>DFT正反变换的运算方式和运算量是相同的，后面的分析以DFT正变换为例。</p>
<p>观察正变换和反变换的公式可知，二者的运算方式和运算量是完全相同的。下面的分析均以DFT正变换为例。(顺便说一句，大家要像熟悉自己的手机一样熟悉旋转因子，闭着眼睛都知道它)<br>观察DFT正变换的公式，容易看出：每计算一个点的数据，需要$N$次复数乘法、$N-1$次复数加法，所以$N$点DFT，需要$N$的平方次复数乘法，$N(N-1)$次复数加法。我们知道，DFT的点数，至少要取成序列的长度，当序列长度很长时，运算量巨大！如下表所示。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">序列长度</th>
<th style="text-align:center">复数乘法$N^2$</th>
<th style="text-align:center">复数加法$N(N-1)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">4</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">64</td>
<td style="text-align:center">56</td>
</tr>
<tr>
<td style="text-align:center">64</td>
<td style="text-align:center">4096</td>
<td style="text-align:center">4032</td>
</tr>
<tr>
<td style="text-align:center">1024</td>
<td style="text-align:center">1048576</td>
<td style="text-align:center">1047552</td>
</tr>
<tr>
<td style="text-align:center">2048</td>
<td style="text-align:center">4194304</td>
<td style="text-align:center">4192256</td>
</tr>
</tbody>
</table>
</div>
<p>以1024点为例，复数乘法的次数100万次之多。</p>
<p>1965年，库利(J.w.Cooley)和图基(J.W.Tukey)在《Mathmatics of Computation》上发表了《AnAlgorithm for the Machine Calculation of Complex Fourier Series》，提出一种DFT的快速算法，后人称为快速傅里叶变换(Fast Fourier Transform ——FFT)。</p>
<h5 id="2-改进DFT计算效率的基本途径"><a href="#2-改进DFT计算效率的基本途径" class="headerlink" title="2. 改进DFT计算效率的基本途径"></a>2. 改进DFT计算效率的基本途径</h5><p><img src="https://pic.imgdb.cn/item/61fa3b0a2ab3f51d91ea2f7a.jpg" style="zoom:70%"></p>
<p>$N$点DFT，直接计算，需要$N$的平方次乘法；分成2个$\dfrac{N}{2}$点DFT分别计算，乘法的次数减少了一半；分成4个$\dfrac{N}{4}$点DFT，乘法的次数又减少了一半。如果能够继续下去，前景很让人向往。</p>
<p>为了能够一直分下去，我们限定$N$为2的整数次幂，即：$N=2^M$，称为<strong>基2FFT</strong>。由此可见，<font color="red"><strong>FFT的基本思想是：把长序列分成几个较短的序列</strong></font>。</p>
<p>但怎么分？不能随便分，基本原则：要保证这几个短序列的DFT组合起来后能够很方便地构成原来长序列的DFT。所以DFT快速算法要解决的两个核心问题是：<strong>怎么分？怎么合？</strong></p>
<p>根据分与合的方式不同，有两种基2FFT算法，分别称为:</p>
<ul>
<li>按时间抽取的FFT算法———Decimation-in-Time，简称DIT-FFT。</li>
<li>按频率抽取的FFT算法———Decimation-in-Frequency，简称DIF-FFT。</li>
</ul>
<p>下面我们分别来归纳总结两种基2FFT算法。</p>
<h4 id="二-两种基2FFT算法"><a href="#二-两种基2FFT算法" class="headerlink" title="(二) 两种基2FFT算法"></a>(二) 两种基2FFT算法</h4><h5 id="1-按时间抽取DIT-FFT算法"><a href="#1-按时间抽取DIT-FFT算法" class="headerlink" title="1. 按时间抽取DIT-FFT算法"></a>1. 按时间抽取DIT-FFT算法</h5><p>以第一次分解($N$点分为2个$\dfrac{N}{2}$点)为例来说明算法原理，首先解决怎么分的问题。</p>
<p><img src="https://pic.imgdb.cn/item/61fa3c602ab3f51d91eb7188.jpg" style="zoom:70%"></p>
<p>通俗地说，就是大家列队、报数（从0开始），报偶数的一组，奇数的一组。</p>
<p><img src="https://pic.imgdb.cn/item/61fa3c902ab3f51d91eba2c8.jpg" style="zoom:70%"></p>
<p>然后解决怎么合的问题，我们略过看似艰苦卓绝实际很简单的推导过程，直接上结论：</p>
<p><img src="https://pic.imgdb.cn/item/61fa3cbd2ab3f51d91ebcde6.jpg" style="zoom:70%"></p>
<p>公式不好看，有人画了一幅图，并且起了个好听的名字：<strong>蝶形运算符号</strong>。下面的动图演示了蝶形运算的过程:</p>
<p><img src="https://pic.imgdb.cn/item/61fa3fae2ab3f51d91ee90ff.gif"></p>
<p>以8点长序列为例，我们来看分解为2个4点长DFT，是如何通过蝶形运算合成8点DFT的： </p>
<p><img src="https://pic.imgdb.cn/item/61fa3ff22ab3f51d91eecd84.gif"></p>
<p>经过第一次分解之后，总的运算量=两个N/2点DFT的运算+N/2个蝶形的运算。而每次蝶形运算，只需要1次乘法，2次加法。所以，总的乘法次数为：</p>
<script type="math/tex; mode=display">
\frac{N^2}{2}+\frac{N}{2} \approx \frac{N^2}{2}</script><p>总加法次数为：</p>
<script type="math/tex; mode=display">
N(\frac{N}{2}-1)+N \approx \frac{N^2}{2}</script><p>当$N$很大时，运算量减少了近一半。</p>
<p>这就给了我们信心，说明这种分解思路是可以有效减少运算量的。我们继续分解下去，经过M-1次分解，分解为N/2 个 2 点长序列。</p>
<p>而2点DFT也用蝶形运算来表示（因为计算机最擅长一致而重复的东西），就得到下面的流图：</p>
<p><img src="https://pic.imgdb.cn/item/61fa40ce2ab3f51d91ef97a2.jpg"></p>
<h5 id="2-按频率抽取DIF-FFT算法"><a href="#2-按频率抽取DIF-FFT算法" class="headerlink" title="2. 按频率抽取DIF-FFT算法"></a>2. 按频率抽取DIF-FFT算法</h5><p>仍以第一次分解（$N$点分为2个$\dfrac N2$点）为例来说明算法原理。</p>
<p><img src="https://pic.imgdb.cn/item/61fa41312ab3f51d91effcba.jpg" style="zoom:80%"></p>
<p>以8点长序列为例，我们来看分解为2个4点长DFT，是如何通过蝶形运算合成8点DFT的： </p>
<p><img src="https://pic.imgdb.cn/item/61fa415c2ab3f51d91f02935.gif" style="zoom:120%"></p>
<p>注意到，输出的频率数据，序号是按照偶数一组、奇数一组的顺序排列的，所以这种算法称为：按频率抽取。我们继续分解下去，经过$M-1$次分解，分解为$N/2$ 个 2 点长序列，就得到下面的流图：</p>
<p><img src="https://pic.imgdb.cn/item/61fa419d2ab3f51d91f0698d.jpg"></p>
<h5 id="3-运算量分析"><a href="#3-运算量分析" class="headerlink" title="3. 运算量分析"></a>3. 运算量分析</h5><p>通过前面的分析可见，两种基2FFT算法，运算量是一样的，N点DFT，就分解成了若干个蝶形的运算而已。多少个蝶形呢？序列长度$N=2^M$，共有 $M$级蝶形，每级$\dfrac N2$个蝶形，共$\dfrac{MN}{2}$个。而每个蝶形是1次复数乘法，2次复数加法。所以总的运算量为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">FFT</th>
<th style="text-align:center">DFT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">复数乘法</td>
<td style="text-align:center">$\dfrac{NM}{2} = \dfrac{N}{2}\log_2N$</td>
<td style="text-align:center">$N^2$</td>
</tr>
<tr>
<td style="text-align:center">复数加法</td>
<td style="text-align:center">$NM = N\log_2N$</td>
<td style="text-align:center">$N(N-1)$</td>
</tr>
</tbody>
</table>
</div>
<p>频率作为自然界的一个基本物理量，是很多领域研究的重要内容。人们很早就认识到，用DFT的方法可以有效进行信号的频率分析。但是因为DFT算法运算量很大，在数字计算机发明以前，运算效率普遍很低的情况下，DFT也更多的是一种理论分析工具，很难被用于实际的信号处理。</p>
<p>FFT的出现，破解了这一历史性难题，极大地促进了数字信号处理这门学科的应用和发展。有人甚至以FFT算法提出的1965年作为数字信号处理这门学科的诞生之年。</p>
<h5 id="4-算法特点"><a href="#4-算法特点" class="headerlink" title="4. 算法特点"></a>4. 算法特点</h5><p>在计算机看来，这两种算法是非常相像的。两者互为转置。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">DIF-FFT</th>
<th style="text-align:center">DIF-FFT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">同址运算</td>
<td style="text-align:center">将蝶形运算的结果仍然保存在原输入量的存储单元中</td>
<td style="text-align:center">将蝶形运算的结果仍然保存在原输入量的存储单元中</td>
</tr>
<tr>
<td style="text-align:center">输入/输出顺序</td>
<td style="text-align:center">输入倒位序,输出自然顺序</td>
<td style="text-align:center">输入自然顺序，输出倒位序</td>
</tr>
<tr>
<td style="text-align:center">蝶形运算</td>
<td style="text-align:center">先乘旋转因子，后加减第$m$级节点间距离:$2^{m-1}$</td>
<td style="text-align:center">先加减，后乘旋转因子第$m$级节点间距离:$2^{L-m}$</td>
</tr>
</tbody>
</table>
</div>
<p>首先来看第一个特点：同址运算（又称同位运算或原位运算），每完成一个蝶形运算，输入的两个数据就没有用的，这就意味着，不需要再重新开辟新的存储单元来保存输出数据，计算结果仍保留在原输入数据占据的存储单元即可。</p>
<p>再来看第二个特点：输入/输出数据的顺序。这是两种算法的不同之处。以DIT-FFT为例来说明为什么会输入倒位序。</p>
<p>还是以8点长数据为例，输入数据的正常顺序是$x(0)、x(1)、x(2)……x(7)$，我们称之为 自然顺序。按照序号的奇偶分为两组，第一组是$x(0)、x(2)、x(4)、x(6)$，第二组是$x(1)、x(3)、x(5)、x(7)$。每个新的组再重新排队报数，按奇偶分，第一组又分成两个组，分别是$x(0)、x(4)$和$x(2)、x(6)$，第二组分成两个组，分别是$x(1)、x(5)$和$x(3)、x(7)$。</p>
<p>也就是说，8点长序列的DIT-FFT，输入数据的顺序是：$x(0)、x(4)、x(2)、x(6)、x(1)、x(5)、x(3)、x(7)$。这个序号的顺序乍看杂乱无章，其实有规律性。0、1、2、3、4、5、6、7的顺序与0、4、2、6、1、3、5、7有何关系的呢？用二进制来写一目了然，看下面的动图：</p>
<p><img src="https://pic.imgdb.cn/item/61fa439a2ab3f51d91f24b96.gif"></p>
<p>倒位序，是将二进制数的最高有效位到最低有效位的位序进行颠倒排列而得到的二进制数。 </p>
<p>DIT-FFT算法中，对时域序列按照序号的奇偶进行分解，造成输入序列的序号按照倒位序排列。</p>
<p>最后再说一说蝶形运算的规律。两种FFT算法，最终都是转换成了M列、每列N/2个、一共MN/2个蝶形运算。但二者蝶形运算的规律有差异。</p>
<ul>
<li>第一个差异：基本蝶形不同。DIT是先乘旋转因子，再加或减；而DIF是先加或减，再乘旋转因子。</li>
<li>第二个差异：两种算法，蝴蝶翅膀的距离（即节点间的距离）和旋转因子的数目恰好相反。</li>
</ul>
<p>仔细观察两种算法的流图，我们会发现，二者互为转置。</p>
<h4 id="三-其他FFT算法简介"><a href="#三-其他FFT算法简介" class="headerlink" title="(三) 其他FFT算法简介"></a>(三) 其他FFT算法简介</h4><h5 id="1-混合基FFT"><a href="#1-混合基FFT" class="headerlink" title="1. 混合基FFT"></a>1. 混合基FFT</h5><p><img src="https://pic.imgdb.cn/item/61fa445b2ab3f51d91f30d11.png" style="zoom:80%"></p>
<h5 id="2-Chirp-z变换"><a href="#2-Chirp-z变换" class="headerlink" title="2. Chirp-z变换"></a>2. Chirp-z变换</h5><p>实际应用中，有时只对信号的某一频段感兴趣，即只需要计算单位圆上某一段的频谱值，而不需要计算[0，Π]区间的所有频谱采样值。此时，可以用”Chirp-z变换“（CZT）。</p>
<p>适用场合：窄带信号的DFT。</p>
<h5 id="3-Goertzel算法"><a href="#3-Goertzel算法" class="headerlink" title="3. Goertzel算法"></a>3. Goertzel算法</h5><p>在某些应用场合，只需计算很少几个频率点的频谱值。例如，双音多频信号（DTMF）的检测。此时可以采用卡泽尔（Goertzel）算法。</p>
<p>除此之外，傅里叶变换的快速算法还有很多种。不过应用最广泛的依然能是基2FFT算法，它是数字信号处理最经典算法之一，几乎各种主流的计算机编程语言都有现成的函数可以调用。不同型号的芯片，硬件开发商也都会给出优化后的FFT算法源代码，一般情况下直接调用就可以。</p>
<h5 id="说明-5"><a href="#说明-5" class="headerlink" title="说明"></a>说明</h5><p><strong>文章转自：个人图书馆/未注明作者，感谢原作者的辛勤付出，如涉及版权，请联系我删除！</strong> <a href="http://www.360doc.com/content/19/0611/18/908538_841808253.shtml">http://www.360doc.com/content/19/0611/18/908538_841808253.shtml</a></p>
<hr>
<h3 id="七、FFT算法的应用"><a href="#七、FFT算法的应用" class="headerlink" title="七、FFT算法的应用"></a>七、FFT算法的应用</h3><h5 id="说明-6"><a href="#说明-6" class="headerlink" title="说明"></a>说明</h5><p><strong>文章转自：个人图书馆/未注明作者，感谢原作者的辛勤付出，如涉及版权，请联系我删除！</strong> </p>
<p><a href="http://www.360doc.com/content/19/0611/18/908538_841808231.shtml">http://www.360doc.com/content/19/0611/18/908538_841808231.shtml</a></p>
<p><a href="http://www.360doc.com/content/19/0611/18/908538_841808103.shtml">http://www.360doc.com/content/19/0611/18/908538_841808103.shtml</a></p>
<hr>
<h3 id="八、Matlab中的fftshift函数"><a href="#八、Matlab中的fftshift函数" class="headerlink" title="八、Matlab中的fftshift函数"></a>八、Matlab中的fftshift函数</h3><p>一般情况下，在<code>fft()</code>之间先对输入数据进行<code>fftshift()</code>处理是没有必要的。没有什么必然性。<code>fftshift()</code>这个函数的存在的目的并不是为了这个，单纯地就是上面所说的为了让频谱观测显得更自然一些而已。 </p>
<p>这里先给出一个简单的解释（有时间再来补详细的解释）。这是从根本上来说是因为<code>fft()</code>处理的离散数据，进行的是离散傅里叶变换（DFT）。如果所要解析的数据本身就是一个离散周期性信号，那<code>fft()</code>给出的结果就反映了真实情况。但是现实应用中，所分析的信号并不是离散周期性信号，而我们所能做的又只能是调用<code>fft()</code>执行离散傅里叶变换进行分析，这种情况下<code>fft()</code>结果并没有完全反映真实情况，而是一种近似的或者“变形”的体现。这个时候我们就需要对<code>fft()</code>结果进行合理的解释间接地得到真实的情况，某种意义上<code>fftshift()</code>将频域数据前后半颠倒以使得频域数据显示出来的效果显得更自然也是属于这种情况。</p>
<p>比如(4)中给出的例子是对对称时域信号进行调用<code>fft()</code>处理，然后将其结果与傅里叶变换的理论解析结果进行对比。原始信号的时间区间是$[-\dfrac{T}{2}, \dfrac{T}{2}]$，这个数据采样后直接用<code>fft()</code>进行处理的话，由于<code>fft()</code>自然地认为数据是从<code>t=0</code>开始，所以可以理解为<code>fft()</code>的输入数据其实是将原始信号在时域上右移$\dfrac{T}{2}$所得到的信号，而<code>fft()</code>的变换结果则代表着这个时域上右移$\dfrac{T}{2}$所得到的信号的周期性拓展后的信号的离散时间傅里叶变换（DTFT）的一个周期内的数据。接下来从这个结果进行解析可反推得到原始信号的DFT结果。在<code>fft()</code>之前做<code>fftshift()</code>可以看作是一个小的trick, as an alternative to the post-analysis of the fft() result, without physical significance and necessity.</p>
<p>参考链接8.1：<a href="https://zhuanlan.zhihu.com/p/396863191">fftshift讲解 - 信号处理小王子的文章 - 知乎</a></p>
<p>参考链接8.2：<a href="https://www.cnblogs.com/zxhyxiao/p/12359243.html">关于MATLAB中fft，fftshift，fftshift(fft(fftshift(x)))，FFT要乘以dx 等问题理解 - 博客园</a></p>
<p>参考链接8.3：<a href="https://blog.csdn.net/chenxy_bwave/article/details/119185124">Matlab fftshift and ifftshift and some confusions - 笨牛慢耕 - CSDN</a></p>
<p>参考链接8.4：<a href="https://www.cnblogs.com/limanjihe/p/10014142.html">fftshift函数详解 - 博客园</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>信号处理与雷达</category>
        <category>信号与系统</category>
      </categories>
      <tags>
        <tag>数字信号</tag>
        <tag>频谱分析</tag>
      </tags>
  </entry>
  <entry>
    <title>英语语法学习</title>
    <url>/2022/04/29/English-grammar-learning/</url>
    <content><![CDATA[<h3 id="第〇章-英语语法体系框架概述"><a href="#第〇章-英语语法体系框架概述" class="headerlink" title="第〇章 英语语法体系框架概述"></a>第〇章 英语语法体系框架概述</h3><p>两个句子最基本的成分是“主语”+“谓语”</p>
<p>主语：一般是指人或物，包括抽象和具体</p>
<p>谓语：动作(动词)/发生了什么事</p>
<p>有哪些不同类型的动作：</p>
<ul>
<li>可以独立完成的动作——主语+不及物动词</li>
</ul>
<p>没有承受者的动词叫做“不及物动词”</p>
<ul>
<li>有1个动作承受者——主语+(单)及物动词+宾语</li>
</ul>
<p>有承受者的动词称为“(单)及物动词”，后接的动作承受者就是“宾语”</p>
<ul>
<li>有个动作承受者——主语+(双)及物动词+间接宾语+直接宾语</li>
</ul>
<p>e.g. My father teaches me English</p>
<p>上面的句子中及物动词为“teach”，教授的知识是英语，而知识的接收对象是“me”，这种情况将宾语分为“直接宾语”(English)，“间接宾语”(me)。此时动词称为“(双)及物动词”</p>
<ul>
<li>只有1个动作承受者(但有补充内容)——主语+复杂及物动词+宾语+宾语补足语</li>
</ul>
<p>e.g. I consider you smart</p>
<p>上面例句中“smart“是补充信息，语法上称为”宾语补足语/补语”，需要宾语补足语的动词称为“复杂及物动词”</p>
<ul>
<li>非动作——主语+系动词+(主语)补语/表语</li>
</ul>
<p>e.g. He is tall. She looks nice.</p>
<p>像例句中is、look等赋予主语某种性质状态的“划等号”的动词称为连系动词(系动词)。系动词后面的信息也属于补语，但是这里是“主语补足语”(又称“表语”)</p>
<p>综上，可以做个小总结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>英语基本句型</th>
</tr>
</thead>
<tbody>
<tr>
<td>主语+动词</td>
</tr>
<tr>
<td>主语+动词+宾语</td>
</tr>
<tr>
<td>主语+动词+间接宾语+直接宾语</td>
</tr>
<tr>
<td>主语+动词+宾语+宾语补足语</td>
</tr>
<tr>
<td>主语+动词+(主语)补语/表语</td>
</tr>
</tbody>
</table>
</div>
<p>注意：严格说来，谓语不同于谓语动词，句子主语后面剩余的部分统称谓语，而谓语动词是谓语的一部分。</p>
<p>定语：主要用来修饰主语或宾语</p>
<p>状语：主要用来修饰谓语动词</p>
<p>e.g. The rabbit ate quickly.</p>
<p>上面例句中quickly就是ate的状语</p>
<p>同位语：主要用来再把主语或者宾语说一遍</p>
<p>e.g. The rabbit, an English teacher, eats carrots.</p>
<p>“an English teacher”就是把主语The rabbit再以不同的方式说一遍</p>
<p>以上是简单句的基本组成成分，但是英语中不可能总是简单句子，更多的是简单句子的叠加，叠加的句子分为两种：</p>
<ul>
<li>compound sentence/复合句/并列句：句子的简单叠加，是并列关系</li>
<li>complex sentence/复杂句：把一个句子套在另一个句子里，作为另一个句子的成分</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/626bda20239250f7c549ac0e.jpg" style="zoom:30%"></p>
<p>从句可以简单理解为把简单句子修改一下来充当另一个句子的句子成分</p>
<p><img src="https://pic.imgdb.cn/item/626be38a239250f7c56180ca.jpg" style="zoom:50%"></p>
<p>词性</p>
<p>名词：表人和物</p>
<p>动词：表示动作</p>
<p>冠词：说明人和事物</p>
<p>代词：替代人和物</p>
<p>形容词：形容人和物</p>
<p>数词：表示数量</p>
<p>副词：修饰动作或形容词</p>
<p>介词：表示和其他词关系的词</p>
<p>连词：连接词和句：and、but、because</p>
<p>叹词：表感叹</p>
<p>谓语动词的“三大本领”：</p>
<ul>
<li>表示动作的时间：比如动作可能发生在过去、现在、将来</li>
<li>表示动作的状态：动作可能是已经完成的状态、正在进行的状态等</li>
</ul>
<p>动作的时间和状态组合一起就是动词的时态</p>
<p><img src="https://pic.imgdb.cn/item/626beae2239250f7c57480cf.jpg"></p>
<p><img src="/2022/04/29/English-grammar-learning/Users\27275\AppData\Local\Temp\1651239758116.png" alt="1651239758116"></p>
<ul>
<li>表示动作的假设、情感、语气(并不是指的是平时说话的语气)等<ul>
<li>虚拟语气：用来表示意愿、和事实相反的假设</li>
<li>陈述语气：</li>
<li>祈使语气：</li>
</ul>
</li>
</ul>
<p>助动词：协助动词充分发挥作用</p>
<p><img src="https://pic.imgdb.cn/item/626bee5e239250f7c57e161c.jpg"></p>
<p>但是有一些助动词喜欢“伪装”，它们在做助动词时(没有实义，只是帮助谓语动词)，但是它们还可以做实义动词甚至其他词性，比如：can可以是“易拉罐”，have可以是“拥有”…</p>
<p>非谓语动词：</p>
<p>当动词在句子中充当除谓语动词以外的词时(主语、宾语、宾语补语、主语补语、定语等)称为非谓语动词，非谓语动词几乎可以取代所有的从句，从而简化句子，但此时这样的动词也就不再具有表示时间、状态、语态、语气的功能了</p>
<p><img src="https://pic.imgdb.cn/item/626bf04d239250f7c5836bc4.jpg"></p>
<p><img src="https://pic.imgdb.cn/item/626bf0d3239250f7c584e808.jpg"></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>英语基础</category>
      </categories>
      <tags>
        <tag>英语语法</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习_吴恩达_Part_3</title>
    <url>/2023/06/15/Foundation-of-DeepLearning-3/</url>
    <content><![CDATA[<h3 id="第七章-超参数调试、Batch正则化和编程框架"><a href="#第七章-超参数调试、Batch正则化和编程框架" class="headerlink" title="第七章 超参数调试、Batch正则化和编程框架"></a>第七章 超参数调试、Batch正则化和编程框架</h3><h4 id="7-1-超参数调试处理"><a href="#7-1-超参数调试处理" class="headerlink" title="7.1 超参数调试处理"></a>7.1 超参数调试处理</h4><p>深度神经网络需要调试的超参数（Hyperparameters）较多，包括：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>参数</strong></th>
<th style="text-align:left"><strong>意义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><script type="math/tex">\alpha</script></td>
<td style="text-align:left">学习因子/速率</td>
</tr>
<tr>
<td style="text-align:left"><script type="math/tex">\beta</script></td>
<td style="text-align:left">动量梯度下降因子</td>
</tr>
<tr>
<td style="text-align:left"><script type="math/tex">\beta_1, \beta_2, \varepsilon</script></td>
<td style="text-align:left">Adam算法参数</td>
</tr>
<tr>
<td style="text-align:left">#layers</td>
<td style="text-align:left">神经网络层数</td>
</tr>
<tr>
<td style="text-align:left">#hidden units</td>
<td style="text-align:left">各层隐藏神经元个数</td>
</tr>
<tr>
<td style="text-align:left">learning rate decay</td>
<td style="text-align:left">学习因子下降参数</td>
</tr>
<tr>
<td style="text-align:left">mini-batch size</td>
<td style="text-align:left">批量训练样本数的包含样本个数</td>
</tr>
<tr>
<td style="text-align:left">。。。</td>
<td style="text-align:left">。。。</td>
</tr>
</tbody>
</table>
</div>
<p>超参数之间也有重要性差异。通常来说，学习因子<script type="math/tex">α</script>是最重要的超参数，也是需要重点调试的超参数。动量梯度下降因子<script type="math/tex">β</script>、各隐藏层神经元个数#hidden units和mini-batch size的重要性仅次于<script type="math/tex">α</script>。然后就是神经网络层数#layers和学习因子下降参数learning rate decay。最后，Adam算法的三个参数<script type="math/tex">β_1, β_2, ε</script>一般常设置为<script type="math/tex">0.9，0.999</script>和<script type="math/tex">10^{−8}</script>，不需要反复调试。当然，这里超参数重要性的排名并不是绝对的，具体情况，具体分析。</p>
<p>如何选择和调试超参数？传统的机器学习中，我们对每个参数等距离选取任意个数的点，然后，分别使用不同点对应的参数组合进行训练，最后根据验证集上的表现好坏，来选定最佳的参数。例如有两个待调试的参数，分别在每个参数上选取5个点，这样构成了5x5=25中参数组合，如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648ae96e1ddac507ccdfd1c7.png" style="zoom:50%"></p>
<p>这种做法在参数比较少的时候效果较好。但是在深度神经网络模型中，我们一般不采用这种均匀间隔取点的方法，比较好的做法是使用<strong>随机选择</strong>。也就是说，对于上面这个例子，我们随机选择25个点，作为待调试的超参数，如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648aea321ddac507cce264c1.png" style="zoom:50%"></p>
<p>随机化选择参数的目的是为了尽可能地得到更多种参数组合。还是上面的例子，如果使用均匀采样的话，每个参数只有5种情况；而使用随机采样的话，每个参数有25种可能的情况，因此更有可能得到最佳的参数组合。</p>
<p>这种做法的另一个好处就是对重要性不同的参数之间的选择效果更好。设hyperparameter1为<script type="math/tex">\alpha</script>，hyperparameter2为<script type="math/tex">\varepsilon</script>，显然二者的重要性是不一样的。如果使用第一种均匀采样的方法，<script type="math/tex">\varepsilon</script>的影响很小，相当于只选择了<script type="math/tex">5</script>个<script type="math/tex">\alpha</script>值。而如果使用第二种随机采样的方法，<script type="math/tex">\varepsilon</script>和<script type="math/tex">\alpha</script>都有可能选择<script type="math/tex">25</script>种不同值。这大大增加了<script type="math/tex">\alpha</script>调试的个数，更有可能选择到最优值。其实，在实际应用中完全不知道哪个参数更加重要的情况下，随机采样的方式能有效解决这一问题，但是均匀采样做不到这点。</p>
<p>在经过随机采样之后，我们可能得到某些区域模型的表现较好。为了得到更精确的最佳参数，我们应该继续对选定的区域进行<strong>由粗到细</strong>的采样(coarse to fine sampling scheme)。也就是放大表现较好的区域，再对此区域做更密集的随机采样。例如，对下图中右下角的方形区域再做25点的随机采样，以获得最佳参数。</p>
<p><img src="https://pic.imgdb.cn/item/648aea541ddac507cce2d585.png" style="zoom:50%"></p>
<h4 id="7-2-为超参数选择合适的范围"><a href="#7-2-为超参数选择合适的范围" class="headerlink" title="7.2 为超参数选择合适的范围"></a>7.2 为超参数选择合适的范围</h4><p>上一部分讲的调试参数使用随机采样，对于某些超参数是可以进行尺度均匀采样的，但是某些超参数需要选择不同的合适尺度进行随机采样。</p>
<p>例如对于超参数#layers和#hidden units，都是正整数，是可以进行均匀随机采样的，即<strong>超参数每次变化的尺度都是一致的</strong>(如每次变化为1，犹如一个刻度尺一样，刻度是均匀的)。</p>
<p>但是，对于某些超参数，可能需要<strong>非均匀随机采样(即非均匀刻度尺)</strong>。例如超参数<script type="math/tex">\alpha</script>，待调范围是<script type="math/tex">[0.0001, 1]</script>。如果使用均匀随机采样，那么有<script type="math/tex">90\%</script>的采样点分布在<script type="math/tex">[0.1, 1]</script>之间，只有<script type="math/tex">10\%</script>分布在<script type="math/tex">[0.0001, 0.1]</script>之间。这在实际应用中是不太好的，因为最佳的<script type="math/tex">\alpha</script>值可能主要分布在<script type="math/tex">[0.0001, 0.1]</script>之间，而<script type="math/tex">[0.1, 1]</script>范围内<script type="math/tex">\alpha</script>值效果并不好。因此我们更关注的是区间<script type="math/tex">[0.0001, 0.1]</script>，应该在这个区间内细分更多刻度。</p>
<p>通常的做法是将<strong>linear scale转换为log scale</strong>，将均匀尺度转化为非均匀尺度，然后再在log scale下进行均匀采样。这样<script type="math/tex">[0.0001, 0.001]</script>，<script type="math/tex">[0.001, 0.01]</script>，<script type="math/tex">[0.01, 0.1]</script>，<script type="math/tex">[0.1, 1]</script>各个区间内随机采样的超参数个数基本一致，也就扩大了之前<script type="math/tex">[0.0001, 0.1]</script>区间内采样值个数。</p>
<p><img src="https://pic.imgdb.cn/item/648aea921ddac507cce3abf1.png"></p>
<p>一般解法是，如果线性区间为<script type="math/tex">[a, b]</script>，令<script type="math/tex">m=\log(a)，n=\log(b)</script>，则对应的<script type="math/tex">\log</script>区间为<script type="math/tex">[m,n]</script>。对<script type="math/tex">\log</script>区间的<script type="math/tex">[m,n]</script>进行随机均匀采样，然后得到的采样值<script type="math/tex">r</script>，最后反推到线性区间，即<script type="math/tex">10^r</script>就是最终采样的超参数。相应的Python语句为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = np.log10(a)</span><br><span class="line">n = np.log10(b)</span><br><span class="line">r = np.random.rand()</span><br><span class="line">r = m + (n-m)*r</span><br><span class="line">r = np.power(<span class="number">10</span>,r)</span><br></pre></td></tr></table></figure>
<p>除了<script type="math/tex">\alpha</script>外，动量梯度因子<script type="math/tex">\beta</script>也是一样，在超参数调试的时候需要进行非均匀采样。一般<script type="math/tex">\beta</script>的取值范围<script type="math/tex">[0.9, 0.999]</script>，<script type="math/tex">1−β</script>的取值范围就在<script type="math/tex">[0.001, 0.1]</script>。那么直接对<script type="math/tex">1−β</script>在<script type="math/tex">[0.001, 0.1]</script>区间内进行<script type="math/tex">\log</script>变换即可。</p>
<blockquote>
<p>这里解释下为什么<script type="math/tex">\beta</script>也需要向<script type="math/tex">\alpha</script>那样做非均匀采样。假设<script type="math/tex">\beta</script>从<script type="math/tex">0.9000</script>变化为<script type="math/tex">0.9005</script>，那么<script type="math/tex">\dfrac 1{1−β}</script>基本没有变化。但假设<script type="math/tex">\beta</script>从<script type="math/tex">0.9990</script>变化为<script type="math/tex">0.9995</script>，那么<script type="math/tex">\dfrac 1{1−β}</script>前后差别<script type="math/tex">1000</script>。<script type="math/tex">\beta</script>越接近<script type="math/tex">1</script>，指数加权平均的个数越多，变化越大。所以对<script type="math/tex">\beta</script>接近<script type="math/tex">1</script>的区间，应该采集得更密集一些。</p>
</blockquote>
<h4 id="7-3-超参数训练的实践：Pandas-vs-Caviar"><a href="#7-3-超参数训练的实践：Pandas-vs-Caviar" class="headerlink" title="7.3 超参数训练的实践：Pandas vs. Caviar"></a>7.3 超参数训练的实践：<strong>Pandas vs. Caviar</strong></h4><p>经过调试选择完最佳的超参数并不是一成不变的，一段时间之后（例如一个月），需要根据新的数据和实际情况，再次调试超参数，以获得实时的最佳模型。</p>
<p>在训练深度神经网络时，一种情况是受计算能力所限，我们只能对一个模型进行训练，调试不同的超参数，使得这个模型有最佳的表现。我们称之为Babysitting one model。另外一种情况是可以对多个模型同时进行训练，每个模型上调试不同的超参数，根据表现情况，选择最佳的模型。我们称之为Training many models in parallel。</p>
<p><img src="https://pic.imgdb.cn/item/648aeab31ddac507cce41e0f.png" style="zoom:50%"></p>
<p>因为第一种情况只使用一个模型，所以类比做Panda approach；第二种情况同时训练多个模型，类比做Caviar  approach。使用哪种模型是由计算资源、计算能力所决定的。一般来说，对于非常复杂或者数据量很大的模型，使用Panda  approach更多一些。</p>
<h4 id="7-4-正则化网络的激活函数"><a href="#7-4-正则化网络的激活函数" class="headerlink" title="7.4 正则化网络的激活函数"></a>7.4 正则化网络的激活函数</h4><p>Sergey Ioffe和Christian Szegedy两位学者提出了Batch Normalization方法。Batch Normalization不仅可以让调试超参数更加简单，而且可以让神经网络模型更加“健壮”。也就是说较好模型可接受的超参数范围更大一些，包容性更强，使得更容易去训练一个深度神经网络。接下来，我们就来介绍什么是Batch Normalization，以及它是如何工作的。</p>
<p>之前，我们在<strong>第五章</strong>中提到过在训练神经网络时，<strong>标准化输入可以提高训练的速度</strong>。方法是对训练数据集进行<strong>归一化</strong>的操作，即将原始数据减去其均值<script type="math/tex">\mu</script>后，再除以其方差<script type="math/tex">σ^2</script>。但是标准化输入只是对输入进行了处理，那么对于神经网络，又该如何对各隐藏层的输入进行标准化处理呢？</p>
<p>其实在神经网络中，第<script type="math/tex">l</script>层隐藏层的输入就是第<script type="math/tex">l−1</script>层隐藏层的输出<script type="math/tex">A^{[l−1]}</script>。对<script type="math/tex">A^{[l−1]}</script>进行标准化处理，从原理上来说可以提高<script type="math/tex">W^{[l]}</script>和<script type="math/tex">b^{[l]}</script>的训练速度和准确度。这种对各隐藏层的标准化处理就是Batch Normalization。值得注意的是，实际应用中，一般是对<script type="math/tex">Z^{[l−1]}</script>进行标准化处理而不是<script type="math/tex">A^{[l−1]}</script>，其实差别不是很大。</p>
<p>Batch Normalization对第<script type="math/tex">l</script>层隐藏层的输入<script type="math/tex">Z^{[l−1]}</script>做如下标准化处理，忽略上标<script type="math/tex">[l−1]</script>：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mu=\frac{1}{m} \sum_{i} z^{(i)} \\
\sigma^{2}=\frac{1}{m} \sum_{i}\left(z_{i}-\mu\right)^{2} \\z_{\text {norm }}^{(i)}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^{2}+\varepsilon}}
\end{gathered}</script><p>其中，<script type="math/tex">m</script>是单个mini-batch包含样本个数，<script type="math/tex">\varepsilon</script>是为了防止分母为零，可取值<script type="math/tex">10^{−8}</script>。这样，使得该隐藏层的所有输入<script type="math/tex">z^{(i)}</script>均值为<script type="math/tex">0</script>，方差为<script type="math/tex">1</script>。但是，大部分情况下并不希望所有的<script type="math/tex">z^{(i)}</script>均值都为<script type="math/tex">0</script>，方差都为<script type="math/tex">1</script>，也不太合理。通常需要对<script type="math/tex">z^{(i)}</script>进行进一步处理：</p>
<script type="math/tex; mode=display">
\tilde{z}^{(i)}=\gamma \cdot z_{\text {norm }}^{(i)}+\beta</script><p>上式中，<script type="math/tex">γ</script>和<script type="math/tex">\beta</script>是learnable parameters，类似于<script type="math/tex">W</script>和<script type="math/tex">b</script>一样，可以通过梯度下降等算法求得。这里，<script type="math/tex">γ</script>和<script type="math/tex">\beta</script>的作用是让<script type="math/tex">\tilde z^{(i)}</script>的均值和方差为任意值，只需调整其值就可以了。例如，令：</p>
<script type="math/tex; mode=display">
\gamma=\sqrt{\sigma^{2}+\varepsilon}, \quad \beta=u</script><p>则<script type="math/tex">\tilde z^{(i)}=z^{(i)}</script>，即identity function。可见，设置<script type="math/tex">γ</script>和<script type="math/tex">\beta</script>为不同的值，可以得到任意的均值和方差。</p>
<p>这样，通过Batch Normalization，对隐藏层的各个<script type="math/tex">z^{[l](i)}</script>进行标准化处理，得到<script type="math/tex">\tilde z^{[l](i)}</script>，替代<script type="math/tex">z^{[l](i)}</script>。</p>
<p>注意，<strong>输入标准化</strong>处理Normalizing inputs和<strong>隐藏层标准化</strong>处理Batch  Normalization是有区别的。Normalizing inputs使所有输入的均值为<script type="math/tex">0</script>，方差为<script type="math/tex">1</script>。而Batch  Normalization可使各隐藏层输入的<strong>均值和方差为任意值</strong>。实际上，从激活函数的角度来说，如果各隐藏层的输入均值在靠近<script type="math/tex">0</script>的区域即处于激活函数的线性区域，这样不利于训练好的非线性神经网络，得到的模型效果也不会太好。这也解释了为什么需要用<script type="math/tex">γ</script>和<script type="math/tex">\beta</script>是来对<script type="math/tex">z^{[l](i)}</script>作进一步处理。</p>
<h4 id="7-5-将Batch-Norm拟合进神经网络"><a href="#7-5-将Batch-Norm拟合进神经网络" class="headerlink" title="7.5 将Batch Norm拟合进神经网络"></a>7.5 将Batch Norm拟合进神经网络</h4><p>我们已经知道了如何对某单一隐藏层的所有神经元进行Batch Norm，接下来将研究如何把Bath Norm应用到整个神经网络中。</p>
<p>对于<script type="math/tex">L</script>层神经网络，经过Batch Norm的作用，整体流程如下：</p>
<p><img src="https://pic.imgdb.cn/item/648aeae71ddac507cce4e5b1.png"></p>
<p>实际上，Batch Norm经常使用在mini-batch上，这也是其名称的由来。值得注意的是，因为Batch Norm对各隐藏层<script type="math/tex">Z^{[l]}=W^{[l]} A^{[l-1]}+b^{[l]}</script>有去均值的操作，所以这里的常数项<script type="math/tex">b^{[l]}</script>可以消去，其数值效果完全可以由<script type="math/tex">\tilde Z^{[l]}</script>中的<script type="math/tex">\beta</script>来实现。因此，我们在使用Batch Norm的时候，可以忽略各隐藏层的常数项<script type="math/tex">b^{[l]}</script>。在使用梯度下降算法时，分别对<script type="math/tex">W^{[l]}，β^{[l]}~ \text 和 ~γ^{[l]}</script>进行迭代更新。除了传统的梯度下降算法之外，还可以使用我们之前介绍过的动量梯度下降、RMSprop或者Adam等优化算法。</p>
<h4 id="7-6-Batch-Norm为什么有效"><a href="#7-6-Batch-Norm为什么有效" class="headerlink" title="7.6 Batch Norm为什么有效"></a>7.6 Batch Norm为什么有效</h4><p>我们可以把输入特征做均值为<script type="math/tex">0</script>，方差为<script type="math/tex">1</script>的规范化处理，来加快学习速度。而Batch Norm也是对隐藏层各神经元的输入做类似的规范化处理。总的来说，Batch Norm不仅能够提高神经网络训练速度，而且能让神经网络的权重<script type="math/tex">W</script>的更新更加“稳健”，尤其在深层神经网络中更加明显。比如神经网络很后面的<script type="math/tex">W</script>对前面的<script type="math/tex">W</script><strong>包容性更强</strong>，即前面的<script type="math/tex">W</script>的变化对后面<script type="math/tex">W</script>造成的影响很小，整体网络更加健壮。</p>
<p>举个例子来说明，假如用一个浅层神经网络(类似逻辑回归)来训练识别猫的模型。如下图所示，提供的所有猫的训练样本都是黑猫。然后，用这个训练得到的模型来对各种颜色的猫样本进行测试，测试的结果可能并不好。其原因是训练样本不具有一般性(即不是所有的猫都是黑猫)，这种训练样本(黑猫)和测试样本(猫)分布的变化称之为<strong>covariate shift</strong>。</p>
<p><img src="https://pic.imgdb.cn/item/648aeafd1ddac507cce53503.png" style="zoom:50%"></p>
<p>对于这种情况，如果实际应用的样本与训练样本分布不同，即发生了covariate shift，则一般是要对模型<strong>重新训练</strong>的。在神经网络，尤其是深度神经网络中，covariate shift会导致模型预测效果变差，重新训练的模型各隐藏层的<script type="math/tex">W^{[l]}</script>和<script type="math/tex">B^{[l]}</script>均产生偏移、变化。而Batch Norm的作用恰恰是减小covariate shift的影响，让模型变得更加健壮，鲁棒性更强。Batch Norm减少了各层<script type="math/tex">W^{[l]}</script>、<script type="math/tex">B^{[l]}</script>之间的耦合性，让各层更加独立，实现自我训练学习的效果。也就是说，如果输入发生covariate shift，那么因为Batch Norm的作用，对个隐藏层输出<script type="math/tex">Z^{[l]}</script>进行均值和方差的归一化处理，<script type="math/tex">W^{[l]}</script>和<script type="math/tex">B^{[l]}</script>更加稳定，使得原来的模型也有不错的表现。针对上面这个黑猫的例子，如果我们使用深层神经网络，使用Batch Norm，那么该模型对花猫的识别能力应该也是不错的。</p>
<p>从另一个方面来说，<strong>Batch Norm也起到轻微的正则化</strong>(regularization)效果。具体表现在：</p>
<ul>
<li><strong>每个mini-batch都进行均值为</strong><script type="math/tex">0</script><strong>，方差为</strong><script type="math/tex">1</script><strong>的归一化操作</strong></li>
<li><strong>每个mini-batch中，对各个隐藏层的</strong><script type="math/tex">Z^{[l]}</script><strong>添加了随机噪声，效果类似于Dropout</strong></li>
<li><strong>mini-batch越小，正则化效果越明显</strong><br>但是，Batch Norm的正则化效果比较微弱，正则化也不是Batch Norm的主要功能。</li>
</ul>
<h4 id="7-7-测试时的Batch-Norm"><a href="#7-7-测试时的Batch-Norm" class="headerlink" title="7.7 测试时的Batch Norm"></a>7.7 测试时的Batch Norm</h4><p>训练过程中，Batch Norm是对单个mini-batch进行操作的，但在测试过程中，如果是单个样本，该如何使用Batch Norm进行处理呢？</p>
<p>首先，回顾一下训练过程中Batch Norm的主要过程：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mu=\frac{1}{m} \sum_{i} z^{(i)} \\
\sigma^{2}=\frac{1}{m} \sum_{i}\left(z^{(i)}-\mu\right)^{2} \\
z_{\text {norm }}^{(i)}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^{2}+\varepsilon}} \\
\tilde{z}^{(i)}=\gamma \cdot z_{\text {norm }}^{(i)}+\beta
\end{gathered}</script><p>其中，<script type="math/tex">\mu</script>和<script type="math/tex">σ^2</script>是对单个mini-batch中所有<script type="math/tex">m</script>个样本求得的。在测试过程中，如果只有一个样本，求其均值和方差是没有意义的，就需要对<script type="math/tex">\mu</script>和<script type="math/tex">σ^2</script>进行估计。估计的方法很多，理论上可以将所有训练集放入最终的神经网络模型中，然后将每个隐藏层计算得到的<script type="math/tex">\mu^{[l]}</script>和<script type="math/tex">σ^{2[l]}</script>直接作为测试过程的<script type="math/tex">\mu</script>和<script type="math/tex">σ^2</script>来使用。但是，实际应用中一般不使用这种方法，而是使用我们之前介绍过的指数加权平均(exponentially weighted average)的方法来预测测试过程单个样本的<script type="math/tex">\mu</script>和<script type="math/tex">σ^2</script>。</p>
<p>指数加权平均的做法为，对第<script type="math/tex">l</script>层隐藏层，考虑所有mini-batch在该隐藏层下的<script type="math/tex">\mu^{[l]}</script>和<script type="math/tex">σ^{2[l]}</script>，然后用指数加权平均的方式来预测得到当前单个样本的<script type="math/tex">\mu^{[l]}</script>和<script type="math/tex">σ^{2[l]}</script>。这样就实现了对测试过程单个样本的均值和方差估计。最后，再利用训练过程得到的<script type="math/tex">\gamma</script>和<script type="math/tex">\beta</script>值计算出各层的<script type="math/tex">\tilde z^{(i)}</script>值。</p>
<h4 id="7-8-Softmax回归"><a href="#7-8-Softmax回归" class="headerlink" title="7.8 Softmax回归"></a>7.8 Softmax回归</h4><p>目前我们介绍的都是二分类问题，神经网络输出层只有一个神经元，表示预测输出<script type="math/tex">\hat y</script>是正类的概率<script type="math/tex">P(y = 1|x)</script>，若<script type="math/tex">\hat y>0.5</script>则判断为正类，否则判断为负类。</p>
<p>对于多分类问题，用<script type="math/tex">C</script>表示种类个数，神经网络中输出层就有<script type="math/tex">C</script>个神经元，即<script type="math/tex">n^{[L]}=C</script>。其中，每个神经元的输出依次对应属于该类的概率，即<script type="math/tex">P(y=c|x)</script>。为了处理多分类问题，一般使用Softmax回归模型。Softmax回归模型输出层的激活函数如下所示：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z^{[L]}=W^{[L]} a^{[L-1]}+b^{[L]} \\
a_{i}^{[L]}=\frac{e^{z_{i}^{[L]}}}{\sum_{i=1}^{C} e^{z_{i}^{[L]}}}
\end{gathered}</script><p>输出层每个神经元的输出<script type="math/tex">a^{[L]}_i</script>对应属于该类的概率，满足：</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{C} a_{i}^{[L]}=1</script><p>所有的<script type="math/tex">a^{[L]}_i</script>，即<script type="math/tex">\hat y</script>，维度为<script type="math/tex">(C, 1)</script>。</p>
<p>下面给出几个简单的线性多分类的例子(只有一个输出层)：</p>
<p><img src="https://pic.imgdb.cn/item/648aeb7e1ddac507cce6d5bb.png"></p>
<p>如果使用神经网络，特别是深层神经网络，可以得到更复杂、更精确的非线性模型。</p>
<h4 id="7-9-训练一个Softmax回归"><a href="#7-9-训练一个Softmax回归" class="headerlink" title="7.9 训练一个Softmax回归"></a>7.9 训练一个Softmax回归</h4><p>Softmax classifier的训练过程与我们之前介绍的二元分类问题有所不同。先来看一下softmax classifier的loss function。举例假如<script type="math/tex">C=4</script>，某个样本的预测输出<script type="math/tex">\hat y</script>和真实输出<script type="math/tex">y</script>为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\hat{y}=\left[\begin{array}{l}0.3 \\0.2 \\0.1 \\0.4
\end{array}\right] 
\qquad y=\left[\begin{array}{l}0 \\1 \\0 \\0
\end{array}\right]
\end{gathered}</script><p>从<script type="math/tex">\hat y</script>值来看，<script type="math/tex">P(y=4|x)=0.4</script>，概率最大，而真实样本属于第<script type="math/tex">2</script>类，因此该预测效果不佳。我们定义softmax classifier的loss function为：</p>
<script type="math/tex; mode=display">
L(\hat{y}, y)=-\sum_{j=1}^{4} y_{j} \cdot \log \hat{y}_{j}</script><p>然而，由于只有当<script type="math/tex">j=2</script>时，<script type="math/tex">y_2=1</script>，其它情况下，<script type="math/tex">y_j=0</script>。所以，上式中的<script type="math/tex">L(\hat y,y)</script>可以简化为：</p>
<script type="math/tex; mode=display">
L(\hat{y}, y)=-y_{2} \cdot \log \hat{y}_{2}=-\log \hat{y}_{2}</script><p>要让<script type="math/tex">L(\hat y,y)</script>更小，就应该让<script type="math/tex">\hat y_2</script>越大越好。<script type="math/tex">\hat y_2</script>反映的是概率，完全符合我们之前的定义。</p>
<p>所有<script type="math/tex">m</script>个样本的cost function为：</p>
<script type="math/tex; mode=display">
J=\frac{1}{m} \sum_{i=1}^{m} L(\hat{y}, y)</script><p>其预测输出向量<script type="math/tex">A^{[L]}</script>即<script type="math/tex">\hat Y</script>的维度为<script type="math/tex">(4, m)</script>。</p>
<p>softmax classifier的反向传播过程仍然使用梯度下降算法，其推导过程与二元分类有一点点不一样。因为只有输出层的激活函数不一样，我们先推导<script type="math/tex">dZ^{[L]}</script>：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathrm d a^{[L]}=-\frac{1}{a^{[L]}} \\
\frac{\partial a^{[L]}}{\partial z^{[L]}}=\frac{\partial}{\partial z^{[L]}} \cdot\left(\frac{e^{z_{i}^{[L]}}}{\sum_{i=1}^{C} e^{z_{i}^{[L]}}}\right)=a^{[L]} \cdot\left(1-a^{[L]}\right) \\
\mathrm d z^{[L]}=d a^{[L]} \cdot \frac{\partial a^{[L]}}{\partial z^{[L]}}=a^{[L]}-1=a^{[L]}-y
\end{gathered}</script><p>对于所有m个训练样本：</p>
<script type="math/tex; mode=display">
d Z^{[L]}=A^{[L]}-Y</script><p>可见<script type="math/tex">\mathrm dZ^{[L]}</script>的表达式与二元分类结果是一致的，虽然推导过程不太一样。然后就可以继续进行反向传播过程的梯度下降算法了，推导过程与二元分类神经网络完全一致。</p>
<h4 id="7-10-深度学习框架"><a href="#7-10-深度学习框架" class="headerlink" title="7.10 深度学习框架"></a>7.10 深度学习框架</h4><p>深度学习框架有很多，例如：</p>
<ul>
<li><strong>Caffe/Caffe2</strong></li>
<li><strong>CNTK</strong></li>
<li><strong>DL4J</strong></li>
<li><strong>Keras</strong></li>
<li><strong>Lasagne</strong></li>
<li><strong>mxnet</strong></li>
<li><strong>PaddlePaddle</strong></li>
<li><strong>TensorFlow</strong></li>
<li><strong>Theano</strong></li>
<li><strong>Torch(Pytorch)</strong><br>一般选择深度学习框架的基本准则是：</li>
<li><strong>Ease of programming(development and deployment)</strong></li>
<li><strong>Running speed</strong></li>
<li><strong>Truly open(open source with good governance)</strong></li>
</ul>
<h4 id="7-11-TensorFlow"><a href="#7-11-TensorFlow" class="headerlink" title="7.11 TensorFlow"></a><strong>7.11 TensorFlow</strong></h4><p>这里简单介绍一下最近几年比较火的一个深度学习框架：TensorFlow。</p>
<p>举个例子来说明，例如cost function是参数w的函数：</p>
<script type="math/tex; mode=display">
J(w) = w^{2}-10 w+25</script><p>如果使用TensorFlow对cost function进行优化，求出最小值对应的<script type="math/tex">w</script>程序如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">w = tf.Variable(<span class="number">0</span>,dtype=tf.float32)</span><br><span class="line"><span class="comment">#cost = tf.add(tf.add(w**2,tf.multiply(-10,w)),25)</span></span><br><span class="line">cost = w**<span class="number">2</span> - <span class="number">10</span>*w +<span class="number">25</span></span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br><span class="line"><span class="comment"># &gt;&gt;0.0</span></span><br><span class="line">session.run(train)</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br><span class="line"><span class="comment"># &gt;&gt;0.1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    session.run(train)</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br><span class="line"><span class="comment"># &gt;&gt;4.99999</span></span><br></pre></td></tr></table></figure>
<p>TensorFlow框架内可以直接调用梯度下降优化算法，不需要我们自己再写程序了，大大提高了效率。在运行<script type="math/tex">1000</script>次梯度下降算法后，<script type="math/tex">w</script>的解为<script type="math/tex">4.99999</script>，已非常接近<script type="math/tex">w</script>的最优值<script type="math/tex">5</script>了。</p>
<p>针对上面这个例子，如果对<script type="math/tex">w</script>前的系数用变量<script type="math/tex">x</script>来代替，程序如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">cofficients = np.array([[<span class="number">1.</span>],[-<span class="number">10.</span>],[<span class="number">25.</span>]])</span><br><span class="line">w = tf.Variable(<span class="number">0</span>,dtype=tf.float32)</span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment">#cost = tf.add(tf.add(w**2,tf.multiply(-10,w)),25)</span></span><br><span class="line"><span class="comment">#cost = w**2 - 10*w +25</span></span><br><span class="line">cost = x[<span class="number">0</span>][<span class="number">0</span>]*w**<span class="number">2</span> + x[<span class="number">1</span>][<span class="number">0</span>]*w + x[<span class="number">2</span>][<span class="number">0</span>]</span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br><span class="line"><span class="comment"># &gt;&gt;0.0</span></span><br><span class="line">session.run(train, feed_dict=(x:coefficients))</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br><span class="line"><span class="comment"># &gt;&gt;0.1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    session.run(train, feed_dict=(x:coefficients))</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br><span class="line"><span class="comment"># &gt;&gt;4.99999</span></span><br></pre></td></tr></table></figure>
<p>结果跟之前一样。此外，还可以更改<script type="math/tex">x</script>即cofficients的值，而得到不同的优化结果<script type="math/tex">w</script>。</p>
<p>另外，上段程序中的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br></pre></td></tr></table></figure>
<p>有另外一种写法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    session.run(init)</span><br><span class="line">    <span class="built_in">print</span>(session.run(w))</span><br></pre></td></tr></table></figure>
<p>TensorFlow的最大优点就是采用<strong>数据流图(data flow graphs)</strong>来进行数值运算。图中的节点(Nodes)表示数学操作，图中的线(edges)则表示在节点间相互联系的多维数据数组，即张量(tensor)。而且它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU(或GPU)，服务器，移动设备等等。<br>关于TensorFlow更多的原理和编程技巧这里就不在赘述了，感兴趣的朋友可以关注更详细的TensorFlow相关文档。</p>
<h3 id="第八章-机器学习策略"><a href="#第八章-机器学习策略" class="headerlink" title="第八章 机器学习策略"></a>第八章 机器学习策略</h3><p>《Structuring Machine Learning Projects》(构建机器学习项目)这门课是Andrw  Ng深度学习专项课程中的第三门课。这门课主要介绍机器学习中的一些策略和方法，让我们能够更快更有效地让机器学习系统工作，该门课共有两周的课时。</p>
<h4 id="8-1-1为什么是Machine-Learning-ML-策略"><a href="#8-1-1为什么是Machine-Learning-ML-策略" class="headerlink" title="8.1.1为什么是Machine Learning(ML)策略"></a>8.1.1为什么是Machine Learning(ML)策略</h4><p>当我们最初得到一个深度神经网络模型时，我们可能希望从很多方面来对它进行优化，例如：</p>
<ul>
<li>Collect more data</li>
<li>Collect more diverse training set</li>
<li>Train algorithm longer with gradient descent</li>
<li>Try Adam instead of gradient descent</li>
<li>Try bigger network</li>
<li>Try smaller network</li>
<li>Try dropout</li>
<li>Add L2 regularization</li>
<li>Network architecture: Activation functions, #hidden units…<br>可选择的方法很多，也很复杂、繁琐。盲目选择、尝试不仅耗费时间而且可能收效甚微。因此，使用快速、有效的策略来优化机器学习模型是非常必要的。</li>
</ul>
<h4 id="8-1-2-正交化"><a href="#8-1-2-正交化" class="headerlink" title="8.1.2 正交化"></a>8.1.2 正交化</h4><p>机器学习中有许多参数、超参数需要调试。通过每次只调试一个参数，保持其它参数不变，而得到的模型某一性能改变是一种最常用的调参策略，称之为<strong>正交化方法(Orthogonalization)</strong>。</p>
<p>Orthogonalization的核心在于<strong>每次调试一个参数只会影响模型的某一个性能</strong>。例如老式电视机旋钮，每个旋钮就对应一个功能，调整旋钮会调整对应的功能，而不会影响其它功能。也就是说彼此旋钮之间是互不影响的，是正交的，这也是Orthogonalization名称的由来。这种方法能够让我们更快更有效地进行机器学习模型的调试和优化。</p>
<p>对应到机器学习监督式学习模型中，可以大致分成四个独立的“功能”，每个“功能”对应一些可调节的唯一的旋钮。四个“功能”如下：</p>
<ul>
<li><strong>Fit training set well on cost function</strong></li>
<li><strong>Fit dev set well on cost function</strong></li>
<li><strong>Fit test set well on cost function</strong></li>
<li><strong>Performs well in real world</strong><br>第一条优化训练集可以通过使用更复杂NN，使用Adam等优化算法来实现；</li>
</ul>
<p>第二条优化验证集可以通过正则化，采用更多训练样本来实现；</p>
<p>第三条优化测试集可以通过使用更多的验证集样本来实现；</p>
<p>第四条提升实际应用模型可以通过更换验证集，使用新的cost function来实现。</p>
<p>概括来说，每一种“功能”对应不同的调节方法。而这些调节方法（旋钮）只会对应一个“功能”，是正交的。</p>
<p>顺便提一下，<strong>early stopping在模型功能调试中并不推荐使用</strong>。因为early stopping在提升验证集性能的同时降低了训练集的性能。也就是说early stopping同时影响两个“功能”，不具有独立性、正交性。</p>
<h4 id="8-1-3-单—数字评估指标"><a href="#8-1-3-单—数字评估指标" class="headerlink" title="8.1.3 单—数字评估指标"></a>8.1.3 单—数字评估指标</h4><p>构建、优化机器学习模型时，单值评价指标非常必要。有了量化的单值评价指标后，我们就能根据这一指标比较不同超参数对应的模型的优劣，从而选择最优的那个模型。</p>
<p>举个例子，比如有A和B两个模型，它们的<strong>准确率(Precision)</strong>和<strong>召回率(Recall)</strong>分别如下：</p>
<p><img src="https://pic.imgdb.cn/item/648aeddf1ddac507ccef3d76.png" style="zoom:80%"></p>
<p>如果只看Precision的话，B模型更好。如果只看Recall的话，A模型更好。实际应用中，我们通常使用单值评价指标<strong>F1 Score</strong>来评价模型的好坏。F1 Score综合了Precision和Recall的大小，计算方法如下：</p>
<script type="math/tex; mode=display">
F 1=\frac{2}{\frac1P+\frac1R}</script><p>然后得到了A和B模型各自的F1 Score：</p>
<p><img src="https://pic.imgdb.cn/item/648aedfc1ddac507ccefa608.png" style="zoom:80%"></p>
<p>从F1 Score来看，A模型比B模型更好一些。通过引入单值评价指标F1 Score，很方便对不同模型进行比较。</p>
<p>除了F1 Score之外，我们还可以使用<strong>平均值</strong>作为单值评价指标来对模型进行评估。如下图所示，A, B, C, D, E, F六个模型对不同国家样本的错误率不同，可以计算其平均性能，然后选择平均错误率最小的那个模型(C模型)。</p>
<p><img src="https://pic.imgdb.cn/item/648aeee81ddac507ccf2bf9c.png"></p>
<h4 id="8-1-4-满足和优化指标"><a href="#8-1-4-满足和优化指标" class="headerlink" title="8.1.4 满足和优化指标"></a>8.1.4 满足和优化指标</h4><p>有时候，要把所有的性能指标都综合在一起，构成单值评价指标是比较困难的。解决办法是，我们可以把某些性能作为<strong>优化指标(Optimizing metic)</strong>，寻求最优化值；而某些性能作为<strong>满意指标(Satisficing metic)</strong>，只要满足阈值就行了。</p>
<p>举个猫类识别的例子，有A，B，C三个模型，各个模型的Accuracy和Running time如下表中所示：</p>
<p><img src="https://pic.imgdb.cn/item/648aef171ddac507ccf359ea.png"></p>
<p>Accuracy和Running time这两个性能不合适综合成单值评价指标。因此，可以将Accuracy作为优化指标(Optimizing metic)，将Running time作为满意指标(Satisficing metic)。也就是说，给Running time设定一个阈值，在其满足阈值的情况下，选择Accuracy最大的模型。如果设定Running time必须在100ms以内，那么很明显，模型C不满足阈值条件，首先剔除；模型B相比较模型A而言，Accuracy更高，性能更好。</p>
<p>概括来说，性能指标(Optimizing metic)是需要优化的，越优越好；而满意指标(Satisficing metic)只要满足设定的阈值就好了。</p>
<h4 id="8-1-5-训练-验证-开发-测试集划分"><a href="#8-1-5-训练-验证-开发-测试集划分" class="headerlink" title="8.1.5 训练_验证(开发)_测试集划分"></a>8.1.5 训练_验证(开发)_测试集划分</h4><p>Train/dev/test sets如何设置对机器学习的模型训练非常重要，合理设置能够大大提高模型训练效率和模型质量。</p>
<p>原则上应该尽量保证dev sets和test sets来源于<strong>同一分布</strong>且都反映了实际样本的情况。如果dev sets和test sets不来自同一分布，那么我们从dev sets上选择的“最佳”模型往往不能够在test sets上表现得很好。</p>
<h4 id="8-1-6-开发集和测试集的大小"><a href="#8-1-6-开发集和测试集的大小" class="headerlink" title="8.1.6 开发集和测试集的大小"></a>8.1.6 开发集和测试集的大小</h4><p>在之前的课程中我们已经介绍过，当样本数量不多(小于一万)的时候，通常将Train/dev/test sets的比例设为60%/20%/20%，在没有dev sets的情况下，Train/test sets的比例设为70%/30%。当样本数量很大(百万级别)的时候，通常将相应的比例设为98%/1%/1%或者99%/1%。</p>
<p>对于dev sets数量的设置，应该遵循的准则是通过dev sets能够检测不同算法或模型的区别，以便选择出更好的模型。</p>
<p>对于test sets数量的设置，应该遵循的准则是通过test sets能够反映出模型在实际中的表现。</p>
<p>实际应用中，可能只有train/dev sets，而没有test sets。这种情况也是允许的，只要算法模型没有对dev sets过拟合。但是，条件允许的话，最好是有test sets，实现<strong>无偏估计</strong>。</p>
<h4 id="8-1-7什么时候该改变开发-测试集和指标"><a href="#8-1-7什么时候该改变开发-测试集和指标" class="headerlink" title="8.1.7什么时候该改变开发_测试集和指标"></a>8.1.7什么时候该改变开发_测试集和指标</h4><p>算法模型的评价标准有时候需要根据实际情况进行动态调整，目的是让算法模型在实际应用中有更好的效果。</p>
<p>举个猫类识别的例子。初始的评价标准是错误率，算法A错误率为3%，算法B错误率为5%。显然，A更好一些。但是，实际使用时发现算法A会通过一些色情图片，但是B没有出现这种情况。从用户的角度来说，他们可能更倾向选择B模型，虽然B的错误率高一些。这时候，我们就需要改变之前单纯只是使用错误率作为评价标准，而考虑新的情况进行改变。例如增加色情图片的权重，增加其代价。</p>
<p>原来的cost function：</p>
<script type="math/tex; mode=display">
J=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)</script><p>更改评价标准后的cost function：</p>
<script type="math/tex; mode=display">
\begin{aligned}J 
&=\frac{1}{w^{(i)}} \sum_{i=1}^{m} w^{(i)} L\left(\hat{y}^{(i)}, y^{(i)}\right) \\
w^{(i)} &= \begin{cases}1, & x^{(i)} \text { is non-porn } \\10, 
& x^{(i)} \text { is porn }
\end{cases}
\end{aligned}</script><p>概括来说，机器学习可分为两个过程：</p>
<pre><code>**(1)** Define a metric to evaluate classifiers

**(2)** How to do well on this metric
</code></pre><p>也就是说，第一步是找靶心，第二步是通过训练，射中靶心。但是在训练的过程中可能会根据实际情况改变算法模型的评价标准，进行动态调整。</p>
<p>另外一个需要动态改变评价标准的情况是dev/test sets与实际使用的样本分布不一致。比如猫类识别样本图像分辨率差异。</p>
<p><img src="https://pic.imgdb.cn/item/648aeffc1ddac507ccf70033.png"></p>
<h4 id="8-1-8-为什么是人的表现"><a href="#8-1-8-为什么是人的表现" class="headerlink" title="8.1.8 为什么是人的表现"></a>8.1.8 为什么是人的表现</h4><p>机器学习模型的表现通常会跟人类水平表现作比较，如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648af01f1ddac507ccf77cd4.png" style="zoom:50%"></p>
<p>图中，横坐标是训练时间，纵坐标是准确性。机器学习模型经过训练会不断接近human-level performance甚至超过它。但是，超过human-level performance之后，准确性会上升得比较缓慢，最终不断接近理想的最优情况，称之为<strong>贝叶斯最优误差(bayes optimal error)</strong>。理论上任何模型都不能超过它，bayes optimal error代表了最佳表现。</p>
<p>实际上，human-level performance在某些方面有不俗的表现。例如图像识别、语音识别等领域，人类是很擅长的。所以，让机器学习模型性能不断接近human-level performance非常必要也做出很多努力：</p>
<ul>
<li>Get labeled data from humans.</li>
<li>Gain insight from manual error analysis: Why did a person get this right?</li>
<li>Better analysis of bias/variance.</li>
</ul>
<h4 id="8-1-9-可避免偏差"><a href="#8-1-9-可避免偏差" class="headerlink" title="8.1.9 可避免偏差"></a>8.1.9 可避免偏差</h4><p>实际应用中，要看human-level error，training error和dev error的相对值。例如猫类识别的例子中，如果human-level error为1%，training error为8%，dev error为10%。由于training error与human-level error相差7%，dev error与training error只相差2%，所以目标是尽量在训练过程中减小training error，即减小偏差bias。如果图片很模糊，肉眼也看不太清，human-level error提高到7.5%。这时，由于training error与human-level error只相差0.5%，dev error与training error只相差2%，所以目标是尽量在训练过程中减小dev error，即方差variance。这是相对而言的。</p>
<p>对于物体识别这类CV问题，human-level error是很低的，很接近理想情况下的bayes optimal error。因此，上面例子中的1%和7.5%都可以近似看成是两种情况下对应的bayes optimal error。实际应用中，我们一般会用human-level error代表bayes optimal error。</p>
<p>通常，<strong>把training error与human-level error的差值称为bias，也称作avoidable bias</strong>；<strong>把dev error与training error之间的差值称为variance</strong>。根据bias和variance值的相对大小，可以知道算法模型是否发生了欠拟合或者过拟合。</p>
<h4 id="8-1-10-理解人的表现"><a href="#8-1-10-理解人的表现" class="headerlink" title="8.1.10 理解人的表现"></a>8.1.10 理解人的表现</h4><p>我们说过human-level performance能够代表bayes optimal error。但是，human-level performance如何定义呢？举个医学图像识别的例子，不同人群的error有所不同：</p>
<ul>
<li><strong>Typical human : 3% error</strong></li>
<li><strong>Typical doctor : 1% error</strong></li>
<li><strong>Experienced doctor : 0.7% error</strong></li>
<li><strong>Team of experienced doctors : 0.5% error</strong><br>不同人群他们的错误率不同。一般来说，我们将表现最好的那一组，即Team of experienced doctors作为human-level performance。那么，这个例子中，human-level error就为0.5%。但是实际应用中，不同人可能选择的human-level performance基准是不同的，这会带来一些影响。</li>
</ul>
<p>假如该模型training error为0.7%，dev error为0.8。如果选择Team of experienced doctors，即human-level error为0.5%，则bias比variance更加突出。如果选择Experienced doctor，即human-level error为0.7%，则variance更加突出。也就是说，选择什么样的human-level error，有时候会影响bias和variance值的相对变化。当然这种情况一般只会在模型表现很好，接近bayes optimal error的时候出现。越接近bayes optimal error，模型越难继续优化，因为这时候的human-level performance可能是比较模糊难以准确定义的。</p>
<h4 id="8-1-11-超过人的表现"><a href="#8-1-11-超过人的表现" class="headerlink" title="8.1.11 超过人的表现"></a>8.1.11 超过人的表现</h4><p>对于自然感知类问题，例如视觉、听觉等，机器学习的表现不及人类。但是在很多其它方面，机器学习模型的表现已经超过人类了，包括：</p>
<ul>
<li>Online advertising</li>
<li>Product recommendations</li>
<li>Logistics(predicting transit time)</li>
<li>Loan approvals<br>实际上，机器学习模型超过human-level performance是比较困难的。但是只要提供足够多的样本数据，训练复杂的神经网络，模型预测准确性会大大提高，很有可能接近甚至超过human-level performance。值得一提的是当算法模型的表现超过human-level performance时，很难再通过人的直觉来解决如何继续提高算法模型性能的问题。</li>
</ul>
<h4 id="8-1-12-改善你模型的表现"><a href="#8-1-12-改善你模型的表现" class="headerlink" title="8.1.12 改善你模型的表现"></a>8.1.12 改善你模型的表现</h4><p>提高机器学习模型性能主要要解决两个问题：<strong>avoidable bias和variance</strong>。我们之前介绍过，training error与human-level error之间的差值反映的是avoidable bias，dev error与training error之间的差值反映的是variance。</p>
<p>解决avoidable bias的常用方法包括：</p>
<ul>
<li>Train bigger model</li>
<li>Train longer/better optimization algorithms: momentum, RMSprop, Adam</li>
<li>NN architecture/hyperparameters search<br>解决variance的常用方法包括：</li>
<li>More data</li>
<li>Regularization: L2, dropout, data augmentation</li>
<li>NN architecture/hyperparameters search</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/648af1351ddac507ccfaeda9.png"></p>
<h4 id="8-2-1-进行误差分析"><a href="#8-2-1-进行误差分析" class="headerlink" title="8.2.1 进行误差分析"></a>8.2.1 进行误差分析</h4><p>对已经建立的机器学习模型进行错误分析(error analysis)十分必要，而且有针对性地、正确地进行error analysis更加重要。</p>
<p>举个例子，猫类识别问题，已经建立的模型的错误率为10%。为了提高正确率，我们发现该模型会将一些狗类图片错误分类成猫。一种常规解决办法是扩大狗类样本，增强模型对够类(负样本)的训练。但是，这一过程可能会花费几个月的时间，耗费这么大的时间成本到底是否值得呢？也就是说扩大狗类样本，重新训练模型，对提高模型准确率到底有多大作用？这时候我们就需要进行error analysis，帮助我们做出判断。</p>
<p>方法很简单，我们可以从分类错误的样本中<strong>统计</strong>出狗类的样本数量。根据狗类样本所占的比重，判断这一问题的重要性。假如狗类样本所占比重仅为5%，即时我们花费几个月的时间扩大狗类样本，提升模型对其识别率，改进后的模型错误率最多只会降低到9.5%。相比之前的10%，并没有显著改善。我们把这种性能限制称为<strong>ceiling on performance</strong>。相反，假如错误样本中狗类所占比重为50%，那么改进后的模型错误率有望降低到5%，性能改善很大。因此，值得去花费更多的时间扩大狗类样本。</p>
<p>这种error analysis虽然简单，但是能够避免花费大量的时间精力去做一些对提高模型性能收效甚微的工作，让我们专注解决影响模型正确率的主要问题，十分必要。</p>
<p>这种error analysis可以同时评估多个影响模型性能的因素，通过各自在错误样本中所占的比例来判断其重要性。例如，猫类识别模型中，可能有以下几个影响因素：</p>
<ul>
<li>Fix pictures of dogs being recognized as cats</li>
<li>Fix great cats(lions, panthers, etc…) being misrecognized</li>
<li>Improve performance on blurry images<br>通常来说，比例越大，影响越大，越应该花费时间和精力着重解决这一问题。这种error analysis让我们改进模型更加有针对性，从而提高效率。</li>
</ul>
<h4 id="8-2-2-清除错误的数据"><a href="#8-2-2-清除错误的数据" class="headerlink" title="8.2.2 清除错误的数据"></a>8.2.2 清除错误的数据</h4><p>监督式学习中，训练样本有时候会出现输出<script type="math/tex">y</script>标注错误的情况，即incorrectly labeled examples。如果这些label标错的情况是随机性的(random errors)，DL算法对其包容性是比较强的，即健壮性好，一般可以直接忽略，无需修复。然而，如果是系统错误(systematic errors)，这将对DL算法造成影响，降低模型性能。</p>
<p>刚才说的是训练样本中出现incorrectly labeled data，如果是dev/test sets中出现incorrectly labeled data，该怎么办呢？</p>
<p>方法很简单，利用上节内容介绍的error analysis，统计dev sets中所有分类错误的样本中incorrectly labeled data所占的比例。根据该比例的大小，决定是否需要修正所有incorrectly labeled data，还是可以忽略。举例说明，若：</p>
<ul>
<li>Overall dev set error: 10%</li>
<li>Errors due incorrect labels: 0.6%</li>
<li>Errors due to other causes: 9.4%<br>上面数据表明Errors due incorrect labels所占的比例仅为0.6%，占dev set error的6%，而其它类型错误占dev set error的94%。因此，这种情况下，可以忽略incorrectly labeled data。</li>
</ul>
<p>如果优化DL算法后，出现下面这种情况：</p>
<ul>
<li>Overall dev set error: 2%</li>
<li>Errors due incorrect labels: 0.6%</li>
<li>Errors due to other causes: 1.4%<br>上面数据表明Errors due incorrect labels所占的比例依然为0.6%，但是却占dev set error的30%，而其它类型错误占dev set error的70%。因此，这种情况下，incorrectly labeled data不可忽略，需要手动修正。</li>
</ul>
<p>我们知道，dev set的主要作用是在不同算法之间进行比较，选择错误率最小的算法模型。但是，如果有incorrectly labeled data的存在，当不同算法错误率比较接近的时候，我们无法仅仅根据Overall dev set error准确指出哪个算法模型更好，必须修正incorrectly labeled data。</p>
<p>关于<strong>修正incorrect dev/test set data</strong>，有几条建议：</p>
<ul>
<li>Apply same process to your dev and test sets to make sure they continue to come from the same distribution</li>
<li>Consider examining examples your algorithm got right as well as ones it got wrong</li>
<li>Train and dev/test data may now come from slightly different distributions</li>
</ul>
<h4 id="8-2-3-快速搭建你的第一个系统"><a href="#8-2-3-快速搭建你的第一个系统" class="headerlink" title="8.2.3 快速搭建你的第一个系统"></a>8.2.3 快速搭建你的第一个系统</h4><p>对于如何构建一个机器学习应用模型，Andrew给出的建议是先快速构建第一个简单模型，然后再反复迭代优化。</p>
<ul>
<li>Set up dev/test set and metric</li>
<li>Build initial system quickly</li>
<li>Use Bias/Variance analysis &amp; Error analysis to prioritize next steps</li>
</ul>
<h4 id="8-2-4-在不同的划分上进行训练并测试"><a href="#8-2-4-在不同的划分上进行训练并测试" class="headerlink" title="8.2.4 在不同的划分上进行训练并测试"></a>8.2.4 在不同的划分上进行训练并测试</h4><p>当train set与dev/test set不来自同一个分布的时候，我们应该如何解决这一问题，构建准确的机器学习模型呢？</p>
<p>以猫类识别为例，train set来自于网络下载(webpages)，图片比较清晰；dev/test set来自用户手机拍摄(mobile app)，图片比较模糊。假如train set的大小为200000，而dev/test set的大小为10000，显然train set要远远大于dev/test set。</p>
<p><img src="https://pic.imgdb.cn/item/648af1f21ddac507ccfd9ad4.png"></p>
<p>虽然dev/test set质量不高，但是模型最终主要应用在对这些模糊的照片的处理上。面对train set与dev/test set分布不同的情况，有两种解决方法。</p>
<p>第一种方法是将train set和dev/test set完全混合，然后在随机选择一部分作为train set，另一部分作为dev/test set。例如，混合210000例样本，然后随机选择205000例样本作为train set，2500例作为dev set，2500例作为test set。这种做法的优点是实现train set和dev/test set分布一致，缺点是dev/test set中webpages图片所占的比重比mobile app图片大得多。例如dev set包含2500例样本，大约有2381例来自webpages，只有119例来自mobile app。这样，dev set的算法模型对比验证，仍然主要由webpages决定，实际应用的mobile app图片所占比重很小，达不到验证效果。因此，这种方法并不是很好。</p>
<p>第二种方法是将原来的train set和一部分dev/test set组合当成train set，剩下的dev/test set分别作为dev set和test set。例如，200000例webpages图片和5000例mobile app图片组合成train set，剩下的2500例mobile app图片作为dev set，2500例mobile app图片作为test set。其关键在于dev/test set全部来自于mobile app。这样保证了验证集最接近实际应用场合。这种方法较为常用，而且性能表现比较好。</p>
<h4 id="8-2-5-不匹配数据划分的偏差和方差"><a href="#8-2-5-不匹配数据划分的偏差和方差" class="headerlink" title="8.2.5 不匹配数据划分的偏差和方差"></a>8.2.5 不匹配数据划分的偏差和方差</h4><p>我们之前介绍过，根据human-level error、training error和dev error的相对值可以判定是否出现了bias或者variance。但是，需要注意的一点是，<strong>如果train set和dev/test set来源于不同分布，则无法直接根据相对值大小来判断。</strong>例如某个模型human-level error为0%，training error为1%，dev error为10%。根据我们之前的理解，显然该模型出现了variance。但是，training error与dev error之间的差值9%可能来自<strong>算法本身(variance)</strong>，也可能来自于<strong>样本分布不同</strong>。比如dev set都是很模糊的图片样本，本身就难以识别，跟算法模型关系不大。因此不能简单认为出现了variance。</p>
<p>在可能伴有train set与dev/test set分布不一致的情况下，定位是否出现variance的方法是<strong>设置train-dev set</strong>。Andrew给train-dev set的定义是：“Same distribution as training set, but not used for training.”也就是说，从原来的train set中分割出一部分作为train-dev set，train-dev set不作为训练模型使用，而是<strong>与dev set一样用于验证</strong>。</p>
<p>这样，我们就有training error、training-dev error和dev error三种error。其中，training error与training-dev error的差值反映了variance；training-dev error与dev error的差值反映了data mismatch problem，即样本分布不一致。</p>
<p>举例说明，如果training error为1%，training-dev error为9%，dev error为10%，则variance问题比较突出。如果training error为1%，training-dev error为1.5%，dev error为10%，则data mismatch problem比较突出。通过引入train-dev set，能够比较准确地定位出现了variance还是data mismatch。</p>
<p>总结一下human-level error、training error、training-dev error、dev error以及test error之间的差值关系和反映的问题：</p>
<p><img src="https://pic.imgdb.cn/item/648af21f1ddac507ccfe4d12.png" style="zoom:60%"></p>
<p>一般情况下，human-level error、training error、training-dev error、dev error以及test error的数值是递增的，但是也会出现dev error和test error下降的情况。这主要可能是因为训练样本比验证/测试样本更加复杂，难以训练。</p>
<p><img src="https://pic.imgdb.cn/item/648af3071ddac507cc01d8e0.png"></p>
<h4 id="8-2-6-解决数据不匹配"><a href="#8-2-6-解决数据不匹配" class="headerlink" title="8.2.6 解决数据不匹配"></a>8.2.6 解决数据不匹配</h4><p>关于如何解决train set与dev/test set样本分布不一致的问题，有两条建议：</p>
<ul>
<li>Carry out manual error analysis to try to understand difference between training dev/test sets</li>
<li>Make training data more similar; or collect more data similar to dev/test sets<br>为了让train set与dev/test set类似，可以用<strong>人工数据合成(artificial data synthesis)</strong>方法。例如说话人识别问题，实际应用场合(dev/test set)是包含背景噪声的，而训练样本train set很可能没有背景噪声。为了让train set与dev/test set分布一致，我们可以在train set上人工添加背景噪声，合成类似实际场景的声音。这样会让模型训练的效果更准确。</li>
</ul>
<p>但需要注意的是，我们不能给每段语音都增加同一段背景噪声，这样会出现对背景噪音的<strong>过拟合</strong>，效果不佳。这就是人工数据合成需要注意的地方。</p>
<h4 id="8-2-7-迁移学习"><a href="#8-2-7-迁移学习" class="headerlink" title="8.2.7 迁移学习"></a>8.2.7 迁移学习</h4><p>深度学习非常强大的一个功能之一就是有时候你可以将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。比如我们已经训练好一个猫类识别的神经网络模型，那么我们可以直接把该模型中的一部分网络结构应用到使用X光片预测疾病的模型中去。这种学习方法被称为<strong>迁移学习(Transfer Learning)</strong>。</p>
<p>如果我们已经有一个训练好的神经网络，用来做图像识别。现在，我们想要构建另外一个通过X光片进行诊断的模型。迁移学习的做法是无需重新构建新的模型，而是利用之前的神经网络模型，只改变样本输入、输出以及输出层的权重系数<script type="math/tex">W^{[L]}, b^{[L]}</script>。也就是说对新的样本<script type="math/tex">(X,Y)</script>，重新训练输出层权重系数<script type="math/tex">W^{[L]}, b^{[L]}</script>，而其它层所有的权重系数<script type="math/tex">W^{[L]}, b^{[L]}</script>保持不变。</p>
<p><img src="https://pic.imgdb.cn/item/648af3561ddac507cc02ff49.png"></p>
<p>迁移学习，重新训练权重系数，如果需要构建新模型的样本数量较少，那么可以像刚才所说的，只训练输出层的权重系数<script type="math/tex">W^{[L]}, b^{[L]}</script>，保持其它层所有的权重系数<script type="math/tex">W^{[L]}, b^{[L]}</script>不变。这种做法相对来说比较简单。如果样本数量足够多，那么也可以只保留网络结构，重新训练所有层的权重系数。这种做法使得模型更加精确，因为毕竟样本对模型的影响最大。选择哪种方法通常由数据量决定。</p>
<p>顺便提一下，如果重新训练所有权重系数，初始<script type="math/tex">W^{[L]}, b^{[L]}</script>由之前的模型训练得到，这一过程称为<strong>pre-training</strong>。之后不断调试、优化<script type="math/tex">W^{[L]}, b^{[L]}</script>的过程称为<strong>fine-tuning</strong>。pre-training和fine-tuning分别对应上图中的黑色箭头和红色箭头。</p>
<p>迁移学习之所以能这么做的原因是，神经网络浅层部分能够检测出许多图片固有<strong>特征</strong>，例如图像边缘、曲线等。使用之前训练好的神经网络部分结果有助于我们更快更准确地提取X光片特征。二者处理的都是图片，而图片处理是有相同的地方，第一个训练好的神经网络已经帮我们实现如何提取图片有用特征了。 因此，即便是即将训练的第二个神经网络样本数目少，仍然可以根据第一个神经网络结构和权重系数得到健壮性好的模型。</p>
<p>迁移学习可以保留原神经网络的一部分，再添加新的网络层。具体问题，具体分析，可以去掉输出层后再增加额外一些神经层。</p>
<p><img src="https://pic.imgdb.cn/item/648af3a01ddac507cc040506.png"></p>
<p>总体来说，迁移学习的应用场合主要包括三点：</p>
<ul>
<li>Task A and B have the same input x.</li>
<li>You have a lot more data for Task A than Task B.</li>
<li>Low level features from A could be helpful for learning B.</li>
</ul>
<h4 id="8-2-8-多任务学习"><a href="#8-2-8-多任务学习" class="headerlink" title="8.2.8 多任务学习"></a>8.2.8 多任务学习</h4><p>多任务学习(multi-task learning)就是构建神经网络同时执行多个任务。这跟二元分类或者多元分类都不同，多任务学习类似将多个神经网络融合在一起，用一个网络模型来实现多种分类效果。如果有<script type="math/tex">C</script>个，那么输出<script type="math/tex">y</script>的维度是<script type="math/tex">(C,1)</script>。</p>
<p>例如汽车自动驾驶中，需要实现的多任务为行人、车辆、交通标志和信号灯。如果检测出汽车和交通标志，则<script type="math/tex">y</script>为：</p>
<script type="math/tex; mode=display">
y = [1,0,0,1]^{\text T}</script><p>多任务学习模型的cost function为：</p>
<script type="math/tex; mode=display">
\begin{equation}
\frac{1}{m} \sum_{i=1}^{m} \sum_{j=1}^{c} L\left(\hat{y}_{j}^{(i)}, y_{j}^{(i)}\right)
\end{equation}</script><p>其中，<script type="math/tex">j</script>表示任务下标，总有<script type="math/tex">c</script>个任务。对应的loss function为：</p>
<script type="math/tex; mode=display">
L\left(\hat{y}_{j}^{(i)}, y_{j}^{(i)}\right)=-y_{j}^{(i)} \log \hat{y}_{j}^{(i)}-\left(1-y_{j}^{(i)}\right) \log \left(1-\hat{y}_{j}^{(i)}\right)</script><p>值得一提的是，Multi-task learning与Softmax regression的区别在于Softmax regression是single label的，即输出向量y只有一个元素为1；而Multi-task learning是multiple labels的，即输出向量y可以有多个元素为1。</p>
<p>多任务学习是使用单个神经网络模型来实现多个任务。实际上，也可以分别构建多个神经网络来实现。但是，如果各个任务之间是相似问题(例如都是图片类别检测)，则可以使用多任务学习模型。另外，多任务学习中，可能存在训练样本<script type="math/tex">Y</script>某些label空白的情况，这并不影响多任务模型的训练。</p>
<p>总体来说，多任务学习的应用场合主要包括三点：</p>
<pre><code>Training on a set of tasks that could benefit from having shared lower-level features.

Usually: Amount of data you have for each task is quite similar.

Can train a big enough neural network to do well on all the tasks.
</code></pre><p>顺便提一下，迁移学习和多任务学习在实际应用中，迁移学习使用得更多一些。</p>
<h4 id="8-2-9-什么是端到端的深度学习"><a href="#8-2-9-什么是端到端的深度学习" class="headerlink" title="8.2.9 什么是端到端的深度学习"></a>8.2.9 什么是端到端的深度学习</h4><p>端到端(end-to-end)深度学习就是将所有不同阶段的数据处理系统或学习系统模块组合在一起，用一个单一的神经网络模型来实现所有的功能。它将所有模块混合在一起，只关心输入和输出。</p>
<p>以语音识别为例，传统的算法流程和end-to-end模型的区别如下：</p>
<p><img src="https://pic.imgdb.cn/item/648af3d61ddac507cc04b689.png"></p>
<p>如果训练样本足够大，神经网络模型足够复杂，那么end-to-end模型性能比传统机器学习分块模型更好。实际上，end-to-end让神经网络模型内部去自我训练模型特征，自我调节，增加了模型整体契合度。</p>
<h4 id="8-2-10-是否要使用端到端的深度学习"><a href="#8-2-10-是否要使用端到端的深度学习" class="headerlink" title="8.2.10 是否要使用端到端的深度学习"></a>8.2.10 是否要使用端到端的深度学习</h4><p>end-to-end深度学习有优点也有缺点。</p>
<p>优点：</p>
<pre><code>Let the data speak

Less hand-designing of components needed
</code></pre><p>缺点：</p>
<pre><code>May need large amount of data

Excludes potentially useful hand-designed
</code></pre><p><strong>到这里这门课程的学习暂告一段落(ps：并不是不学了，接下来学习一些网络CNN、读一些论文看看再决定下一步得计划)</strong></p>
<hr>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>参考链接1：<a href="https://www.bilibili.com/video/BV1FT4y1E74V?p=1">吴恩达深度学习deeplearning.ai</a></p>
<p>参考链接2：<a href="https://blog.csdn.net/wuzhongqiang/article/details/89702268">本笔记参考链接</a></p>
<p>参考链接3：<a href="https://www.zhihu.com/column/c_1007644614324535296">吴恩达机器学习课思维导图</a></p>
<p>参考链接4：<a href="https://blog.csdn.net/weixin_43061212/article/details/82750763?utm_source=app&amp;app_version=4.20.0">DeepLearning吴恩达深度学习课程笔记思维导图</a></p>
<p>参考链接5：<a href="https://www.bilibili.com/video/BV1yg411K72z">三个月从零入门深度学习，保姆级学习路线图</a></p>
<p>参考链接6：<a href="https://www.bilibili.com/video/BV1yg411K72z">【人工智能学习】逐句阅读100篇核心AI论文（双语字幕）</a></p>
<p>参考链接7：<a href="https://zhuanlan.zhihu.com/p/37349519">纯新手自学入门机器/深度学习指南） - 量子位的文章 - 知乎</a></p>
<p>参考链接8：<a href="https://www.bilibili.com/video/BV1Lv41177BW/">Pytorch环境安装配置 - Bilibili</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>编程与深度学习</category>
        <category>深度学习基础</category>
      </categories>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习_吴恩达_Part_2</title>
    <url>/2023/06/15/Foundation-of-DeepLearning-2/</url>
    <content><![CDATA[<h3 id="第四章-深层神经网络"><a href="#第四章-深层神经网络" class="headerlink" title="第四章 深层神经网络"></a>第四章 深层神经网络</h3><p>上节主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是非线性的原因。最后介绍了神经网络参数随机初始化的必要性，特别是权重<script type="math/tex">W</script>，不同神经元的<script type="math/tex">W</script>不能初始化为同一零值。本节课是对上节课的延伸和扩展，讨论更深层的神经网络。</p>
<h4 id="4-1-深层神经网络概述"><a href="#4-1-深层神经网络概述" class="headerlink" title="4.1 深层神经网络概述"></a>4.1 深层神经网络概述</h4><p>深层神经网络其实就是包含更多的隐藏层神经网络。如下图所示，分别列举了逻辑回归、1个隐藏层的神经网络、2个隐藏层的神经网络和5个隐藏层的神经网络它们的模型结构。</p>
<p><img src="https://pic.imgdb.cn/item/648ac9781ddac507cc6cd8f2.png"></p>
<p>命名规则上，一般只参考隐藏层个数和输出层。例如，上图中的逻辑回归又叫1 layer NN，1个隐藏层的神经网络叫做2 layer NN，2个隐藏层的神经网络叫做3 layer NN，以此类推。如果是L-layer NN，则包含了L-1个隐藏层，最后的L层是输出层。</p>
<p>下面以一个4层神经网络为例来介绍关于神经网络的一些标记写法。如图30所示。</p>
<ul>
<li>总层数：用<script type="math/tex">L</script>表示，<script type="math/tex">L=4</script>。输入层是第<script type="math/tex">0</script>层，输出层是第<script type="math/tex">L</script>层。</li>
<li><script type="math/tex">n^{[l]}</script>表示第<script type="math/tex">l</script>层包含的单元个数，<script type="math/tex">l=0,1,⋯,L</script>。这个模型中，<script type="math/tex">n^{[0]}=n_x=3</script>，表示三个输入特征<script type="math/tex">x_1,x_2,x_3</script>。<script type="math/tex">n^{[1]}=5，n^{[2]}=5，n^{[3]}=3，n^{[4]}=n^{[L]}=1</script>。</li>
<li>第<script type="math/tex">l</script>层的激活函数输出用<script type="math/tex">a^{[l]}</script>表示，<script type="math/tex">a^{[l]}=g^{[l]}(z^{[l]})</script>。</li>
<li><script type="math/tex">W^{[l]}</script>表示第<script type="math/tex">l</script>层的权重，用于计算z[l]。<br>另外，我们把输入<script type="math/tex">x</script>记为<script type="math/tex">a^{[0]}</script>，把输出层<script type="math/tex">\hat y</script>记为<script type="math/tex">a^{[L]}</script>。</li>
</ul>
<p>注意，<script type="math/tex">a^{[l]}</script>和<script type="math/tex">W^{[l]}</script>中的上标<script type="math/tex">l</script>都是从<script type="math/tex">1</script>开始的，<script type="math/tex">l=1,⋯,L</script>。</p>
<p><img src="https://pic.imgdb.cn/item/648ac9ba1ddac507cc6ddfdd.png" style="zoom:40%"></p>
<h4 id="4-2-深层神经网络中的前向传播"><a href="#4-2-深层神经网络中的前向传播" class="headerlink" title="4.2 深层神经网络中的前向传播"></a>4.2 深层神经网络中的前向传播</h4><p>对于第<script type="math/tex">l</script>层，其正向传播过程的<script type="math/tex">Z^{[l]}</script>和<script type="math/tex">A^{[l]}</script>可以表示为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
Z^{[l]}=W^{[l]} A^{[l-1]}+b^{[l]} \\
A^{[l]}=g^{[l]}\left(Z^{[l]}\right)
\end{gathered}</script><p>其中，<script type="math/tex">l = 1,...L</script>。</p>
<h4 id="4-3-核对矩阵的维数"><a href="#4-3-核对矩阵的维数" class="headerlink" title="4.3 核对矩阵的维数"></a>4.3 核对矩阵的维数</h4><p>对于单个训练样本，输入<script type="math/tex">x</script>的维度是<script type="math/tex">(n^{[0]},1)</script>神经网络的参数<script type="math/tex">W^{[l]}</script>和<script type="math/tex">b^{[l]}</script>的维度分别是：</p>
<script type="math/tex; mode=display">
\begin{gathered}W^{[l]}:\left(n^{[l]}, n^{[l-1]}\right) \\b^{[l]}:\left(n^{[l]}, 1\right)\end{gathered}</script><p>其中，<script type="math/tex">l=1,⋯,L</script>，<script type="math/tex">n^{[l]}</script>和<script type="math/tex">n^{[l−1]}</script>分别表示第<script type="math/tex">l</script>层和<script type="math/tex">l−1</script>层的所含单元个数。<script type="math/tex">n^{[0]}=n_x</script>，表示输入层特征数目。</p>
<p>顺便提一下，反向传播过程中的<script type="math/tex">\mathrm dW^{[l]}</script>、<script type="math/tex">\mathrm db^{[l]}</script>的维度和<script type="math/tex">W</script>、<script type="math/tex">b</script>的<strong>维度相同</strong>。</p>
<p>正向传播过程中的<script type="math/tex">z^{[l]}</script>和<script type="math/tex">a^{[l]}</script>的维度分别是：</p>
<script type="math/tex; mode=display">
\begin{aligned}&z^{[l]}:\left(n^{[l]}, 1\right) \\&a^{[l]}:\left(n^{[l]}, 1\right)\end{aligned}</script><p>$z^{[l]}$和$a^{[l]}$的维度是一样的，且$\mathrm dz^{[l]}$和$\mathrm da^{[l]}$的维度均与$z^{[l]}$和$a^{[l]}$的维度一致。</p>
<p>对于$m$个训练样本，输入矩阵$X$的维度是$(n^{[0]},m)$。需要注意的是$W^{[l]}$和$b^{[l]}$的维度与只有单个样本是一致的：</p>
<script type="math/tex; mode=display">
\begin{gathered}W^{[l]}:\left(n^{[l]}, n^{[l-1]}\right) \\b^{[l]}:\left(n^{[l]}, 1\right)\end{gathered}</script><p>只不过在运算$Z^{[l]}=W^{[l]}A^{[l−1]}+b^{[l]}$中，$b^{[l]}$会被当成$(n^{[l]},m)$矩阵进行运算，这是因为python的广播性质，且$b^{[l]}$每一列向量都是一样的。$\mathrm dW^{[l]}$和$\mathrm db^{[l]}$的维度分别与$W$和$b$的相同。但是，$Z^{[l]}$和$A^{[l]}$的维度发生了变化：</p>
<script type="math/tex; mode=display">
\begin{gathered}
Z^{[l]}:\left(n^{[l]}, m\right)\\
A^{[l]}:\left(n^{[l]}, m\right)
\end{gathered}</script><p>$\mathrm dZ^{[l]}$和$\mathrm dA^{[l]}$的维度分别与$Z^{[l]}$和$A^{[l]}$的相同。</p>
<h4 id="4-4-为什么使用深层表示"><a href="#4-4-为什么使用深层表示" class="headerlink" title="4.4 为什么使用深层表示"></a>4.4 为什么使用深层表示</h4><p>神经网络能处理很多问题，而且效果显著。其强大能力主要源自神经网络足够<strong>“深”</strong>，也就是说网络层数越多，神经网络就更加复杂和深入，学习也更加准确。接下来，我们从几个例子入手，看一下为什么深度网络能够如此强大。</p>
<p>先来看人脸识别的例子，如下图所示。经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。可以看出，<strong>随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。</strong></p>
<p>语音识别模型也是这个道理。浅层的神经元能够检测一些简单的音调，然后较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。<strong>记住一点，神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大</strong>。</p>
<p><img src="https://pic.imgdb.cn/item/648acaa01ddac507cc70ee96.png"></p>
<p>除了从提取特征复杂度的角度来说明深层网络的优势之外，深层网络还有另外一个优点，就是能够减少神经元个数，从而减少计算量。例如下面这个例子，使用电路理论，计算逻辑输出：</p>
<script type="math/tex; mode=display">
y=x_{1} \oplus x_{2} \oplus x_{3} \oplus \cdots \oplus x_{n}</script><p>其中，⊕表示异或操作。对于这个逻辑运算，如果使用深度网络，深度网络的结构是每层将前一层的两两单元进行异或，最后到一个输出，如下图左边所示。这样，整个深度网络的层数是<script type="math/tex">log_2(n)</script>，不包含输入层。总共使用的神经元个数为：</p>
<script type="math/tex; mode=display">
1+2+\cdots+2^{\log _{2}(n)-1}=1 \cdot \frac{1-2^{\log _{2}(n)}}{1-2}=2^{\log _{2}(n)}-1=n-1</script><p>可见，输入个数是$n$，这种深层网络所需的神经元个数仅仅是$n-1$个。</p>
<p>如果不用深层网络，仅仅使用单个隐藏层，那么需要的神经元个数将是指数级别那么大。Andrew指出，由于包含了所有的逻辑位（0和1），则需要$2^{n−1}$个神经元。<strong>这里笔者推导的是</strong>$2^n$<strong>个神经元，为啥是</strong>$2^{n−1}$<strong>请哪位高手解释下。</strong></p>
<p>比较下来，处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多。这也是深层神经网络的优点之一。</p>
<p>尽管深度学习有着非常显著的优势，Andrew还是建议对实际问题进行建模时，尽量先选择层数少的神经网络模型，这也符合奥卡姆剃刀定律(Occam’s Razor)。对于比较复杂的问题，再使用较深的神经网络模型。</p>
<h4 id="4-5-搭建深层神经网络模块"><a href="#4-5-搭建深层神经网络模块" class="headerlink" title="4.5 搭建深层神经网络模块"></a>4.5 搭建深层神经网络模块</h4><p>下面用流程块图来解释神经网络正向传播和反向传播过程。如下图所示，对于第<script type="math/tex">l</script>层来说，正向传播过程中：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:left"><strong>正向传播</strong></th>
<th style="text-align:left"><strong>反向传播</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>输入</strong></td>
<td style="text-align:left">$a^{[l-1]}$</td>
<td style="text-align:left">$\mathrm da^{[l]}$</td>
</tr>
<tr>
<td style="text-align:left"><strong>输出</strong></td>
<td style="text-align:left">$a^{[l]}$</td>
<td style="text-align:left">$\mathrm da^{[l-1]}, \mathrm dW^{[l]}, \mathrm db^{[l]}$</td>
</tr>
<tr>
<td style="text-align:left"><strong>参数</strong></td>
<td style="text-align:left">$W^{[l]},b^{[l]}$</td>
<td style="text-align:left">$W^{[l]},b^{[l]}$</td>
</tr>
<tr>
<td style="text-align:left"><strong>缓存变量</strong></td>
<td style="text-align:left">$z^{[l]}$</td>
</tr>
</tbody>
</table>
</div>
<p><img src="https://pic.imgdb.cn/item/648acb1f1ddac507cc728405.png" style="zoom:50%"></p>
<p>刚才这是第$l$层的流程块图，对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：</p>
<p><img src="https://pic.imgdb.cn/item/648acb641ddac507cc737af1.png"></p>
<h4 id="4-6-前向和反向传播"><a href="#4-6-前向和反向传播" class="headerlink" title="4.6 前向和反向传播"></a>4.6 前向和反向传播</h4><p>接着上一部分流程块图的内容，推导神经网络正向传播过程和反向传播过程的具体表达式。首先是正向传播过程，令层数为第$l$层，输入是$a^{[l−1]}$，输出是$a^{[l]}$，缓存变量是$z^{[l]}$。其表达式如下：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z^{[l]}=W^{[l]} a^{[l-1]}+b^{[l]} \\
a^{[l]}=g^{[l]}\left(z^{[l]}\right)
\end{gathered}</script><p>$m$个训练样本，向量化形式为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
Z^{[l]}=W^{[l]} A^{[l-1]}+b^{[l]} \\
A^{[l]}=g^{[l]}\left(Z^{[l]}\right)
\end{gathered}</script><p>然后是反向传播过程，输入是$\mathrm da^{[l]}$，输出是$\mathrm da^{[l−1]}, \mathrm  dw^{[l]}, \mathrm db^{[l]}$。其表达式如下：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathrm d z^{[l]}=\mathrm d a^{[l]} * g^{[l]^{\prime}}\left(z^{[l]}\right) \\
\mathrm d W^{[l]}=\mathrm d z^{[l]} \cdot a^{[l-1]} \\
\mathrm d b^{[l]}=\mathrm d z^{[l]} \\
\mathrm d a^{[l-1]}=W^{[l] T} \cdot \mathrm d z^{[l]}
\end{gathered}</script><p>由上述第四个表达式可得$\mathrm da^{[l]}=W^{[l+1]T}⋅\mathrm dz^{[l+1]}​$，将$\mathrm da^{[l]}​$代入第一个表达式中可以得到：</p>
<script type="math/tex; mode=display">
\mathrm d z^{[l]}=W^{[l+1] T} \cdot \mathrm d z^{[l+1]} * g^{[l]^{\prime}}\left(z^{[l]}\right)</script><p>该式非常重要，反映了$\mathrm dz^{[l+1]}$与$\mathrm dz^{[l]}$的递推关系。</p>
<p>$m$个训练样本，向量化形式为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathrm d Z^{[l]}=\mathrm d A^{[l]} * g^{[l]^{\prime}}\left(Z^{[l]}\right) \\
\mathrm d W^{[l]}=\frac{1}{m} \mathrm d Z^{[l]} \cdot A^{[l-1] T} \\
\mathrm d b^{[l]}=\frac{1}{m} n p \cdot \sum\left(\mathrm d Z^{[l]}, \text { axis }=1, \text { keepdim }=\text { True }\right) \\
\mathrm d A^{[l-1]}=W^{[l] T} \cdot\mathrm  d Z^{[l]} \\
\mathrm d Z^{[l]}=W^{[l+1] T} \cdot \mathrm d Z^{[l+1]} * g^{[l]^{\prime}}\left(Z^{[l]}\right)
\end{gathered}</script><h4 id="4-7-参数-VS超参数"><a href="#4-7-参数-VS超参数" class="headerlink" title="4.7 参数 VS超参数"></a>4.7 参数 VS超参数</h4><p>该部分介绍神经网络中的参数（parameters）和超参数（hyperparameters）的概念。</p>
<p>神经网络中的参数就是我们熟悉的<script type="math/tex">W^{[l]}</script>和<script type="math/tex">b^{[l]}</script>。而超参数则是例如学习速率<script type="math/tex">\alpha</script>，训练迭代次数<script type="math/tex">N</script>，神经网络层数<script type="math/tex">L</script>，各层神经元个数<script type="math/tex">n^{[l]}</script>，激活函数<script type="math/tex">g(z)</script>等。之所以叫做超参数的原因是它们决定了参数<script type="math/tex">W^{[l]}</script>和<script type="math/tex">b^{[l]}</script>的值。在后面的第二门课我们还将学习其它的超参数，这里先不讨论。</p>
<p>如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost  function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于validation的方法。</p>
<h3 id="第五章-深度学习的实用层面"><a href="#第五章-深度学习的实用层面" class="headerlink" title="第五章 深度学习的实用层面"></a>第五章 深度学习的实用层面</h3><h4 id="5-1-训练-验证-测试集"><a href="#5-1-训练-验证-测试集" class="headerlink" title="5.1 训练-验证-测试集"></a>5.1 训练-验证-测试集</h4><p>选择最佳的<strong>训练集(Training sets)</strong>、<strong>验证集(Development sets)</strong>、<strong>测试集(Test sets)</strong>对神经网络的性能影响非常重要。除此之外，在构建一个神经网络的时候，需要设置许多参数，例如神经网络的层数、每个隐藏层包含的神经元个数、学习因子(学习速率)、激活函数的选择等等。实际上很难在第一次设置的时候就选择到这些最佳的参数，而是需要通过不断地迭代更新来获得。这个循环迭代的过程是这样的：</p>
<p>(1) 先有个想法Idea，先选择初始的参数值，构建神经网络模型结构；</p>
<p>(2) 然后通过代码Code的形式，实现这个神经网络；</p>
<p>(3) 最后通过实验Experiment验证这些参数对应的神经网络的表现性能。</p>
<p>根据验证结果，我们对参数进行适当的调整优化，再进行下一次的<strong>Idea-&gt;Code-&gt;Experiment</strong>循环。通过很多次的循环，不断调整参数，选定最佳的参数值，从而让神经网络性能最优化。</p>
<p>一般地，我们将所有的样本数据分成三个部分：Train/Dev/Test sets。Train sets用来训练你的算法模型；Dev sets用来验证不同算法的表现情况，从中选择最好的算法模型；Test sets用来测试最好算法的实际表现，作为该算法的无偏估计。</p>
<p>之前人们通常设置Train sets和Test sets的数量比例为70%和30%。如果有Dev sets，则设置比例为60%、20%、20%，分别对应Train/Dev/Test sets。这种比例分配在样本数量不是很大的情况下，例如100,1000,10000，是比较科学的。但是如果数据量很大的时候，例如100万，科学的做法是要将Dev sets和Test sets的比例设置得很低。因为Dev sets的目标是用来比较验证不同算法的优劣，从而选择更好的算法模型就行了。因此，对于大数据样本，Train/Dev/Test sets的比例通常可以设置为98%/1%/1%，或者99%/0.5%/0.5%。样本数据量越大，相应的Dev/Test sets的比例可以设置的越低一些。</p>
<p>现代深度学习还有个重要的问题就是<strong>训练样本和测试样本分布上不匹配</strong>，意思是训练样本和测试样本来自于不同的分布。解决这一问题的比较科学的办法是尽量保证Dev sets和Test sets来自于同一分布。值得一提的是，训练样本非常重要，通常我们可以将现有的训练样本做一些处理，例如<strong>图片的翻转、假如随机噪声等，来扩大训练样本的数量</strong>，从而让该模型更加强大。即使Train sets和Dev/Test sets不来自同一分布，使用这些技巧也能提高模型性能。</p>
<p>最后提一点的是如果<strong>没有Test sets也是没有问题</strong>的。Test sets的目标主要是进行<strong>无偏估计</strong>。我们可以通过Train sets训练不同的算法模型，然后分别在Dev sets上进行验证，根据结果选择最好的算法模型。这样也是可以的，不需要再进行无偏估计了。如果只有Train sets和Dev sets，通常也有人把这里的Dev sets称为Test sets，我们要注意加以区别。</p>
<h4 id="5-2-偏差和方差"><a href="#5-2-偏差和方差" class="headerlink" title="5.2 偏差和方差"></a>5.2 偏差和方差</h4><p>偏差(bias)和方差(Variance)是机器学习领域非常重要的两个概念和需要解决的问题。在传统的机器学习算法中，Bias和Variance是对立的，分别对应着欠拟合和过拟合，我们常常需要在Bias和Variance之间进行权衡。而在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。</p>
<p>如下图所示，显示了二维平面上，high bias，just right，high variance的例子。可见，high bias对应着欠拟合，而high variance对应着过拟合。</p>
<p><img src="https://pic.imgdb.cn/item/648acbe91ddac507cc755d84.png"></p>
<p>上图这个例子中输入特征是二维的，high bias和high variance可以直接从图中分类线看出来。而对于输入特征是高维的情况，如何来判断是否出现了high bias或者high variance呢？</p>
<p>例如猫识别问题，输入是一幅图像，其特征维度很大。这种情况下，我们可以通过两个数值<strong>Train set error</strong>和<strong>Dev set error</strong>来理解bias和variance。</p>
<p>(1) 假设Train set error为1%，而Dev set error为11%，即该算法模型对训练样本的识别很好，但是对验证集的识别却不太好。这说明了该模型对训练样本可能存在<strong>过拟合</strong>，模型泛化能力不强，导致验证集识别率低。这恰恰是<strong>high variance</strong>的表现。</p>
<p>(2) 假设Train set error为15%，而Dev set error为16%，虽然二者error接近，即该算法模型对训练样本和验证集的识别都不是太好。这说明了该模型对训练样本存在<strong>欠拟合</strong>。这恰恰是<strong>high bias</strong>的表现。</p>
<p>(3) 假设Train set error为15%，而Dev set error为30%，说明了该模型既存在high bias也存在high variance(深度学习中最坏的情况)。</p>
<p>(4) 假设Train set error为0.5%，而Dev set error为1%，即low bias和low variance，是最好的情况。</p>
<p>值得一提的是，以上的这些假设都是建立在<strong>base error是0</strong>的基础上，即人类都能正确识别所有猫类图片。base error不同，相应的Train set error和Dev set error会有所变化，但没有相对变化。</p>
<p>一般来说，Train set error体现了是否出现bias，Dev set error体现了是否出现variance(正确地说，应该是Dev set error与Train set error的相对差值)。</p>
<p>我们已经通过二维平面展示了high bias或者high variance的模型，下图展示了high bias and high variance的模型：</p>
<p><img src="https://pic.imgdb.cn/item/648acc1c1ddac507cc760c33.png" style="zoom:60%"></p>
<p>模型既存在high bias也存在high variance，可以理解成某段区域是欠拟合的，某段区域是过拟合的。</p>
<h4 id="5-3-机器学习基础"><a href="#5-3-机器学习基础" class="headerlink" title="5.3 机器学习基础"></a>5.3 机器学习基础</h4><p>机器学习中基本的一个诀窍就是<strong>避免出现high bias和high variance</strong>。</p>
<p><img src="https://pic.imgdb.cn/item/648acc6d1ddac507cc771342.png"></p>
<p><img src="https://pic.imgdb.cn/item/648acd391ddac507cc79e747.png" style="zoom:60%"></p>
<p>这里有几点需要注意的。</p>
<p>第一，解决high bias和high variance的方法是不同的。实际应用中通过Train set error和Dev set error判断是否出现了high bias或者high variance，然后再选择针对性的方法解决问题。</p>
<p>第二，Bias和Variance的折中tradeoff。传统机器学习算法中，Bias和Variance通常是对立的，减小Bias会增加Variance，减小Variance会增加Bias。而在现在的深度学习中，通过使用更复杂的神经网络和海量的训练样本，一般能够同时有效减小Bias和Variance。这也是深度学习之所以如此强大的原因之一。</p>
<h4 id="5-4-正则化"><a href="#5-4-正则化" class="headerlink" title="5.4 正则化"></a>5.4 正则化</h4><p>如果出现了过拟合，即high variance，则需要采用正则化regularization来解决。虽然扩大训练样本数量也是减小high variance的一种方法，但是通常获得更多训练样本的成本太高，比较困难。所以，更可行有效的办法就是使用regularization。</p>
<p>回顾一下之前介绍的Logistic regression。采用L2 regularization，其表达式为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m}\|w\|_{2}^{2} \\
\|w\|_{2}^{2}=\sum_{j=1}^{n_{x}} w_{j}^{2}=w^{T} w
\end{gathered}</script><blockquote>
<p>这里有个问题：<strong>为什么只对w进行正则化而不对b进行正则化呢？</strong>其实也可以对b进行正则化。但是一般w的维度很大，而b只是一个常数。相比较来说，参数很大程度上由w决定，改变b值对整体模型影响较小。所以，一般为了简便，就忽略对b的正则化了。<br>除了L2 regularization之外，还有另外一只正则化方法：L1 regularization。其表达式为：</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{gathered}
J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m}\|w\|_{1} \\
\|w\|_{1}=\sum_{j=1}^{n_{x}}\left|w_{j}\right|
\end{gathered}</script><p>与L2 regularization相比，L1 regularization得到的<script type="math/tex">w</script><strong>更加稀疏</strong>，即很多<script type="math/tex">w</script>为零值，所以其优点是节约存储空间。但实际L1 regularization在解决high variance方面比L2 regularization并不更具优势。而且L1的在微分求导方面比较复杂。所以，一般L2 regularization更加常用。</p>
<p>L1、L2 regularization中的<strong>λ就是正则化参数(超参数的一种)</strong>。可以设置λ为不同的值，在Dev set中进行验证，选择最佳的λ。顺便提一下，在python中，由于lambda是保留字，所以为了避免冲突，我们使用lambd来表示λ。</p>
<p>在深度学习模型中，L2 regularization的表达式为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
J\left(w^{[1]}, b^{[1]}, \cdots, w^{[L]}, b^{[L]}\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m} \sum_{l=1}^{L}\left\|w^{[l]}\right\|^{2} \\
\left\|w^{[l]}\right\|^{2}=\sum_{i=1}^{n^{[l]}} \sum_{j=1}^{n^{[l-1]}}\left(w_{i j}^{[l]}\right)^{2}
\end{gathered}</script><p>通常，我们把<script type="math/tex">||w^{[l]}||^2</script>称为Frobenius范数，记为<script type="math/tex">||w^{[l]}||^2_F</script>。一个矩阵的Frobenius范数就是计算所有元素平方和再开方，如下所示：</p>
<script type="math/tex; mode=display">
\|A\|_{F}=\sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}}</script><p>值得注意的是，由于加入了正则化项，梯度下降算法中的<script type="math/tex">dw^{[l]}</script>计算表达式需要做如下修改：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathrm d w^{[l]}=\mathrm d w_{\text {before }}^{[l]}+\frac{\lambda}{m} w^{[l]} \\w^{[l]}:=w^{[l]}-\alpha \cdot d w^{[l]}
\end{gathered}</script><p>L2 regularization也被称做<strong>权重衰减(weight decay)</strong>。这是因为，由于加上了正则项，<script type="math/tex">\mathrm  dw^{[l]}</script>有个增量，在更新<script type="math/tex">w^{[l]}</script>的时候，会多减去这个增量，使得<script type="math/tex">w^{[l]}</script>比没有正则项的值要小一些。不断迭代更新，不断地减小。</p>
<script type="math/tex; mode=display">
\begin{aligned}
w^{[l]} &:=w^{[l]}-\alpha \cdot \mathrm d w^{[l]} \\
&=w^{[l]}-\alpha \cdot\left(\mathrm d w_{\mathrm{befor} e}^{[l]}+\frac{\lambda}{m} w^{[l]}\right) \\
&=\left(1-\alpha \frac{\lambda}{m}\right) w^{[l]}-\alpha \cdot\mathrm d w_\mathrm{befor}^{[l]}
\end{aligned}</script><p>其中，<script type="math/tex">1-\alpha \frac{\lambda}{m}<1</script></p>
<h4 id="5-5-为什么正则化可以减少过拟合"><a href="#5-5-为什么正则化可以减少过拟合" class="headerlink" title="5.5 为什么正则化可以减少过拟合"></a>5.5 为什么正则化可以减少过拟合</h4><p>为什么正则化能够有效避免high variance，防止过拟合呢？下面通过几个例子说明。</p>
<p>还是之前那张图，从左到右，分别表示了欠拟合，刚好拟合，过拟合三种情况。</p>
<p><img src="https://pic.imgdb.cn/item/648acd901ddac507cc7b0e77.png"></p>
<p>假如我们选择了非常复杂的神经网络模型，如上图左上角所示。在未使用正则化的情况下，我们得到的分类超平面可能是类似上图右侧的过拟合。但是，如果使用L2 regularization，当<em>λ</em>很大时，<script type="math/tex">w^{[l]}≈0</script>意味着该神经网络模型中的某些神经元实际的作用很小，可以忽略。从效果上来看，其实是将某些神经元给忽略掉了。这样原本过于复杂的神经网络模型就变得不那么复杂了，而变得非常简单化了。如下图所示，整个简化的神经网络模型变成了一个逻辑回归模型。问题就从high  variance变成了high bias了。</p>
<p><img src="https://pic.imgdb.cn/item/648acda91ddac507cc7b514b.png" style="zoom:40%"></p>
<p>因此，选择合适大小的<script type="math/tex">\lambda</script>值，就能够同时避免high bias和high variance，得到最佳模型。</p>
<p>还有另外一个直观的例子来解释为什么正则化能够避免发生过拟合。假设激活函数是tanh函数。tanh函数的特点是在<script type="math/tex">z</script>接近零的区域，函数近似是线性的，而当<script type="math/tex">|z|</script>很大的时候，函数非线性且变化缓慢。当使用正则化，<script type="math/tex">\lambda</script>较大，即对权重<script type="math/tex">w^{[l]}</script>的<strong>惩罚较大</strong>，<script type="math/tex">w^{[l]}</script>减小。因为<script type="math/tex">z^{[l]}=w^{[l]}a^{[l]}+b^{[l]}</script>。当<script type="math/tex">w^{[l]}</script>减小的时候，<script type="math/tex">z^{[l]}</script>也会减小。则此时的<script type="math/tex">z^{[l]}</script>分布在tanh函数的近似线性区域。那么这个神经元起的作用就相当于是<strong>linear regression</strong>。如果每个神经元对应的权重<script type="math/tex">w^{[l]}</script>都比较小，那么整个神经网络模型相当于是多个linear regression的组合，即可看成一个linear network。得到的分类超平面就会比较简单，不会出现过拟合现象。</p>
<p><img src="https://pic.imgdb.cn/item/648acdfa1ddac507cc7c246d.png" style="zoom:60%"></p>
<h4 id="5-6-Dropout-正则化"><a href="#5-6-Dropout-正则化" class="headerlink" title="5.6 Dropout 正则化"></a>5.6 <strong>Dropout 正则化</strong></h4><p>除了L2 regularization之外，还有另外一种防止过拟合的有效方法：Dropout。</p>
<p>Dropout是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。</p>
<p><img src="https://pic.imgdb.cn/item/648ace5a1ddac507cc7d6ab8.png" style="zoom:60%"></p>
<p>Dropout有不同的实现方法，接下来介绍一种常用的方法：<strong>Inverted dropout</strong>。假设对于第<script type="math/tex">l</script>层神经元，设定保留神经元比例概率keep_prob=0.8，即该层有20%的神经元停止工作。<script type="math/tex">dl</script>为dropout向量，设置<script type="math/tex">dl</script>为随机vector，其中80%的元素为1，20%的元素为0。在python中可以使用如下语句生成dropout vector：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>],al.shape[<span class="number">1</span>])&lt;keep_prob</span><br></pre></td></tr></table></figure>
<p>然后，第<script type="math/tex">l</script>层经过dropout，随机删减20%的神经元，只保留80%的神经元，其输出为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">al = np.multiply(al,dl)</span><br></pre></td></tr></table></figure>
<p>最后，还要对<script type="math/tex">al</script>进行scale up处理，即：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">al /= keep_prob</span><br></pre></td></tr></table></figure>
<p>以上就是Inverted dropout的方法。之所以要对<script type="math/tex">al</script>进行scale up是为了保证在经过dropout后，<script type="math/tex">al</script><strong>作为下一层神经元的输入值尽量保持不变</strong>。假设第<script type="math/tex">l</script>层有50个神经元，经过dropout后，有10个神经元停止工作，这样只有40神经元有作用。那么得到的<script type="math/tex">al</script>只相当于原来的80%。scale up后，能够尽可能保持<script type="math/tex">al</script>的期望值相比之前没有大的变化。</p>
<p>Inverted dropout的另外一个好处就是在对该dropout后的神经网络进行测试时能够减少scaling问题。因为在训练时，使用scale up保证<script type="math/tex">al</script>的期望值没有大的变化，测试时就不需要再对样本数据进行类似的尺度伸缩操作了。</p>
<p>对于<script type="math/tex">m</script>个样本，单次迭代训练时，随机删除掉隐藏层一定数量的神经元；然后，在删除后的剩下的神经元上正向和反向更新权重w和常数项b；接着，下一次迭代中，再恢复之前删除的神经元，重新随机删除一定数量的神经元，进行正向和反向更新w和b。不断重复上述过程，直至迭代训练完成。</p>
<p><strong>值得注意的是，使用dropout训练结束后，在测试和实际应用模型时，不需要进行dropout和随机删减神经元，所有的神经元都在工作。</strong></p>
<h4 id="5-7-理解Dropout"><a href="#5-7-理解Dropout" class="headerlink" title="5.7 理解Dropout"></a>5.7 理解Dropout</h4><p>Dropout通过每次迭代训练时，随机选择不同的神经元，相当于每次都在不同的神经网络上进行训练，类似机器学习中Bagging的方法(三个臭皮匠，赛过诸葛亮)，能够防止过拟合。</p>
<p>除此之外，还可以从权重<script type="math/tex">w</script>的角度来解释为什么dropout能够有效防止过拟合。对于某个神经元来说，某次训练时，它的某些输入在dropout的作用被过滤了。而在下一次训练时，又有不同的某些输入被过滤。经过多次训练后，某些输入被过滤，某些输入被保留。这样，该神经元就不会受某个输入非常大的影响，影响被均匀化了。也就是说，对应的权重<script type="math/tex">w</script>不会很大。这从从效果上来说，与L2 regularization是类似的，都是对权重<script type="math/tex">w</script>进行“惩罚”，减小了<script type="math/tex">w</script>的值。</p>
<p><img src="https://pic.imgdb.cn/item/648acec31ddac507cc7f27dd.png"></p>
<p>总结一下，对于同一组训练数据，利用不同的神经网络训练之后，求其输出的平均值可以减少overfitting。Dropout就是利用这个原理，每次丢掉一定数量的隐藏层神经元，<strong>相当于在不同的神经网络上进行训练</strong>，这样就<strong>减少了神经元之间的依赖性</strong>，即每个神经元不能依赖于某几个其他的神经元(指层与层之间相连接的神经元)，使神经网络更加能学习到与其他神经元之间的更加健壮robust的特征。</p>
<p>在使用dropout的时候，有几点需要注意。</p>
<p>(1) 不同隐藏层的dropout系数keep_prob可以不同。一般来说，神经元越多的隐藏层，keep_out可以设置得小一些，例如0.5；神经元越少的隐藏层，keep_out可以设置的大一些，例如0.8，设置是1。</p>
<p>(2) 实际应用中，不建议对输入层进行dropout，如果输入层维度很大，例如图片，那么可以设置dropout，但keep_out应设置的大一些，例如0.8，0.9。</p>
<p>总体来说，就是越容易出现overfitting的隐藏层，其keep_prob就设置的相对小一些。没有准确固定的做法，通常可以根据validation进行选择。</p>
<p>Dropout在电脑视觉CV领域应用比较广泛，因为输入层维度较大，而且没有足够多的样本数量。值得注意的是dropout是一种regularization技巧，用来防止过拟合的，最好只在需要regularization的时候使用dropout。</p>
<p>使用dropout的时候，可以通过绘制cost function来进行debug，看看dropout是否正确执行。一般做法是，将所有层的keep_prob全设置为1，再绘制cost function，即涵盖所有神经元，看<script type="math/tex">J</script>是否单调下降。下一次迭代训练时，再将keep_prob设置为其它值。</p>
<h4 id="5-8-其他正则化方法"><a href="#5-8-其他正则化方法" class="headerlink" title="5.8 其他正则化方法"></a>5.8 其他正则化方法</h4><p>除了L2 regularization和dropout regularization之外，还有其它减少过拟合的方法。</p>
<ul>
<li><strong>增加训练样本数量</strong><br>但是通常成本较高，难以获得额外的训练样本。但是，我们可以对已有的训练样本进行一些处理来“制造”出更多的样本，称为data augmentation。例如图片识别问题中，可以对已有的图片进行水平翻转、垂直翻转、任意角度旋转、缩放或扩大等等。如下图所示，这些处理都能“制造”出新的训练样本。虽然这些是基于原有样本的，但是对增大训练样本数量还是有很有帮助的，不需要增加额外成本，却能起到防止过拟合的效果。</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/648acefc1ddac507cc800831.png"></p>
<p>在数字识别中，也可以将原有的数字图片进行任意旋转或者扭曲，或者增加一些noise，如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648acf2b1ddac507cc80def8.png"></p>
<ul>
<li><p><strong>early stopping</strong></p>
<p>一个神经网络模型随着迭代训练次数增加，train set error一般是单调减小的，而dev set error 先减小，之后又增大。也就是说训练次数过多时，模型会对训练样本拟合的越来越好，但是对验证集拟合效果逐渐变差，即发生了过拟合。因此，迭代训练次数不是越多越好，可以通过train set error和dev set error随着迭代次数的变化趋势，选择合适的迭代次数，即early stopping。</p>
</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/648ad1d81ddac507cc8a8a35.png" style="zoom:60%"></p>
<p>然而，Early stopping有其自身缺点。通常来说，机器学习训练模型有两个目标：</p>
<p>一是优化cost function，尽量减小<script type="math/tex">J</script>；</p>
<p>二是防止过拟合。</p>
<p>这两个目标彼此对立的，即<strong>减小</strong><script type="math/tex">J</script><strong>的同时可能会造成过拟合</strong>，反之亦然。我们把这二者之间的关系称为正交化(orthogonalization)。该节课开始部分就讲过，在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。但是，Early stopping的做法通过减少得带训练次数来防止过拟合，这样<script type="math/tex">J</script>就不会足够小。也就是说，early stopping将上述两个目标融合在一起，同时优化，但可能没有“分而治之”的效果好。</p>
<p>与early stopping相比，<strong>L2 regularization可以实现“分而治之”的效果</strong>：迭代训练足够多，减小<script type="math/tex">J</script>，而且也能有效防止过拟合。而L2 regularization的缺点之一是最优的正则化参数<script type="math/tex">\lambda</script>的选择比较复杂。对这一点来说，early stopping比较简单。总的来说，L2 regularization更加常用一些。</p>
<h4 id="5-9-归一化输入"><a href="#5-9-归一化输入" class="headerlink" title="5.9 归一化输入"></a>5.9 归一化输入</h4><p>在训练神经网络时，标准化输入可以提高训练的速度。标准化输入就是对训练数据集进行归一化的操作，即将原始数据减去其均值<script type="math/tex">\mu</script>后，再除以其方差<script type="math/tex">σ^2</script>：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mu=\frac{1}{m} \sum_{i=1}^{m} X^{(i)} \\
\sigma^{2}=\frac{1}{m} \sum_{i=1}^{m}\left(X^{(i)}\right)^{2} \\
X:=\frac{X-\mu}{\sigma^{2}}
\end{gathered}</script><p>以二维平面为例，下图展示了其归一化过程：</p>
<p><img src="https://pic.imgdb.cn/item/648ad21a1ddac507cc8b622e.png"></p>
<p>值得注意的是，由于训练集进行了标准化处理，那么对于测试集或在实际应用时，应该使用同样的<script type="math/tex">\mu</script>和<script type="math/tex">σ^{2}</script>对其进行标准化处理。这样保证了训练集合测试集的标准化操作一致。</p>
<p>之所以要对输入进行标准化操作，主要是为了让所有<strong>输入归一化同样的尺度上</strong>，方便进行梯度下降算法时能够更快更准确地找到全局最优解。假如输入特征是二维的，且<script type="math/tex">x_1</script>的范围是<script type="math/tex">[1,1000]</script>，<script type="math/tex">x_2</script>的范围是<script type="math/tex">[0,1]</script>。如果不进行标准化处理，<script type="math/tex">x_1</script>与<script type="math/tex">x_2</script>之间分布极不平衡，训练得到的<script type="math/tex">w_1</script>和<script type="math/tex">w_2</script>也会在数量级上差别很大。这样导致的结果是cost  function与<script type="math/tex">w</script>和<script type="math/tex">b</script>的关系可能是一个非常细长的椭圆形碗。对其进行梯度下降算法时，由于<script type="math/tex">w_1</script>和<script type="math/tex">w_2</script>数值差异很大，只能选择很小的学习因子<script type="math/tex">\alpha</script>，来避免<script type="math/tex">J</script>发生振荡。一旦<script type="math/tex">\alpha</script>较大，必然发生振荡，<script type="math/tex">J</script>不再单调下降。如下左图所示。</p>
<p>然而，如果进行了标准化操作，<script type="math/tex">x_1</script>与<script type="math/tex">x_2</script>分布均匀，<script type="math/tex">w_1</script>和<script type="math/tex">w_2</script>数值差别不大，得到的cost function与<script type="math/tex">w</script>和<script type="math/tex">b</script>的关系是类似圆形碗。对其进行梯度下降算法时，<script type="math/tex">\alpha</script>可以选择相对大一些，且<script type="math/tex">J</script>一般不会发生振荡，保证了<script type="math/tex">J</script>是单调下降的。如下右图所示。</p>
<p><img src="https://pic.imgdb.cn/item/648ad2621ddac507cc8c5469.png"></p>
<p>另外一种情况，如果输入特征之间的范围本来就比较接近，那么不进行标准化操作也是没有太大影响的。但是，标准化处理在大多数场合下还是值得推荐的。</p>
<p>本小节参考链接：<a href="https://blog.csdn.net/u012768474/article/details/99871942">标准化和归一化的区别</a></p>
<h4 id="5-10-梯度消失与梯度爆炸"><a href="#5-10-梯度消失与梯度爆炸" class="headerlink" title="5.10 梯度消失与梯度爆炸"></a>5.10 梯度消失与梯度爆炸</h4><p>在神经网络尤其是深度神经网络中存在可能存在这样一个问题：梯度消失和梯度爆炸。意思是当训练一个层数非常多的神经网络时，计算得到的梯度可能非常小或非常大，甚至是指数级别的减小或增大。这样会让训练过程变得非常困难。</p>
<p>举个例子来说明，假设一个多层的每层只包含两个神经元的深度神经网络模型，如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648ad2b81ddac507cc8d5d7a.png"></p>
<p>为了简化复杂度便于分析，我们令各层的激活函数为线性函数，即<script type="math/tex">g(Z)=Z</script>。且忽略各层常数项<script type="math/tex">b</script>的影响，令<script type="math/tex">b</script>全部为零。那么，该网络的预测输出<script type="math/tex">\hat Y</script>为：</p>
<script type="math/tex; mode=display">
\hat{Y}=W^{[L]} W^{[L-1]} W^{[L-2]} \ldots W^{[3]} W^{[2]} W^{[1]} X</script><p>如果各层权重<script type="math/tex">W^{[l]}</script>的元素都稍大于<script type="math/tex">1</script>，例如<script type="math/tex">1.5</script>，则预测输出<script type="math/tex">\hat Y</script>将正比于<script type="math/tex">1.5^L</script>。<script type="math/tex">L</script>越大，<script type="math/tex">\hat Y</script>越大，且呈指数型增长。我们称之为数值爆炸。相反，如果各层权重<script type="math/tex">W^{[l]}</script>的元素都稍小于<script type="math/tex">1</script>，例如<script type="math/tex">0.5</script>，则预测输出<script type="math/tex">\hat Y</script>将正比于<script type="math/tex">0.5^L</script>。网络层数<script type="math/tex">L</script>越多，<script type="math/tex">\hat Y</script>呈指数型减小。我们称之为数值消失。</p>
<p>也就是说，如果各层权重<script type="math/tex">W^{[l]}</script>都大于<script type="math/tex">1</script>或者都小于<script type="math/tex">1</script>，那么各层激活函数的输出将随着层数<script type="math/tex">l</script>的增加，呈指数型增大或减小。当层数很大时，出现数值爆炸或消失。同样，这种情况也会引起梯度呈现同样的指数型增大或减小的变化。<script type="math/tex">L</script>非常大时，例如<script type="math/tex">L=150</script>，则梯度会非常大或非常小，引起每次更新的步进长度过大或者过小，这让训练过程十分困难。</p>
<h4 id="5-11-神经网络的初始化权重"><a href="#5-11-神经网络的初始化权重" class="headerlink" title="5.11 神经网络的初始化权重"></a>5.11 神经网络的初始化权重</h4><p>下面介绍如何改善Vanishing and Exploding gradients这类问题，方法是对权重<script type="math/tex">w</script>进行一些初始化处理。</p>
<p>深度神经网络模型中，以单个神经元为例，该层(<script type="math/tex">l</script><em>)</em>的输入个数为<script type="math/tex">n</script>，其输出为：</p>
<script type="math/tex; mode=display">
\begin{gathered}z=w_{1} x_{1}+w_{2} x_{2}+\cdots+w_{n} x_{n} \\a=g(z)\end{gathered}</script><p><img src="https://pic.imgdb.cn/item/648ad35d1ddac507cc902cab.png" style="zoom:40%"></p>
<p>这里忽略了常数项<script type="math/tex">b</script>。为了让<script type="math/tex">z</script>不会过大或者过小，由上面的式子可以有思路是<strong>让</strong><script type="math/tex">w</script><strong>与</strong><script type="math/tex">n</script><strong>有关，且</strong><script type="math/tex">n</script><strong>越大，</strong><script type="math/tex">w</script><strong>应该越小才好</strong>。这样能够保证<script type="math/tex">z</script>不会过大。一种方法是在初始化时，控制<script type="math/tex">w</script>的方差。</p>
<p>(1) 如果激活函数是tanh，<strong>令</strong><script type="math/tex">w</script><strong>方差为</strong><script type="math/tex">1/n</script>;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])*np.sqrt(<span class="number">1</span>/n[l-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>(2) 如果激活函数是ReLU，<strong>令</strong><script type="math/tex">w</script><strong>方差为</strong><script type="math/tex">2/n</script>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])*np.sqrt(<span class="number">2</span>/n[l-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>除此之外，Yoshua Bengio提出了另外一种初始化w的方法，令其方差为<script type="math/tex">\frac{2}{n^{[l−1]}n^{[l]}}</script>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])*np.sqrt(<span class="number">2</span>/n[l-<span class="number">1</span>]*n[l])</span><br></pre></td></tr></table></figure>
<p>至于选择哪种初始化方法因人而异，可以根据不同的激活函数选择不同方法。另外，我们可以对这些初始化方法中设置某些参数，作为超参数，通过验证集进行验证，得到最优参数，来优化神经网络。</p>
<h4 id="5-12-梯度的数值逼近"><a href="#5-12-梯度的数值逼近" class="headerlink" title="5.12  梯度的数值逼近"></a>5.12  梯度的数值逼近</h4><p>Back Propagation神经网络有一项重要的测试是<strong>梯度检查(gradient checking)</strong>。其目的是检查验证反向传播过程中梯度下降算法是否正确。该小节将先介绍如何近似求出梯度值。</p>
<p><img src="https://pic.imgdb.cn/item/648ad5d11ddac507cc9a3d8a.png" style="zoom:70%"></p>
<p>利用微分思想，函数<script type="math/tex">f</script>在点<script type="math/tex">θ</script>处的梯度可以表示成：</p>
<script type="math/tex; mode=display">
\frac{\mathrm df}{\mathrm d\theta} = \lim_{\varepsilon \rightarrow 0}\frac{f(\theta+\varepsilon)-f(\theta-\varepsilon)}{2\varepsilon}</script><p>其中，<script type="math/tex">\varepsilon>0</script>且足够小。</p>
<h4 id="5-13-梯度检验"><a href="#5-13-梯度检验" class="headerlink" title="5.13 梯度检验"></a>5.13 梯度检验</h4><p>介绍完如何近似求出梯度值后，我们将介绍如何进行梯度检查，来验证训练过程中是否出现bugs。</p>
<p>梯度检查首先要做的是分别将<script type="math/tex">W^{[1]}, b^{[1]}, \ldots, W^{[L]}, b^{[L]}</script>这些矩阵构造成一维向量，然后将这些一维向量组合起来构成一个更大的一维向量<script type="math/tex">\theta</script>，这样cost function<script type="math/tex">J(W^{[1]}, b^{[1]}, \cdots, W^{[L]}, b^{[L]})</script>就可以表示成<script type="math/tex">J(\theta)</script>。</p>
<p>然后将反向传播过程通过梯度下降算法得到的<script type="math/tex">\mathrm d W^{[1]}, \mathrm d b^{[1]}, \cdots, \mathrm d W^{[L]}, \mathrm d b^{[L]}</script>按照一样的顺序构造成一个一维向量<script type="math/tex">\mathrm d\theta</script>(与<script type="math/tex">\theta</script>维度相同)。</p>
<p>接着利用<script type="math/tex">J(θ)</script>对每个<script type="math/tex">θ_i</script>计算近似梯度，其值与反向传播算法得到的<script type="math/tex">dθ_i</script>相比较，检查是否一致。例如，对于第<script type="math/tex">i</script>个元素，近似梯度为：</p>
<script type="math/tex; mode=display">
\mathrm d \theta_{\operatorname{approx}}[i]=\frac{J\left(\theta_{1}, \theta_{2}, \cdots, \theta_{i}+\varepsilon, \cdots\right)-J\left(\theta_{1}, \theta_{2}, \cdots, \theta_{i}-\varepsilon, \cdots\right)}{2 \varepsilon}</script><p>计算完所有<script type="math/tex">θ_i</script>的近似梯度后，可以计算<script type="math/tex">\mathrm dθ_{\text {approx}}</script>与<script type="math/tex">dθ</script>的<strong>欧氏(Euclidean)距离</strong>来比较二者的相似度。公式如下：</p>
<script type="math/tex; mode=display">
\frac{\left\| \mathrm d \theta_{\text {approx }}-\mathrm d \theta\right\|_{2}}{\left\|\mathrm d \theta_{\text {approx }}\right\|_{2}+\|\mathrm d \theta\|_{2}}</script><p>一般来说，若欧氏距离越小，例如<script type="math/tex">10^{−7}</script>甚至更小，则表明<script type="math/tex">\mathrm dθ_{\text {approx}}</script>与<script type="math/tex">dθ</script>越接近，即反向梯度计算是正确的，没有bugs。若欧氏距离较大，例如<script type="math/tex">10^{−5}</script>，则表明梯度计算可能出现问题，需要再次检查是否有bugs存在。若欧氏距离很大，例如<script type="math/tex">10^{−3}</script>，甚至更大，则表明<script type="math/tex">\mathrm dθ_{\text {approx}}</script>与<script type="math/tex">\mathrm dθ</script>差别很大，梯度下降计算过程有bugs，需要仔细检查。</p>
<h4 id="5-14-关于梯度检验实现的注记"><a href="#5-14-关于梯度检验实现的注记" class="headerlink" title="5.14 关于梯度检验实现的注记"></a>5.14 关于梯度检验实现的注记</h4><p>在进行梯度检查的过程中有几点需要注意的地方：</p>
<ul>
<li><strong>不要在整个训练过程中都进行梯度检查，仅仅作为debug使用。</strong></li>
<li><strong>如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。</strong></li>
<li><strong>注意不要忽略正则化项，计算近似梯度的时候要包括进去。</strong></li>
<li><strong>梯度检查时关闭dropout，检查完毕后再打开dropout。</strong></li>
<li><strong>随机初始化时运行梯度检查，经过一些训练后再进行梯度检查（不常用）。</strong></li>
</ul>
<h3 id="第六章-优化算法"><a href="#第六章-优化算法" class="headerlink" title="第六章 优化算法"></a>第六章 优化算法</h3><h4 id="6-1-Mini-batch-梯度下降法"><a href="#6-1-Mini-batch-梯度下降法" class="headerlink" title="6.1 Mini-batch 梯度下降法"></a><strong>6.1 Mini-batch 梯度下降法</strong></h4><p>之前介绍的神经网络训练过程是对所有<script type="math/tex">m</script>个样本，称为<strong>batch</strong>，通过向量化计算方式，同时进行的。如果<script type="math/tex">m</script>很大，例如达到百万数量级，训练速度往往会很慢，因为每次迭代都要对所有样本进行进行求和运算和矩阵运算。我们将这种梯度下降算法称为<strong>Batch  Gradient Descent</strong>。</p>
<p>为了解决这一问题，我们可以把<script type="math/tex">m</script>个训练样本分成若干个子集，称为<strong>mini-batches</strong>，这样每个子集包含的数据量就小了，例如只有1000，然后每次在单一子集上进行神经网络训练，速度就会大大提高。这种梯度下降算法叫做<strong>Mini-batch Gradient Descent</strong>。</p>
<p>假设总的训练样本个数<script type="math/tex">m=5000000</script>，其维度为<script type="math/tex">(n_x,m)</script>。将其分成<script type="math/tex">5000</script>个子集，每个mini-batch含有<script type="math/tex">1000</script>个样本。我们将每个mini-batch记为<script type="math/tex">X^{\{ t \}}</script>，其维度为<script type="math/tex">(n_x,1000)</script>。相应的每个mini-batch的输出记为<script type="math/tex">Y^{\{ t \}}</script>，其维度为<script type="math/tex">(1,1000)</script>，且<script type="math/tex">t=1,2,⋯,5000</script>。</p>
<p>这里顺便总结一下我们遇到的神经网络中几类字母的上标含义：</p>
<ul>
<li>$X^{(i)}$<strong>：第</strong>$i$<strong>个样本</strong></li>
<li>$Z^{[l]}$<strong>：神经网络第</strong>$l$<strong>层网络的线性输出</strong></li>
<li>$X^{\{t\}},Y^{\{t\}}$<strong>：第</strong>$t$<strong>组mini-batch</strong></li>
</ul>
<p>ini-batches Gradient Descent的实现过程是先将总的训练样本分成<script type="math/tex">T</script>个子集(mini-batches)，然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至<script type="math/tex">T</script>个mini-batch都训练完毕。</p>
<p><img src="https://pic.imgdb.cn/item/648adc111ddac507ccae58b0.png" style="zoom:70%"></p>
<p>经过<script type="math/tex">T</script>次循环之后，所有<script type="math/tex">m</script>个训练样本都进行了梯度下降计算。这个过程，我们称之为经历了一个<strong>epoch</strong>。对于Batch Gradient Descent而言，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行<script type="math/tex">T</script>次梯度下降算法。</p>
<p>值得一提的是，对于Mini-Batches Gradient Descent，可以进行多次epoch训练。而且，每次epoch，最好是将总体训练数据<strong>重新打乱</strong>、<strong>重新分成</strong><script type="math/tex">T</script><strong>组</strong>mini-batches，这样有利于训练出最佳的神经网络模型。</p>
<h4 id="6-2-理解Mini-batch-梯度下降法"><a href="#6-2-理解Mini-batch-梯度下降法" class="headerlink" title="6.2 理解Mini-batch 梯度下降法"></a>6.2 理解<strong>Mini-batch 梯度下降法</strong></h4><p>Batch gradient descent和Mini-batch gradient descent的cost曲线如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648add1e1ddac507ccb1c568.png"></p>
<p>对于一般的神经网络模型，使用Batch gradient descent，随着迭代次数增加，cost是不断减小的。然而，使用Mini-batch gradient descent，随着在不同的mini-batch上迭代训练，其cost不是单调下降，而是受类似noise的影响，出现振荡。但整体的趋势是下降的，最终也能得到较低的cost值。</p>
<p>之所以出现细微振荡的原因是不同的mini-batch之间是有差异的。例如可能第一个子集<script type="math/tex">(X^{\{1\}},Y^{\{1\}})</script>是好的子集，而第二个子集<script type="math/tex">(X^{\{2\}},Y^{\{2\}})</script>包含了一些噪声noise。出现细微振荡是正常的。</p>
<p>如何选择每个mini-batch的大小，即包含的样本个数呢？有两个极端：</p>
<p>(1) 如果mini-batch size=m，即为Batch gradient descent，只包含一个子集为<script type="math/tex">(X^{\{1\}},Y^{\{1\}}) = (X,Y)</script>；</p>
<p>(2) 如果mini-batch size=1，即为Stachastic gradient descent，每个样本就是一个子集<script type="math/tex">(X^{\{1\}},Y^{\{1\}}) = (x^{(i)},y^{(i)})</script>，共有<script type="math/tex">m</script>个子集。</p>
<p>我们来比较一下Batch gradient descent和Stachastic gradient  descent的梯度下降曲线。如下图所示，蓝线代表Batch gradient descent，紫线代表Stachastic  gradient descent。Batch gradient  descent会比较平稳地接近全局最小值，但是因为使用了所有<script type="math/tex">m</script>个样本，每次前进的速度有些慢。Stachastic gradient  descent每次前进速度很快，但是路线曲折，有较大的振荡，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就不能使用向量化的方法来提高运算速度。</p>
<p><img src="https://pic.imgdb.cn/item/648add4a1ddac507ccb24ba3.png" style="zoom:50%"></p>
<p>实际使用中，mini-batch size不能设置得太大(Batch gradient descent)，也不能设置得太小(Stachastic gradient descent)。这样，相当于结合了Batch gradient descent和Stachastic gradient descent各自的优点，既能使用向量化优化算法，又能叫快速地找到最小值。mini-batch gradient descent的梯度下降曲线如下图绿色所示，每次前进速度较快，且振荡较小，基本能接近全局最小值。</p>
<p><img src="https://pic.imgdb.cn/item/648add6c1ddac507ccb2b3fe.png" style="zoom:50%"></p>
<p>一般来说，如果总体样本数量<script type="math/tex">m</script>不太大时，例如<script type="math/tex">m≤2000</script>，建议直接使用Batch  gradient descent。如果总体样本数量<script type="math/tex">m</script>很大时，建议将样本分成许多mini-batches。推荐常用的mini-batch  size为<script type="math/tex">64</script>,<script type="math/tex">128</script>,<script type="math/tex">256</script>,<script type="math/tex">512</script>。这些都是<strong>2的幂</strong>。之所以这样设置的原因是计算机存储数据一般是2的幂，这样设置可以提高运算速度。</p>
<p>本小节参考链接：<a href="https://www.zhihu.com/question/501172146/answer/2247473794">关于Mini-batch梯度下降法的困惑，为什么会比批量梯度下降法快？</a></p>
<h4 id="6-3-指数加权平均"><a href="#6-3-指数加权平均" class="headerlink" title="6.3 指数加权平均"></a>6.3 指数加权平均</h4><p>举个例子，记录半年内伦敦市的气温变化，并在二维平面上绘制出来，如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648addbb1ddac507ccb3a2ad.png"></p>
<p>看上去，温度数据似乎有noise，而且抖动较大。如果我们希望看到半年内气温的整体变化趋势，可以通过移动平均（moving average）的方法来对每天气温进行平滑处理。</p>
<p>例如我们可以设<script type="math/tex">V_0=0</script>，当成第<script type="math/tex">0</script>天的气温值。</p>
<p>第一天的气温与第<script type="math/tex">0</script>天的气温有关：</p>
<script type="math/tex; mode=display">
V_{1}=0.9 V_{0}+0.1 \theta_{1}</script><p>第二天的气温与第一天的气温有关：</p>
<script type="math/tex; mode=display">
\begin{aligned}
V_{2} &=0.9 V_{1}+0.1 \theta_{2} \\
&=0.9\left(0.9 V_{0}+0.1 \theta_{1}\right)+0.1 \theta_{2} \\
&=0.9^{2} V_{0}+0.9 \cdot 0.1 \theta_{1}+0.1 \theta_{2}
\end{aligned}</script><p>即第<script type="math/tex">t</script>天与第<script type="math/tex">t-1</script>天的气温迭代关系为：</p>
<script type="math/tex; mode=display">
\begin{aligned}V_{t} &=0.9 V_{t-1}+0.1 \theta_{t} \\&=0.9^{t} V_{0}+0.9^{t-1} \cdot 0.1 \theta_{1}+0.9^{t-2} \cdot 0.1 \theta_{2}+\cdots+0.9 \cdot 0.1 \theta_{t-1}+0.1 \theta_{t}\end{aligned}</script><p>经过移动平均处理得到的气温如下图红色曲线所示：</p>
<p><img src="https://pic.imgdb.cn/item/648adf601ddac507ccb8d5ff.png"></p>
<p>这种<strong>滑动平均算法</strong>称为<strong>指数加权平均(exponentially weighted average)</strong>。根据之前的推导公式，其一般形式为：</p>
<script type="math/tex; mode=display">
V_t = (1-\beta)V_{t-1}+\beta \theta_t</script><p>上面的例子中<script type="math/tex">β=0.9</script>。<script type="math/tex">\beta</script><strong>值决定了指数加权平均的天数</strong>，近似表示为：</p>
<script type="math/tex; mode=display">
\frac{1}{1-\beta}</script><p>例如，<script type="math/tex">β=0.9</script>，则<script type="math/tex">1/(1−β)=10</script>，表示将前<script type="math/tex">10</script>天进行指数加权平均。<script type="math/tex">β=0.98</script>，则<script type="math/tex">1/(1−β)=50</script>，表示将前<script type="math/tex">50</script>天进行指数加权平均。<script type="math/tex">\beta</script><strong>越大</strong>，则指数加权平均的天数<strong>越多</strong>，平均后的趋势线就<strong>越平缓</strong>，但是同时也会向右平移。下图绿色曲线和黄色曲线分别表示了<script type="math/tex">β=0.98</script>和<script type="math/tex">β=0.5</script>时，指数加权平均的结果。</p>
<p><img src="https://pic.imgdb.cn/item/648ae02f1ddac507ccbb94b6.png"></p>
<p>这里简单解释一下公式<script type="math/tex">1/{(1−β)}</script>是怎么来的。准确来说，指数加权平均算法跟之前所有天的数值都有关系，根据之前的推导公式就能看出。但是指数是衰减的，<strong>一般认为衰减到</strong><script type="math/tex">1/e</script><strong>就可以忽略不计了</strong>。因此，根据之前的推导公式，我们只要证明</p>
<script type="math/tex; mode=display">
\beta^{\frac{1}{1-\beta}} = \frac1e</script><p>令<script type="math/tex">\dfrac{1}{1-\beta}=N, N>0</script>，则<script type="math/tex">\beta=1-\dfrac{1}{N}, \dfrac{1}{N}<1</script>。即证明转化为：</p>
<script type="math/tex; mode=display">
\left(1-\frac{1}{N}\right)^{N}=\frac{1}{e}</script><p>显然，当<script type="math/tex">N>>0</script>时，上述等式是近似成立的。至此，简单解释了为什么指数加权平均的天数的计算公式为<script type="math/tex">1/{(1−β)}</script>。</p>
<h4 id="6-4-理解指数加权平均"><a href="#6-4-理解指数加权平均" class="headerlink" title="6.4 理解指数加权平均"></a>6.4 理解指数加权平均</h4><p>我们将指数加权平均公式的一般形式写下来：</p>
<script type="math/tex; mode=display">
\begin{aligned}V_{t}=& \beta V_{t-1}+(1-\beta) \theta_{t} \\=&(1-\beta) \theta_{t}+(1-\beta) \cdot \beta \cdot \theta_{t-1}+(1-\beta) \cdot \beta^{2} \cdot \theta_{t-2}+\cdots \\&+(1-\beta) \cdot \beta^{t-1} \cdot \theta_{1}+\beta^{t} \cdot V_{0}\end{aligned}</script><p>上式中<script type="math/tex">\theta_{t}, \theta_{t-1}, \theta_{t-2}, \cdots, \theta_{1}</script>是原始数据值，<script type="math/tex">(1-\beta),(1-\beta) \beta,(1-\beta) \beta^{2}, \cdots,(1-\beta) \beta^{t-1}</script>是类似指数曲线，从右向左，呈指数下降的。<script type="math/tex">V_t</script>的值就是这两个子式的点乘，将原始数据值与衰减指数点乘，相当于做了指数衰减，离得越近，影响越大，离得越远，影响越小，衰减越厉害。</p>
<p><img src="https://pic.imgdb.cn/item/648ae0fd1ddac507ccbe7f06.png" style="zoom:50%"></p>
<p>我们已经知道了指数加权平均的递推公式。实际应用中为了<strong>减少内存</strong>的使用，我们可以使用这样的语句来实现指数加权平均算法：</p>
<p><img src="https://pic.imgdb.cn/item/648ae16e1ddac507ccc01a03.png" style="zoom:70%"></p>
<p>本小节的参考链接：<a href="https://www.jianshu.com/p/41218cb5e099">为什么在优化算法中使用指数加权平均</a></p>
<h4 id="6-5-指数加权平均的偏差修正"><a href="#6-5-指数加权平均的偏差修正" class="headerlink" title="6.5 指数加权平均的偏差修正"></a>6.5 指数加权平均的偏差修正</h4><p>上文中提到当<script type="math/tex">β=0.98</script>时，指数加权平均结果如下图绿色曲线所示。但是实际上，真实曲线如紫色曲线所示。</p>
<p><img src="https://pic.imgdb.cn/item/648ae1b81ddac507ccc14d37.png"></p>
<p>我们注意到，紫色曲线与绿色曲线的区别是，紫色曲线开始的时候相对较低一些。这是因为开始时我们设置<script type="math/tex">V_0=0</script>，所以初始值会相对小一些，直到后面受前面的影响渐渐变小，趋于正常。</p>
<p>修正这种问题的方法是进行<strong>偏移校正(bias correction)</strong>，即在每次计算完<script type="math/tex">V_t</script>后，对<script type="math/tex">V_t</script>进行下式处理：</p>
<script type="math/tex; mode=display">
\frac{V_t}{1-\beta^t}</script><p>在刚开始的时候，<script type="math/tex">t</script>比较小，<script type="math/tex">(1−β^t)<1</script>，这样就将<script type="math/tex">V_t</script>修正得更大一些，效果是把紫色曲线开始部分向上提升一些，与绿色曲线接近重合。随着<script type="math/tex">t</script>增大，<script type="math/tex">(1−β^t)≈1</script>，<script type="math/tex">V_t</script>基本不变，紫色曲线与绿色曲线依然重合。这样就实现了简单的偏移校正，得到我们希望的绿色曲线。</p>
<p>注意机器学习中，偏移校正并不是必须的。因为，在迭代一次次数后(<script type="math/tex">t</script>较大)，<script type="math/tex">V_t</script>受初始值影响微乎其微，紫色曲线与绿色曲线基本重合。所以，一般可以忽略初始迭代过程，等到一定迭代之后再取值，这样就不需要进行偏移校正了。</p>
<h4 id="6-6-动量-momentum-梯度下降法"><a href="#6-6-动量-momentum-梯度下降法" class="headerlink" title="6.6 动量(momentum)梯度下降法"></a>6.6 动量(<strong>momentum</strong>)梯度下降法</h4><p>动量梯度下降与梯度下降相比，就是对梯度使用<a href="https://www.zhihu.com/search?q=%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A30743067%7D">指数加权平均</a>，其他的都保存一致</p>
<p>该部分将介绍动量梯度下降算法，其速度要比传统的梯度下降算法快很多。做法是在每次训练时，对梯度进行指数加权平均处理，然后用得到的梯度值更新权重<script type="math/tex">W</script>和常数项<script type="math/tex">b</script>。下面介绍具体的实现过程。</p>
<p><img src="https://pic.imgdb.cn/item/648ae1d51ddac507ccc1d475.png"></p>
<p>原始的梯度下降算法如上图蓝色折线所示。在梯度下降过程中，梯度下降的振荡较大，尤其对于<script type="math/tex">W、b</script>之间数值范围差别较大的情况。此时每一点处的梯度只与当前方向有关，产生类似折线的效果，前进缓慢。而如果<strong>对梯度进行指数加权平均</strong>，这样使当前梯度不仅与当前方向有关，还与之前的方向有关，这样处理让梯度前进方向更加平滑，减少振荡，能够更快地到达最小值处。</p>
<p>权重<script type="math/tex">W</script>和常数项<script type="math/tex">b</script>的指数加权平均表达式如下：</p>
<script type="math/tex; mode=display">
\begin{gathered}
V_{\mathrm d W}=\beta \cdot V_{\mathrm d W}+(1-\beta) \cdot \mathrm d W \\
V_{\mathrm d b}=\beta \cdot V_{\mathrm d b}+(1-\beta) \cdot \mathrm d b
\end{gathered}</script><p>从动量的角度来看，以权重<script type="math/tex">W</script>为例，<script type="math/tex">V_{\mathrm dW}</script>可以成速度V，<script type="math/tex">\mathrm dW</script>可以看成是加速度<script type="math/tex">a</script>。指数加权平均实际上是计算当前的速度，当前速度由之前的速度和现在的加速度共同影响。而<script type="math/tex">β<1</script>，又能限制速度<script type="math/tex">V_{\mathrm dW}</script>过大。也就是说，当前的速度是渐变的，而不是瞬变的，是动量的过程。这保证了梯度下降的平稳性和准确性，减少振荡，较快地达到最小值处。</p>
<p>动量梯度下降算法的过程如下：</p>
<p><img src="https://pic.imgdb.cn/item/648ae2051ddac507ccc2b4f9.png" style="zoom:70%"></p>
<p>初始时，令<script type="math/tex">V_{\mathrm dW}=0,V_{\mathrm db}=0</script>。一般设置<script type="math/tex">β=0.9</script>，即指数加权平均前10天的数据，实际应用效果较好。</p>
<p>另外，关于偏移校正，可以不使用。因为经过10次迭代后，随着滑动平均的过程，偏移情况会逐渐消失。</p>
<p>补充一下，在其它文献资料中，动量梯度下降还有另外一种写法：</p>
<script type="math/tex; mode=display">
\begin{gathered}
V_{\mathrm d W}=\beta V_{\mathrm d W}+d W \\
V_{\mathrm d b}=\beta V_{\mathrm d b}+\mathrm d b
\end{gathered}</script><p>即消去了<script type="math/tex">dW</script>和<script type="math/tex">db</script>前的系数<script type="math/tex">(1−β)</script>。这样简化了表达式，但是学习因子<script type="math/tex">\alpha</script>相当于变成了<script type="math/tex">\dfrac{α}{1−β}</script>，表示<script type="math/tex">\alpha</script>也受<script type="math/tex">\beta</script>的影响。从效果上来说，这种写法也是可以的，但是不够直观，且调参涉及到<script type="math/tex">\alpha</script>，不够方便。所以，实际应用中，推荐第一种动量梯度下降的表达式。</p>
<h4 id="6-7-root-mean-square-prop-RMSprop"><a href="#6-7-root-mean-square-prop-RMSprop" class="headerlink" title="6.7 root mean square prop(RMSprop)"></a>6.7 <strong>root mean square prop(</strong>RMSprop<strong>)</strong></h4><p>RMSprop是另外一种优化梯度下降速度的算法。每次迭代训练过程中，其权重W和常数项b的更新表达式为：</p>
<script type="math/tex; mode=display">
\begin{gathered}S_{W}=
\beta S_{\mathrm d W}+(1-\beta) d W^{2} \\S_{b}=\beta S_{\mathrm d b}+(1-\beta) \mathrm d b^{2} \\
W:=W-\alpha \frac{\mathrm d W}{\sqrt{S_{W}}}, b:=b-\alpha \frac{\mathrm d b}{\sqrt{S_{b}}}
\end{gathered}</script><p>下面简单解释一下RMSprop算法的原理，仍然以下图为例，为了便于分析，令水平方向为<script type="math/tex">W</script>的方向，垂直方向为<script type="math/tex">b</script>的方向。</p>
<p><img src="https://pic.imgdb.cn/item/648ae2e81ddac507ccc614ab.png"></p>
<p>从图中可以看出，梯度下降(蓝色折线)在垂直方向(<script type="math/tex">b</script>)上振荡较大，在水平方向(<script type="math/tex">W</script>)上振荡较小，表示在<script type="math/tex">b</script>方向上梯度较大，即<script type="math/tex">db</script>较大，而在<script type="math/tex">W</script>方向上梯度较小，即<script type="math/tex">dW</script>较小。因此，上述表达式中<script type="math/tex">S_b</script>较大，而<script type="math/tex">S_W</script>较小。在更新<script type="math/tex">W</script>和<script type="math/tex">b</script>的表达式中，变化值<script type="math/tex">\dfrac{\mathrm dW}{\sqrt{S_W}}</script>较大，而<script type="math/tex">\dfrac{\mathrm db}{\sqrt{S_b}}</script>较小。也就使得<script type="math/tex">W</script>变化得多一些，<script type="math/tex">b</script>变化得少一些。即加快了<script type="math/tex">W</script>方向的速度，减小了<script type="math/tex">b</script>方向的速度，减小振荡，实现快速梯度下降算法，其梯度下降过程如绿色折线所示。总得来说，就是如果哪个方向振荡大，就减小该方向的更新速度，从而减小振荡。</p>
<p>还有一点需要注意的是为了避免RMSprop算法中<strong>分母为零</strong>，通常可以在分母增加一个极小的常数<script type="math/tex">\varepsilon</script>：</p>
<script type="math/tex; mode=display">
W:=W-\alpha \frac{d W}{\sqrt{S_{W}}+\varepsilon}, b:=b-\alpha \frac{d b}{\sqrt{S_{b}}+\varepsilon}</script><p>其中，<script type="math/tex">ε=10^{−8}</script>，或者其它较小值。</p>
<h4 id="6-8-Adam优化算法"><a href="#6-8-Adam优化算法" class="headerlink" title="6.8 Adam优化算法"></a>6.8 Adam优化算法</h4><p>Adam(Adaptive Moment Estimation)算法<strong>结合</strong>了动量梯度下降算法和RMSprop算法。其算法流程为：</p>
<p><img src="https://pic.imgdb.cn/item/648ae7221ddac507ccd65acc.png" style="zoom:70%"></p>
<p>Adam算法包含了几个超参数，分别是：<script type="math/tex">α,β_1,β_2,ε</script>。</p>
<ul>
<li><script type="math/tex">β_1</script>通常设置为<script type="math/tex">0.9</script>；</li>
<li><script type="math/tex">β_2</script>通常设置为<script type="math/tex">0.999</script>；</li>
<li><script type="math/tex">\varepsilon</script>通常设置为<script type="math/tex">10^{−8}</script>。<br>一般只需要对<script type="math/tex">β_1</script>和<script type="math/tex">β_2</script>进行调试。</li>
</ul>
<p>实际应用中，Adam算法结合了动量梯度下降和RMSprop各自的优点，使得神经网络训练速度大大提高。</p>
<h4 id="6-9-学习率衰减"><a href="#6-9-学习率衰减" class="headerlink" title="6.9 学习率衰减"></a>6.9 学习率衰减</h4><p>减小学习因子<script type="math/tex">\alpha</script>也能有效提高神经网络训练速度，这种方法被称为learning rate decay。</p>
<p>Learning rate decay就是随着迭代次数增加，学习因子<script type="math/tex">\alpha</script><strong>逐渐减小</strong>。下面用图示的方式来解释这样做的好处。下图中，蓝色折线表示使用恒定的学习因子<script type="math/tex">\alpha</script>，由于每次训练<script type="math/tex">\alpha</script>相同，步进长度不变，在接近最优值处的振荡也大，在最优值附近较大范围内振荡，与最优值距离就比较远。绿色折线表示使用不断减小的<script type="math/tex">\alpha</script>，随着训练次数增加，<script type="math/tex">\alpha</script>逐渐减小，步进长度减小，使得能够在最优值处较小范围内微弱振荡，不断逼近最优值。相比较恒定的<script type="math/tex">\alpha</script>来说，learning rate decay更接近最优值。</p>
<p><img src="https://pic.imgdb.cn/item/648ae8241ddac507ccdab00c.png"></p>
<p>Learning rate decay中对<script type="math/tex">\alpha</script>可由下列公式得到：</p>
<script type="math/tex; mode=display">
\begin{equation}
\alpha=\frac{1}{1+\text { decay\_rate } * \text { epoch }} \alpha_{0}
\end{equation}</script><p>其中，<script type="math/tex">\text{deacy\_rate}</script>是参数(可调)，<strong>epoch是训练完所有样本的次数</strong>。随着epoch增加，<script type="math/tex">\alpha</script>会不断变小。除了上面计算<script type="math/tex">\alpha</script>的公式之外，还有其它可供选择的计算公式：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\alpha=0.95^{\text {epoch }} \cdot \alpha_{0} \\
\alpha=\frac{k}{\sqrt{\text { epoch }}} \cdot \alpha_{0} \quad \text { or } \quad \frac{k}{\sqrt{t}} \cdot \alpha_{0}
\end{gathered}</script><p>其中，<script type="math/tex">k</script>为可调参数，<script type="math/tex">t</script>为mini-bach number。除此之外，还可以设置<script type="math/tex">\alpha</script>为关于<script type="math/tex">t</script>的离散值，随着<script type="math/tex">t</script>增加，<script type="math/tex">\alpha</script>呈阶梯式减小。当然，也可以根据训练情况灵活调整当前的<script type="math/tex">\alpha</script>值，但会比较耗时间。</p>
<h4 id="6-10局部最优问题"><a href="#6-10局部最优问题" class="headerlink" title="6.10局部最优问题"></a>6.10局部最优问题</h4><p>在使用梯度下降算法不断减小cost function时，可能会得到局部最优解(local optima)而不是全局最优解(global optima)。之前我们对局部最优解的理解是形如碗状的凹槽，如下图左边所示。但是在神经网络中，local optima的概念发生了变化。准确地来说，大部分梯度为零的“最优点”并不是这些凹槽处，而是形如右边所示的马鞍状，称为<strong>saddle point</strong>。也就是说，梯度为零并不能保证都是convex(极小值)，也有可能是concave(极大值)。特别是在神经网络中参数很多的情况下，所有参数梯度为零的点很可能都是右边所示的马鞍状的saddle point，而不是左边那样的local optimum。</p>
<p><img src="https://pic.imgdb.cn/item/648ae86d1ddac507ccdbc60a.png"></p>
<p>类似马鞍状的plateaus会<strong>降低</strong>神经网络学习速度。Plateaus是梯度接近于零的平缓区域，如下图所示。在plateaus上梯度很小，前进缓慢，到达saddle point需要很长时间。到达saddle point后，由于随机扰动，梯度一般能够沿着图中绿色箭头，离开saddle point，继续前进，只是在plateaus上花费了太多时间。</p>
<p><img src="https://pic.imgdb.cn/item/648ae8921ddac507ccdc670a.png"></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>编程与深度学习</category>
        <category>深度学习基础</category>
      </categories>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习_吴恩达_Part_1</title>
    <url>/2022/04/26/Foundation-of-DeepLearning/</url>
    <content><![CDATA[<h3 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h3><h4 id="1-0-深度学习发展历史"><a href="#1-0-深度学习发展历史" class="headerlink" title="1.0 深度学习发展历史"></a>1.0 深度学习发展历史</h4><p><img src="https://pic.imgdb.cn/item/6267f189239250f7c59f6a42.png"></p>
<p>本小节参考链接：<br>参考链接1.1：<a href="https://zhuanlan.zhihu.com/p/29096536">深度学习(deep learning)发展史 - 极海·GeoHey的文章 - 知乎</a><br>参考链接1.2：深度学习发展历程(MindSpore)-哔哩哔哩, <a href="https://b23.tv/t4osJ6I">https://b23.tv/t4osJ6I</a></p>
<h4 id="1-1-欢迎"><a href="#1-1-欢迎" class="headerlink" title="1.1 欢迎"></a>1.1 欢迎</h4><p>从本节课我们将学到：</p>
<ul>
<li>学习神经网络的基础——神经网络与深度学习；</li>
<li>深度学习方面的实践；</li>
<li>如何结构化机器学习工程；</li>
<li>卷积神经网络(经常用于图像)；</li>
<li>序列模型以及如何应用到自然语言处理(NLP)，常见的序列模型有：循环神经网络(RNN)，长短期记忆网络(LSTM)模型。 </li>
</ul>
<h4 id="1-2-什么是神经网络"><a href="#1-2-什么是神经网络" class="headerlink" title="1.2 什么是神经网络"></a>1.2 什么是神经网络</h4><p>深度学习指的是“训练神经网络”</p>
<p>下面以房屋价格预测为例。首先，将已知的六间房子的价格和面积的关系绘制在二维平面上，如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267f293239250f7c5a1d58d.png" style="zoom:40%"></p>
<p>一般地，会用一条直线来拟合图中这些离散点，即建立房价与面积的线性模型。但是从实际考虑，价格永远不会是负数。所以对该直线做一点点修正，让它变成折线的形状，当面积小于某个值时，价格始终为零。如下图蓝色折线所示，就是建立的房价预测模型。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f3cf239250f7c5a4cb1c.png" style="zoom:40%"></p>
<p>其实这个简单的模型（蓝色折线）就可以看成是一个神经网络，而且几乎是一个最简单的神经网络。我们把该房价预测用一个最简单的神经网络模型来表示，如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267f419239250f7c5a57391.png" style="zoom:40%"></p>
<p>上图中的小圆圈就可以视为一个独立的神经元，这个简单网络实现了左边函数的功能值得一提的是，上图神经元的预测函数（蓝色折线）在神经网络应用中比较常见。把这个函数称为<strong>线性整流函数(Rectified Linear Unit, ReLU)</strong>，形如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267f506239250f7c5a7c622.png" style="zoom:70%"></p>
<p>上面是一个最为简单的神经网络，更深的神经网络可以视为：将这些单个的神经元看作乐高积木，通过搭建积木来构建更大更深的网络。把上面举的房价预测的例子变得复杂一些，而不是仅仅使用房屋面积一个判断因素。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f560239250f7c5a8b1e3.png" style="zoom:50%"></p>
<p>在给定这四个输入后，神经网络所做的就是输出房屋的预测价格y。上图中三个神经元所在的位置称之为<strong>中间层或者隐藏层</strong>(x所在的称之为输入层，y所在的称之为输出层)，每个神经元与所有的输入x都有关联(直线相连)。 </p>
<h4 id="1-3-使用神经网络进行监督学习"><a href="#1-3-使用神经网络进行监督学习" class="headerlink" title="1.3 使用神经网络进行监督学习"></a>1.3 使用神经网络进行监督学习</h4><p>由神经网络模型创造的价值基本上都是基于<strong>监督式学习(Supervised Learning)</strong>的。监督式学习与非监督式学习本质区别就是<strong>是否已知训练样本的输出y</strong>。在实际应用中，机器学习解决的大部分问题都属于监督式学习，神经网络模型也大都属于监督式学习。下面我们来看几个监督式学习在神经网络中应用的例子。</p>
<ul>
<li>房屋价格预测。根据训练样本的输入x和输出y，训练神经网络模型，预测房价。</li>
<li>线上广告。输入x是广告和用户个人信息，输出y是用户是否对广告进行点击。神经网络模型经过训练，能够根据广告类型和用户信息对用户的点击行为进行预测，从而向用户提供用户自己可能感兴趣的广告。</li>
<li>电脑视觉(computer vision)。电脑视觉是近些年来越来越火的课题，而电脑视觉发展迅速的原因很大程度上是得益于深度学习。其中，输入x是图片像素值，输出是图片所属的不同类别。</li>
<li>语音识别(speech recognition)。深度学习可以将一段语音信号辨识为相应的文字信息。</li>
<li>智能翻译。例如通过神经网络输入英文，然后直接输出中文。</li>
<li>自动驾驶。通过输入一张图片或者汽车雷达信息，神经网络通过训练来告诉你相应的路况信息并作出相应的决策。 </li>
</ul>
<p><img src="https://pic.imgdb.cn/item/6267f5d9239250f7c5a9dfcd.png" style="zoom:50%"></p>
<p>根据不同的问题和应用场合，应该使用不同类型的神经网络模型。CNN和RNN是比较常用的神经网络模型。下图给出了Standard NN，Convolutional NN和Recurrent NN的神经网络结构图。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f6c4239250f7c5ac1c6c.png"></p>
<p>数据类型一般分为两种：<strong>结构化数据(Structured Data)</strong>和<strong>非结构化数据(Unstructured Data)</strong> </p>
<p><img src="https://pic.imgdb.cn/item/6267f734239250f7c5ad2a26.png" style="zoom:40%"></p>
<h4 id="1-4-为什么深度学习流行起来了"><a href="#1-4-为什么深度学习流行起来了" class="headerlink" title="1.4 为什么深度学习流行起来了"></a>1.4 为什么深度学习流行起来了</h4><p>略</p>
<h3 id="第二章-神经网络基础之逻辑回归"><a href="#第二章-神经网络基础之逻辑回归" class="headerlink" title="第二章 神经网络基础之逻辑回归"></a>第二章 神经网络基础之逻辑回归</h3><p>下面开始介绍神经网络的基础：逻辑回归（Logistic Regression）。通过对逻辑回归模型结构的分析，为后面学习神经网络模型打下基础。 </p>
<h4 id="2-1-二分类-Binary-Classification"><a href="#2-1-二分类-Binary-Classification" class="headerlink" title="2.1 二分类(Binary Classification)"></a>2.1 二分类(Binary Classification)</h4><p>逻辑回归模型一般用来解决二分类(Binary Classification)问题。二分类就是输出只有{0,1\}两个离散值(也有{-1,1}的情况)。以一个图像识别问题为例，判断图片中是否有猫存在，0代表not cat，1代表cat。 </p>
<p><img src="https://pic.imgdb.cn/item/6267f7e7239250f7c5aece58.png" style="zoom:50%"></p>
<p>如上图所示，这是一个典型的二分类问题。一般来说，彩色图片包含RGB三个通道。例如该cat图片的尺寸为<script type="math/tex">(64, 64, 3)</script>。在神经网络模型中，我们首先要将图片输入<script type="math/tex">x</script>(维度是<script type="math/tex">(64, 64, 3)</script>)转化为一维的特征向量(feature vector)。方法是每个通道一行一行取，再连接起来。由于<script type="math/tex">64\times 64\times 3=12288</script>，则转化后的输入特征向量维度为<script type="math/tex">(12288, 1)</script>。此特征向量<script type="math/tex">x</script>是列向量，维度一般记为<script type="math/tex">n_x</script>。</p>
<p>如果训练样本共有<script type="math/tex">m</script>张图片，那么整个训练样本<script type="math/tex">X</script>组成了矩阵，维度是<script type="math/tex">(n_x,m)</script>。注意，这里矩阵<script type="math/tex">X</script>的行<script type="math/tex">n_x</script>代表了每个样本<script type="math/tex">x^{(i)}</script>特征个数，列<script type="math/tex">m</script>代表了样本个数。这里，Andrew解释了<script type="math/tex">X</script>的维度之所以是<script type="math/tex">(n_x, m)</script>而不是<script type="math/tex">(m, n_x)</script>的原因是为了之后矩阵运算的方便。算是Andrew给我们的一个小小的经验吧。而所有训练样本的输出<script type="math/tex">Y</script>也组成了一维的行向量，写成矩阵的形式后，它的维度就是<script type="math/tex">(1, m)</script>。</p>
<blockquote>
<p>后面课程会用到的一些符号<br>用一对<script type="math/tex">(x, y)</script>来表示一个单独的样本，其中<script type="math/tex">x</script>是<script type="math/tex">n_x</script>维特征向量(可记为<script type="math/tex">x \in \mathbb{R}^{n_x}</script>)，<script type="math/tex">y \in \{0, 1\}</script>，训练集由<script type="math/tex">m</script>个训练样本组成，<script type="math/tex">(x^{(i)}, y^{(i)})</script>表示样本<script type="math/tex">n</script>的输入输出。为了便于表示和区分，有时训练集表示为<script type="math/tex">m = m_{\text{train}}</script>，测试集表示为<script type="math/tex">m = m_{\text{test}}</script>，可进一步将训练集表示为更紧凑的形式，用矩阵<script type="math/tex">X</script>表示：<script type="math/tex">X=\left(\begin{array}{cccc}\vdots & \vdots & & \vdots \\x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\\vdots & \vdots & & \vdots \\\end{array}\right)</script>，<script type="math/tex">X</script>的大小为<script type="math/tex">(n_x, m)</script>，<script type="math/tex">X \in \mathbb{R}^{n_x \times m}</script>，输出用<script type="math/tex">Y</script>表示：<script type="math/tex">Y = [y^{(1)}, y^{(1)}, \cdots, y^{(m)}]</script>，<script type="math/tex">Y</script>的大小为<script type="math/tex">(1, m)</script>，<script type="math/tex">Y \in \mathbb{R}^{1\times m}</script></p>
</blockquote>
<h4 id="2-2-logistic回归"><a href="#2-2-logistic回归" class="headerlink" title="2.2 logistic回归"></a>2.2 logistic回归</h4><p>这是一个学习算法，用于监督学习中输出<script type="math/tex">y​</script>是<script type="math/tex">0​</script>或<script type="math/tex">1​</script>的二元分类问题。</p>
<p>逻辑回归中，预测值<script type="math/tex">\hat{h} = \text{P}(y=1|x)</script>表示为输入<script type="math/tex">x</script>输出为<script type="math/tex">y = 1</script>的概率，取值范围在<script type="math/tex">[0,1]</script>之间，这是其与二分类模型不同的地方。使用线性模型，引入参数<script type="math/tex">w</script>和<script type="math/tex">b</script>。权重<script type="math/tex">w</script>的维度是<script type="math/tex">(n_x, 1)</script>，<script type="math/tex">b</script>是一个常数项，即<script type="math/tex">w \in \mathbb{R}^{n_x \times 1}, b \in \mathbb{R}</script>。这样，逻辑回归的线性预测输出可以写成：</p>
<script type="math/tex; mode=display">
\hat{y} = w^{\text{T}}x+b</script><p>值得注意的是，很多其它机器学习资料中，可能把常数<script type="math/tex">b</script>当做<script type="math/tex">w_0</script>处理，并引入<script type="math/tex">x_0=1</script>。这样从维度上来看，<script type="math/tex">x</script>和<script type="math/tex">w</script>都会增加一维。但在本课程中，为了简化计算和便于理解，Andrew建议还是使用上式这种形式将<script type="math/tex">w</script>和<script type="math/tex">b</script>分开比较好。</p>
<p>上式的线性输出区间为整个实数范围，而<strong>逻辑回归要求输出范围在</strong><script type="math/tex">[0,1]</script><strong>之间</strong>，所以还需要对上式的线性函数输出进行处理。方法是引入sigmoid函数，让输出限定在<script type="math/tex">[0,1]</script>之间。这样，逻辑回归的预测输出就可以完整写成：</p>
<script type="math/tex; mode=display">
\hat{y} = \text{sigmoid}(w^{\text T}x+b) = \sigma(w^{\text T}x+b)</script><p>sigmoid函数是一种非线性的S型函数，输出被限定在<script type="math/tex">[0,1]</script>之间，通常被用在神经网络中当作<strong>激活函数(Activation function)</strong>使用。Sigmoid函数的表达式：</p>
<script type="math/tex; mode=display">
\text{sigmoid}(z) = \frac{1}{1+e^{-z}}</script><p> 通过Sigmoid函数，就能够将逻辑回归的输出限定在<script type="math/tex">[0,1]</script>之间了。</p>
<h4 id="2-3-logistic回归损失函数"><a href="#2-3-logistic回归损失函数" class="headerlink" title="2.3 logistic回归损失函数"></a>2.3 logistic回归损失函数</h4><p>逻辑回归中，<script type="math/tex">w</script>和<script type="math/tex">b</script>都是未知参数，需要反复训练优化得到。因此，我们需要定义一个成本函数(cost function)，包含了参数<script type="math/tex">w</script>和<script type="math/tex">b</script>。通过优化cost function，当cost function取值最小时，得到对应的<script type="math/tex">w</script>和<script type="math/tex">b</script>。</p>
<p>如何定义所有<script type="math/tex">m</script>个样本的cost function呢？先从单个样本出发，我们希望该样本的预测值<script type="math/tex">\hat y</script>与真实值越相似越好。我们把单个样本的cost function用<strong>Loss function</strong>来表示，根据以往经验，如果使用平方错误(squared error)来衡量，如下所示：</p>
<script type="math/tex; mode=display">
L(\hat y, y) = \frac{1}{2}(\hat y-y)^2</script><p>但是，对于逻辑回归，我们一般不使用平方错误来作为Loss function。原因是这种Loss function一般是<strong>非凸(non-convex)</strong>的。non-convex函数在使用梯度下降算法时，容易得到局部最小值(local minumum)，即<strong>局部最优化</strong>。而我们最优化的目标是计算得到全局最优化(Global optimization)。因此，我们一般选择的Loss function应该是convex的。因此，我们可以构建另外一种Loss function，且是convex的，如下所示：</p>
<script type="math/tex; mode=display">
L(\hat y, y) = -(y\log \hat y + (1-y)\log(1-\hat y))</script><p>我们来分析一下这个Loss function，它是衡量错误大小的，Loss function越小越好。</p>
<p>当<script type="math/tex">y = 1</script>时，我们带入上式容易得知<script type="math/tex">\hat y\rightarrow1</script>时<script type="math/tex">L(\hat y, y)\rightarrow0</script>，预测效果越好；同理，当<script type="math/tex">y=0</script>时，<script type="math/tex">\hat y \rightarrow 0</script>则<script type="math/tex">L(\hat y,y)\rightarrow0</script>，预测效果越好。后续将会提到这个损失函数是如何推导出来的。</p>
<p>上面介绍的Loss function是针对单个样本的。那对于<script type="math/tex">m</script>个样本，我们定义Cost function，Cost function是<script type="math/tex">m</script>个样本的Loss function的平均值，反映了<script type="math/tex">m</script>个样本的预测输出<script type="math/tex">\hat y</script>与真实样本输出<script type="math/tex">y</script>的平均接近程度。Cost function可表示为：</p>
<script type="math/tex; mode=display">
J(w,b) = \frac1m \sum_{i=0}^{m}L(\hat y^{(i)}, y^{(i)})</script><p>Cost function已经推导出来了，Cost function是关于待求系数w和b的函数。我们的目标就是迭代计算出最佳的w和b值，<strong>最小化Cost function</strong>，让Cost function尽可能地接近于零。</p>
<p>其实逻辑回归问题可以看成是一个简单的神经网络，只包含<strong>一个神经元</strong>。这也是我们这里先介绍逻辑回归的原因。</p>
<h4 id="2-4-梯度下降法"><a href="#2-4-梯度下降法" class="headerlink" title="2.4 梯度下降法"></a>2.4 梯度下降法</h4><p>使用<strong>梯度下降(Gradient Descent)</strong>算法来计算出合适的<script type="math/tex">w</script>和<script type="math/tex">b</script>值，从而最小化<script type="math/tex">m</script>个训练样本的Cost function，即<script type="math/tex">J(w,b)</script>。</p>
<p>由于<script type="math/tex">J(w,b)</script>是convex  function，梯度下降算法是先随机选择一组参数<script type="math/tex">w</script>和<script type="math/tex">b</script>值，然后每次迭代的过程中分别沿着<script type="math/tex">w</script>和<script type="math/tex">b</script>的梯度(偏导数)的反方向前进一小步，不断修正<script type="math/tex">w</script>和<script type="math/tex">b</script>。每次迭代更新后，都能让<script type="math/tex">J(w,b)</script>更接近全局最小值。梯度下降的过程如下图所示。</p>
<p><img src="https://pic.imgdb.cn/item/648a82371ddac507ccb0ec36.jpg" style="zoom:50%"></p>
<p>梯度下降算法每次迭代更新，<script type="math/tex">w</script>和<script type="math/tex">b</script>的修正表达式为：</p>
<script type="math/tex; mode=display">
w:=w-\alpha \frac{\partial J(w, b)}{\partial w}</script><script type="math/tex; mode=display">
b:=b-\alpha \frac{\partial J(w, b)}{\partial b}</script><p>上式中，<script type="math/tex">\alpha</script>是学习因子(learning rate)，表示梯度下降的不仅长度。梯度下降算法能够保证每次迭代w和b都能向着J(w,b)全局最小化的方向进行。其<a href="http://blog.csdn.net/red_stone1/article/details/72229903">数学原理</a>主要是运用泰勒一阶展开来证明的。</p>
<h4 id="2-5-2-6-导数复习"><a href="#2-5-2-6-导数复习" class="headerlink" title="2.5-2.6 导数复习"></a>2.5-2.6 导数复习</h4><p>这一部分的内容相对简单，Andrew主要是给对微积分、求导数不太清楚的同学介绍的。梯度或者导数一定程度上可以看成是斜率。关于求导数的方法这里就不再赘述了。</p>
<h4 id="2-7-计算图"><a href="#2-7-计算图" class="headerlink" title="2.7 计算图"></a>2.7 计算图</h4><p>整个神经网络的训练过程实际上包含了两个过程：<strong>正向传播(Forward Propagation)</strong>和<strong>反向传播(Back Propagation)</strong>。正向传播是从输入到输出，由神经网络计算得到预测输出的过程；反向传播是从输出到输入，对参数w和b计算梯度的过程。下面，我们用<strong>计算图(Computation graph)</strong>的形式来理解这两个过程。</p>
<p>举个简单的例子，假如Cost function为<script type="math/tex">J(a,b,c)=3(a+bc)</script>，包含<script type="math/tex">a</script>，<script type="math/tex">b</script>，<script type="math/tex">c</script>三个变量。我们用<script type="math/tex">u</script>表示<script type="math/tex">bc</script>，<script type="math/tex">v</script>表示<script type="math/tex">a+u</script>，则<script type="math/tex">J=3v</script>。它的计算图可以写成如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648a82ed1ddac507ccb243d5.jpg" style="zoom:50%"></p>
<p>令<script type="math/tex">a=5,b=3,c=2</script>，则<script type="math/tex">u=bc=6,v=a+u=11,J=3v=33</script>。计算图中，这种从左到右，从输入到输出的过程就对应着神经网络或者逻辑回归中输入与权重经过运算计算得到Cost function的正向过程。</p>
<h4 id="2-8-使用计算图求导"><a href="#2-8-使用计算图求导" class="headerlink" title="2.8 使用计算图求导"></a>2.8 使用计算图求导</h4><p>下面我们来介绍反向传播(Back Propagation)，即计算输出对输入的偏导数。</p>
<p><img src="https://pic.imgdb.cn/item/648a83431ddac507ccb2e777.jpg" style="zoom:50%"></p>
<h4 id="2-9-logistic回归中的梯度下降法"><a href="#2-9-logistic回归中的梯度下降法" class="headerlink" title="2.9 logistic回归中的梯度下降法"></a>2.9 logistic回归中的梯度下降法</h4><p>对逻辑回归进行梯度计算。对单个样本而言，逻辑回归Loss function表达式如下：</p>
<script type="math/tex; mode=display">
\begin{gathered}
&z = w^\text Tx+b \\
&\hat y = a = \sigma(z) \\
&L(\hat y,y) = L(a, y) = -(y\log a+(1-y)\log(1-a))
\end{gathered}</script><p>该逻辑回归的正向传播过程非常简单。据上述公式，例如输入样本<script type="math/tex">x</script>有两个特征<script type="math/tex">(x_1,x_2)</script>，相应的权重也有两个<script type="math/tex">(w_1,w_2)</script>，则<script type="math/tex">z = w_1x_1+w_2x_2+b</script>。</p>
<p>然后，计算该逻辑回归的反向传播过程，即由Loss function计算参数<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数：</p>
<script type="math/tex; mode=display">
\begin{gathered}
&\mathrm d a=\frac{\partial L}{\partial a}=-\frac{y}{a}+\frac{1-y}{1-a}\\
&\mathrm d z=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z}=\left(-\frac{y}{a}+\frac{1-y}{1-a}\right) \cdot a(1-a)=a-y
\end{gathered}</script><p>知道了<script type="math/tex">\mathrm dz</script>之后，就可以直接对<script type="math/tex">w_1，w_2</script>和<script type="math/tex">b</script>进行求导了：</p>
<script type="math/tex; mode=display">
\begin{gathered}
&\mathrm d w_1=\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w_1}=x_1 \cdot \mathrm  d z=x_1(a-y)\\
&\mathrm d w_2=\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w_2}=x_2 \cdot \mathrm  d z=x_2(a-y)\\
&\mathrm d b=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial b}=1 \cdot \mathrm  d z=a-y
\end{gathered}</script><p>则梯度下降算法可表示为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
w{1}:=w{1}-\alpha ~\mathrm d w{1} \\
w{2}:=w{2}-\alpha ~\mathrm d w{2} \\
b:=b-\alpha ~\mathrm d b
\end{gathered}</script><p><img src="https://pic.imgdb.cn/item/648a84f21ddac507ccb64b43.jpg" style="zoom:50%"></p>
<h4 id="2-10-m-个样本的梯度下降"><a href="#2-10-m-个样本的梯度下降" class="headerlink" title="2.10 m 个样本的梯度下降"></a>2.10 m 个样本的梯度下降</h4><p>上一部分讲的是对单个样本求偏导和梯度下降。如果有m个样本，其Cost function表达式如下：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z^{(i)}=w^{T} x^{(i)}+b \\
\hat{y}^{(i)}=a^{(i)}=\sigma\left(z^{(i)}\right) \\
J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \hat{y}^{(i)}+\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)\right]
\end{gathered}</script><p>Cost function关于<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数可以写成和平均的形式：</p>
<script type="math/tex; mode=display">
\begin{gathered}
\mathrm d w_{1}=\frac{1}{m} \sum_{i=1}^{m} x_{1}^{(i)}\left(a^{(i)}-y^{(i)}\right) \\
\mathrm d w_{2}=\frac{1}{m} \sum_{i=1}^{m} x_{2}^{(i)}\left(a^{(i)}-y^{(i)}\right) \\
\mathrm d b=\frac{1}{m} \sum_{i=1}^{m} \left(a^{(i)}-y^{(i)}\right)
\end{gathered}</script><p>这样，每次迭代中<script type="math/tex">w</script>和<script type="math/tex">b</script>的梯度有$m$个训练样本计算平均值得到。其算法伪代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">J=<span class="number">0</span>; dw1=<span class="number">0</span>; dw2=<span class="number">0</span>; db=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to m</span><br><span class="line">    z(i) = wx(i)+b;</span><br><span class="line">    a(i) = sigmoid(z(i));</span><br><span class="line">    J += -[y(i)log(a(i))+(<span class="number">1</span>-y(i)）log(<span class="number">1</span>-a(i));</span><br><span class="line">    dz(i) = a(i)-y(i);</span><br><span class="line">    dw1 += x1(i)dz(i);</span><br><span class="line">    dw2 += x2(i)dz(i);</span><br><span class="line">    db += dz(i);</span><br><span class="line">J /= m;</span><br><span class="line">dw1 /= m;</span><br><span class="line">dw2 /= m;</span><br><span class="line">db /= m;</span><br></pre></td></tr></table></figure>
<p>经过每次迭代后，根据梯度下降算法，<script type="math/tex">w</script>和<script type="math/tex">b</script>都进行更新：</p>
<script type="math/tex; mode=display">
\begin{gathered}
w_{1}:=w_{1}-\alpha ~ \mathrm d w_{1} \\
w_{2}:=w_{2}-\alpha ~ \mathrm d w_{2} \\
b:=b-\alpha ~ \mathrm d b
\end{gathered}</script><p>这样经过<script type="math/tex">n</script>次迭代后，整个梯度下降算法就完成了。</p>
<p>值得一提的是，在上述的梯度下降算法中，是利用for循环对每个样本进行dw1，dw2和db的累加计算最后再求平均数的。在深度学习中，样本数量$m$通常很大，使用for循环会让神经网络程序运行得很慢。所以，我们应该尽量避免使用for循环操作，而使用<strong>矩阵运算</strong>，能够大大提高程序运行速度。关于<strong>向量化(vectorization)</strong>的内容我们放在下次笔记中再说。</p>
<h4 id="2-11-2-12-向量化"><a href="#2-11-2-12-向量化" class="headerlink" title="2.11-2.12 向量化"></a>2.11-2.12 向量化</h4><p>深度学习算法中，数据量很大，在程序中应该尽量减少使用loop循环语句，而可以使用向量运算来提高程序运行速度。</p>
<p>向量化(Vectorization)就是利用矩阵运算的思想，大大提高运算速度。</p>
<p>上一部分我们讲了应该尽量避免使用for循环而使用向量化矩阵运算。在python的numpy库中，我们通常使用<strong>np.dot()</strong>函数来进行矩阵运算。</p>
<p>我们将向量化的思想使用在逻辑回归算法上，尽可能减少for循环，而只使用矩阵运算。值得注意的是，算法最顶层的迭代训练的for循环是不能替换的。而每次迭代过程对J，dw，b的计算是可以直接使用矩阵运算。</p>
<h4 id="2-13-向量化logistic回归"><a href="#2-13-向量化logistic回归" class="headerlink" title="2.13 向量化logistic回归"></a>2.13 向量化logistic回归</h4><p>在前面的笔记中我们提到过，整个训练样本构成的输入矩阵<script type="math/tex">X</script>的维度是<script type="math/tex">(,m)</script>，权重矩阵<script type="math/tex">w</script>的维度是<script type="math/tex">(,1)</script>，<script type="math/tex">b</script>是一个常数值，而整个训练样本构成的输出矩阵<script type="math/tex">Y</script>的维度为<script type="math/tex">(1,m)</script>。利用向量化的思想，所有<script type="math/tex">m</script>个样本的线性输出<script type="math/tex">Z</script>可以用矩阵表示：</p>
<script type="math/tex; mode=display">
Z = w^{\text T}X+b</script><p>在python的numpy库中可以表示为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b      <span class="comment"># w.T表示w的转置</span></span><br><span class="line">A = sigmoid(Z)</span><br></pre></td></tr></table></figure>
<p>这样，我们就能够使用向量化矩阵运算代替for循环，对所有<script type="math/tex">m</script>个样本同时运算，大大提高了运算速度。 </p>
<h4 id="2-14-向量化logistic回归的梯度输出"><a href="#2-14-向量化logistic回归的梯度输出" class="headerlink" title="2.14 向量化logistic回归的梯度输出"></a>2.14 向量化logistic回归的梯度输出</h4><p>逻辑回归中的梯度下降算法如何转化为向量化的矩阵形式。对于所有<script type="math/tex">m</script>个样本，<script type="math/tex">\mathrm dZ</script>的维度是<script type="math/tex">(1, m)</script>，可表示为：</p>
<script type="math/tex; mode=display">
\mathrm dZ = A-Y</script><p>$\mathrm d b$可以表示为：</p>
<script type="math/tex; mode=display">
\mathrm d b = \frac{1}{m} \sum_{i=1}^{m}dz^{(i)}</script><p>对应的程序可以写成：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dw = <span class="number">1</span>/m*np.dot(X,dZ.T)</span><br></pre></td></tr></table></figure>
<p>这样，我们把整个逻辑回归中的for循环尽可能用矩阵运算代替，对于单次迭代，梯度下降算法流程如下所示：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A-Y</span><br><span class="line">dw = <span class="number">1</span>/m*np.dot(X,dZ.T)</span><br><span class="line">db = <span class="number">1</span>/m*np.<span class="built_in">sum</span>(dZ)</span><br><span class="line">w = w - alpha*dw</span><br><span class="line">b = b - alpha*db</span><br></pre></td></tr></table></figure><br>其中，<strong>alpha是学习因子</strong>，决定<script type="math/tex">w</script>和<script type="math/tex">b</script>的更新速度。上述代码只是对单次训练更新而言的，外层还需要一个for循环，表示迭代次数。</p>
<h3 id="第三章-浅层神经网络"><a href="#第三章-浅层神经网络" class="headerlink" title="第三章 浅层神经网络"></a>第三章 浅层神经网络</h3><h4 id="3-1-神经网络概览"><a href="#3-1-神经网络概览" class="headerlink" title="3.1 神经网络概览"></a>3.1 神经网络概览</h4><p>首先，我们从整体结构上来大致看一下神经网络模型。</p>
<p>前面的课程中，我们已经使用<strong>计算图</strong>的方式介绍了逻辑回归<strong>梯度下降算法</strong>的<strong>正向传播</strong>和<strong>反向传播</strong>两个过程。如下图所示。神经网络的结构与逻辑回归类似，只是神经网络的层数比逻辑回归多一层，多出来的中间那层称为<strong>隐藏层或中间层</strong>。这样从计算上来说，神经网络的正向传播和反向传播过程只是比逻辑回归多了一次重复的计算。</p>
<p>正向传播过程分成两层，<strong>第一层是输入层到隐藏层</strong>，用上标[1]来表示：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z^{[1]}=W^{[1]} x+b^{[1]} \\
a^{[1]} = \sigma (z^{[1]})
\end{gathered}</script><p>第二层是<strong>隐藏层到输出层</strong>，用上标[2]来表示：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z^{[2]}=W^{[2]} x+b^{[2]} \\
a^{[2]} = \sigma (z^{[2]})
\end{gathered}</script><p>在写法上值得注意的是，方括号上标[i]表示当前所处的层数；圆括号上标(i)表示第i个样本。</p>
<p>同样，<strong>反向传播过程也分成两层</strong>。第一层是输出层到隐藏层，第二层是隐藏层到输入层。其细节部分我们之后再来讨论。</p>
<p><img src="https://pic.imgdb.cn/item/648ac1111ddac507cc54a4ee.png" style="zoom:50%"></p>
<h4 id="3-2-神经网络的表示"><a href="#3-2-神经网络的表示" class="headerlink" title="3.2 神经网络的表示"></a>3.2 神经网络的表示</h4><p>下面我们以图示的方式来介绍单隐藏层的神经网络结构。如下图所示，单隐藏层神经网络就是典型的浅层(shallow)神经网络。</p>
<p><img src="https://pic.imgdb.cn/item/648ac1ad1ddac507cc56265f.png" style="zoom:60%"></p>
<p>结构上，从左到右，可以分成三层：<strong>输入层(Input layer)</strong>，<strong>隐藏层(Hidden layer)</strong>和<strong>输出层(Output  layer)</strong>。输入层和输出层，顾名思义，对应着训练样本的输入和输出，很好理解。隐藏层是抽象的非线性的中间层，这也是其被命名为隐藏层的原因。</p>
<p>在写法上，我们通常把输入矩阵<script type="math/tex">X</script>记为<script type="math/tex">a^{[0]}</script>，把隐藏层输出记为<script type="math/tex">a^{[1]}</script>，上标从<script type="math/tex">0</script>开始。用下标表示第几个神经元，注意下标从<script type="math/tex">1</script>开始。例如<script type="math/tex">a^{[1]}_1</script>表示隐藏层第<script type="math/tex">1</script>个神经元，<script type="math/tex">a^{[1]}_2</script>表示隐藏层第<script type="math/tex">2</script>个神经元等等。这样隐藏层有<script type="math/tex">4</script>个神经元就可以将其输出<script type="math/tex">a^{[1]}</script>写成矩阵的形式：</p>
<script type="math/tex; mode=display">
a^{[1]}=\left[\begin{array}{c}a_{1}^{[1]} \\a_{2}^{[1]} \\a_{3}^{[1]} \\a_{4}^{[1]}\end{array}\right]</script><p>相应的输出层记为<script type="math/tex">a^{[2]}</script>，即<script type="math/tex">\hat y</script>。这种单隐藏层神经网络也称为两层神经网络(2 layer NN)。</p>
<blockquote>
<p>之所以叫两层神经网络是因为，通常我们只会计算隐藏层输出和输出层的输出，输入层是不用计算的。这也是我们把输入层层数上标记为<script type="math/tex">0</script>的原因(<script type="math/tex">a^{[0]}</script>)。<br>关于隐藏层对应的权重<script type="math/tex">W^{[1]}</script>和常数项<script type="math/tex">b^{[1]}</script>，<script type="math/tex">W^{[1]}</script>的维度是<script type="math/tex">(4,3)</script>。这里的<script type="math/tex">4</script>对应着隐藏层神经元个数，<script type="math/tex">3</script>对应着输入层<script type="math/tex">x</script>特征向量包含元素个数。常数项<script type="math/tex">b^{[1]}</script>的维度是<script type="math/tex">(4,1)</script>，这里的<script type="math/tex">4</script>同样对应着隐藏层神经元个数。关于输出层对应的权重<script type="math/tex">W^{[2]}</script>和常数项<script type="math/tex">b^{[2]}</script>，<script type="math/tex">W^{[2]}</script>的维度是<script type="math/tex">(1,4)</script>，这里的<script type="math/tex">1</script>对应着输出层神经元个数，<script type="math/tex">4</script>对应着隐藏层神经元个数。常数项<script type="math/tex">b^{[2]}</script>的维度是<script type="math/tex">(1,1)</script>，因为输出只有一个神经元。</p>
</blockquote>
<p>总结一下，第<script type="math/tex">i</script>层的权重<script type="math/tex">W^{[i]}</script>维度的行等于<script type="math/tex">i</script>层神经元的个数，列等于<script type="math/tex">i-1</script>层神经元的个数；第<script type="math/tex">i</script>层常数项<script type="math/tex">b^{[i]}</script>维度的行等于<script type="math/tex">i</script>层神经元的个数，列始终为<script type="math/tex">1</script>。</p>
<h4 id="3-3-计算神经网络的输出"><a href="#3-3-计算神经网络的输出" class="headerlink" title="3.3 计算神经网络的输出"></a>3.3 计算神经网络的输出</h4><p>本节详细推导神经网络的计算过程。前面讲过两层神经网络可以看成是逻辑回归再重复计算一次。如下图所示，逻辑回归的正向计算可以分解成计算<script type="math/tex">z</script>和<script type="math/tex">a</script>的两部分：</p>
<script type="math/tex; mode=display">
\begin{gathered}
z = w^{\text T}x+b \\
a = \sigma(z)
\end{gathered}</script><p><img src="https://pic.imgdb.cn/item/648ac29f1ddac507cc58f815.png" style="zoom:50%"></p>
<p>对于两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算。每层计算时，要注意对应的上标和下标，一般我们记上标方括号表示layer，下标表示第几个神经元。例如<script type="math/tex">a^{[l]}_i</script>表示第<script type="math/tex">l</script>层的第<script type="math/tex">i</script>个神经元。<strong>注意，</strong><script type="math/tex">i</script><strong>从</strong><script type="math/tex">1</script><strong>开始，</strong><script type="math/tex">l</script><strong>从</strong><script type="math/tex">0</script><strong>开始</strong>。</p>
<p>将从输入层到输出层的计算公式列出来：</p>
<script type="math/tex; mode=display">
\begin{aligned}&z_{1}^{[1]}=w_{1}^{[1] T} x+b_{1}^{[1]}, a_{1}^{[1]}=\sigma\left(z_{1}^{[1]}\right) \\&z_{2}^{[1]}=w_{2}^{[1] T} x+b_{2}^{[1]}, a_{2}^{[1]}=\sigma\left(z_{2}^{[1]}\right) \\&z_{3}^{[1]}=w_{3}^{[1] T} x+b_{3}^{[1]}, a_{3}^{[1]}=\sigma\left(z_{3}^{[1]}\right) \\&z_{4}^{[1]}=w_{4}^{[1] T} x+b_{4}^{[1]}, a_{4}^{[1]}=\sigma\left(z_{4}^{[1]}\right)\end{aligned}</script><p>然后，从隐藏层到输出层的计算公式为：</p>
<script type="math/tex; mode=display">
z^{[2]}_1 = w^{[1] \text T}_1a^{[1]}+b^{[2]}_1,a^{[2]}_1 = \sigma(z^{[2]}_1)</script><p>其中</p>
<script type="math/tex; mode=display">
a^{[1]}=\left[\begin{array}{c}a_{1}^{[1]} \\a_{2}^{[1]} \\a_{3}^{[1]} \\a_{4}^{[1]}\end{array}\right]</script><p>上述每个节点的计算都对应着一次逻辑运算的过程，分别由计算<script type="math/tex">z</script>和<script type="math/tex">a</script>两部分组成。</p>
<p>为了提高程序运算速度，我们引入向量化和矩阵运算的思想，将上述表达式转换成矩阵运算的形式：</p>
<p><img src="https://pic.imgdb.cn/item/648ac37a1ddac507cc5b55f3.png" style="zoom:50%"></p>
<p>之前也介绍过，这里顺便提一下，<script type="math/tex">W^{[1]}</script>的维度是<script type="math/tex">(4,3)</script>，<script type="math/tex">b^{[1]}</script>的维度是<script type="math/tex">(4,1)</script>，<script type="math/tex">W^{[2]}</script>的维度是<script type="math/tex">(1,4)</script>，<script type="math/tex">b^{[2]}</script>的维度是<script type="math/tex">(1,1)</script>。这点需要特别注意。</p>
<h4 id="3-4-多个样本的向量化"><a href="#3-4-多个样本的向量化" class="headerlink" title="3.4 多个样本的向量化"></a>3.4 多个样本的向量化</h4><p>上一部分我们只是介绍了单个样本的神经网络正向传播矩阵运算过程。而对于<script type="math/tex">m</script>个训练样本，我们也可以使用矩阵相乘的形式来提高计算效率。而且它的形式与上一部分单个样本的矩阵运算十分相似，比较简单。</p>
<p>之前我们也介绍过，在书写标记上用上标<script type="math/tex">(i)</script>表示第<script type="math/tex">i</script>个样本，例如<script type="math/tex">x^{(i)}</script>，<script type="math/tex">z^{(i)}</script>，<script type="math/tex">a^{[2](i)}</script>。对于每个样本<script type="math/tex">i</script>，可以使用for循环来求解其正向输出：</p>
<script type="math/tex; mode=display">
\begin{aligned}for \  i = 1 \ to \ m \\&z^{[1](i)}=W^{[1]} x^{(i)}+b^{[1]} \\&a^{[1](i)}=\sigma\left(z^{[1](i)}\right) \\&z^{[2](i)}=W^{[2]} a^{[1](i)}+b^{[2]} \\&a^{[2](i)}=\sigma\left(z^{[2](i)}\right)\end{aligned}</script><p>不使用for循环，利用矩阵运算的思想，输入矩阵<script type="math/tex">X</script>的维度为<script type="math/tex">(n_x,m)</script>。这样，我们可以把上面的for循环写成矩阵运算的形式：</p>
<script type="math/tex; mode=display">
\begin{gathered}Z^{[1]}=W^{[1]} X+b^{[1]} \\A^{[1]}=\sigma\left(Z^{[1]}\right) \\Z^{[2]}=W^{[2]} A^{[1]}+b^{[2]} \\A^{[2]}=\sigma\left(Z^{[2]}\right)\end{gathered}</script><p>其中，<script type="math/tex">Z^{[1]}</script>的维度是<script type="math/tex">(4,m)</script>，<script type="math/tex">4</script>是隐藏层神经元的个数；<script type="math/tex">A^{[1]}</script>的维度与<script type="math/tex">Z^{[1]}</script>相同；<script type="math/tex">Z^{[2]}</script>和<script type="math/tex">A^{[2]}</script>的维度均为<script type="math/tex">(1,m)</script>。对上面这四个矩阵来说，均可以这样来理解：<strong>行表示神经元个数，列表示样本数目</strong><script type="math/tex">m</script>。</p>
<h4 id="3-5向量化实现的解释"><a href="#3-5向量化实现的解释" class="headerlink" title="3.5向量化实现的解释"></a>3.5向量化实现的解释</h4><p>这部分Andrew用图示的方式解释了<script type="math/tex">m</script>个样本的神经网络矩阵运算过程。其实内容比较简单，只要记住上述四个矩阵的行表示神经元个数，列表示样本数目<script type="math/tex">m</script>就行了。</p>
<p>值得注意的是输入矩阵<script type="math/tex">X</script>也可以写成<script type="math/tex">A^{[0]}</script>。</p>
<h4 id="3-6-激活函数"><a href="#3-6-激活函数" class="headerlink" title="3.6 激活函数"></a>3.6 激活函数</h4><p>神经网络隐藏层和输出层都需要<strong>激活函数(activation function)</strong>，在之前的课程中我们都默认使用Sigmoid函数<script type="math/tex">σ(x)</script>作为激活函数。其实，还有其它激活函数可供使用，不同的激活函数有各自的优点。下面我们就来介绍几个不同的激活函数<script type="math/tex">g(x)</script>。</p>
<p><strong>(1) sigmoid函数</strong></p>
<p><img src="https://pic.imgdb.cn/item/648ac3fb1ddac507cc5c8851.png" style="zoom:70%"></p>
<p><strong>(2) tanh函数</strong></p>
<p><img src="https://pic.imgdb.cn/item/648ac49f1ddac507cc5df268.png" style="zoom:70%"></p>
<p><strong>(3) ReLU函数</strong></p>
<p><img src="https://pic.imgdb.cn/item/648ac4d01ddac507cc5e604c.png" style="zoom:70%"></p>
<p><strong>(4) Leaky ReLU函数</strong></p>
<p><img src="https://pic.imgdb.cn/item/648ac4ed1ddac507cc5ea77a.png" style="zoom:70%"></p>
<p>如上图所示，不同激活函数形状不同，<script type="math/tex">a</script>的取值范围也有差异。</p>
<p>如何选择合适的激活函数呢？首先我们来比较sigmoid函数和tanh函数。对于隐藏层的激活函数，一般来说，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果。因此，隐藏层的激活函数，tanh比sigmoid更好一些。而对于输出层的激活函数，因为二分类问题的输出取值为{0,+1}，所以一般会选择sigmoid作为激活函数。</p>
<p>观察sigmoid函数和tanh函数，我们发现有这样一个问题，就是当<script type="math/tex">|z|</script>很大的时候，激活函数的斜率（梯度）很小。因此，在这个区域内，梯度下降算法会运行得比较慢。在实际应用中，应尽量避免使z落在这个区域，使<script type="math/tex">|z|</script>尽可能限定在零值附近，从而提高梯度下降算法运算速度。</p>
<p>为弥补sigmoid函数和tanh函数的这个缺陷，就出现了ReLU激活函数。ReLU激活函数在<script type="math/tex">z</script>大于零时梯度始终为1；在<script type="math/tex">z</script>小于零时梯度始终为0；<script type="math/tex">z</script><strong>等于零时的梯度可以当成1也可以当成0</strong>，实际应用中并不影响。对于隐藏层，选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点，实际应用中，这个缺点影响不是很大。为了弥补这个缺点，出现了Leaky ReLU激活函数，能够保证z小于零是梯度不为0。</p>
<p>最后总结一下，如果是分类问题，输出层的激活函数一般会选择sigmoid函数。但是隐藏层的激活函数通常不会选择sigmoid函数，tanh函数的表现会比sigmoid函数好一些。实际应用中，通常会会选择使用ReLU或者Leaky ReLU函数，保证梯度下降速度不会太小。其实具体选择哪个函数作为激活函数没有一个固定的准确的答案，应该要根据具体实际问题进行验证(validation)。</p>
<h4 id="3-7-为什么需要非线性激活函数"><a href="#3-7-为什么需要非线性激活函数" class="headerlink" title="3.7 为什么需要非线性激活函数"></a>3.7 为什么需要非线性激活函数</h4><p>我们知道上一部分讲的四种激活函数都是<strong>非线性(non-linear)</strong>的。那是否可以使用线性激活函数呢？答案是不行！下面我们就来进行简要的解释和说明。</p>
<p>假设所有的激活函数都是线性的，为了简化计算，我们直接令激活函数<script type="math/tex">g(z)=z</script>，即<script type="math/tex">a=z</script>。那么，浅层神经网络的各层输出为：</p>
<script type="math/tex; mode=display">
\begin{gathered}z^{[1]}=W^{[1]} x+b^{[1]} \\a^{[1]}=z^{[1]} \\z^{[2]}=W^{[2]} a^{[1]}+b^{[2]} \\a^{[2]}=z^{[2]}\end{gathered}</script><p>我们对上式中<script type="math/tex">a^{[2]}</script>进行化简计算：</p>
<script type="math/tex; mode=display">
\begin{align}a^{[2]}& =z^{[2]} \\& =W^{[2]} a^{[1]}+b^{[2]} \\& =W^{[2]}\left(W^{[1]} x+b^{[1]}\right)+b^{[2]} \\&=\left(W^{[2]} W^{[1]}\right) x+\left(W^{[2]} b^{[1]}+b^{[2]}\right) \\& =W^{\prime} x+b^{\prime}\end{align}</script><p>经过推导我们发现<script type="math/tex">a^{[2]}</script>仍是输入变量<script type="math/tex">x</script>的<strong>线性组合</strong>。这表明，使用神经网络与直接使用线性模型的效果并没有什么两样。即便是包含多层隐藏层的神经网络，<strong>如果使用线性函数作为激活函数，最终的输出仍然是输入</strong><script type="math/tex">x</script><strong>的线性模型</strong>。这样的话神经网络就没有任何作用了。因此，<strong>隐藏层的激活函数必须要是非线性的</strong>。</p>
<p>另外，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。</p>
<p>值得一提的是，如果是预测问题而不是分类问题，输出<script type="math/tex">y</script>是连续的情况下，输出层的激活函数可以使用线性函数。如果输出<script type="math/tex">y</script>恒为正值，则也可以使用ReLU激活函数，具体情况，具体分析。</p>
<h4 id="3-8-激活函数的导数"><a href="#3-8-激活函数的导数" class="headerlink" title="3.8 激活函数的导数"></a>3.8 激活函数的导数</h4><p>在梯度下降反向计算过程中少不了计算激活函数的导数即梯度。</p>
<p>(1) sigmoid函数的导数</p>
<script type="math/tex; mode=display">
\begin{gathered}g(z)=\frac{1}{1+e^{(-z)}} \\g^{\prime}(z)=\frac{\mathrm d}{\mathrm d z} g(z)=g(z)(1-g(z))=a(1-a)\end{gathered}</script><p>(2) tanh函数的导数</p>
<script type="math/tex; mode=display">
\begin{gathered}g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}} \\g^{\prime}(z)=\frac{\mathrm d}{\mathrm d z} g(z)=1-(g(z))^{2}=1-a^{2}\end{gathered}</script><p>(3) ReLU函数的导数</p>
<script type="math/tex; mode=display">
\begin{gathered}g(z)=\max (0, z) \\g^{\prime}(z)= \begin{cases}0, & z<0 \\1, & z > 0 \\0\ \text{or}\ 1,& z = 0\end{cases}\end{gathered}</script><p>(4) Leaky ReLU函数的导数</p>
<script type="math/tex; mode=display">
\begin{gathered}g(z)=\max (0.01z, z) \\g^{\prime}(z)= \begin{cases}0.01, & z<0 \\1, & z \geq 0\end{cases}\end{gathered}</script><h4 id="3-9-神经网络的梯度下降法"><a href="#3-9-神经网络的梯度下降法" class="headerlink" title="3.9 神经网络的梯度下降法"></a>3.9 神经网络的梯度下降法</h4><p>你的单隐层神经网络会有<script type="math/tex">W^{[1]}</script>，<script type="math/tex">b^{[1]}</script>，<script type="math/tex">W^{[2]}</script>，<script type="math/tex">b^{[2]}</script>这些参数，还有<script type="math/tex">n_x</script>个表示输入特征的个数，<script type="math/tex">n^{[1]}</script>表示隐藏单元个数, <script type="math/tex">n^{[2]}</script>表示输出单元个数。只介绍这种情况，那么参数:</p>
<p>矩阵<script type="math/tex">W^{[1]}</script>的维度就是<script type="math/tex">(n^{[1]}, n^{[0]})</script>，<script type="math/tex">b^{[1]}</script>就是<script type="math/tex">n^{[1]}</script>维向量，可以写成<script type="math/tex">(n^{[1]},1)</script>，就是一个的列向量。矩阵<script type="math/tex">W^{[2]}</script>的维度就是<script type="math/tex">(n^{[2]},n^{[1]} )</script>, <script type="math/tex">b^{[2]}</script>的维度就是<script type="math/tex">(n^{[2]},1 )</script>。</p>
<p>你还有一个神经网络的成本函数，假设你在做二分类任务，那么你的成本函数等于</p>
<script type="math/tex; mode=display">
J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \frac{1}{m}\sum_{i=1}^{m}{L(\hat y,y)}</script><p>训练参数需要做梯度下降，在训练神经网络的时候，随机初始化参数很重要，而<strong>不是初始化成全零</strong>。当你参数初始化成某些值后，每次梯度下降都会循环计算以下预测值<script type="math/tex">\hat y, (i= 1,2,.... m)</script></p>
<script type="math/tex; mode=display">
\begin{aligned}&\mathrm d W^{[1]}=\frac{\mathrm d J}{\mathrm d W^{[1]}}, \mathrm d b^{[1]}=\frac{\mathrm d J}{\mathrm d b^{[1]}} \\&\mathrm d W^{[2]}=\frac{\mathrm d J}{\mathrm d W^{[2]}}, \mathrm d b^{[2]}=\frac{\mathrm d J}{\mathrm d b^{[2]}}\end{aligned}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{aligned}&W^{[1]} \Rightarrow W^{[1]}-\alpha \mathrm d W^{[1]}, b^{[1]} \Rightarrow b^{[1]}-\alpha \mathrm d b^{[1]} \\&W^{[2]} \Rightarrow W^{[2]}-\alpha\mathrm  d W^{[2]}, b^{[2]} \Rightarrow b^{[2]}-\alpha \mathrm d b^{[2]}\end{aligned}</script><p>使用计算图的方式来推导神经网络反向传播过程。记得之前介绍逻辑回归时，我们就引入了计算图来推导正向传播和反向传播，其过程如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648ac6371ddac507cc6308ed.png"></p>
<p>由于多了一个隐藏层，神经网络的计算图要比逻辑回归的复杂一些，如下图所示。对于单个训练样本，正向过程很容易，反向过程可以根据梯度计算方法逐一推导。</p>
<script type="math/tex; mode=display">
\begin{gathered}\mathrm d z^{[2]}=a^{[2]}-y \\
\mathrm d W^{[2]}=\mathrm d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial W^{[2]}}=\mathrm d z^{[2]} a^{[1] T} \\
\mathrm d b^{[2]}=\mathrm d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial b^{[2]}}=\mathrm d z^{[2]} \cdot 1=\mathrm d z^{[2]} \\
\mathrm d z^{[1]}=\mathrm d z^{[2]} \cdot \frac{\partial z^{[2]}}{\partial a^{[1]}} \cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2] T}\mathrm  d z^{[2]} * g^{[1]^{\prime}}\left(z^{[1]}\right) \\
\mathrm d b^{[1]}=\mathrm d z^{[1]} \cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=\mathrm d z^{[1]} \cdot 1=\mathrm d z^{[1]} \\
\mathrm d z^{[1]}=\mathrm d z^{[1]} \cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=\mathrm d z^{[1]} x^{T}\end{gathered}</script><script type="math/tex; mode=display">
\begin{aligned}&\mathrm d z^{[2]}=A^{[2]}-Y, Y=\left[\begin{array}{lll}y^{[1]} & y^{[2]} & \ldots & \left.y^{[m]}\right]\end{array}\right. \\
&\mathrm d W^{[2]}=\frac{1}{m} \mathrm d z^{[2]} A^{[1] T} \\&\mathrm d b^{[2]}=\frac{1}{m} \text { np.sum }\left(\mathrm d z^{[2]}, \text { axis }=1, \text { keepdims }=\text { True }\right) \\
&\mathrm d z^{[1]}=\underbrace{W^{[2] T} \mathrm d z^{[2]}}_{\left(n^{[1]}, m\right)} \quad \text { activation function of hidden layer } \quad * \underbrace{\left(z^{[1]}\right)}_{\left(n^{[1]}, m\right)} \\
&\mathrm d W^{[1]}=\frac{1}{m} \mathrm d z^{[1]} x^{T} \\
&\underbrace{\mathrm d b^{[1]}}=\frac{1}{m} \text { np.sum }\left(\mathrm d z^{[1]}, \text { axis }=1, \text { keepdims }=\text { True }\right)\end{aligned}</script><p><img src="https://pic.imgdb.cn/item/648ac7ce1ddac507cc677d5e.png"></p>
<p>上述是反向传播的步骤，注：这些都是针对所有样本进行过向量化，<script type="math/tex">Y</script>是<script type="math/tex">1 ∗ m</script>的矩阵；这里np.sum是python的numpy命令，<strong>axis=1表示水平相加求和</strong>，<strong>keepdims防止python输出那些古怪的秩数</strong><script type="math/tex">(n , )</script>，加上这个确保矩阵<script type="math/tex">db^{[2]}</script>这个向量输出维度为<script type="math/tex">( n , 1 )</script>这样标准的形式。</p>
<p>总结一下，浅层神经网络(包含一个隐藏层)，<script type="math/tex">m</script>个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，其向量化矩阵形式如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/648ac8041ddac507cc681841.png"></p>
<h4 id="3-10-选修-直观理解反向传播"><a href="#3-10-选修-直观理解反向传播" class="headerlink" title="3.10 (选修)直观理解反向传播"></a>3.10 (选修)直观理解反向传播</h4><p>本节记录一下我的两个疑问：</p>
<p>(1) 矩阵微积分问题</p>
<p>(2) <script type="math/tex">\mathrm dZ</script>的式子中没有<script type="math/tex">1/m</script>项</p>
<p>本小节笔记参考链接：<a href="https://blog.csdn.net/weixin_36815313/article/details/105341107">3.10 直观理解反向传播-深度学习-Stanford吴恩达教授_赵继超的笔记-CSDN博客</a></p>
<h4 id="3-11-随机初始化"><a href="#3-11-随机初始化" class="headerlink" title="3.11 随机初始化"></a>3.11 随机初始化</h4><p>神经网络模型中的参数权重W是不能全部初始化为零的，接下来我们分析一下原因。</p>
<p>举个简单的例子，一个浅层神经网络包含两个输入，隐藏层包含两个神经元。如果权重<script type="math/tex">W[1]</script>和<script type="math/tex">W^{[2]}</script>都初始化为零，即：</p>
<script type="math/tex; mode=display">
\begin{gathered}W^{[1]}=\left[\begin{array}{ll}0 & 0 \\0 & 0\end{array}\right] \\W^{[2]}=\left[\begin{array}{ll}0 & 0\end{array}\right]\end{gathered}</script><p><img src="https://pic.imgdb.cn/item/648ac8e91ddac507cc6ae7b1.png" style="zoom:60%"></p>
<p>这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即<script type="math/tex">a^{[1]}_1=a^{[1]}_2</script>。经过推导得到<script type="math/tex">dz^{[1]}_1 = dz^{[1]}_2</script>，以及<script type="math/tex">dW^{[1]}_1 = dW^{[1]}_2</script>。因此，这样的结果是隐藏层两个神经元对应的权重行向量<script type="math/tex">W^{[1]}_1</script>和<script type="math/tex">W^{[1]}_2</script>，每次迭代更新都会得到完全相同的结果，<script type="math/tex">W^{[1]}_1</script>始终等于<script type="math/tex">W^{[1]}_2</script>，完全对称。这样隐藏层设置多个神经元就没有任何意义了。值得一提的是，<strong>参数</strong><script type="math/tex">b</script><strong>可以全部初始化为零</strong>，并不会影响神经网络训练效果；<strong>此外权重</strong><script type="math/tex">W</script><strong>不能全初始化为零，也不能全部初始化为一样的值。</strong></p>
<p>我们把这种权重W全部初始化为零带来的问题称为symmetry breaking problem。解决方法也很简单，就是将W进行随机初始化(b可初始化为零)。python里可以使用如下语句进行W和b的初始化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W_1 = np.random.randn((<span class="number">2</span>,<span class="number">2</span>))*<span class="number">0.01</span></span><br><span class="line">b_1 = np.zero((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">W_2 = np.random.randn((<span class="number">1</span>,<span class="number">2</span>))*<span class="number">0.01</span></span><br><span class="line">b_2 = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>这里我们将<script type="math/tex">W^{[1]}_1</script>和<script type="math/tex">W^{[1]}_2</script>乘以<script type="math/tex">0.01</script>的目的是尽量使得权重<script type="math/tex">W</script>初始化比较小的值。之所以让<script type="math/tex">W</script>比较小，是因为如果使用sigmoid函数或者tanh函数作为激活函数的话，<script type="math/tex">W</script>比较小，得到的<script type="math/tex">|z|</script>也比较小(靠近零点)，而<strong>零点区域的梯度比较大</strong>，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解。如果<script type="math/tex">W</script>较大，得到的<script type="math/tex">|z|</script>也比较大，附近曲线平缓，梯度较小，训练过程会慢很多。</p>
<p>当然，如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题。但是，如果输出层是sigmoid函数，则对应的权重最好初始化到比较小的值。</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>编程与深度学习</category>
        <category>深度学习基础</category>
      </categories>
      <tags>
        <tag>深度学习基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Python基础</title>
    <url>/2022/04/17/Foundation-of-Python/</url>
    <content><![CDATA[<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1o4411M71o" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i2.hdslb.com/bfs/archive/213b14bfbdbf54248a3f5fa742cb9f9c8f684274.png)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">40:08:30</span>
            </div>
            <div class="bvideo-info">
                <p class="title">黑马程序员全套Python教程_Python基础入门视频教程，零基础小白自学Python入门教程</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>153.6万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>4.7万</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">黑马程序员</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<h3 id="第〇章-PYTHON简介"><a href="#第〇章-PYTHON简介" class="headerlink" title="第〇章 PYTHON简介"></a>第〇章 PYTHON简介</h3><h4 id="0-1-Python简介"><a href="#0-1-Python简介" class="headerlink" title="0.1 Python简介"></a>0.1 Python简介</h4><p>Python时当前比较流行的<strong>编程语言</strong>。具有简单易学、免费开源、应用领域广泛等特点。</p>
<h4 id="0-2-计算机的组成"><a href="#0-2-计算机的组成" class="headerlink" title="0.2 计算机的组成"></a>0.2 计算机的组成</h4><p><img src="https://pic.imgdb.cn/item/625b8d63239250f7c550004f.jpg"></p>
<p>本节课重点关注<strong>CPU</strong>和<strong>内存</strong>的作用。其中内存是临时存储数据的地方，由于内存的速度远远高于硬盘，所以CPU处理的数据都是从内存调取的。</p>
<h4 id="0-3-Python学习路径"><a href="#0-3-Python学习路径" class="headerlink" title="0.3 Python学习路径"></a>0.3 Python学习路径</h4><p><img src="https://pic.imgdb.cn/item/625b8e94239250f7c551cf9a.jpg"></p>
<h3 id="第一章-PYTHON入门基础"><a href="#第一章-PYTHON入门基础" class="headerlink" title="第一章 PYTHON入门基础"></a>第一章 PYTHON入门基础</h3><h4 id="1-1-解释器的作用和分类"><a href="#1-1-解释器的作用和分类" class="headerlink" title="1.1 解释器的作用和分类"></a>1.1 解释器的作用和分类</h4><h5 id="一-解释器的作用"><a href="#一-解释器的作用" class="headerlink" title="(一) 解释器的作用"></a>(一) 解释器的作用</h5><p>Python解释器的作用：<font color="red"><strong>运行文件</strong></font>。(形象化类比人类翻译官，解释器就是将python语句“翻译为”计算机能读懂的机器码)。</p>
<h5 id="二-解释器的种类"><a href="#二-解释器的种类" class="headerlink" title="(二) 解释器的种类"></a>(二) 解释器的种类</h5><ul>
<li>CPython：C语言开发的解释器【官方】，应用广泛的解释器；</li>
<li>IPython：基于CPython的一种交互式解释器；</li>
<li>其他解释器<ul>
<li>PyPy：基于Python语⾔言开发的解释器器；</li>
<li>Jython：运行在Java平台的解释器器，直接把Python代码编译成Java字节码执行；</li>
<li>IronPython：运行在微软.Net平台上的Python解释器器，可以直接把Python代码编译<br>成.Net的字节码。</li>
</ul>
</li>
</ul>
<h4 id="1-2-下载Python解释器"><a href="#1-2-下载Python解释器" class="headerlink" title="1.2 下载Python解释器"></a>1.2 下载Python解释器</h4><p>安装了andaconda好像就不用单独安装解释器了。</p>
<h4 id="1-3-PyCharm的应用"><a href="#1-3-PyCharm的应用" class="headerlink" title="1.3 PyCharm的应用"></a>1.3 PyCharm的应用</h4><h5 id="一-PyCharm简介"><a href="#一-PyCharm简介" class="headerlink" title="(一) PyCharm简介"></a>(一) PyCharm简介</h5><p>PyCharm是一种<font color="blue">Python <strong>IDE（集成开发环境）</strong></font>，带有一整套可以帮助用户在使用Python语⾔言开发时提高其效率的工具，内部集成的功能有Project管理，智能提示，语法高亮，代码跳转，调试代码，解释代码(解释器)，框架和库等等。</p>
<h5 id="二-PyCharm设置"><a href="#二-PyCharm设置" class="headerlink" title="(二) PyCharm设置"></a>(二) PyCharm设置</h5><p>(1) 界面设置</p>
<p>(2) 设置解释器</p>
<p>(3) 项目管理</p>
<h4 id="1-4-注释"><a href="#1-4-注释" class="headerlink" title="1.4 注释"></a>1.4 注释</h4><p>注释分为两类：单行注释和多行注释</p>
<ul>
<li>单行注释(快捷键：ctrl+/)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注释内容</span></span><br></pre></td></tr></table></figure>
<ul>
<li>多行注释</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">	多行注释1</span></span><br><span class="line"><span class="string">	多行注释2</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	多行1</span></span><br><span class="line"><span class="string">	多行2</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<h4 id="1-5-变量"><a href="#1-5-变量" class="headerlink" title="1.5 变量"></a>1.5 变量</h4><h5 id="一-定义变量"><a href="#一-定义变量" class="headerlink" title="(一) 定义变量"></a>(一) 定义变量</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">变量名 = 值</span><br></pre></td></tr></table></figure>
<p>要注意，变量名的命名要符合<strong>标识符命名规则</strong>：</p>
<ul>
<li>由数字、字母、下划线组成</li>
<li>不能数字开头</li>
<li>不能使用内置关键字</li>
<li><strong>严格区分大小写</strong></li>
</ul>
<h5 id="二-命名习惯"><a href="#二-命名习惯" class="headerlink" title="(二) 命名习惯"></a>(二) 命名习惯</h5><ul>
<li>见名知义。</li>
<li>大驼峰：即每个单词⾸首字⺟母都⼤大写，例如： MyName 。</li>
<li>小驼峰：第二个（含）以后的单词首字母大写，例如： myName 。</li>
<li>下划线：例如： my_name 。</li>
</ul>
<h4 id="1-6-Bug问题"><a href="#1-6-Bug问题" class="headerlink" title="1.6 Bug问题"></a>1.6 Bug问题</h4><p>所谓bug，就是程序中的错误。如果程序有错误，需要程序员排查问题，纠正错误。</p>
<h5 id="一-Debug工具——调试"><a href="#一-Debug工具——调试" class="headerlink" title="(一) Debug工具——调试"></a>(一) Debug工具——调试</h5><p>(1) 步骤1：打断点</p>
<p>单击目标代码的行号右侧空白位置。</p>
<p>(2) 步骤2：调试</p>
<p>打成功断点后，在文件内部任意位置 — 右键 — Debug’文件名’ — 即可调出Debug工具⾯面板 — 单击Step<br>Over/F8，即可按步执行行代码。</p>
<ul>
<li>Debugger</li>
</ul>
<p>​        显示变量量和变量量的细节</p>
<ul>
<li>Console</li>
</ul>
<p>​        输出内容</p>
<h4 id="1-7-数据类型"><a href="#1-7-数据类型" class="headerlink" title="1.7 数据类型"></a>1.7 数据类型</h4><p><img src="https://pic.imgdb.cn/item/625bd2d4239250f7c5c6de00.jpg"></p>
<p>产看变量的类型使用<code>type</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(a)) <span class="comment"># &lt;class &#x27;int&#x27;&gt; -- 整型</span></span><br><span class="line">b = <span class="number">1.1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(b)) <span class="comment"># &lt;class &#x27;float&#x27;&gt; -- 浮点型</span></span><br><span class="line">c = <span class="literal">True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(c)) <span class="comment"># &lt;class &#x27;bool&#x27;&gt; -- 布尔型</span></span><br><span class="line">d = <span class="string">&#x27;12345&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(d)) <span class="comment"># &lt;class &#x27;str&#x27;&gt; -- 字符串</span></span><br><span class="line">e = [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(e)) <span class="comment"># &lt;class &#x27;list&#x27;&gt; -- 列表</span></span><br><span class="line">f = (<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(f)) <span class="comment"># &lt;class &#x27;tuple&#x27;&gt; -- 元组</span></span><br><span class="line">h = &#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(h)) <span class="comment"># &lt;class &#x27;set&#x27;&gt; -- 集合</span></span><br><span class="line">g = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;TOM&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(g)) <span class="comment"># &lt;class &#x27;dict&#x27;&gt; -- 字典</span></span><br></pre></td></tr></table></figure>
<h4 id="1-8-输出"><a href="#1-8-输出" class="headerlink" title="1.8 输出"></a>1.8 输出</h4><h5 id="一-认识格式化符号"><a href="#一-认识格式化符号" class="headerlink" title="(一) 认识格式化符号"></a>(一) 认识格式化符号</h5><ul>
<li>格式化输出<ul>
<li>格式化符号</li>
<li><strong>f-字符串串</strong></li>
</ul>
</li>
<li>print的结束符</li>
</ul>
<h5 id="二-格式化输出"><a href="#二-格式化输出" class="headerlink" title="(二) 格式化输出"></a>(二) 格式化输出</h5><p>(1) 常见格式化符号</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">格式符号</th>
<th style="text-align:center">转换</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">%s</td>
<td style="text-align:center">字符串</td>
</tr>
<tr>
<td style="text-align:center">%d</td>
<td style="text-align:center">有符号的十进制整数</td>
</tr>
<tr>
<td style="text-align:center">%f</td>
<td style="text-align:center">浮点数</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>技巧：</p>
<p>%06d，表示输出的整数显示位数，不足以0补全，超出当前位数则原样输出；<br>%.2f，表示小数点后显示的小数位数。</p>
</blockquote>
<p>(2) 格式化输出示例</p>
<p>格式化字符串除了%s，还可以写为<code>f&#39;&#123;表达式&#125;&#39;</code>(是Python3.6中新增的格式化方法，该方法更简单易读。)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">age = <span class="number">18</span></span><br><span class="line">name = <span class="string">&#x27;TOM&#x27;</span></span><br><span class="line">weight = <span class="number">75.5</span></span><br><span class="line">student_id = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我的名字是TOM</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我的名字是%s&#x27;</span> % name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我的学号是0001</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我的学号是%04d&#x27;</span> % student_id)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我的体重是75.50公斤</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我的体重是%.2f公斤&#x27;</span> % weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我的名字是TOM，今年年18岁了</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我的名字是%s，今年%d岁了&#x27;</span> % (name, age))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我的名字是TOM，明年年19岁了了</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我的名字是%s，明年%d岁了&#x27;</span> % (name, age + <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我的名字是TOM，明年年19岁了了</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;我的名字是<span class="subst">&#123;name&#125;</span>, 明年<span class="subst">&#123;age + <span class="number">1</span>&#125;</span>岁了&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>注意，其中%s的功能相对更强，可以格式化输出字符串、整数、浮点数。</strong></p>
</blockquote>
<h5 id="三-转义字符"><a href="#三-转义字符" class="headerlink" title="(三)  转义字符"></a>(三)  转义字符</h5><ul>
<li><code>\n</code>：换行</li>
<li><code>\t</code>：制表位，一个tab键(4个空格距离)</li>
</ul>
<h5 id="四-结束符"><a href="#四-结束符" class="headerlink" title="(四) 结束符"></a>(四) 结束符</h5><p>想⼀想，为什么两个print会换行输出？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;输出的内容&#x27;</span>, end=<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"><span class="comment"># 修改结束符号为...</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;输出的内容&#x27;</span>, end=<span class="string">&quot;...&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在Python中，print()， <strong>默认自带end=”\n” 这个换行行结束符</strong>，所以导致每两个print 直接会换⾏行行<br>展示，用户可以按需求更改结束符。</p>
<h4 id="1-9-输入"><a href="#1-9-输入" class="headerlink" title="1.9 输入"></a>1.9 输入</h4><h5 id="一-输入语法"><a href="#一-输入语法" class="headerlink" title="(一) 输入语法"></a>(一) 输入语法</h5><p>在Python中，程序接收用户输入的数据的功能即是输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">变量 = <span class="built_in">input</span>(<span class="string">&quot;提示信息&quot;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="二-输入的特点"><a href="#二-输入的特点" class="headerlink" title="(二) 输入的特点"></a>(二) 输入的特点</h5><ul>
<li>当程序执行到input ，等待用户输入，输入完成之后才继续向下执行；</li>
<li>在Python中， input 接收用户输入后，一般存储到变量，方便使用；</li>
<li>在Python中， input 会把接收到的任意用户输入的数据都当做字符串处理。</li>
</ul>
<h5 id="三-演示输入"><a href="#三-演示输入" class="headerlink" title="(三) 演示输入"></a>(三) 演示输入</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">password = <span class="built_in">input</span>(<span class="string">&#x27;请输⼊入您的密码：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;您输⼊入的密码是<span class="subst">&#123;password&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># &lt;class &#x27;str&#x27;&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(password))</span><br></pre></td></tr></table></figure>
<h4 id="1-10-数据类型转换"><a href="#1-10-数据类型转换" class="headerlink" title="1.10 数据类型转换"></a>1.10 数据类型转换</h4><h5 id="一-转换数据类型的作用"><a href="#一-转换数据类型的作用" class="headerlink" title="(一) 转换数据类型的作用"></a>(一) 转换数据类型的作用</h5><p>问：input()接收⽤用户输入的数据都是字符串类型，如果用户输入1，想得到整型该如何操作？<br>答：转换数据类型即可，即将字符串类型转换成整型。</p>
<h5 id="二-转换数据类型的函数"><a href="#二-转换数据类型的函数" class="headerlink" title="(二) 转换数据类型的函数"></a>(二) 转换数据类型的函数</h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">int(x[,base])</td>
<td style="text-align:center">将x转换为一个整数</td>
</tr>
<tr>
<td style="text-align:center">float(x)</td>
<td style="text-align:center">将x转换为一个浮点数</td>
</tr>
<tr>
<td style="text-align:center">str(x)</td>
<td style="text-align:center">将对象 x 转换为字符串</td>
</tr>
<tr>
<td style="text-align:center">tuple(s)</td>
<td style="text-align:center">将序列列 s 转换为一个元组</td>
</tr>
<tr>
<td style="text-align:center">list(s)</td>
<td style="text-align:center">将序列列 s 转换为一个列表</td>
</tr>
<tr>
<td style="text-align:center">eval(str)</td>
<td style="text-align:center">用来计算在字符串中的有效Python表达式,并返回一个对象</td>
</tr>
<tr>
<td style="text-align:center">complex(real [,imag ])</td>
<td style="text-align:center">创建一个复数，real为实部，imag为虚部</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
</tbody>
</table>
</div>
<h4 id="1-11-运算符的分类"><a href="#1-11-运算符的分类" class="headerlink" title="1.11 运算符的分类"></a>1.11 运算符的分类</h4><ul>
<li>算数运算符</li>
<li>赋值运算符</li>
<li>复合赋值运算符</li>
<li>比较运算符</li>
<li>逻辑运算符</li>
</ul>
<h5 id="一-算术运算符"><a href="#一-算术运算符" class="headerlink" title="(一) 算术运算符"></a>(一) 算术运算符</h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">运算符</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">加</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">减</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">*</td>
<td style="text-align:center">乘</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">/</td>
<td style="text-align:center">除</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">//</td>
<td style="text-align:center">整除</td>
<td style="text-align:center">9 // 4 输出结果为 2</td>
</tr>
<tr>
<td style="text-align:center">%</td>
<td style="text-align:center">取余</td>
<td style="text-align:center">9 % 4 输出结果为 1</td>
</tr>
<tr>
<td style="text-align:center">**</td>
<td style="text-align:center">指数</td>
<td style="text-align:center">2 <em>* 4 输出结果为 16，即 2 </em> 2 <em> 2 </em> 2</td>
</tr>
<tr>
<td style="text-align:center">()</td>
<td style="text-align:center">括号</td>
<td style="text-align:center">小括号⽤用来提高运算优先级，即 (1 + 2) * 3 输出结果为 9</td>
</tr>
</tbody>
</table>
</div>
<h5 id="二-赋值运算符"><a href="#二-赋值运算符" class="headerlink" title="(二) 赋值运算符"></a>(二) 赋值运算符</h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">运算符</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">=</td>
<td style="text-align:center">赋值</td>
<td style="text-align:center">将= 右侧的结果赋值给等号左侧的变量</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多变量赋值</span></span><br><span class="line">num1, float1, str1 = <span class="number">10</span>, <span class="number">0.5</span>, <span class="string">&#x27;hello world&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(num1)</span><br><span class="line"><span class="built_in">print</span>(float1)</span><br><span class="line"><span class="built_in">print</span>(str1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多变量量赋相同值</span></span><br><span class="line">a = b = <span class="number">10</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure>
<h5 id="三-复合赋值运算符"><a href="#三-复合赋值运算符" class="headerlink" title="(三) 复合赋值运算符"></a>(三) 复合赋值运算符</h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">运算符</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">+=</td>
<td style="text-align:center">加法赋值运算</td>
<td style="text-align:center">c += a 等价于 c = c + a</td>
</tr>
<tr>
<td style="text-align:center">-=</td>
<td style="text-align:center">减法赋值运算</td>
<td style="text-align:center">c -= a 等价于 c = c - a</td>
</tr>
<tr>
<td style="text-align:center">*=</td>
<td style="text-align:center">乘法赋值运算</td>
<td style="text-align:center">c <em>= a 等价于 c = c </em> a</td>
</tr>
<tr>
<td style="text-align:center">/=</td>
<td style="text-align:center">除法赋值运算</td>
<td style="text-align:center">c /= a 等价于 c = c / a</td>
</tr>
<tr>
<td style="text-align:center">//=</td>
<td style="text-align:center">整除赋值运算</td>
<td style="text-align:center">c //= a 等价于 c = c // a</td>
</tr>
<tr>
<td style="text-align:center">%=</td>
<td style="text-align:center">取余赋值运算</td>
<td style="text-align:center">c %= a 等价于 c = c % a</td>
</tr>
<tr>
<td style="text-align:center">**=</td>
<td style="text-align:center">指数赋值运算</td>
</tr>
</tbody>
</table>
</div>
<h5 id="四-关系-比较运算符"><a href="#四-关系-比较运算符" class="headerlink" title="(四) 关系/比较运算符"></a>(四) 关系/比较运算符</h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">运算符</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">==</td>
<td style="text-align:center">相等</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">!=</td>
<td style="text-align:center">不相等</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">&gt;</td>
<td style="text-align:center">大于</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">>=</td>
<td style="text-align:center">大于等于</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">&lt;</td>
<td style="text-align:center">小于</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">&lt;=</td>
<td style="text-align:center">小于等于</td>
</tr>
</tbody>
</table>
</div>
<h5 id="五-逻辑运算符"><a href="#五-逻辑运算符" class="headerlink" title="(五) 逻辑运算符"></a>(五) 逻辑运算符</h5><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">运算符</th>
<th style="text-align:center">逻辑表达式</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">实例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">and</td>
<td style="text-align:center">x and y</td>
<td style="text-align:center">布尔”与”：若x为False，x and y返回False，否则返回y值</td>
<td style="text-align:center">True and False，返回False</td>
</tr>
<tr>
<td style="text-align:center">or</td>
<td style="text-align:center">x or y</td>
<td style="text-align:center">布尔”或”：若x是True，返回 True，否则返回y的值</td>
<td style="text-align:center">False or True，返回True</td>
</tr>
<tr>
<td style="text-align:center">not</td>
<td style="text-align:center">not x</td>
<td style="text-align:center">布尔”非”：若x为True，返回False 。若x为False，返回 True</td>
<td style="text-align:center">not True 返回 False</td>
</tr>
</tbody>
</table>
</div>
<h5 id="六-小结"><a href="#六-小结" class="headerlink" title="(六) 小结"></a>(六) 小结</h5><ul>
<li>算数运算的优先级<ul>
<li>混合运算优先级顺序： () 高于 <em>* 高于 </em> / // % 高于 + -</li>
</ul>
</li>
</ul>
<h3 id="第二章-流程控制"><a href="#第二章-流程控制" class="headerlink" title="第二章 流程控制"></a>第二章 流程控制</h3><h4 id="2-1-条件语句"><a href="#2-1-条件语句" class="headerlink" title="2.1 条件语句"></a>2.1 条件语句</h4><p>条件语句，即条件成立执行某些代码，条件不成立则不执行这些代码。</p>
<h5 id="一-if语句语法"><a href="#一-if语句语法" class="headerlink" title="(一) if语句语法"></a>(一) if语句语法</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> 条件:</span><br><span class="line">	条件成立执行的代码<span class="number">1</span></span><br><span class="line">	条件成立执行的代码<span class="number">2</span></span><br><span class="line">	......</span><br></pre></td></tr></table></figure>
<h5 id="二-if-else语句语法"><a href="#二-if-else语句语法" class="headerlink" title="(二) if-else语句语法"></a>(二) if-else语句语法</h5><p>作用：条件成立执行if下方的代码; 条件不成立执行else下方的代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> 条件:</span><br><span class="line">	条件成立执行的代码<span class="number">1</span></span><br><span class="line">	条件成立执行的代码<span class="number">2</span></span><br><span class="line">	......</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	条件不成立执行的代码<span class="number">1</span></span><br><span class="line">	条件不成立执行的代码<span class="number">2</span></span><br><span class="line">	......</span><br></pre></td></tr></table></figure>
<h5 id="三-多重判断：if-elif-else语句语法"><a href="#三-多重判断：if-elif-else语句语法" class="headerlink" title="(三) 多重判断：if-elif-else语句语法"></a>(三) 多重判断：if-elif-else语句语法</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> 条件<span class="number">1</span>:</span><br><span class="line">	条件<span class="number">1</span>成立执行的代码<span class="number">1</span></span><br><span class="line">	条件<span class="number">1</span>成立执行的代码<span class="number">2</span></span><br><span class="line">	......</span><br><span class="line"><span class="keyword">elif</span> 条件<span class="number">2</span>：</span><br><span class="line">	条件<span class="number">2</span>成立执行的代码<span class="number">1</span></span><br><span class="line">	条件<span class="number">2</span>成立执行的代码<span class="number">2</span></span><br><span class="line">	......</span><br><span class="line"><span class="keyword">elif</span> 条件<span class="number">3</span>:</span><br><span class="line">    ......</span><br><span class="line">......</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	以上条件都不成立执行的代码</span><br></pre></td></tr></table></figure>
<h5 id="四-if嵌套语法"><a href="#四-if嵌套语法" class="headerlink" title="(四) if嵌套语法"></a>(四) if嵌套语法</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> 条件<span class="number">1</span>：</span><br><span class="line">	条件<span class="number">1</span>成立执行的代码</span><br><span class="line">	条件<span class="number">1</span>成立执行的代码</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">if</span> 条件<span class="number">2</span>：</span><br><span class="line">		条件<span class="number">2</span>成立执行的代码</span><br><span class="line">		条件<span class="number">2</span>成立执行的代码</span><br></pre></td></tr></table></figure>
<h5 id="五-示例——猜拳游戏"><a href="#五-示例——猜拳游戏" class="headerlink" title="(五) 示例——猜拳游戏"></a>(五) 示例——猜拳游戏</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">提示：0-石头，1-剪刀，2-布</span></span><br><span class="line"><span class="string">1. 出拳</span></span><br><span class="line"><span class="string">玩家输入出拳</span></span><br><span class="line"><span class="string">电脑随机出拳</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. 判断输赢</span></span><br><span class="line"><span class="string">玩家获胜</span></span><br><span class="line"><span class="string">平局</span></span><br><span class="line"><span class="string">电脑获胜</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入random模块</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment"># 计算电脑出拳的随机数字</span></span><br><span class="line">computer = random.randint(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(computer)</span><br><span class="line">player = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请出拳：0-石头，1-剪刀，2-布：&#x27;</span>))</span><br><span class="line"><span class="comment"># 玩家胜利 p0:c1 或 p1:c2 或 p2:c0</span></span><br><span class="line"><span class="keyword">if</span> ((player == <span class="number">0</span>) <span class="keyword">and</span> (computer == <span class="number">1</span>) <span class="keyword">or</span> ((player == <span class="number">1</span>) <span class="keyword">and</span> (computer == <span class="number">2</span>) <span class="keyword">or</span></span><br><span class="line">((player == <span class="number">2</span>) <span class="keyword">and</span> (computer == <span class="number">0</span>)):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;玩家获胜&#x27;</span>)</span><br><span class="line"><span class="comment"># 平局：玩家 == 电脑</span></span><br><span class="line"><span class="keyword">elif</span> player == computer:</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;平局&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;电脑获胜&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="六-三元运算符"><a href="#六-三元运算符" class="headerlink" title="(六) 三元运算符"></a>(六) 三元运算符</h5><p>三元运算符也叫三目运算符或三元表达式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">条件成立执行的表达式 <span class="keyword">if</span> 条件 <span class="keyword">else</span> 条件不成立执行的表达式</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = <span class="number">1</span></span><br><span class="line">b = <span class="number">2</span></span><br><span class="line">c = a <span class="keyword">if</span> a &gt; b <span class="keyword">else</span> b</span><br><span class="line"><span class="built_in">print</span>(c)  <span class="comment"># 输出c=b=2</span></span><br></pre></td></tr></table></figure>
<h4 id="2-2-循环语句"><a href="#2-2-循环语句" class="headerlink" title="2.2 循环语句"></a>2.2 循环语句</h4><h5 id="一-循环语句的分类"><a href="#一-循环语句的分类" class="headerlink" title="(一) 循环语句的分类"></a>(一) 循环语句的分类</h5><p>在Python中，循环分为while和for两种，最终实现效果相同。</p>
<h5 id="二-while循环的语法"><a href="#二-while循环的语法" class="headerlink" title="(二) while循环的语法"></a>(二) while循环的语法</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> 条件:</span><br><span class="line">	条件成立重复执行的代码<span class="number">1</span></span><br><span class="line">	条件成立重复执行的代码<span class="number">2</span></span><br><span class="line">	......</span><br></pre></td></tr></table></figure>
<p>示例：计算1-100累加和</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">i = <span class="number">1</span></span><br><span class="line">result = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt;= <span class="number">100</span>:</span><br><span class="line">	result += i</span><br><span class="line">	i += <span class="number">1</span></span><br><span class="line"><span class="comment"># 输出5050</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<h5 id="三-while循环嵌套"><a href="#三-while循环嵌套" class="headerlink" title="(三) while循环嵌套"></a>(三) while循环嵌套</h5><p>略。。。</p>
<h5 id="四-for循环的语法"><a href="#四-for循环的语法" class="headerlink" title="(四) for循环的语法"></a>(四) for循环的语法</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> 临时变量 <span class="keyword">in</span> 序列:</span><br><span class="line">	重复执行的代码<span class="number">1</span></span><br><span class="line">	重复执行的代码<span class="number">2</span></span><br><span class="line">	......</span><br></pre></td></tr></table></figure>
<h5 id="五-循环与else"><a href="#五-循环与else" class="headerlink" title="(五) 循环与else"></a>(五) 循环与else</h5><p><strong>循环和else配合使用，else下方缩进的代码指的是当循环正常结束之后要执行的代码。</strong></p>
<font color="red">所谓else指的是循环正常结束之后要执行的代码，即如果是break终止循环的情况，else下方缩进的代码将不执行，但是对于continue语句则else正常执行。</font>

<p>(1) while…else</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> 条件:</span><br><span class="line">	条件成立重复执行的代码</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	循环正常结束之后要执行的代码</span><br></pre></td></tr></table></figure>
<p>(2) for…else</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> 临时变量量 <span class="keyword">in</span> 序列列:</span><br><span class="line">	重复执行的代码</span><br><span class="line">	...</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	循环正常结束之后要执行的代码</span><br></pre></td></tr></table></figure>
<h5 id="六-退出循环的两种方式——break、continue"><a href="#六-退出循环的两种方式——break、continue" class="headerlink" title="(六) 退出循环的两种方式——break、continue"></a>(六) 退出循环的两种方式——break、continue</h5><p>break和continue是循环中满足一定条件退出循环的两种不同方式。</p>
<ul>
<li>break控制循环流程，即终止此循环，跳出循环</li>
<li>continue控制循环流程，即退出当前一次循环继而执行下一次循环，不跳出循环</li>
</ul>
<h3 id="第三章-数据类型"><a href="#第三章-数据类型" class="headerlink" title="第三章 数据类型"></a>第三章 数据类型</h3><h4 id="3-1-字符串"><a href="#3-1-字符串" class="headerlink" title="3.1 字符串"></a>3.1 字符串</h4><h5 id="一-认识字符串"><a href="#一-认识字符串" class="headerlink" title="(一) 认识字符串"></a>(一) 认识字符串</h5><p>字符串是 Python 中最常用的数据类型。我们一般使用单/双/三引号来创建字符串。创建字符串很简单，只要为变量分配一个值即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name1 = <span class="string">&#x27;Tom&#x27;</span></span><br><span class="line">name2 = <span class="string">&quot;Rose&quot;</span></span><br><span class="line">name3 = <span class="string">&#x27;&#x27;&#x27; Tom &#x27;&#x27;&#x27;</span></span><br><span class="line">name4 = <span class="string">&quot;&quot;&quot; Rose &quot;&quot;&quot;</span></span><br><span class="line">a = <span class="string">&#x27;&#x27;&#x27; i am Tom,</span></span><br><span class="line"><span class="string">		nice to meet you! &#x27;&#x27;&#x27;</span></span><br><span class="line">b = <span class="string">&quot;&quot;&quot; i am Rose,</span></span><br><span class="line"><span class="string">		nice to meet you! &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>两个注意事项：</p>
<ul>
<li>注意1：三引号形式的字符串支持换行</li>
<li>注意2：用单引号创建一个带有单引号的字符串I’m Tom需要使用反斜杠进行转义，e.g.<code>&#39;I\&#39;m Tom&#39;</code>，但是用双引号就不需要，e.g.<code>&quot;I&#39;m Tom&quot;</code></li>
</ul>
<h5 id="二-字符串的下标索引"><a href="#二-字符串的下标索引" class="headerlink" title="(二) 字符串的下标索引"></a>(二) 字符串的下标索引</h5><p>“下标” 又叫“索引” ，就是编号。比如火车座位号，座位号的作用：按照编号快速找到对应的座位。同<br>理，下标的作用即是通过下标快速找到对应的数据。</p>
<ul>
<li><strong>python中对字符串的下标索引使用<code>[]</code>完成</strong></li>
<li><strong>字符串下标从0开始</strong></li>
<li><strong>可以使用负号索引</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name = <span class="string">&quot;abcdef&quot;</span></span><br><span class="line"><span class="built_in">print</span>(name[<span class="number">1</span>])  <span class="comment"># b</span></span><br><span class="line"><span class="built_in">print</span>(name[<span class="number">0</span>])  <span class="comment"># a</span></span><br><span class="line"><span class="built_in">print</span>(name[-<span class="number">1</span>])  <span class="comment"># f</span></span><br></pre></td></tr></table></figure>
<h5 id="三-字符串切片操作"><a href="#三-字符串切片操作" class="headerlink" title="(三) 字符串切片操作"></a>(三) 字符串切片操作</h5><p>切片是指对操作的对象截取其中一部分的操作。字符串、列表、元组都支持切片操作。</p>
<p><strong>(1) 语法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">序列[开始位置下标:结束位置下标:步长]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意</p>
<ol>
<li>不包含结束位置下标对应的数据（即<strong>左闭右开区间</strong>）， 正负整数均可；</li>
<li>步长是选取间隔，正负整数均可，默认步长为1。</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name = <span class="string">&quot;abcdefg&quot;</span></span><br><span class="line"><span class="built_in">print</span>(name[<span class="number">2</span>:<span class="number">5</span>:<span class="number">1</span>]) <span class="comment"># cde</span></span><br><span class="line"><span class="built_in">print</span>(name[<span class="number">2</span>:<span class="number">5</span>]) <span class="comment"># cde</span></span><br><span class="line"><span class="built_in">print</span>(name[:-<span class="number">1</span>]) <span class="comment"># abcdef, 负1表示倒数第⼀一个数据</span></span><br><span class="line"><span class="built_in">print</span>(name[-<span class="number">4</span>:-<span class="number">1</span>]) <span class="comment"># def</span></span><br></pre></td></tr></table></figure>
<h5 id="四-字符串查找操作"><a href="#四-字符串查找操作" class="headerlink" title="(四) 字符串查找操作"></a>(四) 字符串查找操作</h5><p>所谓字符串查找方法即是查找子串在字符串中的位置或出现的次数。</p>
<p><strong>(1) find函数</strong></p>
<p>find()：检测某个子串是否包含在这个字符串中，如果在返回这个子串开始的位置下标，否则返回-1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串序列.find(子串, 开始位置下标, 结束位置下标)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mystr = <span class="string">&quot;hello world and itcast and itheima and Python&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.find(<span class="string">&#x27;and&#x27;</span>)) <span class="comment"># 12</span></span><br><span class="line"><span class="built_in">print</span>(mystr.find(<span class="string">&#x27;and&#x27;</span>, <span class="number">15</span>, <span class="number">30</span>)) <span class="comment"># 23</span></span><br><span class="line"><span class="built_in">print</span>(mystr.find(<span class="string">&#x27;ands&#x27;</span>)) <span class="comment"># -1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：开始和结束位置下标可以省略，表示在整个字符串序列中查找。</p>
</blockquote>
<p><strong>(2) index函数</strong></p>
<p>index()：检测某个子串是否包含在这个字符串中，如果在返回这个子串开始的位置下标，否则报异常。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串序列.index(子串, 开始位置下标, 结束位置下标)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mystr = <span class="string">&quot;hello world and itcast and itheima and Python&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.index(<span class="string">&#x27;and&#x27;</span>)) <span class="comment"># 12</span></span><br><span class="line"><span class="built_in">print</span>(mystr.index(<span class="string">&#x27;and&#x27;</span>, <span class="number">15</span>, <span class="number">30</span>)) <span class="comment"># 23</span></span><br><span class="line"><span class="built_in">print</span>(mystr.index(<span class="string">&#x27;ands&#x27;</span>)) <span class="comment"># 报错</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：开始和结束位置下标可以省略，表示在整个字符串序列中查找。</p>
</blockquote>
<p><strong>(3) rfind、rindex函数</strong></p>
<p>rfind()： 和find()功能相同，但查找方向为右侧开始。<br>rindex()：和index()功能相同，但查找方向为右侧开始。</p>
<p><strong>(4) count函数</strong></p>
<p>count()：返回某个子串在字符串中出现的次数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串序列.count(子串, 开始位置下标, 结束位置下标)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mystr = <span class="string">&quot;hello world and itcast and itheima and Python&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.count(<span class="string">&#x27;and&#x27;</span>)) <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(mystr.count(<span class="string">&#x27;ands&#x27;</span>)) <span class="comment"># 0</span></span><br><span class="line"><span class="built_in">print</span>(mystr.count(<span class="string">&#x27;and&#x27;</span>, <span class="number">0</span>, <span class="number">20</span>)) <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：开始和结束位置下标可以省略，表示在整个字符串序列中查找。</p>
</blockquote>
<h5 id="五-字符串修改操作"><a href="#五-字符串修改操作" class="headerlink" title="(五) 字符串修改操作"></a>(五) 字符串修改操作</h5><p>所谓修改字符串，指的就是通过函数的形式修改字符串中的数据。</p>
<p><strong>(1) replace函数</strong></p>
<p>replace()：替换字符串</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串序列.replace(旧子串, 新子串, 替换次数)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：替换次数如果查出子串出现次数，则替换次数为该子串出现次数。</p>
</blockquote>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mystr = <span class="string">&quot;hello world and itcast and itheima and Python&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.replace(<span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;he&#x27;</span>))</span><br><span class="line"><span class="comment"># 结果：hello world he itcast he itheima he Python</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.replace(<span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;he&#x27;</span>, <span class="number">10</span>))</span><br><span class="line"><span class="comment"># 结果：hello world he itcast he itheima he Python</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr)</span><br><span class="line"><span class="comment"># 结果：hello world and itcast and itheima and Python</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：数据按照是否能直接修改分为<strong>可变类型</strong>和<strong>不可变类型</strong>两种。字符串类型的数据修改的时候不能改变原有字符串，属于不能直接修改数据的类型即是不可变类型。</p>
</blockquote>
<p><strong>(2) split函数</strong></p>
<p>split()：按照指定字符分割字符串。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串序列.split(分割字符, num)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：num表示的是分割字符出现的次数，即将来返回数据个数为num+1个。</p>
</blockquote>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mystr = <span class="string">&quot;hello world and itcast and itheima and Python&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.split(<span class="string">&#x27;and&#x27;</span>))</span><br><span class="line"><span class="comment"># 结果：[&#x27;hello world &#x27;, &#x27; itcast &#x27;, &#x27; itheima &#x27;, &#x27; Python&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.split(<span class="string">&#x27;and&#x27;</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 结果：[&#x27;hello world &#x27;, &#x27; itcast &#x27;, &#x27; itheima and Python&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line"><span class="comment"># 结果：[&#x27;hello&#x27;, &#x27;world&#x27;, &#x27;and&#x27;, &#x27;itcast&#x27;, &#x27;and&#x27;, &#x27;itheima&#x27;, &#x27;and&#x27;, &#x27;Python&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.split(<span class="string">&#x27; &#x27;</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 结果：[&#x27;hello&#x27;, &#x27;world&#x27;, &#x27;and itcast and itheima and Python&#x27;]</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如果分割字符是原有字符串中的子串，分割后则丢失该子串。</p>
</blockquote>
<p><strong>(3) join函数</strong></p>
<p>join()：用一个字符或子串合并字符串，即是将多个字符串合并为一个新的字符串。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符或子串.join(多字符串组成的序列)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：序列包括 列表、元组、集合等，但是序列中的元素必须是字符串</p>
</blockquote>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list1 = [<span class="string">&#x27;chuan&#x27;</span>, <span class="string">&#x27;zhi&#x27;</span>, <span class="string">&#x27;bo&#x27;</span>, <span class="string">&#x27;ke&#x27;</span>]</span><br><span class="line">t1 = (<span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;cc&#x27;</span>, <span class="string">&#x27;ddd&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;_&#x27;</span>.join(list1))</span><br><span class="line"><span class="comment"># 结果：chuan_zhi_bo_ke</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;...&#x27;</span>.join(t1))</span><br><span class="line"><span class="comment"># 结果：aa...b...cc...ddd</span></span><br></pre></td></tr></table></figure>
<p><strong>(4) 其他常用函数</strong></p>
<p>capitalize()：将字符串第一个字符转换成大写。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串序列.capitalize()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：capitalize()函数转换后，只字符串串第一个字符大写，其他的字符全都小写。</p>
</blockquote>
<p>title()：将字符串每个单词首字母转换成大写。</p>
<p>lower()：将字符串中大写转小写。</p>
<p>upper()：将字符串中小写转大写。</p>
<p>lstrip()：删除字符串左侧空白字符。</p>
<p>rstrip()：删除字符串右侧空白字符。</p>
<p>strip()：删除字符串两侧空白字符。</p>
<p><strong>(5) ljust函数</strong></p>
<p>ljust()：返回一个原字符串左对齐，并使用指定字符(默认空格)填充至对应长度的新字符串。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串序列.ljust(长度, 填充字符)</span><br></pre></td></tr></table></figure>
<p>rjust()：返回一个原字符串右对齐,并使用指定字符(默认空格)填充至对应长度的新字符串，语法和ljust()相同。</p>
<p>center()：返回一个原字符串居中对齐,并使用指定字符(默认空格)填充至对应长度的新字符串，语法和ljust()相同。</p>
<h5 id="六-字符串判断操作"><a href="#六-字符串判断操作" class="headerlink" title="(六) 字符串判断操作"></a>(六) 字符串判断操作</h5><p>所谓判断即是判断真假，返回的结果是布尔型数据类型：True 或 False。</p>
<p><strong>(1) startswith函数</strong></p>
<p>startswith()：检查字符串是否是以指定子串开头，是则返回True，否则返回False。如果设置开始和结束位置下标，则在指定范围内检查。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串序列.startswith(子串, 开始位置下标, 结束位置下标)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mystr = <span class="string">&quot;hello world and itcast and itheima and Python &quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.startswith(<span class="string">&#x27;hello&#x27;</span>))</span><br><span class="line"><span class="comment"># 结果：True</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr.startswith(<span class="string">&#x27;hello&#x27;</span>, <span class="number">5</span>, <span class="number">20</span>))</span><br><span class="line"><span class="comment"># 结果False</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) endswith函数</strong></p>
<p>endswith()：检查字符串是否是以指定子串结尾，是则返回True，否则返回False。如果设置开始和结束位置下标，则在指定范围内检查。</p>
<p>语法与startwith()相同</p>
<p><strong>(3) isalpha函数</strong></p>
<p>isalpha()：如果字符串中的所有字符都是字母，则返回True，否则返回False。此函数用于检查参数是否包含任何字母字符 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串.isalpha()</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mystr1 = <span class="string">&#x27;hello&#x27;</span></span><br><span class="line">mystr2 = <span class="string">&#x27;hello12345&#x27;</span></span><br><span class="line">mystr3 = <span class="string">&#x27;hello world&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr1.isalpha())  <span class="comment"># 结果：True</span></span><br><span class="line"><span class="built_in">print</span>(mystr2.isalpha())  <span class="comment"># 结果：False</span></span><br><span class="line"><span class="built_in">print</span>(mystr3.isalpha())  <span class="comment"># 结果：False</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：<strong>空格不算字母</strong></p>
</blockquote>
<p><strong>(4) isdigit函数</strong></p>
<p>isdigit()：如果字符串只包含数字则返回True，否则返回False。</p>
<p>用法与isalpha()相同。</p>
<p><strong>(5) isalnum函数</strong></p>
<p>isalnum()：如果字符串至少有一个字符并且所有字符都是字母或数字则返回True，否则返回False。常用于检测字符串是否由字母和数字组成。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字符串.isalnum()</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mystr1 = <span class="string">&#x27;aaa12345&#x27;</span></span><br><span class="line">mystr2 = <span class="string">&#x27;12345-&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mystr1.isalnum())  <span class="comment"># 结果：True</span></span><br><span class="line"><span class="built_in">print</span>(mystr2.isalnum())  <span class="comment"># 结果：False</span></span><br></pre></td></tr></table></figure>
<p><strong>(6) isspace函数</strong></p>
<p>isspace()：如果字符串中只包含空白(空格)，则返回True，否则返回False。</p>
<h4 id="3-2-列表-或list"><a href="#3-2-列表-或list" class="headerlink" title="3.2 列表[]或list()"></a>3.2 列表<code>[]</code>或<code>list()</code></h4><h5 id="一-列表的应用常景"><a href="#一-列表的应用常景" class="headerlink" title="(一) 列表的应用常景"></a>(一) 列表的应用常景</h5><p>思考：有一个人的姓名(TOM)怎么书写存储程序？<br>答：变量。<br>思考：如果一个班级100位学生，每个人的姓名都要存储，应该如何书写程序？声明100个变量吗？<br>答：列表即可， 列表一次性可以存储多个数据。</p>
<h5 id="二-列表的格式"><a href="#二-列表的格式" class="headerlink" title="(二) 列表的格式"></a>(二) 列表的格式</h5><p>语法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[数据<span class="number">1</span>, 数据<span class="number">2</span>, 数据<span class="number">3</span>, 数据<span class="number">4</span>, ......]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>列表可以一次性存储多个数据，且可以为<strong>不同数据类型</strong>。</p>
<p>列表中的元素可以<strong>重复</strong></p>
<p>列表里面的元素可以是列表</p>
</blockquote>
<h5 id="三-列表查找操作"><a href="#三-列表查找操作" class="headerlink" title="(三) 列表查找操作"></a>(三) 列表查找操作</h5><p><strong>(1) 下标查找法</strong></p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list[<span class="number">0</span>]) <span class="comment"># Tom</span></span><br><span class="line"><span class="built_in">print</span>(name_list[<span class="number">1</span>]) <span class="comment"># Lily</span></span><br><span class="line"><span class="built_in">print</span>(name_list[<span class="number">2</span>]) <span class="comment"># Rose</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) 函数查找法</strong></p>
<ul>
<li>index()：返回指定数据所在位置的下标 。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">列表序列.index(数据, 开始位置下标, 结束位置下标)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(name_list.index(<span class="string">&#x27;Lily&#x27;</span>, <span class="number">0</span>, <span class="number">2</span>)) <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如果查找的数据不存在则<strong>报错</strong>。</p>
</blockquote>
<ul>
<li>count()：统计指定数据在当前列列表中出现的次数。</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(name_list.count(<span class="string">&#x27;Lily&#x27;</span>)) <span class="comment"># 2</span></span><br></pre></td></tr></table></figure>
<ul>
<li>len()：访问列表长度，即列表中数据的个数。</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(name_list)) <span class="comment"># 3</span></span><br></pre></td></tr></table></figure>
<h5 id="四-列表判断操作"><a href="#四-列表判断操作" class="headerlink" title="(四) 列表判断操作"></a>(四) 列表判断操作</h5><ul>
<li>in：判断指定数据在某个列表序列，如果在返回True，否则返回False。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Lily&#x27;</span> <span class="keyword">in</span> name_list)  <span class="comment"># 结果：True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Lilys&#x27;</span> <span class="keyword">in</span> name_list)  <span class="comment"># 结果：False</span></span><br></pre></td></tr></table></figure>
<ul>
<li>not in：判断指定数据不在某个列表序列，如果不在返回True，否则返回False。</li>
</ul>
<blockquote>
<p>in、not in是公共操作，字符串、列表、字典、元组等都有相应的用法。</p>
</blockquote>
<h5 id="五-列表增加操作"><a href="#五-列表增加操作" class="headerlink" title="(五) 列表增加操作"></a>(五) 列表增加操作</h5><p><strong>(1) append函数</strong></p>
<p>append()：列表<strong>结尾</strong>追加数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">列表序列.append(数据)</span><br></pre></td></tr></table></figure>
<p>示例1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">name_list.append(<span class="string">&#x27;xiaoming&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list)</span><br><span class="line"><span class="comment"># 结果：[&#x27;Tom&#x27;, &#x27;Lily&#x27;, &#x27;Rose&#x27;, &#x27;xiaoming&#x27;]</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>列表追加数据的时候，直接在原列表里面追加了指定数据，即修改了原列表，故列表为可变类型数据。</p>
</blockquote>
<p>如果append()追加的数据是<strong>一个序列</strong>，则追加整个序列到列表</p>
<p>示例2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">name_list.append([<span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;xiaohong&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list)</span><br><span class="line"><span class="comment"># 结果：[&#x27;Tom&#x27;, &#x27;Lily&#x27;, &#x27;Rose&#x27;, [&#x27;xiaoming&#x27;, &#x27;xiaohong&#x27;]]</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) extend函数</strong></p>
<p>extend()：列表结尾追加数据，如果数据是一个序列，则将这个序列的数据逐一添加到列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">列表序列.extend(数据)</span><br></pre></td></tr></table></figure>
<p>示例1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">name_list.extend(<span class="string">&#x27;xiaoming&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list)</span><br><span class="line"><span class="comment"># 结果：[&#x27;Tom&#x27;, &#x27;Lily&#x27;, &#x27;Rose&#x27;, &#x27;x&#x27;, &#x27;i&#x27;, &#x27;a&#x27;, &#x27;o&#x27;, &#x27;m&#x27;, &#x27;i&#x27;, &#x27;n&#x27;, &#x27;g&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>示例2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">name_list.extend([<span class="string">&#x27;xiaoming&#x27;</span>, <span class="string">&#x27;xiaohong&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list)</span><br><span class="line"><span class="comment"># 结果：[&#x27;Tom&#x27;, &#x27;Lily&#x27;, &#x27;Rose&#x27;, &#x27;xiaoming&#x27;, &#x27;xiaohong&#x27;]</span></span><br></pre></td></tr></table></figure>
<p><strong>(3) insert函数</strong></p>
<p>insert()：指定位置新增数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">列表序列.insert(位置下标, 数据)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">name_list.insert(<span class="number">1</span>, <span class="string">&#x27;xiaoming&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list)</span><br><span class="line"><span class="comment"># 结果：[&#x27;Tom&#x27;, &#x27;xiaoming&#x27;, &#x27;Lily&#x27;, &#x27;Rose&#x27;]</span></span><br></pre></td></tr></table></figure>
<h5 id="六-列表删除操作"><a href="#六-列表删除操作" class="headerlink" title="(六) 列表删除操作"></a>(六) 列表删除操作</h5><p><strong>(1) del函数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">del</span> 目标</span><br></pre></td></tr></table></figure>
<p>示例1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> name_list</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list)</span><br><span class="line"><span class="comment"># 结果：报错提示：name &#x27;name_list&#x27; is not defined</span></span><br></pre></td></tr></table></figure>
<p>示例2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line"><span class="keyword">del</span> name_list[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list)</span><br><span class="line"><span class="comment"># 结果：[&#x27;Lily&#x27;, &#x27;Rose&#x27;]</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) pop函数</strong></p>
<p>pop()：删除指定下标的数据(默认为最后一个)，并返回该数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">列表序列.pop(下标)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">del_name = name_list.pop(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(del_name)  <span class="comment"># 结果：Lily</span></span><br><span class="line"><span class="built_in">print</span>(name_list)  <span class="comment"># 结果：[&#x27;Tom&#x27;, &#x27;Rose&#x27;]</span></span><br></pre></td></tr></table></figure>
<p><strong>(3) remove函数</strong></p>
<p>remove()：移除列表中某个数据的<strong>第一个</strong>匹配项。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">列表序列.remove(数据)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">name_list.remove(<span class="string">&#x27;Rose&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list)  <span class="comment"># 结果：[&#x27;Tom&#x27;, &#x27;Lily&#x27;, &#x27;Rose&#x27;]</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：只删除第一个匹配项。</p>
</blockquote>
<p><strong>(4) clear函数</strong></p>
<p>clear()：清空列列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">name_list.clear()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list) <span class="comment"># 结果： []</span></span><br></pre></td></tr></table></figure>
<h5 id="七-列表修改操作"><a href="#七-列表修改操作" class="headerlink" title="(七) 列表修改操作"></a>(七) 列表修改操作</h5><p><strong>(1) 下标修改法</strong></p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">name_list[<span class="number">0</span>] = <span class="string">&#x27;aaa&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_list)</span><br><span class="line"><span class="comment"># 结果：[&#x27;aaa&#x27;, &#x27;Lily&#x27;, &#x27;Rose&#x27;]</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) 函数修改法</strong></p>
<ul>
<li>reverse()：将列表元素逆序排列</li>
</ul>
<p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">列表.reverse()</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_list = [<span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">8</span>]</span><br><span class="line">num_list.reverse()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(num_list)</span><br><span class="line"><span class="comment"># 结果：[8, 6, 3, 2, 5, 1]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>sort()：将列表元素递增或递减排序</li>
</ul>
<p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">列表.sort(key=<span class="literal">None</span>, reverse=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>参数：</p>
<ul>
<li>key<strong>接受的是一个只有一个形参的函数</strong>，<strong>key接收的函数返回值或者字典的键，表示此元素的权值，sort将按照权值大小进行排序</strong> ，参考示例2。</li>
<li>reverse表示排序规则，reverse = True降序， reverse = False升序（默认）</li>
</ul>
</blockquote>
<p>示例1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_list = [<span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">8</span>]</span><br><span class="line">num_list.sort()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(num_list)</span><br><span class="line"><span class="comment"># 结果：[1, 2, 3, 5, 6, 8]，默认升序</span></span><br></pre></td></tr></table></figure>
<p>示例2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取列表的第二个元素</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeSecond</span>(<span class="params">elem</span>):</span></span><br><span class="line">    <span class="keyword">return</span> elem[<span class="number">1</span>]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 列表</span></span><br><span class="line">random = [(<span class="number">2</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>), (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">3</span>)]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 指定第二个元素排序</span></span><br><span class="line">random.sort(key=takeSecond)</span><br><span class="line"><span class="comment"># 输出类别</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;排序列表：&#x27;</span>, random)</span><br><span class="line"><span class="comment"># 结果：排序列表：[(4, 1), (2, 2), (1, 3), (3, 4)]</span></span><br></pre></td></tr></table></figure>
<h5 id="八-列表复制操作"><a href="#八-列表复制操作" class="headerlink" title="(八) 列表复制操作"></a>(八) 列表复制操作</h5><p>copy()：将列表复制一次</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">name_li2 = name_list.copy()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(name_li2)</span><br><span class="line"><span class="comment"># 结果：[&#x27;Tom&#x27;, &#x27;Lily&#x27;, &#x27;Rose&#x27;]</span></span><br></pre></td></tr></table></figure>
<h5 id="九-列表遍历操作"><a href="#九-列表遍历操作" class="headerlink" title="(九) 列表遍历操作"></a>(九) 列表遍历操作</h5><p><strong>(1) while循环法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; <span class="built_in">len</span>(name_list):</span><br><span class="line">	<span class="built_in">print</span>(name_list[i])</span><br><span class="line">	i += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) for循环法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> name_list:</span><br><span class="line">	<span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>
<h5 id="十-列表嵌套操作"><a href="#十-列表嵌套操作" class="headerlink" title="(十) 列表嵌套操作"></a>(十) 列表嵌套操作</h5><p>所谓列表嵌套指的就是一个列表里面包含了其他的子列表。<br>应用场景：要存储班级一、二、三个班级学生姓名，且每个班级的学生姓名在一个列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name_list = [[<span class="string">&#x27;小明&#x27;</span>, <span class="string">&#x27;小红&#x27;</span>, <span class="string">&#x27;小绿&#x27;</span>], [<span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;Lily&#x27;</span>, <span class="string">&#x27;Rose&#x27;</span>], [<span class="string">&#x27;张三&#x27;</span>, <span class="string">&#x27;李四&#x27;</span>, <span class="string">&#x27;王五&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<p>如何查找到数据”李四”？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第一步：按下标查找到李四所在的列表</span></span><br><span class="line"><span class="built_in">print</span>(name_list[<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 第⼆步：从李四所在的列表里面，再按下标找到数据李四</span></span><br><span class="line"><span class="built_in">print</span>(name_list[<span class="number">2</span>][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h4 id="3-3-元组"><a href="#3-3-元组" class="headerlink" title="3.3 元组()"></a>3.3 元组<code>()</code></h4><h5 id="一-元组的应用场景"><a href="#一-元组的应用场景" class="headerlink" title="(一) 元组的应用场景"></a>(一) 元组的应用场景</h5><p>思考：如果想要存储多个数据，但是这些数据是不能修改的数据，怎么做？<br>答：列表？列表可以一次性存储多个数据，但是<strong>列表中的数据允许更改</strong>。一个元组可以存储多个数据，<strong>元组内的数据是不能修改的</strong>。</p>
<h5 id="二-元组的格式"><a href="#二-元组的格式" class="headerlink" title="(二) 元组的格式"></a>(二) 元组的格式</h5><p>元组特点：定义元组使用<strong>小括号</strong>，且逗号隔开各个数据，数据可以是不同的数据类型。</p>
<p>示例1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 多个数据元组</span></span><br><span class="line">t1 = (<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="comment"># 单个数据元组</span></span><br><span class="line">t2 = (<span class="number">10</span>,)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如果定义的元组只有一个数据，那么这个数据后面<strong>要添加逗号</strong>，否则数据类型为唯一的这个数据的数据类型，如示例2所示</p>
</blockquote>
<p>示例2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t2 = (<span class="number">10</span>,)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(t2)) <span class="comment"># tuple</span></span><br><span class="line">t3 = (<span class="number">20</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(t3)) <span class="comment"># int</span></span><br><span class="line">t4 = (<span class="string">&#x27;hello&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(t4)) <span class="comment"># str</span></span><br></pre></td></tr></table></figure>
<h5 id="三-元组常见操作——查找"><a href="#三-元组常见操作——查找" class="headerlink" title="(三)元组常见操作——查找"></a>(三)元组常见操作——查找</h5><font color="red">**元组数据不支持修改(增加、删除等)，只支持查找**</font>

<ul>
<li>按下标查找数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuple1 = (<span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="string">&#x27;ccc&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="number">12</span>, <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(tuple1[<span class="number">0</span>]) <span class="comment"># aa</span></span><br></pre></td></tr></table></figure>
<ul>
<li>index()：查找某个数据，如果数据存在返回对应的下标，否则报错，语法和列表、字符串的index方法相同。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuple1 = (<span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="string">&#x27;ccc&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="number">12</span>, <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(tuple1.index(<span class="string">&#x27;aa&#x27;</span>)) <span class="comment"># 0</span></span><br></pre></td></tr></table></figure>
<ul>
<li>count()：统计某个数据在当前元组出现的次数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuple1 = (<span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="string">&#x27;ccc&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="number">12</span>, <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(tuple1.count(<span class="string">&#x27;bb&#x27;</span>)) <span class="comment"># 2</span></span><br></pre></td></tr></table></figure>
<ul>
<li>len()：统计元组中数据的个数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuple1 = (<span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="string">&#x27;ccc&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="number">12</span>, <span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(tuple1)) <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<ul>
<li>特殊修改方法。。。</li>
</ul>
<blockquote>
<p>注意：元组内的直接数据如果修改则立即报错</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuple1 = (<span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="string">&#x27;cc&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>)</span><br><span class="line">tuple1[<span class="number">0</span>] = <span class="string">&#x27;aaa&#x27;</span></span><br><span class="line"><span class="comment"># 报错</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>但是如果元组里面有列表，修改列表里面的数据则是支持的，故自觉很重要。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tuple2 = (<span class="number">10</span>, <span class="number">20</span>, [<span class="string">&#x27;aa&#x27;</span>, <span class="string">&#x27;bb&#x27;</span>, <span class="string">&#x27;cc&#x27;</span>], <span class="number">50</span>, <span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(tuple2[<span class="number">2</span>]) <span class="comment"># 访问到列列表</span></span><br><span class="line"></span><br><span class="line">tuple2[<span class="number">2</span>][<span class="number">0</span>] = <span class="string">&#x27;aaaaa&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(tuple2)</span><br><span class="line"><span class="comment"># 结果：(10, 20, [&#x27;aaaaa&#x27;, &#x27;bb&#x27;, &#x27;cc&#x27;], 50, 30)</span></span><br></pre></td></tr></table></figure>
<h4 id="3-4-字典-或dict"><a href="#3-4-字典-或dict" class="headerlink" title="3.4 字典{}或dict()"></a>3.4 字典<code>&#123;&#125;</code>或<code>dict()</code></h4><h5 id="一-字典简介"><a href="#一-字典简介" class="headerlink" title="(一) 字典简介"></a>(一) 字典简介</h5><p><strong>字典类似与matlab的结构体</strong>。</p>
<p>字典里面的数据是以<strong>键值对</strong>形式出现，字典数据和数据顺序没有关系，即<strong>字典不支持下标</strong>，后期无论数据如何变化，只需要按照对应的键的名字查找数据即可。</p>
<h5 id="二-创建字典的语法"><a href="#二-创建字典的语法" class="headerlink" title="(二) 创建字典的语法"></a>(二) 创建字典的语法</h5><ul>
<li>符号为大括号</li>
<li>数据为键值对形式出现</li>
<li>各个键值对之间用逗号隔开</li>
</ul>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 有数据字典</span></span><br><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 空字典</span></span><br><span class="line">dict2 = &#123;&#125;</span><br><span class="line">dict3 = <span class="built_in">dict</span>()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：一般称冒号前面的为键(key)，简称k；冒号后面的为值(value)，简称v。</p>
</blockquote>
<h5 id="三-字典修改-增添操作"><a href="#三-字典修改-增添操作" class="headerlink" title="(三) 字典修改/增添操作"></a>(三) 字典修改/增添操作</h5><p>语法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字典序列[key] = 值</span><br></pre></td></tr></table></figure>
<p>注意：如果key存在则修改这个key对应的值；如果key不存在则新增此键值对。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">dict1[<span class="string">&#x27;name&#x27;</span>] = <span class="string">&#x27;Rose&#x27;</span></span><br><span class="line"><span class="comment"># 结果：&#123;&#x27;name&#x27;: &#x27;Rose&#x27;, &#x27;age&#x27;: 20, &#x27;gender&#x27;: &#x27;男&#x27;&#125;</span></span><br><span class="line"><span class="built_in">print</span>(dict1)</span><br><span class="line">dict1[<span class="string">&#x27;id&#x27;</span>] = <span class="number">110</span></span><br><span class="line"><span class="comment"># &#123;&#x27;name&#x27;: &#x27;Rose&#x27;, &#x27;age&#x27;: 20, &#x27;gender&#x27;: &#x27;男&#x27;, &#x27;id&#x27;: 110&#125;</span></span><br><span class="line"><span class="built_in">print</span>(dict1)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：由例子可知<strong>字典为可变类型</strong>。</p>
</blockquote>
<h5 id="四-字典删除操作"><a href="#四-字典删除操作" class="headerlink" title="(四) 字典删除操作"></a>(四) 字典删除操作</h5><p><strong>(1) del() / del</strong></p>
<p>del() / del：删除字典或删除字典中指定键值对</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"><span class="keyword">del</span> dict1[<span class="string">&#x27;gender&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dict1)</span><br><span class="line"><span class="comment"># 结果：&#123;&#x27;name&#x27;: &#x27;Tom&#x27;, &#x27;age&#x27;: 20&#125;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如删除的键字典里没有则会报错</p>
</blockquote>
<p><strong>(2) clear()</strong></p>
<p>clear()：清空字典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line">dict1.clear()</span><br><span class="line"><span class="built_in">print</span>(dict1) <span class="comment"># &#123;&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="五-字典查找操作"><a href="#五-字典查找操作" class="headerlink" title="(五) 字典查找操作"></a>(五) 字典查找操作</h5><p><strong>(1) 按key值查找</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dict1[<span class="string">&#x27;name&#x27;</span>]) <span class="comment"># Tom</span></span><br><span class="line"><span class="built_in">print</span>(dict1[<span class="string">&#x27;id&#x27;</span>]) <span class="comment"># 报错</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) get函数</strong></p>
<p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">字典序列.get(key, 默认值)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如果当前查找的key不存在则返回第二个参数(默认值)，如果省略第二个参数，则返回None。</p>
</blockquote>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dict1.get(<span class="string">&#x27;name&#x27;</span>)) <span class="comment"># Tom</span></span><br><span class="line"><span class="built_in">print</span>(dict1.get(<span class="string">&#x27;id&#x27;</span>, <span class="number">110</span>)) <span class="comment"># 110</span></span><br><span class="line"><span class="built_in">print</span>(dict1.get(<span class="string">&#x27;id&#x27;</span>)) <span class="comment"># None</span></span><br></pre></td></tr></table></figure>
<p><strong>(3) keys函数</strong></p>
<p>keys()函数：查找字典中的所有键key，返回<strong>可迭代对象(可用循环遍历的对象)</strong></p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dict1.keys()) </span><br><span class="line"><span class="comment"># dict_keys([&#x27;name&#x27;, &#x27;age&#x27;, &#x27;gender&#x27;])</span></span><br></pre></td></tr></table></figure>
<p><strong>(4) values函数</strong></p>
<p>value()函数：查找字典中所有键对应的值，返回<strong>可迭代对象(可用循环遍历的对象)</strong></p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dict1.values()) </span><br><span class="line"><span class="comment"># dict_values([&#x27;Tom&#x27;, 20, &#x27;男&#x27;])</span></span><br></pre></td></tr></table></figure>
<p><strong>(5) items函数</strong></p>
<p>items()函数：查找字典中所有的键及其对应的值，返回<strong>可迭代对象(可用循环遍历的对象)</strong></p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dict1.items()) </span><br><span class="line"><span class="comment"># dict_items([(&#x27;name&#x27;, &#x27;Tom&#x27;), (&#x27;age&#x27;, 20), (&#x27;gender&#x27;,&#x27;男&#x27;)])， 形式是元组</span></span><br></pre></td></tr></table></figure>
<h5 id="六-字典遍历操作"><a href="#六-字典遍历操作" class="headerlink" title="(六) 字典遍历操作"></a>(六) 字典遍历操作</h5><p><strong>(1) 遍历字典的key</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> dict1.keys():</span><br><span class="line">	<span class="built_in">print</span>(key)	</span><br></pre></td></tr></table></figure>
<p><strong>(2) 遍历字典的value</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> dict1.values():</span><br><span class="line">	<span class="built_in">print</span>(value)</span><br></pre></td></tr></table></figure>
<p><strong>(3) 遍历字典的元素</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> dict1.items():</span><br><span class="line">	<span class="built_in">print</span>(item)</span><br></pre></td></tr></table></figure>
<p><strong>(4) 遍历字典的键值对</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> dict1.items():</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;key&#125;</span> = <span class="subst">&#123;value&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="3-5-集合-或set"><a href="#3-5-集合-或set" class="headerlink" title="3.5 集合{}或set()"></a>3.5 集合<code>&#123;&#125;</code>或<code>set()</code></h4><h5 id="一-创建集合"><a href="#一-创建集合" class="headerlink" title="(一) 创建集合"></a>(一) 创建集合</h5><p>创建集合使用<code>&#123;&#125;</code> 或<code>set()</code> ， 但是如果<strong>要创建空集合只能使用set() ，因为{}用来创建空字典</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s2 = &#123;<span class="number">10</span>, <span class="number">30</span>, <span class="number">20</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">30</span>, <span class="number">50</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(s2)</span><br><span class="line">s3 = <span class="built_in">set</span>(<span class="string">&#x27;abcdefg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(s3)</span><br><span class="line">s4 = <span class="built_in">set</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(s4)) <span class="comment"># set</span></span><br><span class="line">s5 = &#123;&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(s5)) <span class="comment"># dict</span></span><br></pre></td></tr></table></figure>
<p>特点：</p>
<ul>
<li>集合可以<strong>去掉重复数据</strong></li>
<li>集合数据是<strong>无序的，故不支持下标</strong></li>
</ul>
<h5 id="二-集合增加数据操作"><a href="#二-集合增加数据操作" class="headerlink" title="(二) 集合增加数据操作"></a>(二) 集合增加数据操作</h5><p><strong>(1) add函数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s1 = &#123;<span class="number">10</span>, <span class="number">20</span>&#125;</span><br><span class="line">s1.add(<span class="number">100</span>)</span><br><span class="line">s1.add(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(s1) <span class="comment"># &#123;100, 10, 20&#125;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>因为<strong>集合有去重功能</strong>，所以，当向集合内追加的数据是当前集合已有数据的话，则不不进⾏行行任何操作。</p>
</blockquote>
<p><strong>(2) update函数</strong></p>
<p>update()：追加的数据是<strong>序列</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s1 = &#123;<span class="number">10</span>, <span class="number">20</span>&#125;</span><br><span class="line"><span class="comment"># s1.update(100) # 报错</span></span><br><span class="line">s1.update([<span class="number">100</span>, <span class="number">200</span>])</span><br><span class="line">s1.update(<span class="string">&#x27;abc&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(s1)</span><br><span class="line"><span class="comment"># 结果 &#123;a, 100, 200, 10, &#x27;b&#x27;, &#x27;c&#x27;, 20&#125;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>因为集合无顺序，所有结果的顺序无所谓</p>
</blockquote>
<h5 id="三-集合删除数据操作"><a href="#三-集合删除数据操作" class="headerlink" title="(三) 集合删除数据操作"></a>(三) 集合删除数据操作</h5><p><strong>(1) remove函数</strong></p>
<p>remove()：删除集合中的指定数据，如果数据不存在则报错。</p>
<p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">集合.remove(元素)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s1 = &#123;<span class="number">10</span>, <span class="number">20</span>&#125;</span><br><span class="line">s1.remove(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(s1)</span><br><span class="line"><span class="comment"># s1=&#123;20&#125;</span></span><br><span class="line">s1.remove(<span class="number">50</span>) <span class="comment"># 报错</span></span><br><span class="line"><span class="built_in">print</span>(s1)</span><br></pre></td></tr></table></figure>
<p><strong>(2) discard函数</strong></p>
<p>discard()：删除集合中的指定数据，如果数据不存在也不会报错。</p>
<p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">集合.discard(元素)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s1 = &#123;<span class="number">10</span>, <span class="number">20</span>&#125;</span><br><span class="line">s1.discard(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(s1)</span><br><span class="line">s1.discard(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(s1)</span><br></pre></td></tr></table></figure>
<p><strong>(3) pop函数</strong></p>
<p>pop()：<strong>随机删除</strong>集合中的某个数据，并<strong>返回这个数据</strong>。</p>
<p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">集合.pop()</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s1 = &#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>&#125;</span><br><span class="line">del_num = s1.pop()</span><br><span class="line"><span class="built_in">print</span>(del_num)</span><br><span class="line"><span class="built_in">print</span>(s1)</span><br></pre></td></tr></table></figure>
<h5 id="四-集合查找数据操作"><a href="#四-集合查找数据操作" class="headerlink" title="(四) 集合查找数据操作"></a>(四) 集合查找数据操作</h5><ul>
<li>in：判断数据在集合序列列</li>
<li>not in：判断数据不不在集合序列列</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s1 = &#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="number">10</span> <span class="keyword">in</span> s1)</span><br><span class="line"><span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">10</span> <span class="keyword">not</span> <span class="keyword">in</span> s1)</span><br><span class="line"><span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<h3 id="第四章-公共操作"><a href="#第四章-公共操作" class="headerlink" title="第四章 公共操作"></a>第四章 公共操作</h3><p>Python中常见的数据结构可以统称为<strong>容器(container)</strong>。序列(如列表和元组)、映射(如字典)以及集合(set)是三类主要的容器。 </p>
<h4 id="4-1-运算符"><a href="#4-1-运算符" class="headerlink" title="4.1 运算符"></a>4.1 运算符</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">运算符</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">支持的数据(容器)类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">合并</td>
<td style="text-align:center">字符串、列表、元组</td>
</tr>
<tr>
<td style="text-align:center">*</td>
<td style="text-align:center">复制</td>
<td style="text-align:center">字符串、列表、元组</td>
</tr>
<tr>
<td style="text-align:center">in</td>
<td style="text-align:center">元素是否存在</td>
<td style="text-align:center">字符串、列表、元组、字典、集合</td>
</tr>
<tr>
<td style="text-align:center">not in</td>
<td style="text-align:center">元素是否不存在</td>
<td style="text-align:center">字符串、列表、元组、字典、集合</td>
</tr>
</tbody>
</table>
</div>
<h5 id="一-合并"><a href="#一-合并" class="headerlink" title="(一) +合并"></a>(一) <code>+</code>合并</h5><p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">支持的数据类型的变量 + 支持的数据类型的变量</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 字符串</span></span><br><span class="line">str1 = <span class="string">&#x27;aa&#x27;</span></span><br><span class="line">str2 = <span class="string">&#x27;bb&#x27;</span></span><br><span class="line">str3 = str1 + str2</span><br><span class="line"><span class="built_in">print</span>(str3)  <span class="comment"># aabb</span></span><br><span class="line"><span class="comment"># 2. 列表</span></span><br><span class="line">list1 = [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">list2 = [<span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line">list3 = list1 + list2</span><br><span class="line"><span class="built_in">print</span>(list3)  <span class="comment"># [1, 2, 10, 20]</span></span><br><span class="line"><span class="comment"># 3. 元组</span></span><br><span class="line">t1 = (<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">t2 = (<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">t3 = t1 + t2</span><br><span class="line"><span class="built_in">print</span>(t3)  <span class="comment"># (10, 20, 100, 200)</span></span><br></pre></td></tr></table></figure>
<h5 id="二-复制"><a href="#二-复制" class="headerlink" title="(二) *复制"></a>(二) <code>*</code>复制</h5><p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">支持的数据类型的变量 * 要复制的次数num</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 字符串</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">10</span>) <span class="comment"># ----------</span></span><br><span class="line"><span class="comment"># 2. 列表</span></span><br><span class="line">list1 = [<span class="string">&#x27;hello&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(list1 * <span class="number">4</span>) <span class="comment"># [&#x27;hello&#x27;, &#x27;hello&#x27;, &#x27;hello&#x27;, &#x27;hello&#x27;]</span></span><br><span class="line"><span class="comment"># 3. 元组</span></span><br><span class="line">t1 = (<span class="string">&#x27;world&#x27;</span>,)</span><br><span class="line"><span class="built_in">print</span>(t1 * <span class="number">4</span>) <span class="comment"># (&#x27;world&#x27;, &#x27;world&#x27;, &#x27;world&#x27;, &#x27;world&#x27;)</span></span><br></pre></td></tr></table></figure>
<h5 id="三-in或not-in"><a href="#三-in或not-in" class="headerlink" title="(三) in或not in"></a>(三) <code>in</code>或<code>not in</code></h5><p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 字符串</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a&#x27;</span> <span class="keyword">in</span> <span class="string">&#x27;abcd&#x27;</span>) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> <span class="string">&#x27;abcd&#x27;</span>) <span class="comment"># False</span></span><br><span class="line"><span class="comment"># 2. 列表</span></span><br><span class="line">list1 = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a&#x27;</span> <span class="keyword">in</span> list1) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> list1) <span class="comment"># False</span></span><br><span class="line"><span class="comment"># 3. 元组</span></span><br><span class="line">t1 = (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;aa&#x27;</span> <span class="keyword">in</span> t1) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;aa&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> t1) <span class="comment"># True</span></span><br><span class="line"><span class="comment"># 4. 字典</span></span><br><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;gender&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;name&#x27;</span> <span class="keyword">in</span> dict1) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Tom&#x27;</span> <span class="keyword">in</span> dict1.values()) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：对于字典数据类型<strong>直接</strong>使用in/not in<strong>只能判断键</strong>，不能用来判断键对应的值</p>
</blockquote>
<h4 id="4-2-公共方法"><a href="#4-2-公共方法" class="headerlink" title="4.2 公共方法"></a>4.2 公共方法</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">函数</th>
<th style="text-align:left">描述</th>
<th>支持容器</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">len()</td>
<td style="text-align:left">计算容器中元素个数</td>
<td>字符串、列表、元组、字典、集合</td>
</tr>
<tr>
<td style="text-align:left">del 或 del()</td>
<td style="text-align:left">删除</td>
<td>字符串、列表、字典</td>
</tr>
<tr>
<td style="text-align:left">max()</td>
<td style="text-align:left">返回容器中元素最大值</td>
<td>字符串、列表、元组、集合、字典</td>
</tr>
<tr>
<td style="text-align:left">min()</td>
<td style="text-align:left">返回容器中元素最小值</td>
<td>字符串、列表、元组、集合、字典</td>
</tr>
<tr>
<td style="text-align:left">range(start,end, step)</td>
<td style="text-align:left">生成从start到end的数字，步长为 step，供for循环使用</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">enumerate()</td>
<td style="text-align:left">函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序</td>
</tr>
</tbody>
</table>
</div>
<h5 id="一-len-函数"><a href="#一-len-函数" class="headerlink" title="(一) len()函数"></a>(一) len()函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 字符串</span></span><br><span class="line">str1 = <span class="string">&#x27;abcdefg&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(str1)) <span class="comment"># 7</span></span><br><span class="line"><span class="comment"># 2. 列表</span></span><br><span class="line">list1 = [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(list1)) <span class="comment"># 4</span></span><br><span class="line"><span class="comment"># 3. 元组</span></span><br><span class="line">t1 = (<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(t1)) <span class="comment"># 5</span></span><br><span class="line"><span class="comment"># 4. 集合</span></span><br><span class="line">s1 = &#123;<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(s1)) <span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 5. 字典</span></span><br><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Rose&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">18</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(dict1)) <span class="comment"># 2</span></span><br></pre></td></tr></table></figure>
<h5 id="二-del-函数"><a href="#二-del-函数" class="headerlink" title="(二) del()函数"></a>(二) del()函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 字符串</span></span><br><span class="line">str1 = <span class="string">&#x27;abcdefg&#x27;</span></span><br><span class="line"><span class="keyword">del</span> str1</span><br><span class="line"><span class="built_in">print</span>(str1)</span><br><span class="line"><span class="comment"># 2. 列表</span></span><br><span class="line">list1 = [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>]</span><br><span class="line"><span class="keyword">del</span>(list1[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(list1) <span class="comment"># [20, 30, 40]</span></span><br></pre></td></tr></table></figure>
<h5 id="三-max-、min-函数"><a href="#三-max-、min-函数" class="headerlink" title="(三) max()、min()函数"></a>(三) max()、min()函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 字符串串</span></span><br><span class="line">str1 = <span class="string">&#x27;abcdefg&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">max</span>(str1)) <span class="comment"># g</span></span><br><span class="line"><span class="comment"># 2. 列列表</span></span><br><span class="line">list1 = [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">max</span>(list1)) <span class="comment"># 40</span></span><br><span class="line"><span class="comment"># 3.字典</span></span><br><span class="line">dict1 = &#123;<span class="string">&#x27;1&#x27;</span>: <span class="string">&#x27;Tom&#x27;</span>, <span class="string">&#x27;2&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;7&#x27;</span>: <span class="string">&#x27;男&#x27;</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">max</span>(dict1)) <span class="comment"># 7 返回键的最大值</span></span><br></pre></td></tr></table></figure>
<h5 id="四-range-函数"><a href="#四-range-函数" class="headerlink" title="(四) range()函数"></a>(四) range()函数</h5><p>注意：range()⽣生成的序列<strong>不包含end数字</strong>，<strong>其返回值是一个可迭代对象</strong>。</p>
<p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">range</span>(开始数字, 结束数字, 步幅)</span><br></pre></td></tr></table></figure>
<p>示例1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1 3 5 7 9</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>):</span><br><span class="line">	<span class="built_in">print</span>(i)</span><br><span class="line"><span class="comment"># 0 1 2 3 4 5 6 7 8 9</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">	<span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>
<p>示例2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line"><span class="comment"># 结果 range(1, 10, 2)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>其返回值是一个可迭代对象</p>
</blockquote>
<h5 id="五-enumerate-函数"><a href="#五-enumerate-函数" class="headerlink" title="(五) enumerate()函数"></a>(五) enumerate()函数</h5><p>enumerate()：返回N个元组，每个元组形式为(索引值，对应的值)</p>
<p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">enumerate</span>(可遍历对象, start=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：start参数用来设置遍历数据的下标的起始值，默认为0。</p>
</blockquote>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list1 = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">enumerate</span>(list1):</span><br><span class="line">	<span class="built_in">print</span>(i)</span><br><span class="line"><span class="comment"># (0, &#x27;a&#x27;)</span></span><br><span class="line"><span class="comment"># (1, &#x27;b&#x27;)</span></span><br><span class="line"><span class="comment"># (2, &#x27;c&#x27;)</span></span><br><span class="line"><span class="keyword">for</span> index, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(list1, start=<span class="number">1</span>):</span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&#x27;下标是<span class="subst">&#123;index&#125;</span>, 对应的字符是<span class="subst">&#123;char&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 下标是1, 对应的字符是a</span></span><br><span class="line"><span class="comment"># 下标是2, 对应的字符是b</span></span><br><span class="line"><span class="comment"># 下标是3, 对应的字符是c</span></span><br></pre></td></tr></table></figure>
<h4 id="4-3-容器类型转换"><a href="#4-3-容器类型转换" class="headerlink" title="4.3 容器类型转换"></a>4.3 容器类型转换</h4><h5 id="一-tuple-函数"><a href="#一-tuple-函数" class="headerlink" title="(一) tuple()函数"></a>(一) tuple()函数</h5><p>tuple()：将某个序列转换成元组</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list1 = [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>, <span class="number">20</span>]</span><br><span class="line">s1 = &#123;<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>, <span class="number">400</span>, <span class="number">500</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">tuple</span>(list1))</span><br><span class="line"><span class="comment"># (10, 20, 30, 40, 50, 20)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">tuple</span>(s1))</span><br><span class="line"><span class="comment"># (100, 200, 300, 400, 500)</span></span><br></pre></td></tr></table></figure>
<h5 id="二-list-函数"><a href="#二-list-函数" class="headerlink" title="(二) list()函数"></a>(二) list()函数</h5><p>list()：将某个序列转换成列表</p>
<h5 id="三-set-函数"><a href="#三-set-函数" class="headerlink" title="(三) set()函数"></a>(三) set()函数</h5><p>set()：将某个序列转换成集合</p>
<blockquote>
<p>注意：</p>
<ol>
<li>集合可以快速完成列表去重</li>
<li>集合不支持下标</li>
</ol>
</blockquote>
<h4 id="4-4-推导式"><a href="#4-4-推导式" class="headerlink" title="4.4 推导式"></a>4.4 推导式</h4><p>Python中只有列表、字典、集合有推导式，字典和元组没有。</p>
<h5 id="一-列表推导式"><a href="#一-列表推导式" class="headerlink" title="(一) 列表推导式"></a>(一) 列表推导式</h5><p>作用：用一个表达式创建一个<strong>有规律</strong>的列表或控制一个有规律列表。列表推导式又叫列表生成式。</p>
<p>示例1：传统方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个0-10的列表</span></span><br><span class="line"><span class="comment"># ========while方法实现==========</span></span><br><span class="line"><span class="comment"># 1. 准备⼀一个空列列表</span></span><br><span class="line">list1 = []</span><br><span class="line"><span class="comment"># 2. 书写循环，依次追加数字到空列列表list1中</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; <span class="number">10</span>:</span><br><span class="line">	list1.append(i)</span><br><span class="line">	i += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(list1)</span><br><span class="line"><span class="comment"># ========for方法实现==========</span></span><br><span class="line">list1 = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">	list1.append(i)</span><br><span class="line"><span class="built_in">print</span>(list1)</span><br></pre></td></tr></table></figure>
<p><strong>示例2：列表推导式</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list1 = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)]</span><br><span class="line"><span class="built_in">print</span>(list1)</span><br></pre></td></tr></table></figure>
<ul>
<li>带if的列表推导式</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建0-10的偶数列表</span></span><br><span class="line"><span class="comment"># 方法一：range()步长实现</span></span><br><span class="line">list1 = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2</span>)]</span><br><span class="line"><span class="built_in">print</span>(list1)</span><br><span class="line"><span class="comment"># 方法二：if实现</span></span><br><span class="line">list2 = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>) <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(list2)</span><br></pre></td></tr></table></figure>
<ul>
<li>多个for循环实现列表推导式</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 需求：创建列表如下：</span></span><br><span class="line"><span class="comment"># [(1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]</span></span><br><span class="line">list1 = [(i, j) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">3</span>) <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)]</span><br><span class="line"><span class="built_in">print</span>(list1)</span><br></pre></td></tr></table></figure>
<h5 id="二-字典推导式"><a href="#二-字典推导式" class="headerlink" title="(二) 字典推导式"></a>(二) 字典推导式</h5><p>思考：如果有如下两个列表：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list1 = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>]</span><br><span class="line">list2 = [<span class="string">&#x27;Tom&#x27;</span>, <span class="number">20</span>, <span class="string">&#x27;man&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>如何快速合并为一个字典？</p>
<p>答：字典推导式；字典推导式作用：快速合并列表为字典或提取字典中目标数据。</p>
<p>示例1：创建一个字典：字典key是1-5数字，value是这个数字的2次方</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;i: i**<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)&#125;</span><br><span class="line"><span class="built_in">print</span>(dict1) <span class="comment"># &#123;1: 1, 2: 4, 3: 9, 4: 16&#125;</span></span><br></pre></td></tr></table></figure>
<p>示例2：将两个列表合并为一个字典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list1 = [<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>]</span><br><span class="line">list2 = [<span class="string">&#x27;Tom&#x27;</span>, <span class="number">20</span>, <span class="string">&#x27;man&#x27;</span>]</span><br><span class="line">dict1 = &#123;list1[i]: list2[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(list1))&#125;</span><br><span class="line"><span class="built_in">print</span>(dict1)</span><br></pre></td></tr></table></figure>
<p>示例3：提取字典中目标数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">counts = &#123;<span class="string">&#x27;MBP&#x27;</span>: <span class="number">268</span>, <span class="string">&#x27;HP&#x27;</span>: <span class="number">125</span>, <span class="string">&#x27;DELL&#x27;</span>: <span class="number">201</span>, <span class="string">&#x27;Lenovo&#x27;</span>: <span class="number">199</span>, <span class="string">&#x27;acer&#x27;</span>: <span class="number">99</span>&#125;</span><br><span class="line"><span class="comment"># 需求：提取上述电脑数量量大于等于200的字典数据</span></span><br><span class="line">count1 = &#123;key: value <span class="keyword">for</span> key, value <span class="keyword">in</span> counts.items() <span class="keyword">if</span> value &gt;= <span class="number">200</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(count1) <span class="comment"># &#123;&#x27;MBP&#x27;: 268, &#x27;DELL&#x27;: 201&#125;</span></span><br></pre></td></tr></table></figure>
<h5 id="三-集合推导式"><a href="#三-集合推导式" class="headerlink" title="(三) 集合推导式"></a>(三) 集合推导式</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 需求：创建一个集合，数据为下方列表的2次方</span></span><br><span class="line">list1 = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">set1 = &#123;i ** <span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> list1&#125;</span><br><span class="line"><span class="built_in">print</span>(set1) <span class="comment"># &#123;1, 4&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="第五章-函数"><a href="#第五章-函数" class="headerlink" title="第五章 函数"></a>第五章 函数</h3><h4 id="5-1-函数的作用"><a href="#5-1-函数的作用" class="headerlink" title="5.1 函数的作用"></a>5.1 函数的作用</h4><p>函数就是将一段具有独立功能的代码块整合到一个整体并命名，在需要的位置调用这个名称即可完成对应的需求。<br>函数在开发过程中，可以更高效的实现<strong>代码重用</strong>。</p>
<h4 id="5-2-函数的使用步骤"><a href="#5-2-函数的使用步骤" class="headerlink" title="5.2 函数的使用步骤"></a>5.2 函数的使用步骤</h4><h5 id="一-定义函数"><a href="#一-定义函数" class="headerlink" title="(一) 定义函数"></a>(一) 定义函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> 函数名(<span class="params">参数</span>):</span></span><br><span class="line">	代码<span class="number">1</span></span><br><span class="line">	代码<span class="number">2</span></span><br><span class="line">    ...</span><br><span class="line">    (<span class="keyword">return</span> ...)</span><br></pre></td></tr></table></figure>
<h5 id="二-调用函数"><a href="#二-调用函数" class="headerlink" title="(二) 调用函数"></a>(二) 调用函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">函数名(参数)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：</p>
<ol>
<li>不同的需求，参数可有可无；</li>
<li>在Python中，函数必须先定义后使用。</li>
</ol>
</blockquote>
<h5 id="三-函数的说明文档"><a href="#三-函数的说明文档" class="headerlink" title="(三) 函数的说明文档"></a>(三) 函数的说明文档</h5><p>思考：定义一个函数后，程序员如何书写程序能够快速提示这个函数的作用？<br>答：注释</p>
<p>思考：如果代码多，我们是不是需要在很多代码中找到这个函数定义的位置才能看到注释？如果想更方便的查看函数的作用怎么办？<br>答：函数的说明文档</p>
<ul>
<li>定义函数的说明文档</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> 函数名(<span class="params">参数</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot; 说明文档的位置 &quot;&quot;&quot;</span></span><br><span class="line">	代码</span><br><span class="line">	......</span><br></pre></td></tr></table></figure>
<ul>
<li>查看函数的说明文档</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">help</span>(函数名)</span><br></pre></td></tr></table></figure>
<h5 id="四-函数嵌套调用"><a href="#四-函数嵌套调用" class="headerlink" title="(四) 函数嵌套调用"></a>(四) 函数嵌套调用</h5><p>所谓函数嵌套调用指的是一个函数里面又调用了另外一个函数。</p>
<h5 id="五-函数使用步骤小结"><a href="#五-函数使用步骤小结" class="headerlink" title="(五) 函数使用步骤小结"></a>(五) 函数使用步骤小结</h5><ul>
<li>函数的参数：函数调用的时候可以传入真实数据，增大函数的使用的灵活性<ul>
<li>形参：函数定义时书写的参数(非真实数据)</li>
<li>实参：函数调用时书写的参数(真实数据)</li>
</ul>
</li>
</ul>
<h4 id="5-3-函数的详细理解"><a href="#5-3-函数的详细理解" class="headerlink" title="5.3 函数的详细理解"></a>5.3 函数的详细理解</h4><h5 id="一-变量作用域"><a href="#一-变量作用域" class="headerlink" title="(一) 变量作用域"></a>(一) 变量作用域</h5><p>变量量作用域指的是变量生效的范围，主要分为两类：局部变量和全局变量。</p>
<p><strong>(1) 局部变量</strong></p>
<p>所谓局部变量是定义在函数体内部的变量，即只在函数体内部生效。</p>
<p>局部变量的作用：在函数体内部，临时保存数据，即当函数调用完成后，则销毁局部变量。</p>
<p><strong>(2) 全局变量</strong><br>所谓全局变量，指的是在函数体内、外都能生效的变量。</p>
<p>思考：如果有一个数据，在函数A和函数B中都要使用，该怎么办？<br>答：将这个数据存储在一个全局变量里面。</p>
<h5 id="二-函数的返回值"><a href="#二-函数的返回值" class="headerlink" title="(二) 函数的返回值"></a>(二) 函数的返回值</h5><p>如果一个函数要有多个返回值，该如何书写代码？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">return_num</span>():</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">result = return_num()</span><br><span class="line"><span class="built_in">print</span>(result) <span class="comment"># (1, 2)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：</p>
<ol>
<li>return后面可以连接列表、元组或字典，以返回多个值。</li>
</ol>
</blockquote>
<h5 id="三-函数的参数"><a href="#三-函数的参数" class="headerlink" title="(三) 函数的参数"></a>(三) 函数的参数</h5><p><strong>(1) 位置参数</strong></p>
<p>位置参数：调用函数时根据函数定义的参数位置来传递参数。</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_info</span>(<span class="params">name, age, gender</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&#x27;您的名字是<span class="subst">&#123;name&#125;</span>, 年龄是<span class="subst">&#123;age&#125;</span>, 性别是<span class="subst">&#123;gender&#125;</span>&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">user_info(<span class="string">&#x27;TOM&#x27;</span>, <span class="number">20</span>, <span class="string">&#x27;男&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：传递和定义参数的<strong>顺序及个数必须一致</strong>。</p>
</blockquote>
<p><strong>(2) 关键字参数</strong></p>
<p>函数调用，通过“<font color="red"><strong>键=值</strong></font>”形式加以指定。可以让函数更加清晰、容易使用，同时也清除了参数的顺序需求。</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_info</span>(<span class="params">name, age, gender</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&#x27;您的名字是<span class="subst">&#123;name&#125;</span>, 年龄是<span class="subst">&#123;age&#125;</span>, 性别是<span class="subst">&#123;gender&#125;</span>&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">user_info(<span class="string">&#x27;Rose&#x27;</span>, age=<span class="number">20</span>, gender=<span class="string">&#x27;女&#x27;</span>)</span><br><span class="line">user_info(<span class="string">&#x27;小明&#x27;</span>, gender=<span class="string">&#x27;男&#x27;</span>, age=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：函数调用时，如果有位置参数时，<strong>位置参数必须在关键字参数的前面，但关键字参数之间不存在先后顺序</strong>。</p>
</blockquote>
<p><strong>(3) 缺省参数</strong></p>
<p>缺省参数也叫默认参数，用于定义函数，为参数提供默认值，调用函数时可不传该默认参数的值</p>
<p>（注意：<strong>所有位置参数必须出现在默认参数前，包括函数定义和调用</strong>）。</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_info</span>(<span class="params">name, age, gender=<span class="string">&#x27;男&#x27;</span></span>):</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">f&#x27;您的名字是<span class="subst">&#123;name&#125;</span>, 年龄是<span class="subst">&#123;age&#125;</span>, 性别是<span class="subst">&#123;gender&#125;</span>&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">user_info(<span class="string">&#x27;TOM&#x27;</span>, <span class="number">20</span>)</span><br><span class="line"><span class="comment"># 您的名字是TOM, 年龄是20, 性别是男</span></span><br><span class="line">user_info(<span class="string">&#x27;Rose&#x27;</span>, <span class="number">18</span>, <span class="string">&#x27;女&#x27;</span>)</span><br><span class="line"><span class="comment"># 您的名字是Rose, 年龄是18, 性别是女</span></span><br></pre></td></tr></table></figure>
<p><strong>(4) 不定长参数</strong></p>
<p>不定长参数也叫可变参数。用于不确定调用的时候会传递多少个参数(不传参也可以)的场景。此时，可用包裹(packing)位置参数，或者包裹关键字参数，来进行参数传递，会显得非常方便。</p>
<ul>
<li>包裹位置传递</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_info</span>(<span class="params">*args</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(args)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">user_info(<span class="string">&#x27;TOM&#x27;</span>)</span><br><span class="line"><span class="comment"># (&#x27;TOM&#x27;,)</span></span><br><span class="line"></span><br><span class="line">user_info(<span class="string">&#x27;TOM&#x27;</span>, <span class="number">18</span>)</span><br><span class="line"><span class="comment"># (&#x27;TOM&#x27;, 18)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：传进函数的所有参数都会被args变量收集，它会根据传进参数的位置合并为一个<font color="red"><strong>元组(tuple)</strong></font>，args是元组类型，这就是包裹位置传递。</p>
</blockquote>
<ul>
<li>包裹关键字传递</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">user_info</span>(<span class="params">**kwargs</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(kwargs)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">user_info(name=<span class="string">&#x27;TOM&#x27;</span>, age=<span class="number">18</span>, <span class="built_in">id</span>=<span class="number">110</span>)</span><br><span class="line"><span class="comment"># &#123;&#x27;name&#x27;: &#x27;TOM&#x27;, &#x27;age&#x27;: 18, &#x27;id&#x27;: 110&#125;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>综上：无论是包裹位置传递还是包裹关键字传递，都是一个组包的过程。</p>
</blockquote>
<h5 id="四-拆包和交换变量值"><a href="#四-拆包和交换变量值" class="headerlink" title="(四) 拆包和交换变量值"></a>(四) 拆包和交换变量值</h5><p><strong>(1) 拆包</strong></p>
<ul>
<li>拆包：元组</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">return_num</span>():</span></span><br><span class="line">	<span class="keyword">return</span> <span class="number">100</span>, <span class="number">200</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num1, num2 = return_num()</span><br><span class="line"><span class="built_in">print</span>(num1) <span class="comment"># 100</span></span><br><span class="line"><span class="built_in">print</span>(num2) <span class="comment"># 200</span></span><br></pre></td></tr></table></figure>
<ul>
<li>拆包：字典</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;TOM&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">18</span>&#125;</span><br><span class="line">a, b = dict1</span><br><span class="line"><span class="comment"># 对字典进行拆包，取出来的是字典的key</span></span><br><span class="line"><span class="built_in">print</span>(a) <span class="comment"># name</span></span><br><span class="line"><span class="built_in">print</span>(b) <span class="comment"># age</span></span><br><span class="line"><span class="built_in">print</span>(dict1[a]) <span class="comment"># TOM</span></span><br><span class="line"><span class="built_in">print</span>(dict1[b]) <span class="comment"># 18</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) 交换变量值</strong></p>
<p>需求：有变量a = 10 和b = 20 ，交换两个变量的值。</p>
<ul>
<li>方法1：借助第三变量存储数据。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 定义中间变量量</span></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line"><span class="comment"># 2. 将a的数据存储到c</span></span><br><span class="line">c = a</span><br><span class="line"><span class="comment"># 3. 将b的数据20赋值到a，此时a = 20</span></span><br><span class="line">a = b</span><br><span class="line"><span class="comment"># 4. 将之前c的数据10赋值到b，此时b = 10</span></span><br><span class="line">b = c</span><br><span class="line"><span class="built_in">print</span>(a) <span class="comment"># 20</span></span><br><span class="line"><span class="built_in">print</span>(b) <span class="comment"># 10</span></span><br></pre></td></tr></table></figure>
<ul>
<li>方法2</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a, b = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">a, b = b, a</span><br><span class="line"><span class="built_in">print</span>(a) <span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span>(b) <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<h5 id="五-引用"><a href="#五-引用" class="headerlink" title="(五) 引用"></a>(五) 引用</h5><p><strong>(1) 了解引用</strong></p>
<p>在python中，值是靠引用来传递来的。<br>我们可以用<code>id()</code>来判断两个变量是否为同一个值的引用。 我们可以<strong>将id值理解为那块内存的地址标识</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. int类型</span></span><br><span class="line">a = <span class="number">1</span></span><br><span class="line">b = a</span><br><span class="line"><span class="built_in">print</span>(b) <span class="comment"># 1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a)) <span class="comment"># 140708464157520</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(b)) <span class="comment"># 140708464157520</span></span><br><span class="line">a = <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(b) <span class="comment"># 1,说明int类型为不可变类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a)) <span class="comment"># 140708464157552，此时得到是的数据2的内存地址</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(b)) <span class="comment"># 140708464157520</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 列表</span></span><br><span class="line">aa = [<span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line">bb = aa</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(aa)) <span class="comment"># 2325297783432</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(bb)) <span class="comment"># 2325297783432</span></span><br><span class="line">aa.append(<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(bb) <span class="comment"># [10, 20, 30], 列列表为可变类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(aa)) <span class="comment"># 2325297783432</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(bb)) <span class="comment"># 2325297783432</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) 引用当做实参</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test1</span>(<span class="params">a</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(a)</span><br><span class="line">	<span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br><span class="line">	a += a</span><br><span class="line">	<span class="built_in">print</span>(a)</span><br><span class="line">	<span class="built_in">print</span>(<span class="built_in">id</span>(a))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># int：计算前后id值不不同</span></span><br><span class="line">b = <span class="number">100</span></span><br><span class="line">test1(b)</span><br><span class="line"><span class="comment"># 列表：计算前后id值相同</span></span><br><span class="line">c = [<span class="number">11</span>, <span class="number">22</span>]</span><br><span class="line">test1(c)</span><br></pre></td></tr></table></figure>
<h5 id="六-可变与不可变类型"><a href="#六-可变与不可变类型" class="headerlink" title="(六) 可变与不可变类型"></a>(六) 可变与不可变类型</h5><p>所谓可变类型与不可变类型是指：数据能够直接进行修改，如果能直接修改那么就是可变，否则是不可变。</p>
<ul>
<li><p><strong>可变类型</strong></p>
<p>  列表<br>  字典<br>  集合</p>
</li>
<li><p><strong>不可变类型</strong></p>
<p>  整型<br>  浮点型<br>  字符串<br>  元组</p>
</li>
</ul>
<p>不可变数据类型： 当该数据类型的对应变量的值发生了改变，那么它对应的内存地址也会发生改变，对于这种数据类型，就称不可变数据类型。</p>
<p>可变数据类型：当该数据类型的对应变量的值发生了改变，那么它对应的内存地址不发生改变，对于这种数据类型，就称可变数据类型。</p>
<p>总结：不可变数据类型更改后地址发生改变，可变数据类型更改地址不发生改变</p>
<h4 id="5-4-递归函数"><a href="#5-4-递归函数" class="headerlink" title="5.4 递归函数"></a>5.4 递归函数</h4><p>递归是一种编程思想</p>
<ul>
<li>函数内部自己调用自己</li>
<li>递归函数必须有出口</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 需求：3以内数字累加和 3+2+1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum_numbers</span>(<span class="params">num</span>):</span></span><br><span class="line">	<span class="comment"># 1.如果是1，直接返回1 -- 出口</span></span><br><span class="line">	<span class="keyword">if</span> num == <span class="number">1</span>:</span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">	<span class="comment"># 2.如果不不是1，重复执行累加:</span></span><br><span class="line">	result = num + sum_numbers(num-<span class="number">1</span>)</span><br><span class="line">	<span class="comment"># 3.返回累加结果</span></span><br><span class="line">	<span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sum_result = sum_numbers(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(sum_result)</span><br><span class="line"><span class="comment"># 输出结果为6</span></span><br></pre></td></tr></table></figure>
<h4 id="5-5-lambda表达式-匿名函数"><a href="#5-5-lambda表达式-匿名函数" class="headerlink" title="5.5 lambda表达式/匿名函数"></a>5.5 lambda表达式/匿名函数</h4><p>如果一个函数有一个返回值，并且只有一句代码，可以使用 lambda简化。</p>
<h5 id="一-lambda的语法"><a href="#一-lambda的语法" class="headerlink" title="(一) lambda的语法"></a>(一) lambda的语法</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lambda</span> 参数列表 ： 表达式</span><br></pre></td></tr></table></figure>
<ul>
<li>lambda表达式的参数可有可无，函数的参数在lambda表达式中完全适用</li>
<li>lambda函数能接收任何数量的参数但只能返回一个表达式的值</li>
<li>该表达式必须有返回值，表达式处可以视为：<code>return 表达式</code></li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算a + b</span></span><br><span class="line"><span class="comment"># 方法1：普通函数实现=======================</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">a, b</span>):</span></span><br><span class="line">	<span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result = add(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="comment"># 方法2：lambda表达式=======================</span></span><br><span class="line"><span class="built_in">print</span>((<span class="keyword">lambda</span> a, b: a + b)(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h5 id="二-lambda的参数形式"><a href="#二-lambda的参数形式" class="headerlink" title="(二) lambda的参数形式"></a>(二) lambda的参数形式</h5><p><strong>(1) 无参数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>((<span class="keyword">lambda</span> : <span class="number">100</span>)())</span><br></pre></td></tr></table></figure>
<p><strong>(2) 一个参数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>((<span class="keyword">lambda</span> a: a)(<span class="string">&#x27;hello world&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p><strong>(3) 默认参数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>((<span class="keyword">lambda</span> a, b, c=<span class="number">100</span>: a + b + c)(<span class="number">10</span>, <span class="number">20</span>))</span><br></pre></td></tr></table></figure>
<p><strong>(4) 可变位置参数：*args</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>((<span class="keyword">lambda</span> *args: args)(<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p> 注意：这里的可变参数传入到lambda之后，返回值为元组。</p>
</blockquote>
<p> <strong>(5) 可变关键字参数：*kwargs</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>((<span class="keyword">lambda</span> **kwargs: kwargs)(name=<span class="string">&#x27;python&#x27;</span>, age=<span class="number">20</span>))</span><br></pre></td></tr></table></figure>
<h5 id="三-lambda表达式的应用"><a href="#三-lambda表达式的应用" class="headerlink" title="(三) lambda表达式的应用"></a>(三) lambda表达式的应用</h5><p><strong>(1) 带判断的lambda</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>((<span class="keyword">lambda</span> a, b: a <span class="keyword">if</span> a &gt; b <span class="keyword">else</span> b)(<span class="number">1000</span>, <span class="number">500</span>))</span><br></pre></td></tr></table></figure>
<p><strong>(2) 列表数据按字典key的值排序</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">students = [</span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;TOM&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">20</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;ROSE&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">19</span>&#125;,</span><br><span class="line">&#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Jack&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">22</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按name值升序排列</span></span><br><span class="line">students.sort(key=<span class="keyword">lambda</span> x: x[<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(students)</span><br><span class="line"><span class="comment"># 按name值降序排列</span></span><br><span class="line">students.sort(key=<span class="keyword">lambda</span> x: x[<span class="string">&#x27;name&#x27;</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(students)</span><br><span class="line"><span class="comment"># 按age值升序排列</span></span><br><span class="line">students.sort(key=<span class="keyword">lambda</span> x: x[<span class="string">&#x27;age&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(students)</span><br></pre></td></tr></table></figure>
<h4 id="5-6-高阶函数"><a href="#5-6-高阶函数" class="headerlink" title="5.6 高阶函数"></a>5.6 高阶函数</h4><p><strong>把函数作为另一个函数的参数传入</strong>，这样的函数称为高阶函数，高阶函数是函数式编程的体现。函数式编程就是指这种高度抽象的编程范式。</p>
<h5 id="一-体验高阶函数"><a href="#一-体验高阶函数" class="headerlink" title="(一) 体验高阶函数"></a>(一) 体验高阶函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 完成计算任意两个数字的绝对值之和</span></span><br><span class="line"><span class="comment"># 方法1：普通函数法====================</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_num</span>(<span class="params">a, b</span>):</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">abs</span>(a) + <span class="built_in">abs</span>(b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result = add_num(-<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(result) <span class="comment"># 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：高阶函数法====================</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum_num</span>(<span class="params">a, b, f</span>):</span></span><br><span class="line">	<span class="keyword">return</span> f(a) + f(b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result = sum_num(-<span class="number">1</span>, <span class="number">2</span>, <span class="built_in">abs</span>)</span><br><span class="line"><span class="built_in">print</span>(result) <span class="comment"># 3</span></span><br></pre></td></tr></table></figure>
<p>两种方法对比之后，发现方法2的代码会更加简洁。</p>
<p>函数式编程大量使用函数，减少了代码的重复，提高了灵活性，因此程序比较短，开发速度较快。</p>
<h5 id="二-内置高阶函数"><a href="#二-内置高阶函数" class="headerlink" title="(二) 内置高阶函数"></a>(二) 内置高阶函数</h5><p><strong>(1) map()函数</strong></p>
<p><code>map(func, lst)</code>，将传入的函数变量<code>func</code>作用到<code>lst</code>列表变量的每个元素中，并将结果组成新的列表(Python2)/迭代器(可迭代对象，Python3)返回。</p>
<p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">map</span>(function, iterable, ...)</span><br></pre></td></tr></table></figure>
<p>示例1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ：计算list1序列中各个数字的2次方</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="keyword">return</span> x ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">list1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">result = <span class="built_in">map</span>(func, list1)</span><br><span class="line"><span class="built_in">print</span>(result) <span class="comment"># &lt;map object at 0x0000013769653198&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(result)) <span class="comment"># [1, 4, 9, 16, 25]</span></span><br></pre></td></tr></table></figure>
<p>示例2</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x ** <span class="number">2</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]))   <span class="comment"># 使用 lambda 匿名函数</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>]</span><br></pre></td></tr></table></figure>
<p><strong>(2) reduce()函数</strong></p>
<p><code>reduce(func(x,y)，lst)</code>，其中<code>func</code>必须有两个参数。每次<code>func</code>计算的结果继续和序列的下一个元素做累积计算。</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算list1序列中各个数字的累加和</span></span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">a, b</span>):</span></span><br><span class="line">	<span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line">list1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">result = functools.reduce(func, list1)</span><br><span class="line"><span class="built_in">print</span>(result) <span class="comment"># 15</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>reduce()在python3中不属于内置函数，需要从functools模块调用</p>
</blockquote>
<p><strong>(3) filter()函数</strong></p>
<p><code>filter(func, lst)</code>函数用于过滤序列，过滤掉不符合条件的元素，返回一个filter对象。如果要转换为列表，可以使用list()来转换。和map()类似，filter()也接收一个函数和一个序列。和map()不同的时，filter()把传入的函数依次作用于每个元素，然后根据返回值是<strong>True还是False</strong>决定保留还是丢弃该元素。 </p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">x</span>):</span></span><br><span class="line">	<span class="keyword">return</span> x % <span class="number">2</span> == <span class="number">0</span> <span class="comment">#返回True或False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">list1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line">result = <span class="built_in">filter</span>(func, list1)</span><br><span class="line"><span class="built_in">print</span>(result) <span class="comment"># &lt;filter object at 0x0000017AF9DC3198&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(result)) <span class="comment"># [2, 4, 6, 8, 10]</span></span><br></pre></td></tr></table></figure>
<h3 id="第六章-文件操作"><a href="#第六章-文件操作" class="headerlink" title="第六章 文件操作"></a>第六章 文件操作</h3><h4 id="6-1-文件操作的作用"><a href="#6-1-文件操作的作用" class="headerlink" title="6.1 文件操作的作用"></a>6.1 文件操作的作用</h4><p>思考：文件操作包含什么？<br>答：打开、关闭、读、写、复制….</p>
<p>思考：文件操作的的作用是什么？<br>答：读取内容、写入内容、备份内容……</p>
<p>文件操作的作用就是把一些内容(数据)存储存放起来，可以让程序下一次执行的时候直接使用，而不必重新制作一份，省时省力。</p>
<h4 id="6-2-文件的基本操作"><a href="#6-2-文件的基本操作" class="headerlink" title="6.2 文件的基本操作"></a>6.2 文件的基本操作</h4><h5 id="一-打开-创建文件——open"><a href="#一-打开-创建文件——open" class="headerlink" title="(一) 打开/创建文件——open()"></a>(一) 打开/创建文件——open()</h5><p>在python，使用open函数，可以打开一个已经存在的文件，或者创建一个新文件，语法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">open</span>(name, mode)</span><br></pre></td></tr></table></figure>
<p>name：是要打开的目标文件名的字符串(可以包含文件所在的具体路径)。<br>mode：设置打开文件的模式(访问模式)：只读、写入、追加等。</p>
<p><strong>(1) 打开文件模式</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>r</td>
<td>以只读方式打开文件。文件的指针将会放在文件的开头，这是<strong>默认模式</strong>。</td>
</tr>
<tr>
<td>rb</td>
<td>以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。</td>
</tr>
<tr>
<td>r+</td>
<td>打开一个文件用于读写。文件指针将会放在文件的开头。</td>
</tr>
<tr>
<td>rb+</td>
<td>以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。</td>
</tr>
<tr>
<td>w</td>
<td>打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容<strong>会被删除</strong>。如果该文件不存在，创建新文件。</td>
</tr>
<tr>
<td>wb</td>
<td>以二进制格式打开一个文件只用于写入。如果该文件已存在则打开⽂文件，并从开头开始编辑，即原有内容<strong>会被删除</strong>。如果该文件不存在，创建新文件。</td>
</tr>
<tr>
<td>w+</td>
<td>打开一个文件用于读写。如果该文件已存在则打开⽂文件，并从开头开始编辑，即原有内容<strong>会被删除</strong>。如果该文件不存在，创建新文件。</td>
</tr>
<tr>
<td>wb+</td>
<td>以二进制格式打开一个文件用于读写。如果该文件已存在则打开⽂文件，并从开头开始编辑，即原有内容<strong>会被删除</strong>。如果该文件不存在，创建新文件。</td>
</tr>
<tr>
<td>a</td>
<td>打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。</td>
</tr>
<tr>
<td>ab</td>
<td>以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。</td>
</tr>
<tr>
<td>a+</td>
<td>打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。</td>
</tr>
<tr>
<td>ab+</td>
<td>以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>注意：对于a的模式打开文件，由于文件指针在文件的结尾处，因此无法使用read()读出内容，但是可以使用seek()函数改变文件指针</p>
<p>注意：使用w模式打开文件，一旦文件打开就自动删除文件的内容</p>
</blockquote>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;test,txt&#x27;</span>, <span class="string">&#x27;w)</span></span><br></pre></td></tr></table></figure>
<h5 id="二-文件对象方法"><a href="#二-文件对象方法" class="headerlink" title="(二) 文件对象方法"></a>(二) 文件对象方法</h5><p><strong>(1) 写方法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">文件对象.write(<span class="string">&#x27;内容&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 打开文件</span></span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;test.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="comment"># 2.文件写入</span></span><br><span class="line">f.write(<span class="string">&#x27;hello world&#x27;</span>)</span><br><span class="line"><span class="comment"># 3. 关闭文件</span></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：</p>
<ol>
<li>w 和a 模式：如果文件不存在则创建该文件；如果文件存在， w 模式先清空再写入， a 模式直接末尾追加。</li>
<li>r 模式：如果文件不存在则报错。</li>
</ol>
</blockquote>
<p><strong>(2) 读方法</strong></p>
<p><strong>① read()函数</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">文件对象.read(num)</span><br></pre></td></tr></table></figure>
<p>num表示要从文件中读取的数据的长度（单位是<strong>字节</strong>），如果没有传入num，那么就表示读取文件中所有的数据。</p>
<p><strong>② readline()函数</strong></p>
<p>readline()一次读取一行内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;test.txt&#x27;</span>)</span><br><span class="line">content = f.readline()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;第一行：<span class="subst">&#123;content&#125;</span>&#x27;</span>)</span><br><span class="line">content = f.readline()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;第二行：<span class="subst">&#123;content&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 关闭文件</span></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
<p><strong>③ readlines()函数</strong></p>
<p>readlines可以按照行的方式把整个文件中的内容进行一次性读取，并且返回的是一个<strong>列表</strong>，其中每一行的数据为一个元素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;test.txt&#x27;</span>)</span><br><span class="line">content = f.readlines()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(content) <span class="comment"># [&#x27;hello world\n&#x27;, &#x27;abcdefg\n&#x27;, &#x27;aaa\n&#x27;, &#x27;bbb\n&#x27;, &#x27;ccc&#x27;]</span></span><br><span class="line"><span class="comment"># 关闭文件</span></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
<p><strong>(3) 文件指针方法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">文件对象.seek(偏移量, 起始位置)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>偏移量：n，单位字节</p>
<p>起始位置：<br>0：文件开头，1：当前位置，2：文件结尾</p>
</blockquote>
<p><strong>(4) 关闭文件</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">文件对象.close()</span><br></pre></td></tr></table></figure>
<h5 id="三-文件备份"><a href="#三-文件备份" class="headerlink" title="(三) 文件备份"></a>(三) 文件备份</h5><h4 id="6-3-文件和文件夹的操作"><a href="#6-3-文件和文件夹的操作" class="headerlink" title="6.3 文件和文件夹的操作"></a>6.3 文件和文件夹的操作</h4><p>在Python中文件和文件夹的操作要借助<strong>os模块</strong>里面的相关功能，具体步骤如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 导入os模块</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 2. 使用os模块相关功能</span></span><br><span class="line">os.函数名()</span><br></pre></td></tr></table></figure>
<h5 id="一-文件重命名"><a href="#一-文件重命名" class="headerlink" title="(一) 文件重命名"></a>(一) 文件重命名</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.rename(目标文件名, 新文件名)</span><br></pre></td></tr></table></figure>
<h5 id="二-删除文件"><a href="#二-删除文件" class="headerlink" title="(二) 删除文件"></a>(二) 删除文件</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.remove(目标文件名)</span><br></pre></td></tr></table></figure>
<h5 id="三-创建文件夹"><a href="#三-创建文件夹" class="headerlink" title="(三) 创建文件夹"></a>(三) 创建文件夹</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.mkdir(文件夹名字)</span><br></pre></td></tr></table></figure>
<h5 id="四-删除文件夹"><a href="#四-删除文件夹" class="headerlink" title="(四) 删除文件夹"></a>(四) 删除文件夹</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.rmdir(文件夹名字)</span><br></pre></td></tr></table></figure>
<h5 id="五-获取当前目录"><a href="#五-获取当前目录" class="headerlink" title="(五) 获取当前目录"></a>(五) 获取当前目录</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.getcwd()</span><br></pre></td></tr></table></figure>
<h5 id="六-改变默认目录"><a href="#六-改变默认目录" class="headerlink" title="(六) 改变默认目录"></a>(六) 改变默认目录</h5><p>os.chdir()方法用于改变当前工作目录到指定的路径。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.chdir(目录)</span><br></pre></td></tr></table></figure>
<h5 id="七-获取目录列表"><a href="#七-获取目录列表" class="headerlink" title="(七) 获取目录列表"></a>(七) 获取目录列表</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.listdir(目录)</span><br></pre></td></tr></table></figure>
<h4 id="6-4-文件路径操作"><a href="#6-4-文件路径操作" class="headerlink" title="6.4 文件路径操作"></a>6.4 文件路径操作</h4><h5 id="一-Windows下的路径写法"><a href="#一-Windows下的路径写法" class="headerlink" title="(一) Windows下的路径写法"></a>(一) Windows下的路径写法</h5><p><strong>由于windows的路径分隔符使用的是反斜杠\，它刚刚好是转义字符，故可能发生冲突</strong> </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 错误示范：反斜杠似乎被解释为转义字符而不是分隔字符</span></span><br><span class="line">path = <span class="string">&quot;F:\Python\test.txt&quot;</span>   </span><br><span class="line">path = <span class="string">&quot;F:\\Python\test.txt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;以下三种方式都是可行的&quot;&quot;&quot;</span></span><br><span class="line">path = <span class="string">&quot;F:\Python\\test.txt&quot;</span></span><br><span class="line">path = <span class="string">&quot;F:\\Python\\test.txt&quot;</span></span><br><span class="line">path = <span class="string">&quot;F:\Python\\test.txt&quot;</span>   </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;绝对路径的最后一个反斜杠使用了双反斜杠时就正常&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;原因是最后一个反斜杠可以与它紧挨着的字符t构成转义效果&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong>使用绝对路径的时候，要注意在绝对路径中使用的反斜杠与它后面紧跟着的字符会不会构成转义效果，如果会产生转义效果，如\t \n \r等等，则必须使用双反斜杠来取消单反斜杠的转义效果。如果产生了转义字符带来的错误，会出现<strong>open()</strong>提示 <strong>“Invalid arguments”</strong> </p>
<font color="blue">**Python中windows路径的3种写法**</font>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;C:\\Users\\10799\\Desktop\\learn\\C\\homework\\5&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">r&quot;C:\Users\10799\Desktop\learn\C\homework\5&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;C:/Users/10799/Desktop/learn/C/homework/5&quot;</span></span><br></pre></td></tr></table></figure>
<p>但是不可以这样写：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;C:\Users\10799\Desktop\learn\C\homework\5&quot;</span></span><br></pre></td></tr></table></figure>
<h5 id="二-绝对路径与相对路径"><a href="#二-绝对路径与相对路径" class="headerlink" title="(二) 绝对路径与相对路径"></a>(二) 绝对路径与相对路径</h5><p><strong>相对路径相关写法：</strong></p>
<p><code>../</code> 表示当前文件所在的目录的上一级目录<br><code>./</code>表示当前文件所在的目录(可以省略)<br><code>/</code>表示当前站点的根目录(域名映射的硬盘目录) </p>
<h3 id="第七章-面向对象基础"><a href="#第七章-面向对象基础" class="headerlink" title="第七章 面向对象基础"></a>第七章 面向对象基础</h3><h4 id="7-1-理解面向对象"><a href="#7-1-理解面向对象" class="headerlink" title="7.1 理解面向对象"></a>7.1 理解面向对象</h4><p>面向对象就是将编程当成是一个事物，对外界来说，事物是直接使用的，不用去管他内部的情况。而编程就是设置事物能够做什么事。</p>
<p>面向对象是一种抽象化的编程思想，很多编程语言中都有的一种思想。<br>例如：洗衣服<br>思考：几种途径可以完成洗衣服？<br>答： 手洗 和 机洗。<br>手洗：找盆 - 放水 - 加洗衣粉 - 浸泡 - 搓洗 - 拧干水 - 倒水 - 漂洗N次 - 拧干 - 晾晒。<br>机洗：打开洗衣机 - 放衣服 - 加洗衣粉 - 按下开始按钮 - 晾晒。<br>思考：对比两种洗衣服途径，同学们发现了了什么？<br>答：机洗更简单<br>思考：机洗，只需要找到一台洗衣机，加入简单操作就可以完成洗衣服的工作，而不需要关心洗衣机内部发⽣生了了什么事情。</p>
<ul>
<li>面向对象的三大特征<ul>
<li>封装<ul>
<li>将属性和方法书写到类的里面的操作即为封装</li>
<li>封装可以为属性和⽅方法添加私有权限</li>
</ul>
</li>
<li>继承<ul>
<li>子类默认继承父类的所有属性和方法</li>
<li>子类可以重写父类属性和方法</li>
</ul>
</li>
<li>多态<ul>
<li>传入不同的对象，产生不同的结果</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="7-2-类和对象"><a href="#7-2-类和对象" class="headerlink" title="7.2 类和对象"></a>7.2 类和对象</h4><font color="red">**类和对象的关系：用类去实例化/创建一个对象。**</font>

<h5 id="一-理解类和对象"><a href="#一-理解类和对象" class="headerlink" title="(一) 理解类和对象"></a>(一) 理解类和对象</h5><p><strong>(1) 类</strong></p>
<p>类是对一系列具有相同特征和行为的事物的统称，是一个抽象的概念，不是真实存在的事物。</p>
<ul>
<li>特征即是属性——变量</li>
<li>行为即是方法——函数</li>
</ul>
<p>类比如是制造洗衣机时要用到的图纸，也就是说类是用来创建对象。</p>
<p><strong>(2) 对象</strong></p>
<p>对象是类创建出来的真实存在的事物，例如：洗衣机。也就是说开发中先有类，再有对象。</p>
<h5 id="二-面向对象实现方法"><a href="#二-面向对象实现方法" class="headerlink" title="(二) 面向对象实现方法"></a>(二) 面向对象实现方法</h5><p><strong>(1) 定义类</strong></p>
<p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> 类名():</span></span><br><span class="line">	代码</span><br><span class="line">	......</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：类名要满足标识符命名规则，同时遵循<strong>大驼峰命名</strong>习惯。e.g. 每个单词的首字母都采用大写字母,例如:FirstName </p>
</blockquote>
<p><strong>(2) 创建对象</strong></p>
<p>对象又名实例。语法如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">对象名 = 类名()</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建对象</span></span><br><span class="line">haier1 = Washer()</span><br><span class="line"><span class="comment"># &lt;__main__.Washer object at 0x0000018B7B224240&gt;</span></span><br><span class="line"><span class="built_in">print</span>(haier1)</span><br><span class="line"><span class="comment"># haier对象调用实例方法</span></span><br><span class="line">haier1.wash()</span><br></pre></td></tr></table></figure>
<p><strong>(3) self</strong></p>
<p>self指的是调用该函数的对象。</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 定义类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Washer</span>():</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">wash</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我会洗衣服&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(self)  <span class="comment"># &lt;__main__.Washer object at 0x0000024BA2B34240&gt;</span></span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 2. 创建对象</span></span><br><span class="line">haier1 = Washer()</span><br><span class="line"><span class="built_in">print</span>(haier1)  <span class="comment"># &lt;__main__.Washer object at 0x0000018B7B224240&gt;</span></span><br><span class="line"><span class="comment"># haier1对象调用实例方法</span></span><br><span class="line">haier1.wash()</span><br><span class="line">haier2 = Washer()</span><br><span class="line"><span class="built_in">print</span>(haier2)  <span class="comment"># &lt;__main__.Washer object at 0x0000022005857EF0&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：打印对象和self得到的结果是一致的，都是当前对象的内存中存储地址。</p>
</blockquote>
<h4 id="7-3-添加和获取对象属性"><a href="#7-3-添加和获取对象属性" class="headerlink" title="7.3 添加和获取对象属性"></a>7.3 添加和获取对象属性</h4><p>属性即是特征，比如：洗衣机的宽度、高度、重量…<br>对象属性既可以在类外面添加和获取，也能在类里面添加和获取。</p>
<h5 id="一-类外面添加对象属性"><a href="#一-类外面添加对象属性" class="headerlink" title="(一) 类外面添加对象属性"></a>(一) 类外面添加对象属性</h5><p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">对象名.属性名 = 值</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">haier1.width = <span class="number">500</span></span><br><span class="line">haier1.height = <span class="number">800</span></span><br></pre></td></tr></table></figure>
<h5 id="二-类外面获取对象属性"><a href="#二-类外面获取对象属性" class="headerlink" title="(二) 类外面获取对象属性"></a>(二) 类外面获取对象属性</h5><p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">对象名.属性名</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;haier1洗衣机的宽度是<span class="subst">&#123;haier1.width&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;haier1洗衣机的高度是<span class="subst">&#123;haier1.height&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="三-类里面获取对象属性"><a href="#三-类里面获取对象属性" class="headerlink" title="(三) 类里面获取对象属性"></a>(三) 类里面获取对象属性</h5><p>语法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.属性名</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Washer</span>():</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">print_info</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="comment"># 类里面获取实例属性</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;haier1洗衣机的宽度是<span class="subst">&#123;self.width&#125;</span>&#x27;</span>)</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;haier1洗衣机的高度是<span class="subst">&#123;self.height&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment"># 创建对象</span></span><br><span class="line">haier1 = Washer()</span><br><span class="line"><span class="comment"># 添加实例例属性</span></span><br><span class="line">haier1.width = <span class="number">500</span></span><br><span class="line">haier1.height = <span class="number">800</span></span><br><span class="line">haier1.print_info()</span><br></pre></td></tr></table></figure>
<h4 id="7-4-魔法方法"><a href="#7-4-魔法方法" class="headerlink" title="7.4 魔法方法"></a>7.4 魔法方法</h4><p>在Python中，<code>__xx__()</code>的函数叫做魔法方法，指的是具有<strong>特殊功能</strong>的函数。</p>
<p>魔法方法是python内置方法，<strong>不需要主动调用</strong>，存在的目的是为了给python的解释器进行调用，几乎每个魔法方法都有一个对应的内置函数，或者运算符，当我们对这个对象使用这些函数或者运算符时就会调用类中的对应魔法方法，可以理解为重写这些python的内置函数(类似于方法重载)。 </p>
<h5 id="一-init-魔法方法"><a href="#一-init-魔法方法" class="headerlink" title="(一) __init__()魔法方法"></a>(一) <code>__init__()</code>魔法方法</h5><p>思考：洗衣机的宽度高度是与生俱来的属性，可不可以在生产过程中就赋予这些属性呢？<br>答：理应如此。</p>
<p><strong>(1) <code>__init__()</code> 方法</strong></p>
<p><code>__init__()</code> 方法的作用：初始化对象。</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Washer</span>():</span></span><br><span class="line">	<span class="comment"># 定义初始化功能的函数</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="comment"># 添加实例属性</span></span><br><span class="line">		self.width = <span class="number">500</span></span><br><span class="line">		self.height = <span class="number">800</span></span><br><span class="line">         <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">print_info</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="comment"># 类里面调用实例属性</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;洗衣机的宽度是<span class="subst">&#123;self.width&#125;</span>, 高度是<span class="subst">&#123;self.height&#125;</span>&#x27;</span>)</span><br><span class="line">         <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line">haier1 = Washer()</span><br><span class="line">haier1.print_info()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：<br><code>__init__()</code> 方法，在创建一个对象时默认被调用，不需要手动调用；</p>
<p><code>__init__(self)</code>中的self参数，不需要开发者传递，python解释器会自动把当前的对象引用传递过去；</p>
<p><code>__init__()</code> 方法不需要调用是指在在创建对象时会自动调用，但是如果在没有创建对象时，需要调用</p>
</blockquote>
<p><strong>(2) 带参数的<code>__init__()</code></strong></p>
<p>思考：一个类可以创建多个对象，如何对不同的对象设置不同的初始化属性呢？<br>答：传参数。</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Washer</span>():</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, width, height</span>):</span></span><br><span class="line">		self.width = width</span><br><span class="line">		self.height = height</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">print_info</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;洗衣机的宽度是<span class="subst">&#123;self.width&#125;</span>&#x27;</span>)</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;洗衣机的高度是<span class="subst">&#123;self.height&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">haier1 = Washer(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">haier1.print_info()</span><br><span class="line">haier2 = Washer(<span class="number">30</span>, <span class="number">40</span>)</span><br><span class="line">haier2.print_info()</span><br></pre></td></tr></table></figure>
<h5 id="二-str-魔法方法"><a href="#二-str-魔法方法" class="headerlink" title="(二) __str__()魔法方法"></a>(二) <code>__str__()</code>魔法方法</h5><p>当使用print输出对象的时候，默认打印对象的内存地址。如果类定义了了<code>__str__()</code>方法，那么就会打印从<br>在这个方法中 return 的数据。</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Washer</span>():</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, width, height</span>):</span></span><br><span class="line">		self.width = width</span><br><span class="line">		self.height = height</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__str__</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">return</span> <span class="string">&#x27;这是海尔洗衣机的说明书&#x27;</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">haier1 = Washer(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line"><span class="built_in">print</span>(haier1) <span class="comment"># 这是海尔洗衣机的说明书</span></span><br></pre></td></tr></table></figure>
<h5 id="三-del-魔法方法"><a href="#三-del-魔法方法" class="headerlink" title="(三) __del__()魔法方法"></a>(三) <code>__del__()</code>魔法方法</h5><p>当删除对象时，python解释器也会默认调用<code>__del__()</code> 方法。(即使没有删除语句，在程序结束时也会调用该函数)</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Washer</span>():</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, width, height</span>):</span></span><br><span class="line">		self.width = width</span><br><span class="line">		self.height = height</span><br><span class="line">        </span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__del__</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;self&#125;</span>对象已经被删除&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">haier1 = Washer(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line"><span class="keyword">del</span> haier1 <span class="comment"># &lt;__main__.Washer object at 0x0000026118223278&gt;对象已经被删除</span></span><br></pre></td></tr></table></figure>
<h5 id="四-call-魔法方法"><a href="#四-call-魔法方法" class="headerlink" title="(四) __call__()魔法方法"></a>(四) <code>__call__()</code>魔法方法</h5><p><code>__call__()</code>方法的作用其实是把一个类的实例化对象变成了可调用对象，只要类里实现了<code>__call__()</code>方法就行。</p>
<p>如当类里没有实现<code>__call__()</code>时，<strong>此时的对象p只是个类的实例，不是一个可调用的对象</strong>，当调用它时会报错：’Person’ object is not callable</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"><span class="built_in">object</span></span>)</span></span><br><span class="line"><span class="class">	<span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>)</span></span><br><span class="line"><span class="class">    	<span class="title">self</span>.<span class="title">name</span> = <span class="title">name</span></span></span><br><span class="line"><span class="class">        <span class="title">pass</span></span></span><br><span class="line"><span class="class">    <span class="title">pass</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">Wjh</span> = <span class="title">Person</span>(<span class="params"><span class="string">&#x27;WJH&#x27;</span></span>)</span></span><br><span class="line"><span class="class"># 未写方法<span class="title">__call__</span>时强行调用对象</span></span><br><span class="line"><span class="class"><span class="title">Wjh</span>(<span class="params"><span class="string">&#x27;Bob&#x27;</span></span>)</span></span><br><span class="line"><span class="class"># 报错：&#x27;<span class="title">Person</span>&#x27; <span class="title">object</span> <span class="title">is</span> <span class="title">not</span> <span class="title">callable</span>        </span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"><span class="built_in">object</span></span>)</span></span><br><span class="line"><span class="class">	<span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>)</span></span><br><span class="line"><span class="class">    	<span class="title">self</span>.<span class="title">name</span> = <span class="title">name</span></span></span><br><span class="line"><span class="class">        <span class="title">pass</span></span></span><br><span class="line"><span class="class">     <span class="title">def</span> <span class="title">__call__</span>(<span class="params">self, friend</span>)</span></span><br><span class="line"><span class="class">    	<span class="title">print</span>(<span class="params"><span class="string">f&#x27;我的名字是<span class="subst">&#123;self.name&#125;</span>，我的朋友叫<span class="subst">&#123;friend&#125;</span>&#x27;</span></span>)</span></span><br><span class="line"><span class="class">        <span class="title">pass</span></span></span><br><span class="line"><span class="class">    <span class="title">pass</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">Wjh</span> = <span class="title">Person</span>(<span class="params"><span class="string">&#x27;WJH&#x27;</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">Wjh</span>(<span class="params"><span class="string">&#x27;Bob&#x27;</span></span>)</span></span><br><span class="line"><span class="class"># 我的名字是<span class="title">WJH</span>，我的朋友叫<span class="title">Bob</span></span></span><br></pre></td></tr></table></figure>
<h3 id="第八章-面向对象—继承"><a href="#第八章-面向对象—继承" class="headerlink" title="第八章 面向对象—继承"></a>第八章 面向对象—继承</h3><h4 id="8-1-继承的概念"><a href="#8-1-继承的概念" class="headerlink" title="8.1 继承的概念"></a>8.1 继承的概念</h4><p>生活中的继承，一般指的是子女继承父辈的财产。</p>
<p>Python面向对象的继承指的是<strong>多个类之间的所属关系</strong>，即子类默认继承父类的所有属性和方法，具体如<br>下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 父类A</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.num = <span class="number">1</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">info_print</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(self.num)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="comment"># 子类B</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>(<span class="params">A</span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result = B()</span><br><span class="line">result.info_print() <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<p>在Python中，所有类默认继承object类，object类是顶级类或基类；其他子类叫做派生类。</p>
<ul>
<li><font color="blue">**子类不重写`__init__`，实例化子类时，会自动调用父类定义的`__init__`。**</font> </li>
<li><strong>如果子类重写了<code>__init__</code> 方法，那么父类的属性是不能直接调用的。</strong> </li>
</ul>
<h4 id="8-2-单继承"><a href="#8-2-单继承" class="headerlink" title="8.2 单继承"></a>8.2 单继承</h4><p>一个子类只有一个直接父类时称这个继承关系为单继承。</p>
<blockquote>
<p> 故事主线：一个煎饼果子老师傅，在煎饼果子界摸爬滚打多年，研发了一套精湛的摊煎饼果子的技术。师⽗父要把这套技术传授给他的唯一的最得意的徒弟。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 师⽗父类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[古法煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">         </span><br><span class="line"><span class="comment"># 2. 徒弟类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Prentice</span>(<span class="params">Master</span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 创建对象daqiu</span></span><br><span class="line">daqiu = Prentice()</span><br><span class="line"><span class="comment"># 4. 对象访问实例例属性</span></span><br><span class="line"><span class="built_in">print</span>(daqiu.kongfu)</span><br></pre></td></tr></table></figure>
<h4 id="8-3-多继承"><a href="#8-3-多继承" class="headerlink" title="8.3 多继承"></a>8.3 多继承</h4><p>一个子类有两个或以上直接父类时称这个继承关系为多继承。 </p>
<blockquote>
<p>故事推进：daqiu是个爱学习的好孩子，想学习更多的煎饼果子技术，于是，在百度搜索到黑马程序员，报班学习煎饼果子技术。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[古法煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 创建学校类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">School</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[黑马煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Prentice</span>(<span class="params">School, Master</span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">daqiu = Prentice()</span><br><span class="line"><span class="built_in">print</span>(daqiu.kongfu)</span><br><span class="line">daqiu.make_cake()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：<strong>当一个类有多个父类的时候，默认使用第一个父类的同名属性和方法。</strong></p>
</blockquote>
<h4 id="8-4-子类重写父类同名方法和属性"><a href="#8-4-子类重写父类同名方法和属性" class="headerlink" title="8.4 子类重写父类同名方法和属性"></a>8.4 子类重写父类同名方法和属性</h4><p>子类和父类具有同名属性和方时，默认使用子类的同名属性和方法。</p>
<p>一个对象调用一个方法，如果本类中没有这个方法那就会按照mro顺序去找与他相关的类是否有这个方法，如果找到顶级类object依然没有这个方法，那就会报错。包括后面的super().方法()也是按照mro顺序去找的，至于mro是怎么定义的，大家可以搜索下【拓扑排序】 </p>
<blockquote>
<p>故事：daqiu掌握了师父和培训的技术后，自己潜心钻研出自己的独门配方的一套全新的煎饼果子技术。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    	self.kongfu = <span class="string">&#x27;[古法煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">School</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[黑马煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 独创配方</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Prentice</span>(<span class="params">School, Master</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[独创煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">daqiu = Prentice()</span><br><span class="line"><span class="built_in">print</span>(daqiu.kongfu)</span><br><span class="line">daqiu.make_cake()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Prentice.__mro__) <span class="comment"># 通过打印__mro__可以查看类的继承层级顺序</span></span><br></pre></td></tr></table></figure>
<h4 id="8-5-子类调用父类的同名方法和属性"><a href="#8-5-子类调用父类的同名方法和属性" class="headerlink" title="8.5 子类调用父类的同名方法和属性"></a>8.5 子类调用父类的同名方法和属性</h4><blockquote>
<p>故事：很多顾客都希望也能吃到古法和黑马的技术的煎饼果子。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    	self.kongfu = <span class="string">&#x27;[古法煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">School</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[黑马煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Prentice</span>(<span class="params">School, Master</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[独创煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="comment"># 若先调用了父类的属性和方法，父类属性会覆盖子类属性，故在调用属性前，先调用自己子类的初始化</span></span><br><span class="line">		self.__init__()</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">		<span class="comment"># 调用父类方法，但是为保证调用到的也是父类的属性，必须在调用方法前调用父类的初始化</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_master_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		Master.__init__(self)</span><br><span class="line">		Master.make_cake(self)</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_school_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		School.__init__(self)</span><br><span class="line">		School.make_cake(self)</span><br><span class="line">        </span><br><span class="line">daqiu = Prentice()</span><br><span class="line">daqiu.make_cake()</span><br><span class="line">daqiu.make_master_cake()</span><br><span class="line">daqiu.make_school_cake()</span><br><span class="line">daqiu.make_cake()</span><br></pre></td></tr></table></figure>
<h4 id="8-6-多层继承"><a href="#8-6-多层继承" class="headerlink" title="8.6 多层继承"></a>8.6 多层继承</h4><p>一个子类可以有一个父类，一个父类还可以有一个父类。</p>
<blockquote>
<p>故事：N年后，daqiu老了，想要把所有技术传承给自己的徒弟。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    	self.kongfu = <span class="string">&#x27;[古法煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">School</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[黑马煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Prentice</span>(<span class="params">School, Master</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[独创煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="comment"># 若先调用了父类的属性和方法，父类属性会覆盖子类属性，故在调用属性前，先调用自己子类的初始化</span></span><br><span class="line">		self.__init__()</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">		<span class="comment"># 调用父类方法，但是为保证调用到的也是父类的属性，必须在调用方法前调用父类的初始化</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_master_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		Master.__init__(self)</span><br><span class="line">		Master.make_cake(self)</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_school_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		School.__init__(self)</span><br><span class="line">		School.make_cake(self)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 徒孙类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tusun</span>(<span class="params">Prentice</span>):</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">xiaoqiu = Tusun()</span><br><span class="line">xiaoqiu.make_cake()  <span class="comment"># 运用[独创煎饼果子配方]制作煎饼果子</span></span><br><span class="line">xiaoqiu.make_school_cake()  <span class="comment"># 运用[黑马煎饼果子配方]制作煎饼果子</span></span><br><span class="line">xiaoqiu.make_master_cake()  <span class="comment"># 运用[古法煎饼果子配方]制作煎饼果子</span></span><br></pre></td></tr></table></figure>
<h4 id="8-7-super-调用父类方法"><a href="#8-7-super-调用父类方法" class="headerlink" title="8.7 super()调用父类方法"></a>8.7 super()调用父类方法</h4><p>使用super()可以自动查找父类。调用顺序遵循 <code>__mro__</code> 类属性的顺序。比较<strong>适合单继承</strong>使用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Master</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[古法煎饼果子配方]&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">School</span>(<span class="params">Master</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[黑马煎饼果子配方]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">		<span class="comment"># 方法2.1</span></span><br><span class="line">		<span class="comment"># super(School, self).__init__()</span></span><br><span class="line">		<span class="comment"># super(School, self).make_cake()</span></span><br><span class="line">		<span class="comment"># 方法2.2</span></span><br><span class="line">		<span class="built_in">super</span>().__init__()</span><br><span class="line">		<span class="built_in">super</span>().make_cake()</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Prentice</span>(<span class="params">School</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.kongfu = <span class="string">&#x27;[独创煎饼果子技术]&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.__init__()</span><br><span class="line">		<span class="built_in">print</span>(<span class="string">f&#x27;运用<span class="subst">&#123;self.kongfu&#125;</span>制作煎饼果子&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 子类调用父类的同名方法和属性：把父类的同名属性和方法再次封装</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_master_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		Master.__init__(self)</span><br><span class="line">		Master.make_cake(self)</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_school_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">		School.__init__(self)</span><br><span class="line">		School.make_cake(self)</span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 一次性调用父类的同名属性和方法</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">make_old_cake</span>(<span class="params">self</span>):</span></span><br><span class="line">	<span class="comment"># 方法一：代码冗余；父类名如果变化，这里代码需要频繁修改</span></span><br><span class="line">		<span class="comment"># Master.__init__(self)</span></span><br><span class="line">		<span class="comment"># Master.make_cake(self)</span></span><br><span class="line">		<span class="comment"># School.__init__(self)</span></span><br><span class="line">		<span class="comment"># School.make_cake(self)</span></span><br><span class="line">	<span class="comment"># 方法二: super()</span></span><br><span class="line">	<span class="comment"># 方法2.1 super(当前类名, self).函数()</span></span><br><span class="line">	<span class="comment"># super(Prentice, self).__init__()</span></span><br><span class="line">	<span class="comment"># super(Prentice, self).make_cake()</span></span><br><span class="line">	<span class="comment"># 方法2.2 super().函数()</span></span><br><span class="line">	<span class="built_in">super</span>().__init__()</span><br><span class="line">	<span class="built_in">super</span>().make_cake()</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">daqiu = Prentice()</span><br><span class="line">daqiu.make_old_cake()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意： super() 在 python2、3中的区别</p>
<p>python3直接写成 super().方法名（参数） </p>
<p>python2必须写成 super（父类，self）.方法名（参数） </p>
</blockquote>
<h4 id="8-8-私有权限"><a href="#8-8-私有权限" class="headerlink" title="8.8 私有权限"></a>8.8 私有权限</h4><p>主要介绍了Python 私有属性与私有方法，私有属性是指在 Python 的面向对象开发过程中，对象的某些属性只想在对象的内部被使用，但不想在外部被访问到这些属。</p>
<blockquote>
<p>参考笔记：Python 私有属性与私有方法<a href="https://www.jb51.net/article/239280.htm">https://www.jb51.net/article/239280.htm</a></p>
</blockquote>
<h5 id="一-定义私有属性和方法"><a href="#一-定义私有属性和方法" class="headerlink" title="(一) 定义私有属性和方法"></a>(一) 定义私有属性和方法</h5><p> <strong>(1) 私有属性/方法是什么？</strong></p>
<font color="red">**两个下划线开头的属性是私有的(private)。其他为公共的(public)**。</font>

<ul>
<li><strong>私有属性：</strong></li>
</ul>
<p>是指在 Python 的面向对象开发过程中，对象的某些属性只想在对象的内部被使用，但不想在外部被访问到这些属性。<strong>即：</strong>私有属性是对象不愿意公开的属性。</p>
<ul>
<li><strong>私有方法：</strong></li>
</ul>
<p>是指在 Python 的面向对象开发过程中，对象的某些方法或者称为函数只想在对象的内部被使用，但不想在外部被访问到这些方法或函数。<strong>即：</strong>私有方法是对象不愿意公开的方法或函数。</p>
<p><strong>(2) 示例语法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Staff</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, s_name, s_salary</span>):</span></span><br><span class="line">      self.s_name = s_name</span><br><span class="line">      self.__salary = s_salary</span><br><span class="line">      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__secret</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;%s 的工资是 %d&quot;</span> % (self.s_name, self.__salary))</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>(1). __salary是以两个下划线开头来定义的私有属性。</li>
<li>(2). __secret(self)是以两个下划线开头来定义的私有方法。</li>
</ul>
</blockquote>
<h5 id="二-私有属性的调用"><a href="#二-私有属性的调用" class="headerlink" title="(二) 私有属性的调用"></a>(二) 私有属性的调用</h5><ul>
<li>类内部可以访问私有属性(方法)</li>
<li>类外部不能直接访问私有属性(方法)</li>
<li>类外部可以通过”_类名__私有属性(方法)名”访问私有属性(方法)(<font color="red"><strong>伪</strong></font>私有属性和私有方法)</li>
</ul>
<blockquote>
<p>方法本质上也是属性！只不过是可以通过()执行而执行</p>
</blockquote>
<h5 id="三-获取和修改私有属性值"><a href="#三-获取和修改私有属性值" class="headerlink" title="(三) 获取和修改私有属性值"></a>(三) 获取和修改私有属性值</h5><p>在Python中，一般定义公有方法函数名<code>get_xx</code>用来获取私有属性，定义<code>set_xx</code>用来修改私有属性值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"><span class="built_in">object</span></span>):</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.name = <span class="string">&quot;小明&quot;</span></span><br><span class="line">        self.__age = <span class="number">20</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">#获取私有属性的值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_age</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.__age</span><br><span class="line"> </span><br><span class="line">    <span class="comment">#设置私有属性的值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_age</span>(<span class="params">self, new_age</span>):</span></span><br><span class="line">        self.__age = new_age</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#定义一个对象</span></span><br><span class="line">p = Person()</span><br><span class="line"><span class="comment">#强行获取私有属性</span></span><br><span class="line"><span class="comment">#崇尚一切靠自觉</span></span><br><span class="line"><span class="built_in">print</span>(p._Person__age)</span><br><span class="line"><span class="built_in">print</span>(p.name)</span><br><span class="line"><span class="comment">#想在类的外面获取对象的属性</span></span><br><span class="line">ret = p.get_age()</span><br><span class="line"><span class="built_in">print</span>(ret)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#想在类的外面修改对象私有属性的值</span></span><br><span class="line">p.set_age(<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(p.get_age())</span><br></pre></td></tr></table></figure>
<h3 id="第九章-面向对象—多态—其他知识"><a href="#第九章-面向对象—多态—其他知识" class="headerlink" title="第九章 面向对象—多态—其他知识"></a>第九章 面向对象—多态—其他知识</h3><h4 id="9-1-多态"><a href="#9-1-多态" class="headerlink" title="9.1 多态"></a>9.1 多态</h4><h5 id="一-了解多态"><a href="#一-了解多态" class="headerlink" title="(一) 了解多态"></a>(一) 了解多态</h5><p>多态指的是一类事物有多种形态，（一个抽象类有多个子类，因而多态的概念依赖于继承）。</p>
<ul>
<li><strong>定义：</strong></li>
</ul>
<p>多态是一种使用对象的方式，子类重写父类方法，调用不同子类对象的相同父类方法，可以产生不同的执行结果。</p>
<ul>
<li><strong>好处：</strong></li>
</ul>
<p>调用灵活，有了多态更容易写出通用的代码，做出通用的编程，以适应需求的不断变化！</p>
<ul>
<li><strong>实现步骤：</strong><ul>
<li>定义父类，并提供公共方法</li>
<li>定义子类，并重写父类方法</li>
<li>传递子类对象给调用者，可以看到不同子类执行效果不同</li>
</ul>
</li>
</ul>
<h5 id="二-多态示例"><a href="#二-多态示例" class="headerlink" title="(二) 多态示例"></a>(二) 多态示例</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">work</span>(<span class="params">self</span>):</span> <span class="comment"># 父类提供统一的方法，哪怕是空方法</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;指哪打哪...&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArmyDog</span>(<span class="params">Dog</span>):</span> <span class="comment"># 继承Dog类</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">work</span>(<span class="params">self</span>):</span> <span class="comment"># 子类重写父类同名方法</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;追击敌人...&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DrugDog</span>(<span class="params">Dog</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">work</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;追查毒品...&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">work_with_dog</span>(<span class="params">self, dog</span>):</span> <span class="comment"># 传入不同的对象执行不同的代码，即不同的work函数</span></span><br><span class="line">		dog.work()</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">ad = ArmyDog()</span><br><span class="line">dd = DrugDog()</span><br><span class="line">daqiu = Person()</span><br><span class="line">daqiu.work_with_dog(ad)</span><br><span class="line">daqiu.work_with_dog(dd)</span><br></pre></td></tr></table></figure>
<h4 id="9-2-类属性和实例属性"><a href="#9-2-类属性和实例属性" class="headerlink" title="9.2 类属性和实例属性"></a>9.2 类属性和实例属性</h4><h5 id="一-类属性"><a href="#一-类属性" class="headerlink" title="(一) 类属性"></a>(一) 类属性</h5><p><strong>(1) 设置和访问类属性</strong></p>
<ul>
<li>类属性就是<strong>类对象</strong>所拥有的属性，它被<strong>该类的所有实例对象</strong>所共有。</li>
<li>类属性可以使用<strong>类对象</strong>或<strong>实例对象</strong>访问。</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	tooth = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">wangcai = Dog()</span><br><span class="line">xiaohei = Dog()</span><br><span class="line"><span class="built_in">print</span>(Dog.tooth) <span class="comment"># 10</span></span><br><span class="line"><span class="built_in">print</span>(wangcai.tooth) <span class="comment"># 10</span></span><br><span class="line"><span class="built_in">print</span>(xiaohei.tooth) <span class="comment"># 10</span></span><br></pre></td></tr></table></figure>
<p><strong>(2) 类属性的优点</strong></p>
<ul>
<li>类的实例 记录的某项数据 始终保持一致时，则定义类属性。</li>
<li>实例属性要求每个对象为其单独开辟一份内存空间来记录数据，而类属性为全类所共有，仅占用一份内存，更加节省内存空间。</li>
</ul>
<p><strong>(3) 修改类属性</strong></p>
<p>类属性只能通过类对象修改，不能通过实例对象修改，如果通过实例对象修改类属性，表示的是创建了了一个实例属性。</p>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	tooth = <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">wangcai = Dog()</span><br><span class="line">xiaohei = Dog()</span><br><span class="line"><span class="comment"># 修改类属性</span></span><br><span class="line">Dog.tooth = <span class="number">12</span></span><br><span class="line"><span class="built_in">print</span>(Dog.tooth) <span class="comment"># 12</span></span><br><span class="line"><span class="built_in">print</span>(wangcai.tooth) <span class="comment"># 12</span></span><br><span class="line"><span class="built_in">print</span>(xiaohei.tooth) <span class="comment"># 12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不能通过对象修改属性，如果这样操作，实则是创建了一个实例属性</span></span><br><span class="line">wangcai.tooth = <span class="number">20</span></span><br><span class="line"><span class="built_in">print</span>(Dog.tooth) <span class="comment"># 12</span></span><br><span class="line"><span class="built_in">print</span>(wangcai.tooth) <span class="comment"># 20</span></span><br><span class="line"><span class="built_in">print</span>(xiaohei.tooth) <span class="comment"># 12</span></span><br></pre></td></tr></table></figure>
<h5 id="二-实例属性"><a href="#二-实例属性" class="headerlink" title="(二) 实例属性"></a>(二) 实例属性</h5><p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.age = <span class="number">5</span></span><br><span class="line">        </span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">info_print</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(self.age)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">wangcai = Dog()</span><br><span class="line"><span class="built_in">print</span>(wangcai.age) <span class="comment"># 5</span></span><br><span class="line"><span class="comment"># print(Dog.age) # 报错：实例属性不能通过类访问</span></span><br><span class="line">wangcai.info_print() <span class="comment"># 5</span></span><br></pre></td></tr></table></figure>
<h4 id="9-3-类方法和静态方法"><a href="#9-3-类方法和静态方法" class="headerlink" title="9.3 类方法和静态方法"></a>9.3 类方法和静态方法</h4><h5 id="一-类方法"><a href="#一-类方法" class="headerlink" title="(一) 类方法"></a>(一) 类方法</h5><p><strong>(1) 类方法特点</strong></p>
<ul>
<li>第一个形参是类对象的方法</li>
<li>需要用装饰器<code>@classmethod</code>来标识其为类方法，对于类方法，第一个参数必须是类对象，一般以<code>cls</code>作为第一个参数。</li>
</ul>
<p><strong>(2) 类方法使用场景</strong></p>
<ul>
<li>当方法中 需要使用类对象 (如访问私有类属性等)时，定义类方法</li>
<li>类方法一般和类属性配合使用</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	__tooth = <span class="number">10</span> <span class="comment"># 私有-类-属性</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">	@classmethod</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">get_tooth</span>(<span class="params">cls</span>):</span></span><br><span class="line">		<span class="keyword">return</span> cls.__tooth</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">wangcai = Dog()</span><br><span class="line">result = wangcai.get_tooth()</span><br><span class="line"><span class="built_in">print</span>(result) <span class="comment"># 10</span></span><br></pre></td></tr></table></figure>
<p><strong>(二) 静态方法</strong></p>
<p><strong>(1) 静态方法特点</strong></p>
<ul>
<li>需要通过装饰器<code>@staticmethod</code>来进行修饰，静态方法既不需要传递类对象也不需要传递实例对象(<strong>形参没有self/cls</strong>）。</li>
<li>静态方法也能够通过<strong>实例对象</strong>和<strong>类对象</strong>去访问。</li>
</ul>
<p><strong>(2) 静态方法使用场景</strong></p>
<ul>
<li>当方法中既不需要使用实例对象(如实例对象，实例属性)，也不需要使用类对象(如类属性、类方法、创建实例等)时，定义静态方法</li>
<li>取消不需要的参数传递，有利于减少不必要的内存占用和性能消耗</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"><span class="meta">	@staticmethod</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">info_print</span>():</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&#x27;这是一个狗类，用于创建狗实例....&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">wangcai = Dog()</span><br><span class="line"><span class="comment"># 静态方法既可以使用对象访问又可以使用类访问</span></span><br><span class="line">wangcai.info_print()</span><br><span class="line">Dog.info_print()</span><br></pre></td></tr></table></figure>
<h3 id="第十章-模块和包"><a href="#第十章-模块和包" class="headerlink" title="第十章 模块和包"></a>第十章 模块和包</h3><h4 id="10-1-模块"><a href="#10-1-模块" class="headerlink" title="10.1 模块"></a>10.1 模块</h4><p>Python 模块(Module)，是一个 Python文件，以 .py 结尾，包含了Python对象定义和Python语句句。模块能定义函数，类和变量，模块里也能包含可执行的代码。</p>
<h5 id="一-导入模块"><a href="#一-导入模块" class="headerlink" title="(一) 导入模块"></a>(一) 导入模块</h5><ul>
<li>import 模块名</li>
<li>from 模块名 import 功能名</li>
<li>from 模块名 import *</li>
<li>import 模块名 as 别名</li>
<li>from 模块名 import 功能名 as 别名</li>
</ul>
<h5 id="二-制作模块"><a href="#二-制作模块" class="headerlink" title="(二) 制作模块"></a>(二) 制作模块</h5><p>在Python中，每个Python文件都可以作为一个模块，模块的名字就是文件的名字。也就是说自定义模块名必须要符合标识符命名规则。</p>
<p><strong>(1) 定义模块</strong></p>
<p>新建一个Python文件，命名为<code>my_module1.py</code> ，并定义<code>testA</code>函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testA</span>(<span class="params">a, b</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(a + b)</span><br></pre></td></tr></table></figure>
<p><strong>(2) 测试模块</strong></p>
<p>在实际开中，当一个开发人员编写完一个模块后，为了让模块能够在项目中达到想要的效果，这个开发人员会自行在py文件中添加一些测试信息，例如在<code>my_module1.py</code>文件中添加测试代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testA</span>(<span class="params">a, b</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(a + b)</span><br><span class="line">    </span><br><span class="line">testA(<span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>此时，无论是当前文件，还是其他已经导入了该模块的文件，在运行的时候都会自动执行testA函数的调用。解决办法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testA</span>(<span class="params">a, b</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(a + b)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 只在当前文件中调用该函数，其他导入的文件内不符合该条件，则不执行testA函数调用</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">	testA(<span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：<code>__name__</code>是一个系统变量，当在自己文件内时其值为<code>__main__</code>，当在其他文件调用时其值为文件名</p>
</blockquote>
<p><strong>(3) 调用模块</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> my_module1</span><br><span class="line">my_module1.testA(<span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>(4) 注意事项</strong></p>
<p>如果使用<code>from .. import ..</code>或<code>from .. import *</code>导入多个模块的时候，且模块内有同名功能。当调用这个同名功能的时候，调用到的是<strong>后面导入的模块的功能</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my_module1: 模块1代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_test</span>(<span class="params">a, b</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(a + b)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># my_module2:模块2代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_test</span>(<span class="params">a, b</span>):</span></span><br><span class="line">	<span class="built_in">print</span>(a - b)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 导入模块和调⽤用功能代码</span></span><br><span class="line"><span class="keyword">from</span> my_module1 <span class="keyword">import</span> my_test</span><br><span class="line"><span class="keyword">from</span> my_module2 <span class="keyword">import</span> my_test</span><br><span class="line"><span class="comment"># my_test函数是模块2中的函数</span></span><br><span class="line">my_test(<span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 0</span></span><br></pre></td></tr></table></figure>
<h5 id="三-模块定位顺序"><a href="#三-模块定位顺序" class="headerlink" title="(三) 模块定位顺序"></a>(三) 模块定位顺序</h5><p>当导入一个模块，Python解析器对模块位置的搜索顺序是：</p>
<ol>
<li>当前目录</li>
<li>如果不在当前目录，Python则搜索在shell变量PYTHONPATH下的每个目录。</li>
<li>如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/模块搜索路径存储在system模块的sys.path变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。<br>注意：</li>
</ol>
<ul>
<li>自己的文件名不要和已有模块名重复，否则导致模块功能无法使用</li>
<li>使用<code>from 模块名 import 功能</code>时，若功能名字和自己定义的函数名重复，调用到的是最后定义或导入的功能。</li>
<li>使用<code>import 模块名</code>时，则会导致名字覆盖</li>
</ul>
<h5 id="四-all-变量"><a href="#四-all-变量" class="headerlink" title="(四) __all__变量"></a>(四) <code>__all__</code>变量</h5><p>如果一个模块文件中有<code>__all__</code>变量，当使用<code>from xxx import *</code>导入时，只能导入这个列表中的元素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my_module1模块代码</span></span><br><span class="line">__all__ = [<span class="string">&#x27;testA&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testA</span>():</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;testA&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testB</span>():</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;testB&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>导入模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> my_module1 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">testA() <span class="comment"># &#x27;testA&#x27;</span></span><br><span class="line">testB() <span class="comment"># __all__列表没有testB，调用报错</span></span><br></pre></td></tr></table></figure>
<h4 id="10-2-包"><a href="#10-2-包" class="headerlink" title="10.2 包"></a>10.2 包</h4><p>包将有联系的模块组织在一起，即放到同一个文件夹下，并且在这个文件夹创建一个名字为<code>__init__.py</code>文件，那么这个文件夹就称之为包。</p>
<h5 id="一-制作包"><a href="#一-制作包" class="headerlink" title="(一) 制作包"></a>(一) 制作包</h5><p>[New] — [Python Package] — 输入包名 — [OK] — 新建功能模块(有联系的模块)。<br>注意：新建包后，包内部会自动创建<code>__init__.py</code>文件，这个文件控制着包的导入行为。</p>
<p><strong>(1) 快速示例</strong></p>
<ol>
<li>新建包mypackage</li>
<li>新建包内模块： my_module1 和 my_module2</li>
<li>模块内代码如下</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my_module1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_print1</span>():</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;my_module1&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># my_module2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_print2</span>():</span></span><br><span class="line">	<span class="built_in">print</span>(<span class="string">&#x27;my_module2&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h5 id="二-导入包"><a href="#二-导入包" class="headerlink" title="(二) 导入包"></a>(二) 导入包</h5><p><strong>(1) 方法一</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> 包名.模块名</span><br><span class="line"><span class="comment"># 调用方法</span></span><br><span class="line">包名.模块名.目标函数</span><br></pre></td></tr></table></figure>
<p><strong>(2) 方法二</strong></p>
<p>注意：必须在<code>__init__.py</code>文件中添加<code>__all__ = []</code>，控制允许导入的模块列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> 包名 <span class="keyword">import</span> *</span><br><span class="line"><span class="comment"># 调用方法</span></span><br><span class="line">模块名.目标</span><br></pre></td></tr></table></figure>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> my_package <span class="keyword">import</span> *</span><br><span class="line">my_module1.info_print1()</span><br></pre></td></tr></table></figure>
<h3 id="第十一章-异常处理"><a href="#第十一章-异常处理" class="headerlink" title="第十一章 异常处理"></a>第十一章 异常处理</h3><hr>
<ul>
<li>Python的模块(module)、包(package)和库的区别<ul>
<li>模块(module)其实就是文件，里面定义了一些函数、类、变量等，可以作为module的文件类型有”.py”、”.pyo”、”.pyc”、”.pyd”、”.so”、”.dll” 等；</li>
<li>包(package)是多个模块module的聚合体形成的文件夹，里面可以是多个py文件，也可以嵌套文件夹；</li>
<li>库是参考其他编程语言的说法，是指完成一定功能的代码集合，在python中的形式就是模块和包；</li>
</ul>
</li>
<li>如何导入python包和模块<ul>
<li>模块的导入 就是直接import文件名即可 ；</li>
<li>导入包不能随便使用其中的模块，要导入到具体模块或者变量的层次 </li>
</ul>
</li>
</ul>
<hr>
<p>参考笔记：E:\A.毕业设计\前期基础知识学习\01-Python基础课件.pdf</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>编程与深度学习</category>
        <category>编程语言学习</category>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python编程基础</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch基础</title>
    <url>/2022/04/24/Foundation-of-Pytorch/</url>
    <content><![CDATA[<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1hE411t7RN" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i0.hdslb.com/bfs/archive/aedf385ac09752547290e85319b88081c812cf06.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">09:50:38</span>
            </div>
            <div class="bvideo-info">
                <p class="title">PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>312.9万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>3.4万</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">我是土堆</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<h3 id="第一章-Pytorch加载数据"><a href="#第一章-Pytorch加载数据" class="headerlink" title="第一章 Pytorch加载数据"></a>第一章 Pytorch加载数据</h3><h4 id="1-1-Pytorch加载数据初识"><a href="#1-1-Pytorch加载数据初识" class="headerlink" title="1.1 Pytorch加载数据初识"></a>1.1 Pytorch加载数据初识</h4><h5 id="一-常见的数据组织形式"><a href="#一-常见的数据组织形式" class="headerlink" title="(一) 常见的数据组织形式"></a>(一) 常见的数据组织形式</h5><ul>
<li>每一类事物分别放在单独的文件夹内，文件夹的名称即为该类的名字；</li>
<li>将数据集划分为训练集和测试集两个文件夹(即每类事物混合放)，并会提供一个训练集的数据对应的label文件；</li>
<li>直接在数据文件的命名中体现出label名</li>
</ul>
<h5 id="二-使用的类"><a href="#二-使用的类" class="headerlink" title="(二) 使用的类"></a>(二) 使用的类</h5><p>pytorch中加载数据主要涉及两个类——<font clolor="red"><strong>Dataset、Dataloader</strong></font></p>
<p>直白的说：</p>
<ul>
<li>Dataset作用：提供一种方式获取数据已经数据对应的label，获取总共有多少数据</li>
<li>Dataloader作用：为后面的网络提供不同的数据形式，如：mini_batch大小、是否打乱等</li>
</ul>
<h4 id="1-2-Dataset类"><a href="#1-2-Dataset类" class="headerlink" title="1.2 Dataset类"></a>1.2 Dataset类</h4><p>Dataset是一个抽象类，所有的数据集都要继承这个类，所有的子类都应该重写<code>__getitem__</code>魔法方法(获取每一个数据及其对应的label)，可以选择重写<code>__len__</code>魔法方法(获取数据的总长度)</p>
<p>在Python中我们可以使用<code>__getitem__</code>、<code>__len__</code>等方法去创建类似于序列和映射的类。这种方法的好处是可以像列表一样使用索引功能访问元素。 </p>
<h5 id="一-getitem-魔法方法"><a href="#一-getitem-魔法方法" class="headerlink" title="(一) __getitem__()魔法方法"></a>(一) <code>__getitem__()</code>魔法方法</h5><p>如果在类中定义了<code>__getitem__()</code>方法，那么他的实例对象（假设为P）就可以这样<code>P[idx]</code>取值。当实例对象做<code>P[idx]</code>运算时，就会调用类中的<code>__getitem__()</code>方法。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataTest</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, <span class="built_in">id</span>, address</span>):</span></span><br><span class="line">        self.<span class="built_in">id</span> = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">        self.address = address</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;调用了__getitem__方法&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">id</span>[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = DataTest(<span class="number">1</span>, <span class="string">&quot;192.168.2.11&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 调用了__getitem__方法</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure>
<h5 id="二-len-魔法方法"><a href="#二-len-魔法方法" class="headerlink" title="(二) __len__()魔法方法"></a>(二) <code>__len__()</code>魔法方法</h5><p>如果一个类表现得像一个list，要获取有多少个元素，就得用<code>len()</code>函数。要让<code>len()</code>函数工作正常，类必须提供一个特殊方法<code>__len__()</code>，它返回元素的个数。</p>
<ul>
<li>类的对象能否使用<code>len()</code>函数，仅仅取决于其是否实现了<code>__len__()</code>函数而已。正如基本的str，tuple，list，dict，set等，它们可以使用<code>len()</code>函数，也仅仅是因为它们的类实现了<code>__len__()</code>函数而已； </li>
<li>自己创建的类和python中的基础类型（int,float,str,tuple,forzenset,list,set,dict）等均是平等的地位， 差别仅在于您自己创建的类中是否实现了与这些基础类中相同的功能(或者魔术函数)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 例如，写一个Students类，把名字传进去：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Students</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args</span>):</span></span><br><span class="line">        self.names = args</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.names)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment"># 只要正确实现了__len__()方法，就可以用len()函数返回Students实例的“长度”：</span></span><br><span class="line">ss = Students(<span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Alice&#x27;</span>, <span class="string">&#x27;Tim&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(ss))</span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure>
<h4 id="1-3-Tensorboard类"><a href="#1-3-Tensorboard类" class="headerlink" title="1.3 Tensorboard类"></a>1.3 Tensorboard类</h4><h5 id="一-Tensorboard介绍"><a href="#一-Tensorboard介绍" class="headerlink" title="(一) Tensorboard介绍"></a>(一) Tensorboard介绍</h5><p>Tensorboard原本是Google TensorFlow的可视化工具，可以用于记录训练数据、评估数据、网络结构、图像等，并且可以在web上展示，对于观察神经网络的过程非常有帮助。PyTorch也推出了自己的可视化工具，一个是tensorboardX包，一个是<code>torch.utils.tensorboard</code>，二者的使用相差不大。</p>
<h5 id="二-Tensorboard的使用"><a href="#二-Tensorboard的使用" class="headerlink" title="(二) Tensorboard的使用"></a>(二) Tensorboard的使用</h5><p>首先展示该包的使用的大致流程 </p>
<ul>
<li>Step1：导入tensorboard，实例化SummaryWriter类，指明记录日记路径等信息</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="comment"># 实例化SummaryWriter，并指明日志存放路径。在当前目录如果每月logs目录将自动创建</span></span><br><span class="line"><span class="comment"># 如果不写log_dir，系统将会创建runs目录</span></span><br><span class="line">writer = SummaryWriter(log_dir = ‘logs’)</span><br><span class="line"><span class="comment"># 调用实例</span></span><br><span class="line">writer.add_xxx()</span><br><span class="line"><span class="comment"># 关闭writer</span></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>Step2：调用相应的API，接口一般格式为：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_xxx(tag_name, <span class="built_in">object</span>, iteration-number)</span><br></pre></td></tr></table></figure>
<ul>
<li>Step3：启动tensorboard，在命令行中输入</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=r’加logs所在路径’</span><br></pre></td></tr></table></figure>
<ul>
<li>Step4：复制网址在浏览器中打开</li>
</ul>
<h5 id="三-使用各种add方法记录数据"><a href="#三-使用各种add方法记录数据" class="headerlink" title="(三) 使用各种add方法记录数据"></a>(三) 使用各种add方法记录数据</h5><p><strong>(1) 单条曲线(scalar)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_scalar(tag, scalar_value, global_step=<span class="literal">None</span>, walltime=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>参数：</strong><ul>
<li>tag ( string ) – 数据标识符</li>
<li>scalar_value ( float或string/blobname ) – 要保存的值(对应Y轴)</li>
<li>global_step ( int ) – 要记录的全局步长值(对应X轴)</li>
<li>walltime ( float ) – 记录训练的时间，默认 walltime (time.time()) 秒</li>
<li>new_style ( boolean ) – 是使用新样式（张量字段）还是旧样式（simple_value 字段）。新样式可能会导致更快的数据加载。</li>
</ul>
</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>) :</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y = 2x&#x27;</span>, x, <span class="number">2</span> * x)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p><strong>(2) 多条曲线(scalars)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_scalars( main_tag , tag_scalar_dict , global_step = <span class="literal">None</span> , walltime = <span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>参数：</strong><ul>
<li>main_tag ( string ) – 标签的父名称</li>
<li>tag_scalar_dict ( dict ) – 存储标签和对应值的键值对</li>
<li>global_step ( int ) – 要记录的全局步长值</li>
<li>walltime ( float ) – 记录训练的时间，默认 walltime (time.time()) 秒</li>
</ul>
</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line">r = <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>) :</span><br><span class="line">    writer.add_scalars(<span class="string">&#x27;run_14h&#x27;</span>, &#123;<span class="string">&#x27;xsinx&#x27;</span> : x * np.sin(x / r), </span><br><span class="line">                                  <span class="string">&#x27;xcosx&#x27;</span> : x * np.cos(x / r), </span><br><span class="line">                                  <span class="string">&#x27;xtanx&#x27;</span> : x * np.tan(x / r)&#125;, x)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p><strong>(3) 图片(image)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">add_image(tag, img_tensor, global_step=<span class="literal">None</span>, walltime=<span class="literal">None</span>, dataformats = ‘CHW’)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>参数：</strong><ul>
<li>tag ( string ) – 数据标识符</li>
<li>img_tensor ( torch.Tensor , numpy.array , or string/blobname ) – 图像数据</li>
<li>global_step ( int ) – 要记录的全局步长值</li>
<li>walltime ( float ) – 记录训练的时间，默认 walltime (time.time()) 秒</li>
</ul>
</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">img = cv.imread(<span class="string">&#x27;zhou.jpg&#x27;</span>, cv.IMREAD_COLOR)<span class="comment">#输入图像要是3通道的，所以读取彩色图像</span></span><br><span class="line">img = cv.cvtColor(img, cv.COLOR_BGR2RGB)</span><br><span class="line">img = torch.tensor(img.transpose(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))<span class="comment">#cv读取为numpy图像为(H * W * C)，所以要进行轴转换</span></span><br><span class="line">writer.add_image(<span class="string">&#x27;zhou_ge&#x27;</span>, img, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p><strong>(4) 其他</strong></p>
<p>直方图(histogram)、渲染(figure)、网络(graph)等。</p>
<blockquote>
<p>请参考博文：<a href="https://blog.csdn.net/weixin_53598445/article/details/121301078">详解Tensorboard及使用教程 - 八岁爱玩耍 - CSDN</a></p>
</blockquote>
<h4 id="1-4-Transforms类"><a href="#1-4-Transforms类" class="headerlink" title="1.4 Transforms类"></a>1.4 Transforms类</h4><h5 id="一-Transforms结构"><a href="#一-Transforms结构" class="headerlink" title="(一) Transforms结构"></a>(一) Transforms结构</h5><p>torchvision 中的 transforms 主要是对图片进行一些变换。</p>
<p>tranforms对应 tranforms.py 文件，里面定义了很多类，输入一个图片对象，返回经过处理的图片对象。</p>
<p>transforms 更多指的是python文件，transforms.py 就像一个工具箱，里面定义的各种类就像各种工具，特定格式的图片就是输入对象，经过工具处理，输出期望的图片结果。 </p>
<p><img src="https://pic.imgdb.cn/item/62661394239250f7c57c4a9c.jpg" style="zoom:50%"></p>
<h5 id="二-Python中的用法"><a href="#二-Python中的用法" class="headerlink" title="(二) Python中的用法"></a>(二) Python中的用法</h5><p>使用 transforms 的方法就是先实例化选中的类，然后用实例化的对象去处理图片就行。 </p>
<h5 id="三-补充：ToTensor"><a href="#三-补充：ToTensor" class="headerlink" title="(三) 补充：ToTensor"></a>(三) 补充：ToTensor</h5><p>将第一节中的代码复制到 python 控制台，回车，可在右侧看到各种变量和对象的具体信息： </p>
<p>tensor 数据类型可以理解为包装了反向神经网络一些理论基础参数。在神经网络中，要将数据先转换为Tensor类型，再进行训练。</p>
<h5 id="四-图像的读取与显示"><a href="#四-图像的读取与显示" class="headerlink" title="(四) 图像的读取与显示"></a>(四) 图像的读取与显示</h5><p><strong>(1) 图像读取</strong></p>
<p>之前PIL image已经学会了读取，numpy.ndarray 最常用的读取方法就是 opencv。 </p>
<ul>
<li><p>区别1</p>
<ul>
<li>img = cv2.imread(path)，这是opencv中的处理图片的函数，使用时需 import cv2  </li>
<li>img = Image.open(path)，这是PIL中的一个处理图片的函数，使用时需 from PIL import Image </li>
</ul>
</li>
<li><p>区别2</p>
<ul>
<li><p>cv2.imread()读取的是图像的真实数据</p>
</li>
<li><p>Image.open()函数只是保持了图像被读取的状态，但是图像的真实数据并未被读取，因此如果对需要操作图像每个元素，如输出某个像素的RGB值等，需要执行对象的load()方法读取数据。具体如下： </p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;lena.jpg&quot;</span>)</span><br><span class="line">img = img.load()</span><br><span class="line"><span class="built_in">print</span>(img[<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line"><span class="comment"># result：(255, 201, 166)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>区别3</p>
<ul>
<li><p>cv2.imread()得到的img数据类型是np.array()类型</p>
</li>
<li><p>Image.open(）得到的img数据类型呢是Image对象，不是普通的数组。</p>
</li>
</ul>
</li>
<li><p>区别4</p>
<ul>
<li><p>cv2.imread()读取通道的顺序为BGR</p>
</li>
<li><p>Image.open()函数默认彩色图像读取通道的顺序为RGB</p>
</li>
</ul>
</li>
</ul>
<p><strong>(2) 图像显示</strong></p>
<p>图像显示时常见方法有两种</p>
<ul>
<li><p>一种是matplotlib的plt.imshow()方法</p>
</li>
<li><p>一种是opencv的cv2.imshow()。</p>
</li>
</ul>
<p>两个函数的<strong>输入都要求是数组</strong>。因此Image读取的图片要先转化为数组，再进行图片的显示。plt函数读入的顺序为RGB，cv2.imshow()读入的顺序是BGR。</p>
<ul>
<li><p>因此image与plt.imshow()配合使用</p>
</li>
<li><p>opencv的方法配套使用</p>
</li>
</ul>
<h5 id="五-常用的Transforms类-方法"><a href="#五-常用的Transforms类-方法" class="headerlink" title="(五) 常用的Transforms类/方法"></a>(五) 常用的Transforms类/方法</h5><p><strong>(1) ToTensor()</strong>：将图片对象类型转为 tensor</p>
<p><strong>(2) Normalize()</strong>：对图像像素进行归一化计算</p>
<p><strong>(3) Resize()</strong>：重新设置 PIL Image的大小，返回也是PIL Image格式，效果是缩放，不是裁剪</p>
<p><strong>(4) Compose()</strong>：输入为 transforms 类型参数的列表</p>
<p>Compose()中的参数需要是一个列表，python中列表的表示形式为 [数据1，数据2，…]，在 Compose 中，数据需要是 transforms 类型，所以 Cmpose( [transforms 参数1, transforms 参数2], …)</p>
<p>目的是将几个 transforms 操作打包成一个，比如要先进行大小调整，然后进行归一化计算，返回 tensor 类型，则可以将 ToTensor、Normalize、Resize，按操作顺序输入到 Compose 中。</p>
<p><strong>(5) RandomCrop</strong>：随机裁剪</p>
<h4 id="1-5-torchvision类"><a href="#1-5-torchvision类" class="headerlink" title="1.5 torchvision类"></a>1.5 torchvision类</h4><h5 id="一-简介"><a href="#一-简介" class="headerlink" title="(一) 简介"></a>(一) 简介</h5><p>在Pytorch官网，torchvision 文档列出了很多科研或者毕设常用的一些数据集，如入门数据集 MNIST，用于手写文字。这些数据集位于 torchvision.datasets 模块，可以通过该模块对数据集进行下载，转换等操作。</p>
<p>torchvision 还有 io模块，但不常用；<strong>torchvision.models 会提供一些训练好的神经网络模型</strong>，在之后会用到；torchvision.transforms 之前已经学习过了，主要提供一些数据处理的工具。</p>
<p>接下来主要讲解如何联合使用 torchvision.datasets 和 torchvision.transforms</p>
<h5 id="二-CIFAR数据集"><a href="#二-CIFAR数据集" class="headerlink" title="(二) CIFAR数据集"></a>(二) CIFAR数据集</h5><p>用到的数据集是 CIFIAR，点击官网文档进行查看。</p>
<p><strong>(1) 数据集下载</strong></p>
<p><a href="https://www.cs.toronto.edu/~kriz/cifar.html">数据集官网</a>，该数据集包含了 60000 张 32*32 像素的 10 个类别的彩色图片，每个种类 6000 张图片，其中 60000 张中 50000 张是训练图片，10000 张是测试图片。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">若有 ssl 报错，添加如下代码</span></span><br><span class="line"><span class="string">import ssl</span></span><br><span class="line"><span class="string">ssl._create_default_https_context = ssl._create_unverified_context</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>然后运行，torchvision 就会自动进行 CIFAR10 数据集的下载。 </p>
<p>这里分别下载训练集和测试集，下载好后会放到所设置的路径下，这里下载的数据集会被放带当前目录的 dataset目录下。</p>
<p><strong>(2) 数据集的使用</strong></p>
<p>查看下测试数据集中每个数据包含什么：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line"></span><br><span class="line">ssl._create_default_https_context = ssl._create_unverified_context</span><br><span class="line"></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(test_set.classes)</span><br><span class="line"></span><br><span class="line">img, target = test_set[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"><span class="built_in">print</span>(test_set.classes[target])</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure>
<p>即一个数据单元里包含输入图片和对应的 tag，这里用数字进行映射，数字 3 也就是表示 cat ，可用 img.show() 查看下图片。因为这个数据集比较小，只有百 MB，图片像素只有 32*32，所以模糊，这里是将其分类为猫。 </p>
<h4 id="1-6-DataLoader类"><a href="#1-6-DataLoader类" class="headerlink" title="1.6 DataLoader类"></a>1.6 DataLoader类</h4><h5 id="一-简介-1"><a href="#一-简介-1" class="headerlink" title="(一) 简介"></a>(一) 简介</h5><p><strong>dataset类</strong>：在程序中起到的作用是告诉程序数据在哪，以及每个索引所对应的数据是什么。相当于一系列的存储单元，每个单元都存储了数据。这里可以类比成一幅扑克牌，一张扑克牌就是一个数据，一幅扑克牌就是一个完整的数据集。</p>
<p><strong>dataloader类</strong>： 是一个加载器，将数据加载到神经网络中。类比成手（神经网络），dataloader 每次从dataset 中去取数据，每次取多少，怎么取，通过 dataloader 参数进行设置。用手去抓扑克牌，每次抓几张，用一只手去抓取，还是用两只手，这就是 dataloader 要做的事，可以通过参数进行一个设置。</p>
<p><a href="https://blog.csdn.net/weixin_44227733/article/details/%5Btorch.utils.data%20%E2%80%94%20PyTorch%201.10%20documentation%5D%28https%3A//pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader%29">Pytoch 官网</a>对 dataloader 的介绍，各个参数都有详细的描述，这里就不再赘述。 </p>
<p><img src="https://pic.imgdb.cn/item/62668aa5239250f7c59926e0.jpg"></p>
<h5 id="二-dataloader-的使用"><a href="#二-dataloader-的使用" class="headerlink" title="(二) dataloader 的使用"></a>(二) dataloader 的使用</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor())</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据集中第一张图片及 target</span></span><br><span class="line">img, target = test_data[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img.shape)  <span class="comment"># 查看图片大小</span></span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;dataloader&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="comment"># print(imgs.shape)</span></span><br><span class="line">    <span class="comment"># print(targets)</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;test_data&quot;</span>, imgs, step)  <span class="comment"># 注意加 s</span></span><br><span class="line">    step = step + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：</p>
<p>1、shuffle打乱是指在epoch阶段打乱，不是mini_batch级打乱</p>
</blockquote>
<h3 id="第二章-神经网络的基本骨架nn-Module的使用"><a href="#第二章-神经网络的基本骨架nn-Module的使用" class="headerlink" title="第二章 神经网络的基本骨架nn.Module的使用"></a>第二章 神经网络的基本骨架nn.Module的使用</h3><h5 id="一-torch-nn简介"><a href="#一-torch-nn简介" class="headerlink" title="(一) torch.nn简介"></a>(一) torch.nn简介</h5><p>搭建<a href="https://so.csdn.net/so/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;spm=1001.2101.3001.7020">神经网络</a>常用的工具在 torch.nn 模块，<a href="https://pytorch.org/docs/stable/nn.html">官网</a></p>
<p><img src="https://pic.imgdb.cn/item/62668fd5239250f7c5a5f02b.jpg"></p>
<p>Containers 中文翻译为容器，但这里可以理解为骨架，往这个骨架中添加一些内容就可以构成一个神经网络。</p>
<p>Convolution Layers、Pooling Layers、Paading Layers 都是要添加进网络的各层。</p>
<p>Containers 中 Module 是最常用的，它是所有神经网络的基本类，给所有神经网络提供基本的骨架。</p>
<p><img src="https://pic.imgdb.cn/item/6266900d239250f7c5a68458.jpg"></p>
<p><img src="https://pic.imgdb.cn/item/62669121239250f7c5a91eef.jpg"></p>
<h5 id="二-简单示例"><a href="#二-简单示例" class="headerlink" title="(二) 简单示例"></a>(二) 简单示例</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># Alt+insert 可重写方法或实现方法（Windows）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)  <span class="comment"># 将 1.0 这个数转换成 tensor 数据类型</span></span><br><span class="line">output = jiaolong(x)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<h5 id="三-卷积操作"><a href="#三-卷积操作" class="headerlink" title="(三) 卷积操作"></a>(三) 卷积操作</h5><p>略</p>
<h3 id="第三章-神经网络"><a href="#第三章-神经网络" class="headerlink" title="第三章 神经网络"></a>第三章 神经网络</h3><h5 id="一-卷积层"><a href="#一-卷积层" class="headerlink" title="(一) 卷积层"></a>(一) 卷积层</h5><p>官网 Pytorch 的 nn 模块有 Convolution Layers，有3种卷积操作，nn.Conv1d、nn.Conv2d、nn.Conv3d 分别对应一维二维以及三维。</p>
<p>注：在Pytorch 官网文档左侧，有 torch.nn 和 torch.nn.fuctional，torch.nn 是对 torch.nn.fuctional 进行了一个封装，方便用户使用。想细致的了解一些 nn 模块中的函数可以从 torch.nn.fuctional 入手。这里主要介绍 nn.Conv2d，打开 torch.nn.fuctional 对应页面，可以看到对 conv2d 函数的介绍。</p>
<p>conv2d 需要的参数有 输入 input、权重 weight（更专业的名称是卷积核）、偏置 bias、步长 stride、填充Padding等。 </p>
<p><img src="https://pic.imgdb.cn/item/62673ce8239250f7c50e52b6.jpg"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">ssl._create_default_https_context = ssl._create_unverified_context</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line"><span class="built_in">print</span>(jiaolong)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = jiaolong(imgs)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    <span class="comment"># torch.Size([64, 3, 32, 32])</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)</span><br><span class="line">    <span class="comment"># torch.Size([64, 6, 30, 30]) -&gt; torch.Size([xx, 3, 30, 30])</span></span><br><span class="line">    output = torch.reshape(output, (-<span class="number">1</span>,<span class="number">3</span>, <span class="number">30</span>, <span class="number">30</span>))  <span class="comment"># 第一个数不知道是多少用 -1，它会自动根据值计算</span></span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output,step)</span><br><span class="line">    step += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/62673e27239250f7c510a187.jpg"></p>
<h5 id="二-池化层"><a href="#二-池化层" class="headerlink" title="(二) 池化层"></a>(二) 池化层</h5><p><strong>最大池化</strong>(又称“下采样”)的作用是在保存数据特征的前提下去减小数据量。 池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。本质是将采样，可以大幅减少网络参数量。</p>
<p><img src="https://pic.imgdb.cn/item/626745b0239250f7c51dba58.jpg"></p>
<p><code>ceil_mode:</code>当ceil_mode为 True时，将用<code>ceil</code>(向上取整)模式代替<code>floor</code>(向下取整) 模式去计算输出。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor())</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">36</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"># 最大池化无法对 Long 数据类型进行实现</span></span><br><span class="line"><span class="string">input = torch.tensor([[1, 2, 0, 3, 1],</span></span><br><span class="line"><span class="string">                     [0, 1, 2, 3, 1],</span></span><br><span class="line"><span class="string">                     [1, 2, 1, 0, 0],</span></span><br><span class="line"><span class="string">                     [5, 2, 3, 1, 1],</span></span><br><span class="line"><span class="string">                     [2, 1, 0, 1, 1]], dtype=torch.float32)</span></span><br><span class="line"><span class="string">input = torch.reshape(input, (-1, 1, 5, 5))</span></span><br><span class="line"><span class="string">print(input.shape)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.maxpool1 = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        output = self.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line"><span class="comment"># output = jiaolong(input)</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs_maxpool&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)</span><br><span class="line">    output = jiaolong(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h5 id="三-非线性激活"><a href="#三-非线性激活" class="headerlink" title="(三) 非线性激活"></a>(三) 非线性激活</h5><p>非线性激活主要目的就是给网络增加非线性特征，以便训练出符合要求的泛化模型。常用的有</p>
<p>ReLU函数、Sigmoid函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ReLU</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor([[<span class="number">1</span>, -<span class="number">0.5</span>],</span><br><span class="line">                      [-<span class="number">1</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        inplace 为替换的意思</span></span><br><span class="line"><span class="string">        如 input = -1 </span></span><br><span class="line"><span class="string">           ReLU(input, inplace = True)</span></span><br><span class="line"><span class="string">           input = 0</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.relu = ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        output = self.relu(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line">output = jiaolong(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<h5 id="四-正则化层Normalization"><a href="#四-正则化层Normalization" class="headerlink" title="(四) 正则化层Normalization"></a>(四) 正则化层Normalization</h5><p>将数据进行正则化可以加快神经网络的训练速度</p>
<h5 id="五-线性层"><a href="#五-线性层" class="headerlink" title="(五) 线性层"></a>(五) 线性层</h5><p>线性层又叫全连接层，其中每个神经元与上一层所有神经元相连，多看<a href="https://blog.csdn.net/weixin_44227733/article/details/%5Btorch.nn%20%E2%80%94%20PyTorch%201.10%20documentation%5D%28https%3A//pytorch.org/docs/stable/nn.html%29">官方文档</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear = Linear(<span class="number">196608</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        output = self.linear(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)</span><br><span class="line">    output = torch.flatten(imgs)  <span class="comment"># 展平</span></span><br><span class="line">    <span class="comment"># output = torch.reshape(imgs, (1, 1, 1, -1))</span></span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line">    output = jiaolong(output)</span><br><span class="line">    <span class="built_in">print</span>((output.shape))</span><br></pre></td></tr></table></figure>
<h5 id="六-神经网络-搭建小实战和Sequential的使用"><a href="#六-神经网络-搭建小实战和Sequential的使用" class="headerlink" title="(六) 神经网络-搭建小实战和Sequential的使用"></a>(六) 神经网络-搭建小实战和Sequential的使用</h5><p><strong>(1) Sequential的使用</strong></p>
<p>Sequential 是一个时序容器。Modules 会以他们传入的顺序被添加到容器中。包含在 <a href="https://blog.csdn.net/weixin_44227733/article/details/%5Btorch.nn%20%E2%80%94%20PyTorch%201.10%20documentation%5D%28https%3A//pytorch.org/docs/stable/nn.html%29">PyTorch 官网</a>中 torch.nn 模块中的 Containers 中，在<a href="https://so.csdn.net/so/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;spm=1001.2101.3001.7020">神经网络</a>搭建的过程中如果使用 Sequential，代码更简洁。 </p>
<p><strong>(2) 搭建神经网络小示例</strong></p>
<p><img src="https://pic.imgdb.cn/item/62676095239250f7c5567e82.jpg"></p>
<p><img src="https://pic.imgdb.cn/item/626760a6239250f7c556a3ea.jpg"></p>
<p>搭建上述神经网络的具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Jiaolong, self).__init__()</span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool1 = MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool2 = MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        self.conv3 = Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool3 = MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        self.flatten = Flatten()</span><br><span class="line">        self.linear1 = Linear(<span class="number">1024</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear2 = Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line"><span class="built_in">print</span>(jiaolong)</span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))  <span class="comment"># 指定数据创建的形状，都是1</span></span><br><span class="line">output = jiaolong(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure>
<p>现以Sequential搭建上述一模一样的神经网络，并借助tensorboard显示计算图的具体信息。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Jiaolong, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line"><span class="comment"># print(jiaolong)</span></span><br><span class="line"><span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))  <span class="comment"># 指定数据创建的形状，都是1</span></span><br><span class="line">output = jiaolong(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># print(output.shape)</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">writer.add_graph(jiaolong, <span class="built_in">input</span>)  <span class="comment"># 计算图</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h3 id="第四章-损失函数与反向传播"><a href="#第四章-损失函数与反向传播" class="headerlink" title="第四章 损失函数与反向传播"></a>第四章 损失函数与反向传播</h3><h4 id="4-1-交叉熵基本表示"><a href="#4-1-交叉熵基本表示" class="headerlink" title="4.1 交叉熵基本表示"></a>4.1 交叉熵基本表示</h4><p>交叉熵常用于分类问题的loss函数，其形式为</p>
<script type="math/tex; mode=display">
\mathrm{loss}(x, class) = -\log(\frac{\exp(x[class])}{\sum_{j}\exp(x[j])}) = -x[class]+\log(\sum_{j}exp(x[j]))</script><h4 id="4-2-示例"><a href="#4-2-示例" class="headerlink" title="4.2 示例"></a>4.2 示例</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss, MSELoss</span><br><span class="line"></span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">inputs = torch.reshape(inputs, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">loss = L1Loss(reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">result = loss(inputs, targets)</span><br><span class="line"></span><br><span class="line">loss_mse = MSELoss()</span><br><span class="line">result_mse = loss_mse(inputs, targets)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result_mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 交叉熵</span></span><br><span class="line">x = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line">y = torch.tensor([<span class="number">1</span>])</span><br><span class="line">x = torch.reshape(x, (<span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">loss_cross = nn.CrossEntropyLoss()</span><br><span class="line">result_cross = loss_cross(x, y)</span><br><span class="line"><span class="built_in">print</span>(result_cross)</span><br></pre></td></tr></table></figure>
<p>现在以 CIFAR10 数据集为例，在上一篇文章中最后搭建的神经网络中使用 CrossEntropyLoss 函数作为<a href="https://so.csdn.net/so/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020">损失函数</a>，讲解在神经网络中如何使用损失函数。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Sequential, Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;D:\Code\Project\learn_pytorch\pytorch_p17-21\data&quot;</span>, 					train=<span class="literal">False</span>,download=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor())</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Jiaolong, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = jiaolong(imgs)</span><br><span class="line">    result_loss = loss(outputs, targets)</span><br><span class="line">    <span class="built_in">print</span>(result_loss)</span><br></pre></td></tr></table></figure>
<h3 id="第五章-优化器"><a href="#第五章-优化器" class="headerlink" title="第五章 优化器"></a>第五章 优化器</h3><p>优化器：神经网络的学习的目的就是寻找合适的参数，使得损失函数的值尽可能小。解决这个问题的过程为称为最优化。解决这个问题使用的算法叫做优化器。在 PyTorch 官网中，将优化器放置在 torch.optim 中，并详细介绍了各种优化器的使用方法。</p>
<p>现以 CIFAR10 数据集为例，损失函数选取交叉熵函数，优化器选择 SGD 优化器，搭建神经网络，并计算其损失值，用优化器优化各个参数，使其朝梯度下降的方向调整。设置 epoch，让其执行 20 次，并将每一次完整的训练的损失函数值求和输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Sequential, Conv2d, MaxPool2d, Flatten, Linear</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;D:\Code\Project\learn_pytorch\pytorch_p17-21\data&quot;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor())</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Jiaolong, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line"><span class="comment"># 构建 SGD 优化器，其中 jiaolong.parameters() 表示：待优化参数的 iterable 或者是定义了参数组的 dict，lr=0.01 表示学习率</span></span><br><span class="line">optim = torch.optim.SGD(jiaolong.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = jiaolong(imgs)</span><br><span class="line">        result_loss = loss(outputs, targets)</span><br><span class="line">        <span class="comment"># 将上一轮计算的梯度清零，避免上一轮的梯度值会影响下一轮的梯度值计算</span></span><br><span class="line">        optim.zero_grad()</span><br><span class="line">        <span class="comment"># 反向传播过程，在反向传播过程中会计算每个参数的梯度值</span></span><br><span class="line">        result_loss.backward()</span><br><span class="line">        <span class="comment"># 所有的 optimizer 都实现了 step() 方法，该方法会更新所有的参数。</span></span><br><span class="line">        optim.step()</span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line">    <span class="built_in">print</span>(running_loss)</span><br></pre></td></tr></table></figure>
<h3 id="第六章-现有网络模型的使用与修改"><a href="#第六章-现有网络模型的使用与修改" class="headerlink" title="第六章 现有网络模型的使用与修改"></a>第六章 现有网络模型的使用与修改</h3><p>PyTorch是一个开源的Python机器学习库，基于Torch，用于自然语言处理等应用程序。他提供了大量的模型供我们所使用，如下图所示： </p>
<p><img src="https://pic.imgdb.cn/item/6267a215239250f7c5f7a34d.jpg"></p>
<p>下面，我们选择其中一个网络进行使用，介绍如何使用、并修改 pytorch 本身为我们提供的现有网络。最后介绍一下模型的保存和修改。</p>
<h4 id="6-1-pytorch-现有网络的使用与修改"><a href="#6-1-pytorch-现有网络的使用与修改" class="headerlink" title="6.1 pytorch 现有网络的使用与修改"></a>6.1 pytorch 现有网络的使用与修改</h4><p>下面以 VGG(Very Deep Convolutional Networks for Large-Scale Image Recognition)的使用为例，进行介绍该网络。</p>
<h5 id="一-VGG-16-简介"><a href="#一-VGG-16-简介" class="headerlink" title="(一) VGG 16 简介"></a>(一) VGG 16 简介</h5><p>VGG16网络是14年牛津大学计算机视觉组和Google DeepMind公司研究员一起研发的深度网络模型。该网络一共有16个训练参数的网络，该网络的具体网络结构如下所示：</p>
<p><img src="https://pic.imgdb.cn/item/6267a311239250f7c5f9aae8.jpg"></p>
<p>不难看出，该网络主要用于对 224 x 224 的图像进行1000分类。下面我们查看 VGG 在 pytorch 上的官方文档。</p>
<h5 id="二-VGG-16-doc"><a href="#二-VGG-16-doc" class="headerlink" title="(二) VGG 16 doc"></a>(二) VGG 16 doc</h5><p>从帮助文档中，我们可以清楚的看到 pytorch 为我们提供了各种版本的 VGG，我们选择 VGG 16 进行查看。</p>
<p><img src="https://pic.imgdb.cn/item/6267a34e239250f7c5fa28a9.jpg"></p>
<h5 id="三-VGG16-的简单使用"><a href="#三-VGG16-的简单使用" class="headerlink" title="(三) VGG16 的简单使用"></a>(三) VGG16 的简单使用</h5><p>从VGG 16的帮助文档可以得知，该模型训练的数据是 <code>ImageNet</code>，我们进入 torchvision.datasets 查看 ImageNet</p>
<p>但是该数据集实在是太大了，根本下不了，还是不搞了。建立一个该网络的模型查看参数: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torchvision </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn </span><br><span class="line"><span class="comment"># import torchvision.models</span></span><br><span class="line"></span><br><span class="line">vgg_model_pretrained = torchvision.models.vgg16(pretrained=<span class="literal">True</span>, progress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">vgg_model_original = torchvision.models.vgg16(pretrained=<span class="literal">False</span>, progress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg_model_original)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg_model_pretrained)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="6-2-修改网络"><a href="#6-2-修改网络" class="headerlink" title="6.2 修改网络"></a>6.2 修改网络</h4><p>使用以下语句可以实现对网络的修改</p>
<h5 id="一-添加网络结构"><a href="#一-添加网络结构" class="headerlink" title="(一) 添加网络结构"></a>(一) 添加网络结构</h5><p><code>vgg_model_pretrained.add_module()</code></p>
<h5 id="二-修改网络结构"><a href="#二-修改网络结构" class="headerlink" title="(二) 修改网络结构"></a>(二) 修改网络结构</h5><p><code>vgg_model_original.classifier[num]</code></p>
<blockquote>
<p>num表示要修改网络的哪一层</p>
</blockquote>
<h5 id="三-伪代码示例"><a href="#三-伪代码示例" class="headerlink" title="(三) 伪代码示例"></a>(三) 伪代码示例</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 添加：vgg_model_pretrained.add_module()</span></span><br><span class="line">vgg_model_original.classifier.add_module(<span class="string">&#x27;15&#x27;</span>, nn.Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>))</span><br><span class="line"><span class="built_in">print</span>(vgg_model_original)</span><br><span class="line"><span class="comment"># 修改：vgg_model_original.classifier[num]</span></span><br><span class="line">vgg_model_original.classifier[<span class="number">7</span>] = nn.Linear(in_features=<span class="number">1000</span>, out_features=<span class="number">15</span>, bias=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg_model_original)</span><br></pre></td></tr></table></figure>
<h3 id="第七章-网络模型的保存与读取"><a href="#第七章-网络模型的保存与读取" class="headerlink" title="第七章 网络模型的保存与读取"></a>第七章 网络模型的保存与读取</h3><p>在搭建自己的神经网络模型之后，需要将模型进行保存，同时也需要读取或加载现有的神经网络模型。</p>
<h4 id="7-1-加载模型"><a href="#7-1-加载模型" class="headerlink" title="7.1 加载模型"></a>7.1 加载模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># File : model_load.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1 -&gt; 保存方式1，加载模型</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2 ，加载模型</span></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">vgg16.load_state_dict(torch.load(<span class="string">&quot;vgg16_method2.pth&quot;</span>))</span><br><span class="line"><span class="comment"># model = torch.load(&quot;vgg16_method2.pth&quot;)</span></span><br><span class="line"><span class="built_in">print</span>(vgg16)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式 1 陷阱</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Jiaolong</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">jiaolong = Jiaolong()</span><br><span class="line">torch.save(jiaolong, <span class="string">&#x27;jiaolong_method1.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="7-2-保存模型"><a href="#7-2-保存模型" class="headerlink" title="7.2 保存模型"></a>7.2 保存模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># File : model_save.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 保存方式 1，模型结构+模型参数</span></span><br><span class="line">torch.save(vgg16, <span class="string">&quot;vgg16_method1.pth&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存方式2，模型参数（官方推荐）</span></span><br><span class="line">torch.save(vgg16.state_dict(), <span class="string">&#x27;vgg16_method2.pth&#x27;</span>)  <span class="comment"># 将模型的状态（参数）保存成字典形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式 1 陷阱，需要将模型写过来，但无需创建实例</span></span><br><span class="line"><span class="comment"># 可以引入，from model_save import *</span></span><br><span class="line">model = torch.load(<span class="string">&#x27;jiaolong_method1.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)  <span class="comment"># 报错</span></span><br></pre></td></tr></table></figure>
<h3 id="第八章-完整地模型训练-验证套路"><a href="#第八章-完整地模型训练-验证套路" class="headerlink" title="第八章 完整地模型训练/验证套路"></a>第八章 完整地模型训练/验证套路</h3><h4 id="8-1-训练套路"><a href="#8-1-训练套路" class="headerlink" title="8.1 训练套路"></a>8.1 训练套路</h4><h4 id="8-2-验证套路"><a href="#8-2-验证套路" class="headerlink" title="8.2 验证套路"></a>8.2 验证套路</h4><p>验证不是指的时在测试集上测试，而是找一张没有在数据集中的图片进行训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">image_path = <span class="string">&quot;imgs/dog.png&quot;</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line"><span class="built_in">print</span>(image)</span><br><span class="line">image = image.convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line"><span class="comment"># 因为png格式为四个通道，除了RGB三个通道外，还有一个透明度通道。所以调用此语句保留其颜色通道。</span></span><br><span class="line"></span><br><span class="line">transform = torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">                                            torchvision.transforms.ToTensor()])</span><br><span class="line"><span class="comment"># 这个照片size=258x208，因为模型为32×32，所以改一下</span></span><br><span class="line">image = transform(image)</span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载网络模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tudui</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()</span><br><span class="line">        <span class="comment"># 为了避免上下两个def都写一整串，将整个网络放到序列当中，前边有讲过，注释了很多代码有老乡的那一篇</span></span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),   <span class="comment">#  卷积</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),             <span class="comment">#  池化</span></span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">64</span>),   <span class="comment"># 最后两步的展平</span></span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&quot;tudui_0.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># img输入到这个模型中</span></span><br><span class="line"><span class="comment"># 这里一开始用的gpu的模型，会报错，换成cpu的&quot;tudui_0.pth&quot;模型就行了</span></span><br><span class="line">image = torch.reshape(image, (<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"><span class="comment"># 这几步比较重要且容易忘记======================</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(image)</span><br><span class="line"><span class="comment"># 这几步比较重要且容易忘记======================</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="comment"># 显示是哪一类的概率最大</span></span><br><span class="line"><span class="built_in">print</span>(output.argmax(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h3 id="第九章-利用GPU进行训练"><a href="#第九章-利用GPU进行训练" class="headerlink" title="第九章 利用GPU进行训练"></a>第九章 利用GPU进行训练</h3><h4 id="9-1-方法-1"><a href="#9-1-方法-1" class="headerlink" title="9.1 方法 1"></a>9.1 方法 1</h4><p>采用方法 1 实现GPU训练网络模型只需要将原来的 CPU 版本的代码进行小量修改即可，具体修改的位置包括下面3个地方：</p>
<ul>
<li>网络模型</li>
<li>数据（输入、标注）</li>
<li>损失函数</li>
</ul>
<p>只需找到上述 3 个位置的代码加上<code>.cuda()</code>操作即可实现将CPU版本的代码修改为GPU版本的代码，现以上一篇博文中完整的模型训练代码为例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 网络模型</span></span><br><span class="line">junheng = junheng.cuda()</span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_function = loss_function.cuda()</span><br><span class="line"><span class="comment"># 数据（输入、标注）——包括训练和测试部分</span></span><br><span class="line">imgs = imgs.cuda()</span><br><span class="line">labels = labels.cuda()</span><br></pre></td></tr></table></figure>
<p>注意：最好可以在使用<code>cuda()</code>前加一个判断<code>if torch.cuda.is_available()</code></p>
<h4 id="9-2-方法2"><a href="#9-2-方法2" class="headerlink" title="9.2 方法2"></a>9.2 方法2</h4><p>打开 google colab（科学上网）</p>
<h4 id="9-3-方法3"><a href="#9-3-方法3" class="headerlink" title="9.3 方法3"></a>9.3 方法3</h4><p>用的<code>.to</code>操作来指定模型训练的设备，具体是CPU还是GPU，如果是GPU，还能具体到时哪一块GPU。</p>
<p>具体方法总结如下：</p>
<ul>
<li>定义具体的 device，例如：device=torch.device(“cpu”) 或者 device=torch.device(“cuda:0”) 或者 device=torch.device(“cuda:1”)；</li>
<li>利用 .to() 操作训练使用的具体设备，例如 .to(device)。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 定义训练设备</span></span><br><span class="line">mydevice = torch.device(<span class="string">&quot;cpu&quot;</span>)	<span class="comment"># 设备在cpu运行</span></span><br><span class="line">mydevice = torch.device(<span class="string">&quot;cuda&quot;</span>)	<span class="comment"># 设备在gpu运行</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">另一种写法</span></span><br><span class="line"><span class="string">mydevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 网络模型使用训练设备</span></span><br><span class="line">junheng = junheng.to(mydevice)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 损失函数使用训练设备</span></span><br><span class="line">loss_function = loss_function.to(mydevice)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 数据（输入、标注）使用训练设备——包括训练和测试部分</span></span><br><span class="line">imgs = imgs.to(mydevice)</span><br><span class="line">labels = labels.to(mydevice)</span><br></pre></td></tr></table></figure>
<h3 id="参考笔记"><a href="#参考笔记" class="headerlink" title="参考笔记"></a>参考笔记</h3><p><a href="https://so.csdn.net/so/search?q=PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&amp;t=blog&amp;u=weixin_44227733">PyTorch深度学习笔记 - 小于同学饿了 - CSDN</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>编程与深度学习</category>
        <category>深度学习基础</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Pytorch基础</tag>
      </tags>
  </entry>
  <entry>
    <title>信息搜索技巧</title>
    <url>/2023/01/12/Google-Search-Tips/</url>
    <content><![CDATA[<h3 id="1-搜索引擎命令大全"><a href="#1-搜索引擎命令大全" class="headerlink" title="1 搜索引擎命令大全"></a>1 搜索引擎命令大全</h3><h4 id="1-1-site：在某个网站或域名下搜索"><a href="#1-1-site：在某个网站或域名下搜索" class="headerlink" title="1.1 site：在某个网站或域名下搜索"></a>1.1 site：在某个网站或域名下搜索</h4><blockquote>
<p>在特定的网站下进行检索，不仅可以搜索网页，还可以搜索某个站点下的所有图片、视频、新闻等。 例如：樱花 site:http ://edu.cn表示在中国教育网下搜索樱花</p>
</blockquote>
<p><strong>使用方式以及示例：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">二战纪录片 site:zhihu.com; baidu.com</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfae4ebe43e0d30ea30f62.jpg"></p>
<p><strong>作用：</strong></p>
<ul>
<li>替代站内搜索</li>
<li>指定网站搜索、缩小范围、提供精准结果</li>
<li>搜索不能直接访问的网站；死网站；绕过注册</li>
</ul>
<h4 id="1-2-Filetype：搜索某种类型的文件"><a href="#1-2-Filetype：搜索某种类型的文件" class="headerlink" title="1.2 Filetype：搜索某种类型的文件"></a>1.2 Filetype：搜索某种类型的文件</h4><blockquote>
<p>搜索指定的文件类型，用来查找文献、电子书、PPT 等非常有用 支持的文件类型：ppt、ppts、doc、docx、xlsx、pdf、txt 等（不同的搜索引擎支持的文件类型不同）</p>
</blockquote>
<p><strong>使用方式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">百年孤独 filetype:pdf</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfaed9be43e0d30ea3fedc.jpg"></p>
<h4 id="1-3-逻辑与、或：AND、OR"><a href="#1-3-逻辑与、或：AND、OR" class="headerlink" title="1.3 逻辑与、或：AND、OR"></a>1.3 逻辑与、或：AND、OR</h4><ul>
<li>注意AND、OR必须大写，否则会被认为是普通的单词，而不是搜索指令。</li>
<li>与普通编程语言不一样的是，OR指令优先于AND指令。</li>
<li>简写：AND指令一般以空格代替，不明确写出。另外AND指令还可以用+代替。OR指令可用 | 代替。</li>
</ul>
<p><strong>使用方式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">手机 OR 相机 电脑 OR 鼠标 OR 键盘</span><br><span class="line">它等同于 (手机 OR 相机) (电脑 OR 鼠标 OR 键盘)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfb025be43e0d30ea61e37.jpg"></p>
<p><strong>4、逻辑非：-</strong></p>
<blockquote>
<p>逻辑非 - ，也就是减号，代表搜索不包含减号后面的词的页面。使用这个指令时减号前面必须是空格，减号后面没有空格，紧跟着需要排除的词。Google 和 baidu 都支持这个指令。</p>
</blockquote>
<p><strong>使用方式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">搜索引擎 历史 -文化 -中国历史 -世界历史</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfb271be43e0d30eac68af.jpg"></p>
<p><strong>5、双引号</strong></p>
<blockquote>
<p>把搜索词放在双引号中，代表完全匹配搜索，也就是说搜索结果返回的页面包含双引号中出现的所有的词，连顺序也必须完全匹配。bd和Google 都支持这个指令。</p>
</blockquote>
<p><strong>使用方法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">“模拟”信号处理</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfb1a0be43e0d30ea9c058.jpg"></p>
<h4 id="1-6-通配符：-和"><a href="#1-6-通配符：-和" class="headerlink" title="1.6 通配符：?和*"></a>1.6 通配符：?和*</h4><ul>
<li>与我们通常的习惯一致，“*”代表一连串字符，“?”代表单个字符。</li>
<li>样例：</li>
</ul>
<p>使用方法：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">示例<span class="number">1</span>：progr?m</span><br><span class="line"># 不要指望Google可用通过progr?m搜索到program，因为progr和m分别被当作一个独立的单词</span><br><span class="line"># 并且这两个单词之间允许出现一个字符，如此而已。</span><br><span class="line"></span><br><span class="line">示例<span class="number">2</span>：the * of money</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfb33cbe43e0d30eae38b9.jpg"></p>
<p><img src="https://pic.imgdb.cn/item/63bfb54bbe43e0d30eb18c22.jpg"></p>
<h4 id="1-7-in指令-位置关键词查找"><a href="#1-7-in指令-位置关键词查找" class="headerlink" title="1.7 in指令: 位置关键词查找"></a>1.7 in指令: 位置关键词查找</h4><ul>
<li>intitle: 在网页标题中查找。这通常对讨论比较多的热门话题很有效。<ul>
<li>例如：intitle:”GeForce 7800”+测试</li>
</ul>
</li>
<li>allintitle: 该指令属于排他指令，不能与其他指令混用。<ul>
<li>例如：allintitile:”GeForce 7800” 测试 与 intitle:”GeForce 7800”+测试 的含义一致。但是，allintitile是排他的，不能加上其他非intitle方面的限制条件。</li>
<li>注意：在这里，你会发现用+代替AND指令是很有意思的。如果没有+指令，我们就需要写：intitle:”GeForce 7800” intitle:测试，因为Google不支持这样的写法：intitle:(“GeForce 7800” 测试)</li>
</ul>
</li>
<li>inurl: 在网页的url地址中查找。<ul>
<li>例如：inurl:dowload 表示查找url中包含download的网页。</li>
</ul>
</li>
<li>allinurl: 结果中带有“XXX”和“YYY”，相当于“inurl:XXX inurl:YYY”</li>
<li>inanchor: 在网页的链接锚中查找。<ul>
<li>需要注意区别于inurl：inurl是网页本身的url地址，而inanchor是查找网页内容中的超链接。</li>
<li>例如：inanchor:download，你可能会发现有FlashGet最佳的下载管理程式，而该页面中根本就没有download字样。</li>
</ul>
</li>
<li>allinanchor</li>
<li>intext: 在正文中检索。</li>
<li>allintext</li>
</ul>
<h5 id="1-7-1-inurl"><a href="#1-7-1-inurl" class="headerlink" title="1.7.1 inurl"></a>1.7.1 inurl</h5><blockquote>
<p>用于指定搜索查询词出现在url 中的页面。baidu 和Google 都支持inurl 指令。inurl 指令支持中文和英文。</p>
</blockquote>
<p><strong>使用方法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">inurl:大学</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfb648be43e0d30eb309a0.jpg"></p>
<h5 id="1-7-2-inanchor"><a href="#1-7-2-inanchor" class="headerlink" title="1.7.2 inanchor"></a>1.7.2 inanchor</h5><blockquote>
<p>inanchor:指令返回的结果是导入链接锚文字中包含搜索词的页面。百度不支持inanchor。 比如在Google 搜索 ：inanchor:点击这里 返回的结果页面本身并不一定包含“点击这里”这四个字，而是指向这些页面的链接锚文字中出现了“点击这里”这四个字。 可以用来找到某个关键词的竞争对收，而且这些竞争对手往往是做过SEO 的。研究竞争对手页面有哪些外部链接，就可以找到很多链接资源。</p>
</blockquote>
<p><strong>使用方法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">inanchor:点击这里</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfb74cbe43e0d30eb4bf3b.jpg"></p>
<h5 id="1-7-3-intitle"><a href="#1-7-3-intitle" class="headerlink" title="1.7.3 intitle"></a>1.7.3 intitle</h5><blockquote>
<p>指定返回的页面 title 中包含关键词。Google 和bd都支持intitle 指令。</p>
</blockquote>
<p><strong>使用方法：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">intitle:知乎科技</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfb787be43e0d30eb5333f.jpg"></p>
<h5 id="1-7-4-allintitle"><a href="#1-7-4-allintitle" class="headerlink" title="1.7.4 allintitle"></a>1.7.4 allintitle</h5><blockquote>
<p>搜索返回的是页面标题中包含多组关键词的文件。 例如 ：allintitle:知乎 手机 科技 就相当于：intitle:知乎 intitle:手机 intitle:科技，返回的是标题中中既包含“知乎”，也包含“手机”和”科技”的页面</p>
</blockquote>
<p><strong>使用方式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">allintitle:知乎 手机 科技</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfb81cbe43e0d30eb62a14.jpg"></p>
<h4 id="1-8-link-搜索所有链接到某个URL地址的网页"><a href="#1-8-link-搜索所有链接到某个URL地址的网页" class="headerlink" title="1.8 link: 搜索所有链接到某个URL地址的网页"></a>1.8 link: 搜索所有链接到某个URL地址的网页</h4><ul>
<li>最重要的用途：迅速判断一个网页的“热度”。</li>
<li>例如：link:<a href="http://www.newhua.com">http://www.newhua.com</a> 表示所有指向“华军软件园”外部链接。</li>
<li>注意：和其他指令类似，link:和url之间不能有空格。之所以特别指出这一点，是因为网上有教程说link和allinurl等一样，是排他指令，中间可以有空格。我不太了解Google搜索指令的发展历史，只能猜测曾经如此。</li>
</ul>
<p><strong>使用方式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">link:www.newhua.com</span><br></pre></td></tr></table></figure>
<h4 id="1-9-related-寻找某网页的“类似网页”"><a href="#1-9-related-寻找某网页的“类似网页”" class="headerlink" title="1.9 related: 寻找某网页的“类似网页”"></a>1.9 related: 寻找某网页的“类似网页”</h4><ul>
<li>例如：related:<a href="http://www.newhua.com">http://www.newhua.com</a> 表示找和“华军软件园”类似的网页。</li>
<li>这条指令其实Google自己用得很广泛，只是可能没有引起你的注意罢了。在Google的每条搜索结果中，均有一个“类似网页”的链接。就是它了。</li>
</ul>
<p><strong>使用方式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">related:www.newhua.com</span><br></pre></td></tr></table></figure>
<h4 id="1-10-数值范围："><a href="#1-10-数值范围：" class="headerlink" title="1.10 数值范围：.."></a>1.10 数值范围：..</h4><ul>
<li>例如：数码相机 600..900 万像素 3000..4000 元</li>
<li>注意：“900”与“万”之间必须有空格。</li>
</ul>
<p><strong>使用方式：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">数码相机 600..900 万像素 3000..4000 元</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/63bfb914be43e0d30eb7abde.jpg"></p>
<h3 id="2-高级搜索技巧"><a href="#2-高级搜索技巧" class="headerlink" title="2 高级搜索技巧"></a>2 高级搜索技巧</h3><blockquote>
<p>google 在界面上提供搜索条件查询「限制语言」、「按照时间」、「精确匹配」（百度等其他同理）</p>
</blockquote>
<p><img src="https://pic.imgdb.cn/item/63bfb9c7be43e0d30eb93c45.jpg"></p>
<blockquote>
<p>google 高级搜索界面，其实就是前面介绍的命令的界面版（百度等其他同理） </p>
</blockquote>
<p><img src="https://pic.imgdb.cn/item/63bfba8dbe43e0d30eba9658.jpg"></p>
<p><img src="https://pic.imgdb.cn/item/63bfbaf5be43e0d30ebb408e.jpg"></p>
<p>相关链接：<a href="https://www.freecodecamp.org/chinese/news/how-to-google-like-a-pro-10-tips-for-effective-googling/">如何像专家一样高效使用 Google 搜索</a></p>
<hr>
<p>本文转自简书(侵删)：<br>作者：非凡公社<br>链接：<a href="https://www.jianshu.com/p/22aae651cc68">https://www.jianshu.com/p/22aae651cc68</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
      </categories>
      <tags>
        <tag>搜索技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数_Part1</title>
    <url>/2022/05/03/LinearAlgebra1/</url>
    <content><![CDATA[<h3 id="1-线性代数基础"><a href="#1-线性代数基础" class="headerlink" title="1 线性代数基础"></a>1 线性代数基础</h3><h4 id="1-1-方程组的几何解释基础"><a href="#1-1-方程组的几何解释基础" class="headerlink" title="1.1 方程组的几何解释基础"></a>1.1 方程组的几何解释基础</h4><p>本节主要介绍线性代数的基础。首先从解方程开始，学习线性代数的应用之一就是求解复杂的方程问题，本节核心之一就是从row picture(行图像)和column picture(列图像)的角度解方程。</p>
<h5 id="1-1-1-二维行图像"><a href="#1-1-1-二维行图像" class="headerlink" title="1.1.1 二维行图像"></a>1.1.1 二维行图像</h5><p>如下所示，一个普通的方程组：</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{r}
2 x-y=0 \\
-x+2 y=3
\end{array}\right.</script><p>按行将方程组写成矩阵形式：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{cc}
2 & -1 \\
-1 & 2
\end{array}\right]\left[\begin{array}{l}
x \\
y
\end{array}\right]=\left[\begin{array}{l}
0 \\
3
\end{array}\right]</script><p>分别可以记为：</p>
<ul>
<li>系数矩阵($\boldsymbol A$): 将方程组系数按行提取出来，构造完成的一个矩阵。</li>
</ul>
<ul>
<li>未知向量($\boldsymbol x$): 将方程组的未知数提取出来，按列构成一个向量。 </li>
</ul>
<ul>
<li>向量($\boldsymbol b$): 将等号右侧结果按列提取，构成一个向量。</li>
</ul>
<p>从行的角度来看，$2x-y=0$和$-x+2y = 3$分别表示两条二维平面中的直线，如果这两条直线相交，那么交点的坐标$(x^<em>, y^</em>)$即为方程组的解。</p>
<p>更确切的讲，如果两条直线相交于一点，那么该方程组有且仅有一个解，即为交点的坐标；如果两条直线重合，那么说明这两条直线方程实际上是同一条直线，此时直线上的所有点的坐标均为方程组的解；如果两条直线平行但不重合，则说明不存在点的坐标同时满足这两条直线的方程，此时方程组无解。</p>
<h5 id="1-1-2-二维列图像"><a href="#1-1-2-二维列图像" class="headerlink" title="1.1.2 二维列图像"></a>1.1.2 二维列图像</h5><p>从列图像的角度，再次求解上面的方程，即将方程按列提取，得到的矩阵为：</p>
<script type="math/tex; mode=display">
x\left[\begin{array}{c}
2 \\
-1
\end{array}\right]+y\left[\begin{array}{c}
-1 \\
2
\end{array}\right]=\left[\begin{array}{l}
0 \\
3
\end{array}\right]</script><p>使用列向量构成系数矩阵，将问题转化为：将向量$\boldsymbol \alpha=\left[\begin{array}{c}2 \\ -1\end{array}\right]$与向量$\boldsymbol \beta=\left[\begin{array}{c}-1 \\ 2\end{array}\right]$ 任意组合，使其结果构成$\boldsymbol \gamma=\left[\begin{array}{c}0 \\ 3\end{array}\right]$，也就是“线性组合”，它是贯穿线性代数的基本方法。$x, y$称为线性组合的系数，因此线性方程组就可以理解为：<br>是否存在合适的线性组合系数$x, y$，使得$\boldsymbol \alpha, \boldsymbol \beta$的线性组合 $x\boldsymbol \alpha + y\boldsymbol \beta$恰好等于$\boldsymbol \gamma$。如果存在，线性组合的系数为多少？</p>
<p>值得一提的是，从列的角度看待线性方程组是一种非常重要的理解方式，以后会经常用到这样的思想。</p>
<p>对于一般的$n$维线性方程组$\boldsymbol{Ax = b}$，其中$\boldsymbol A$是$n \times n$维系数矩阵，$\boldsymbol x$是$n$维列向量。$\boldsymbol b$是方程组右端的$n$维列向量。不妨设$\boldsymbol A$由$n$个列向量$(\boldsymbol \alpha_1, \boldsymbol \alpha_2, \cdots, \boldsymbol \alpha_n)$组成，$\boldsymbol x=\left[\begin{array}{c}x_1 \\ x_2 \\ \vdots \\ x_n\end{array}\right]$，则方程组$\boldsymbol{Ax = b}$可以表示为：</p>
<script type="math/tex; mode=display">
\left(\boldsymbol \alpha_{1}, \boldsymbol \alpha_{2}, \cdots, \boldsymbol \alpha_{n}\right)\left(\begin{array}{c}
x_1 \\ x_2 \\ \vdots \\ x_n \end{array}\right)= \boldsymbol b</script><p>即</p>
<script type="math/tex; mode=display">
x_1 \boldsymbol \alpha_{1} + x_2 \boldsymbol \alpha_{2} + \cdots + x_n\boldsymbol \alpha_{n} = \boldsymbol b</script><p>由此可以看出，矩阵$\boldsymbol A$乘以向量$\boldsymbol x$相当于对$\boldsymbol A$的$n$个列向量作线性组合，线性组合的系数即为向量$\boldsymbol x$各对应的分量。因此对线性方程组可以理解为：是否存在合适的线性组合系数，使得$\boldsymbol A$的列向量的线性组合恰好为$\boldsymbol b$。如果存在，线性组合的系数为多少？这些线性组合的系数就构成了$\boldsymbol{Ax = b}$的解向量$\boldsymbol x$。</p>
<h4 id="1-2-线性方程组有解情况"><a href="#1-2-线性方程组有解情况" class="headerlink" title="1.2 线性方程组有解情况"></a>1.2 线性方程组有解情况</h4><p>首先考虑对于任意的$n$维列向量$\boldsymbol x$，当$\boldsymbol x$变动时，$\boldsymbol{Ax}$也在变动，当 $\boldsymbol x$取遍所有的$n$维列向量时，$\boldsymbol{Ax}$就能取遍所有$\boldsymbol A$的列向量的线性组合，也就是说，所有的$\boldsymbol{Ax}$就构成了$\boldsymbol A$的列向量张成的线性空间$\boldsymbol{V} = \text{span} \begin{Bmatrix} \boldsymbol \alpha_1, \boldsymbol \alpha_2, \cdots, \boldsymbol \alpha_n \end{Bmatrix}$ (span是一组集合，它包含两个向量之间的全部线性组合)，因此：</p>
<script type="math/tex; mode=display">
\boldsymbol{Ax = b}有解 \Longleftrightarrow \boldsymbol b \in \boldsymbol{V} = \text{span} \begin{Bmatrix} \boldsymbol \alpha_1, \boldsymbol \alpha_2, \cdots, \boldsymbol \alpha_n \end{Bmatrix}</script><p>又由于</p>
<script type="math/tex; mode=display">
\boldsymbol b \in \boldsymbol{V} = \text{span} \begin{Bmatrix} \boldsymbol \alpha_1, \boldsymbol \alpha_2, \cdots, \boldsymbol \alpha_n \end{Bmatrix}</script><p>因此也就得出了</p>
<script type="math/tex; mode=display">
\boldsymbol{Ax = b}有解 \Longleftrightarrow \text{Rank}(\boldsymbol A) = \text{Rank}(\boldsymbol A, \boldsymbol b)</script><p>特别地，若$\boldsymbol A$的$n$个列向量线性无关，则这$n$个列向量就构成了$n$维向量空间的一组基。此时对任意的向量均可由$\boldsymbol A$的列向量线性表出，也即是一定有解。换言之，如果$\boldsymbol A$<strong>可逆/非奇异</strong>，则一定有解。</p>
<h4 id="1-3-矩阵乘法理解"><a href="#1-3-矩阵乘法理解" class="headerlink" title="1.3 矩阵乘法理解"></a>1.3 矩阵乘法理解</h4><p>有了对线性方程组的这些认识，我们可以更好地理解矩阵乘法。</p>
<p><strong>(一) 向量右乘矩阵</strong></p>
<p>首先考虑列向量$\boldsymbol x \in \mathbb{R}^n$右乘矩阵$\boldsymbol A \in \mathbb{R}^{n \times n}$。先从行的角度考虑，不妨设：</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{c}
\boldsymbol{a}_{1}^{\mathrm{T}} \\
\boldsymbol{\alpha}_{2}^{\mathrm{T}} \\
\vdots \\
\boldsymbol{\alpha}_{n}^{\mathrm{T}}
\end{array}\right), \boldsymbol{x}=\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)</script><p>则有：</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{x}=\left(\begin{array}{c}
\boldsymbol{\alpha}_{1}^{\mathrm{T}} \\
\boldsymbol{\alpha}_{2}^{\mathrm{T}} \\
\vdots \\
\boldsymbol{\alpha}_{n}^{\mathrm{T}}
\end{array}\right) \boldsymbol{x}=\left(\begin{array}{c}
\boldsymbol{\alpha}_{1}^{\mathrm{T}} \boldsymbol{x} \\
\boldsymbol{\alpha}_{2}^{\mathrm{T}} \boldsymbol{x} \\
\vdots \\
\boldsymbol{a}_{n}^{\mathrm{T}} \boldsymbol{x}
\end{array}\right)=\left(\begin{array}{c}
\boldsymbol{\alpha}_{1} \cdot \boldsymbol{x} \\
\boldsymbol{\alpha}_{2} \cdot \boldsymbol{x} \\
\vdots \\
\boldsymbol{\alpha}_{n} \cdot \boldsymbol{x}
\end{array}\right)</script><p>由此可知，从行的角度来看，$\boldsymbol{Ax}$相当于分别用$\boldsymbol A$的行点乘$\boldsymbol x$，这就是矩阵乘法的定义。<br>下面从列的角度考虑，这是一种非常重要的理解方式。不妨设：</p>
<script type="math/tex; mode=display">
\boldsymbol A = (\boldsymbol \beta_1, \boldsymbol \beta_2, \cdots, \boldsymbol \beta_n)</script><p>则有：</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{x}=\left(\boldsymbol{\beta}_{1}, \boldsymbol{\beta}_{2}, \cdots, \boldsymbol{\beta}_{n}\right)\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)=x_{1} \boldsymbol{\beta}_{1}+x_{2} \boldsymbol{\beta}_{2}+\cdots+x_{n} \boldsymbol{\beta}_{n}</script><p>由此即知，列向量$\boldsymbol x$右乘矩阵$\boldsymbol A$即是对$\boldsymbol A$的列向量作线性组合，$\boldsymbol x$的各分量即为线性组合的系数。</p>
<p><strong>(二) 向量左乘矩阵</strong></p>
<p>下面考虑行向量$\boldsymbol y^{\mathrm T}$左乘矩阵$\boldsymbol A \in \mathbb{R}^{n \times n}$，其中$\boldsymbol y \in \mathbb{R}^{n}$，不妨设：</p>
<script type="math/tex; mode=display">
\boldsymbol{y}^{\mathrm{T}}=\left(y_{1}, y_{2}, \cdots, y_{n}\right)</script><p>则有：</p>
<script type="math/tex; mode=display">
\boldsymbol{y}^{\mathrm{T}} \boldsymbol{A}=\left(y_{1}, y_{2}, \cdots, y_{n}\right)\left(\begin{array}{c}
\boldsymbol{\alpha}_{1}^{\mathrm{T}} \\
\boldsymbol{\alpha}_{2}^{\mathrm{T}} \\
\vdots \\
\boldsymbol{a}_{n}^{\mathrm{T}}
\end{array}\right)=y_{1} \boldsymbol{\alpha}_{1}^{\mathrm{T}}+y_{2} \boldsymbol{\alpha}_{2}^{\mathrm{T}}+\cdots y_{n} \boldsymbol{\alpha}_{n}^{\mathrm{T}}</script><p>由此即知，行向量$\boldsymbol y^{\mathrm T}$左乘矩阵$\boldsymbol A$相当于对$\boldsymbol A$的行向量作线性组合，$\boldsymbol y^{\mathrm T}$的各分量即为线性组合的系数。</p>
<p>综上所述，列向量$\boldsymbol x$右乘矩阵$\boldsymbol A$相当于对$\boldsymbol A$的列向量作线性组合，$\boldsymbol x$的各分量即为线性组合的系数；行向量$\boldsymbol y^{\mathrm T}$左乘矩阵$\boldsymbol A$相当于对$\boldsymbol A$的行向量作线性组合，$\boldsymbol y^{\mathrm T}$的各分量即为线性组合的系数。</p>
<p><strong>(三) 矩阵乘以矩阵</strong></p>
<p>对于矩阵与矩阵的乘法，只需把矩阵按行或列分块，即可按上述向量乘矩阵的方式理解。</p>
<script type="math/tex; mode=display">
\boldsymbol{A B}=\boldsymbol{A}\left(\boldsymbol{\beta}_{1}, \boldsymbol{\beta}_{2}, \cdots, \boldsymbol{\beta}_{n}\right)=\left(\boldsymbol{A} \boldsymbol{\beta}_{1}, \boldsymbol{A} \boldsymbol{\beta}_{2}, \cdots, \boldsymbol{A} \boldsymbol{\beta}_{n}\right)=\left(\begin{array}{c}
\boldsymbol{\alpha}_{1}^{T} \\
\boldsymbol{\alpha}_{2}^{T} \\
\vdots \\
\boldsymbol{\alpha}_{n}^{T}
\end{array}\right) \boldsymbol{B}=\left(\begin{array}{c}
\boldsymbol{\alpha}_{1}^{T} \boldsymbol{B} \\
\boldsymbol{\alpha}_{2}^{T} \boldsymbol{B} \\
\vdots \\
\boldsymbol{\alpha}_{n}^{T} \boldsymbol{B}
\end{array}\right)</script><p>也即是，矩阵$\boldsymbol B$右乘矩阵$\boldsymbol A$相当于对$\boldsymbol A$的列作线性组合，$\boldsymbol B$的各列的分量即为线性组合的系数；矩阵$\boldsymbol A$左乘矩阵$\boldsymbol B$相当于对$\boldsymbol B$的行作线性组合，$\boldsymbol A$的各行的分量即为线性组合的系数。这种理解方式也有助于我们更快地进行矩阵乘法的计算。</p>
<h3 id="2-矩阵消元"><a href="#2-矩阵消元" class="headerlink" title="2 矩阵消元"></a>2 矩阵消元</h3><p>对于线性方程组：</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{r}
x+2 y+z=2 \\
3 x+8 y+z=12 \\
4 y+z=2
\end{array}\right.</script><p>首先通过消元来简化方程组，再通过回代求得方程组的解。考虑方程组系数矩阵$\boldsymbol A$及其右端向量$\boldsymbol b$：</p>
<script type="math/tex; mode=display">
\boldsymbol A=\left(\begin{array}{lll}
1 & 2 & 1 \\
3 & 8 & 1 \\
0 & 4 & 1
\end{array}\right), \boldsymbol b=\left(\begin{array}{c}
2 \\
12 \\
2
\end{array}\right)</script><p>我们称：</p>
<script type="math/tex; mode=display">
(\boldsymbol A,\boldsymbol b)=\left(\begin{array}{lll}
1 & 2 & 1 & 2\\
3 & 8 & 1 & 12\\
0 & 4 & 1 & 2
\end{array}\right)</script><p>为增广矩阵(augmented matrix)。下面对增广矩阵$(\boldsymbol A,\boldsymbol b)$进行消元：</p>
<p><img src="https://pic.imgdb.cn/item/62ab15940947543129489e3a.jpg"></p>
<p>其中，方框中框起来的元素1,2,5称为主元(pivot)，注意主元不能为0。下面通过回代求得线性方程组的解。</p>
<p>首先由增广矩阵的第3行可知$z = -2$，将$z = -2$代入第2行得$y = 1$，再将$z = -2, y = 1$代入第1行得$x = 2$。因此方程组的解为$x = 2, y = 1, z = 2$。</p>
<p>我们将$\boldsymbol A$通过消元后得到的上三角矩阵(upper triangular)记为$\boldsymbol U$，即：</p>
<script type="math/tex; mode=display">
\boldsymbol U=\left(\begin{array}{ccc}
1 & 2 & 1 \\
0 & 2 & -2 \\
0 & 0 & 5
\end{array}\right)</script><p>下面从矩阵乘法角度来说明$\boldsymbol A$是如何变成$\boldsymbol U$的。先将$\boldsymbol A$的第1行的−3倍加到第2行得到了$\boldsymbol A_1=\left(\begin{array}{ccc} 1 &amp; 2 &amp; 1 \\ 0 &amp; 2 &amp; -2 \\ 0 &amp; 4 &amp; 1 \end{array}\right)$，回忆一下矩阵乘法，一个矩阵左乘矩阵$\boldsymbol A$相当于对$\boldsymbol A$的行作线性组合，因此我们要找到一个合适的矩阵$\boldsymbol X$使得$\boldsymbol{XA = A_1}$，由$\boldsymbol A$和$\boldsymbol A_1$的第1行和第3行相同可知，矩阵$\boldsymbol X$的第1行和第3行分别为$(1,0,0),(0,0,1)$。 又由将$\boldsymbol A$的第1行的-3倍加到第2行得到$\boldsymbol A_1$可知，$\boldsymbol X$的第2行为$(−3,1,0)$，即：</p>
<script type="math/tex; mode=display">
\boldsymbol X=\left(\begin{array}{ccc}
1 & 0 & 0 \\
-3 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)</script><p>我们将这个矩阵称为$\boldsymbol E_{21}$，因为它把$\boldsymbol A$的$(2,1)$位置的元素消成了0。这个矩阵称为<strong>初等矩阵</strong>或消元矩阵(elementary matrix or elimination matrix)。同理可知，第二次变换的矩阵为：</p>
<script type="math/tex; mode=display">
\boldsymbol E_{32}=\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & -2 & 1
\end{array}\right)</script><p>$\boldsymbol E_{32}$同样是初等矩阵。因此我们即得：</p>
<script type="math/tex; mode=display">
\boldsymbol E_{32} \boldsymbol E_{21} \boldsymbol A = \boldsymbol U</script><p>这就是矩阵消元的乘法表示。</p>
<h3 id="3-矩阵乘法与逆"><a href="#3-矩阵乘法与逆" class="headerlink" title="3 矩阵乘法与逆"></a>3 矩阵乘法与逆</h3><h4 id="3-1-矩阵乘法的五种理解方式"><a href="#3-1-矩阵乘法的五种理解方式" class="headerlink" title="3.1 矩阵乘法的五种理解方式"></a>3.1 矩阵乘法的五种理解方式</h4><h5 id="3-1-1-定义的角度"><a href="#3-1-1-定义的角度" class="headerlink" title="3.1.1 定义的角度"></a>3.1.1 定义的角度</h5><p>设$\boldsymbol{C = AB}$，则矩阵$\boldsymbol A$的$(i,j)$处的元素为$\boldsymbol A$的第$i$行与$\boldsymbol B$的第$j$列的各元素相乘之和，即：</p>
<script type="math/tex; mode=display">
c_{i j}=a_{i 1} b_{1 j}+a_{i 2} b_{2 j}+\cdots+a_{i n} b_{n j}=\sum_{k=1}^{n} a_{i k} b_{k j}</script><p>也即是$\boldsymbol A$的第$i$行与$\boldsymbol B$的第$j$列点乘所得到的结果。</p>
<h5 id="3-1-2-列的角度"><a href="#3-1-2-列的角度" class="headerlink" title="3.1.2 列的角度"></a>3.1.2 列的角度</h5><p>设矩阵$\boldsymbol B$为：</p>
<script type="math/tex; mode=display">
\boldsymbol B = (\boldsymbol \beta_1, \boldsymbol \beta_2, \cdots, \boldsymbol \beta_n)</script><p>则：</p>
<script type="math/tex; mode=display">
\boldsymbol{AB} = \boldsymbol A (\boldsymbol \beta_1, \boldsymbol \beta_2, \cdots, \boldsymbol \beta_n) = (\boldsymbol A \boldsymbol \beta_1, \boldsymbol A \boldsymbol \beta_2, \cdots, \boldsymbol A \boldsymbol \beta_n)</script><p>因此，从列的角度来看，矩阵$\boldsymbol B$右乘矩阵$\boldsymbol A$所得到的矩阵的每一列都是$\boldsymbol A$的列的线性组合，线性组合的系数分别是$\boldsymbol B$的各列的分量。</p>
<h5 id="3-1-3-行的角度"><a href="#3-1-3-行的角度" class="headerlink" title="3.1.3 行的角度"></a>3.1.3 行的角度</h5><p>设矩阵$\boldsymbol A$为：</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{c}
\boldsymbol{a}_{1}^{\mathrm{T}} \\
\boldsymbol{\alpha}_{2}^{\mathrm{T}} \\
\vdots \\
\boldsymbol{\alpha}_{n}^{\mathrm{T}}
\end{array}\right)</script><p>则有：</p>
<script type="math/tex; mode=display">
\boldsymbol{AB}=\left(\begin{array}{c}
\boldsymbol{a}_{1}^{\mathrm{T}} \boldsymbol B\\
\boldsymbol{\alpha}_{2}^{\mathrm{T}} \boldsymbol B\\
\vdots \\
\boldsymbol{\alpha}_{n}^{\mathrm{T}} \boldsymbol B
\end{array}\right)</script><p>因此，从行的角度来看，矩阵$\boldsymbol A$左乘矩阵$\boldsymbol B$所得到的矩阵的每一行都是$\boldsymbol B$的行的线性组合，线性组合的系数分别是$\boldsymbol A$的各行的分量。</p>
<h5 id="3-1-4-从列乘以行的角度"><a href="#3-1-4-从列乘以行的角度" class="headerlink" title="3.1.4 从列乘以行的角度"></a>3.1.4 从列乘以行的角度</h5><p>设矩阵$\boldsymbol A$、$\boldsymbol B$分别为：</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2} \cdots, \boldsymbol{\alpha}_{n}\right), \boldsymbol{B}=\left(\begin{array}{c}
\boldsymbol{\beta}_{1}^{\mathrm{T}} \\
\boldsymbol{\beta}_{2}^{\mathrm{T}} \\
\vdots \\
\boldsymbol{\beta}_{n}^{\mathrm{T}}
\end{array}\right)</script><p>则有：</p>
<script type="math/tex; mode=display">
\boldsymbol{A B}=\boldsymbol{\alpha}_{1} \boldsymbol{\beta}_{1}^{\mathrm{T}}+\boldsymbol{\alpha}_{2} \boldsymbol{\beta}_{2}^{\mathrm{T}}+\cdots+\boldsymbol{\alpha}_{n} \boldsymbol{\beta}_{n}^{\mathrm{T}}=\sum_{k=1}^{n} \boldsymbol{\alpha}_{k} \boldsymbol{\beta}_{k}^{\mathrm{T}}</script><p>由于列向量乘以行向量得到的是一个矩阵，因此从列乘以行的角度来看，矩阵$\boldsymbol A$乘以$\boldsymbol B$得到的是$n$个矩阵之和，其中第$i$个矩阵由$\boldsymbol A$的第$i$列乘以$\boldsymbol B$的第$j$行得到。</p>
<h5 id="3-1-5-分块乘法-block-multiplication"><a href="#3-1-5-分块乘法-block-multiplication" class="headerlink" title="3.1.5 分块乘法(block multiplication)"></a>3.1.5 分块乘法(block multiplication)</h5><p>矩阵乘法同样可以分块来乘，只要分块的大小能够使乘法有意义即可(分块的大小要相互匹配)如</p>
<script type="math/tex; mode=display">
\boldsymbol{A B}=\left(\begin{array}{ll}
\boldsymbol{A}_{1} & \boldsymbol{A}_{2} \\
\boldsymbol{A}_{3} & \boldsymbol{A}_{4}
\end{array}\right)\left(\begin{array}{ll}
\boldsymbol{B}_{1} & \boldsymbol{B}_{2} \\
\boldsymbol{B}_{3} & \boldsymbol{B}_{4}
\end{array}\right)=\left(\begin{array}{ll}
\boldsymbol{A}_{1} \boldsymbol{B}_{1}+\boldsymbol{A}_{2} \boldsymbol{B}_{3} & \boldsymbol{A}_{1} \boldsymbol{B}_{2}+\boldsymbol{A}_{2} \boldsymbol{B}_{4} \\
\boldsymbol{A}_{3} \boldsymbol{B}_{1}+\boldsymbol{A}_{4} \boldsymbol{B}_{3} & \boldsymbol{A}_{3} \boldsymbol{B}_{2}+\boldsymbol{A}_{4} \boldsymbol{B}_{4}
\end{array}\right)</script><h4 id="3-2-矩阵的逆"><a href="#3-2-矩阵的逆" class="headerlink" title="3.2 矩阵的逆"></a>3.2 矩阵的逆</h4><h5 id="3-2-1-矩阵逆的定义"><a href="#3-2-1-矩阵逆的定义" class="headerlink" title="3.2.1 矩阵逆的定义"></a>3.2.1 矩阵逆的定义</h5><p>如果存在矩阵$\boldsymbol B$使得$\boldsymbol{A B} = \boldsymbol{B A} = \boldsymbol I$，则矩阵$\boldsymbol B$称为矩阵$\boldsymbol A$的逆矩阵(inverse matrix)，记为$\boldsymbol A^{-1}$。一个矩阵可逆那么它是<strong>非奇异矩阵</strong>。<br>如果存在矩阵$\boldsymbol B$使得$\boldsymbol{AB = I}$，我们称$\boldsymbol B$是$\boldsymbol A$的右逆(right inverse)，事实上，可以证明$\boldsymbol B$还是$\boldsymbol A$的左逆(left inverse)，即$\boldsymbol{BA = I}$，因此，直接称满足$\boldsymbol{A B} = \boldsymbol{B A} = \boldsymbol I$的矩阵$\boldsymbol B$为$\boldsymbol A$的逆矩阵(inverse matrix)，即为$\boldsymbol A^{-1}$。</p>
<h5 id="3-2-2-判断矩阵是否存在逆"><a href="#3-2-2-判断矩阵是否存在逆" class="headerlink" title="3.2.2 判断矩阵是否存在逆"></a>3.2.2 判断矩阵是否存在逆</h5><p>以矩阵$\boldsymbol A$为例：</p>
<script type="math/tex; mode=display">
\boldsymbol A=\left(\begin{array}{cc}
1 & 3 \\
2 & 6 
\end{array}\right)</script><p>为例，如果从行列式的角度来看，由于$\boldsymbol A$的行列式为零，显然$\boldsymbol A$不可逆。但是，有没有其他方式来说明$\boldsymbol A$不可逆呢？<br>注意到$\boldsymbol A$的两列是线性相关的(都是$\left[\begin{array}{c}1 \\ 2\end{array}\right]$的倍数)，假设存在矩阵$\boldsymbol B$使得$\boldsymbol{AB = I}$，再来回忆下矩阵的乘法可知，$\boldsymbol{AB}$的每一列都是$\boldsymbol A$的列的线性组合，因此$\boldsymbol{AB}$的每一列也都是$\left[\begin{array}{c}1 \\ 2\end{array}\right]$的倍数，显然是不可能等于单位矩阵$\boldsymbol I$的，因此$\boldsymbol A$不可逆。</p>
<ul>
<li>或者我们可以再换一种方式来说明：<br>如果存在向量$\boldsymbol x \neq \boldsymbol 0$使得$\boldsymbol{Ax = 0}$，那么$\boldsymbol A$不可逆。<br>这个结论的证明是显然的，假设$\boldsymbol A$可逆，那么$\boldsymbol{Ax = 0}$两边同时乘以$\boldsymbol A^{-1}$，则得到$\boldsymbol{x = 0}$，矛盾，因此$\boldsymbol A$不可逆。<br>显然，可取$\boldsymbol x=\left[\begin{array}{c}-3 \\ 1\end{array}\right]$，则$\boldsymbol{Ax = 0}$，因此，A 不可逆。</li>
</ul>
<h5 id="3-2-3-求矩阵的逆"><a href="#3-2-3-求矩阵的逆" class="headerlink" title="3.2.3 求矩阵的逆"></a>3.2.3 求矩阵的逆</h5><p>利用<strong><font color="red">Gauss-Jordan</font></strong> 消元法，对矩阵$(\boldsymbol{A, I})$通过行变换消元，当$(\boldsymbol{A, I})$中的$\boldsymbol A$变为$\boldsymbol I$时，$(\boldsymbol{A, I})$中的矩阵$\boldsymbol I$就变成了$\boldsymbol A^{-1}$，即：</p>
<script type="math/tex; mode=display">
(\boldsymbol{A, I}) \frac{\text { 行变换消元, 相当于在左边乘以矩阵 } \boldsymbol{X}}{\text { 当 } \boldsymbol{X A=I} \text { 时, } \boldsymbol X \text { 即为 } \boldsymbol A^{-1}, \boldsymbol I \text { 就变为 } \boldsymbol A^{-1}}\left(\boldsymbol I, \boldsymbol A^{-1}\right)</script><p>这种方法的原理可以从矩阵线性变换来考虑，初等行变换就是线性变换，其实就是对矩阵$\boldsymbol A$、$\boldsymbol I$同时进行相同的线性变换，当$\boldsymbol A$变成了$\boldsymbol I$，此时$\boldsymbol I$就变成了$\boldsymbol A^{-1}$。<br>发现矩阵的逆与线性相关/无关、矩阵的秩、矩阵行列式有很多相互关联关系。</p>
<h3 id="4-矩阵的LU分解"><a href="#4-矩阵的LU分解" class="headerlink" title="4 矩阵的LU分解"></a>4 矩阵的LU分解</h3><p>从另一种角度来看待Gauss消元(本质上，LU分解是高斯消元的一种表达方式)。</p>
<p>首先考虑没有行交换的情形（也就是主元位置的元素不为0）。对矩阵$\boldsymbol A$进行Gauss消元相当于用一系列初等矩阵左乘$\boldsymbol A$从而得到上三角矩阵$\boldsymbol U$。</p>
<p>以$3 \times 3$矩阵为例。设$\boldsymbol A$是一个$3 \times 3$矩阵，$\boldsymbol E_{21}$、$\boldsymbol E_{31}$、$\boldsymbol E_{32}$是初等矩阵(将$\boldsymbol E_{ij}$位置的元素消为0），$\boldsymbol U$是消元后所得到的上三角矩阵，即：</p>
<script type="math/tex; mode=display">
\boldsymbol E_{32}\boldsymbol E_{31}\boldsymbol E_{21} \boldsymbol A= \boldsymbol U</script><p>因此：</p>
<script type="math/tex; mode=display">
\boldsymbol A = \boldsymbol E_{21}^{-1}\boldsymbol E_{31}^{-1}\boldsymbol E_{32}^{-1} \boldsymbol U</script><p>记：</p>
<script type="math/tex; mode=display">
\boldsymbol{E}=\boldsymbol{E}_{32} \boldsymbol{E}_{31} \boldsymbol{E}_{21}, \boldsymbol{L}=\boldsymbol{E}_{21}^{-1} \boldsymbol{E}_{31}^{-1} \boldsymbol{E}_{32}^{-1}</script><p>则以上两式即为：</p>
<script type="math/tex; mode=display">
\boldsymbol{EA = U}, \boldsymbol{A = LU}</script><p>而当我们写成$\boldsymbol{A = LU}$的形式时，显然$\boldsymbol L$是对角元素全为1的<strong>下三角矩阵</strong>(一般认为，下三角矩阵的左乘代表了对矩阵进行行变换)，且$\boldsymbol L$下三角部分各位置的元素可通过消元过程快速确定。<br>因此，我们只需记录消元所用的乘数，就能快速地确定矩阵$\boldsymbol L$(注意我们这里所讨论的是没有行交换的情形)，不需要进行任何计算，这就是我们使用形式$\boldsymbol{A = LU}$的好处。<br>数学家们喜欢0，喜欢1，喜欢对称，$\boldsymbol{A = LU}$显然不那么对称，$\boldsymbol{U}$对角线上是主元，$\boldsymbol{L}$对角线上是1，这太不美观了实际上，我们还可以进一步分解：</p>
<script type="math/tex; mode=display">
\boldsymbol U = \boldsymbol{DW} = \left[\begin{array}{cccc}
d_{1} & & & \\
& d_{2} & & \\
& & \ddots & \\
& & & d_{n}
\end{array}\right]\left[\begin{array}{cccc}
1 & u_{12} / d_{1} & u_{13} / d_{1} & \cdot \\
& 1 & u_{23} / d_{2} & \cdot \\
& & \ddots & \vdots \\
& & & 1
\end{array}\right]</script><p>因此，有：</p>
<script type="math/tex; mode=display">
\boldsymbol{A = LDW}</script><p>此处$\boldsymbol{DW = U}$，$\boldsymbol D$是一个只有主对角线元素的矩阵，$\boldsymbol W$是对角元素全为1的上三角矩阵。</p>
<blockquote>
<p>你一定跟我当时一样心中一万匹羊驼在奔腾，觉得折腾这玩意有啥用啊，折腾过来折腾过去，没啥用啊，这么弄的目的是啥嘛，但是当天晚上回家看数值分析的书，刚好也讲这个过程，原来这么做的目的是为了减轻计算，举个例子$\boldsymbol{Ax = b}$这种计算过程在工程应用里非常常见，而且多半时间是$\boldsymbol A$不变，不同的$\boldsymbol b$来解不同的$\boldsymbol x$，那么按照高斯消元法，每次要从头消元，因为$\boldsymbol b$改变了增广矩阵，但是很多计算是冗余的，所以使用三角矩阵的好处就是可以大大减少冗余计算。<br>第一步：就是把矩阵分解成 LU 或者 LDU 形式（factor）<br>第二步：通过回代，把x求出来（solve）<br><img src="https://pic.imgdb.cn/item/62ab37de0947543129a17814.jpg" style="zoom:80%"><br>过程(1)(2)并不需要求逆，而是通过回代的过程进行，根据计算时间复杂度（也就是计算量，计算次数），factor的时间复杂度是$O(\dfrac{1}{3}n^3)$，solve的时间复杂度大概是$O(n^2)$，如果你对时间复杂度不了解，可以去看《算法导论》的最开始那一章，这个理论还是非常有用的，尤其是对研究算法的童鞋。通过回代而不是消元，能够降低不少多余的计算。</p>
</blockquote>
<h3 id="5-转置、置换、空间-mathbb-R-n"><a href="#5-转置、置换、空间-mathbb-R-n" class="headerlink" title="5 转置、置换、空间$\mathbb{R}^n$"></a>5 转置、置换、空间$\mathbb{R}^n$</h3><h4 id="5-1-置换矩阵-permutation-matrix"><a href="#5-1-置换矩阵-permutation-matrix" class="headerlink" title="5.1 置换矩阵(permutation matrix)"></a>5.1 置换矩阵(permutation matrix)</h4><p>置换矩阵可以用来行行交换。由上一节我们知道，一个矩阵若恰好不需要行变换就能完成$\boldsymbol{A = LU}$分解是十分简单，但是当被分解的矩阵的行主元有零时，则需要行变换才能完成分解，所以此时就变成了：</p>
<script type="math/tex; mode=display">
\boldsymbol{PA = LU}</script><ul>
<li>置换矩阵的每一行和每一列都恰好有一个1，其余的元素都是0；</li>
<li>置换矩阵可由单位矩阵经过行或列交换得到；</li>
<li>一个矩阵乘以置换矩阵，相当于对矩阵的行或列进行交换；</li>
<li>置换矩阵的性质：$\boldsymbol P^{-1} =\boldsymbol P^{\mathrm T}$，即置换矩阵都是<strong>正交矩阵</strong>。</li>
<li>由于置换矩阵的每一行都可以看作取自单位矩阵的某一行，因此$n \times n$维置换矩阵共有$n!$个。</li>
</ul>
<h4 id="5-2-转置"><a href="#5-2-转置" class="headerlink" title="5.2 转置"></a>5.2 转置</h4><p>矩阵$\boldsymbol A$的转置记为$\boldsymbol A^{\mathrm T}$，满足：</p>
<script type="math/tex; mode=display">
\boldsymbol A_{ij}^{\mathrm T} = \boldsymbol A_{ji}</script><p>若矩阵$\boldsymbol A$满足$\boldsymbol A = \boldsymbol A^{\mathrm T}$，则称$\boldsymbol A$为对称矩阵。对于任何一个矩阵$\boldsymbol A$，不管$\boldsymbol A$是长方形矩阵还是方阵，$\boldsymbol {AA}^{\mathrm T}$、$\boldsymbol A^{\mathrm T}\boldsymbol A$一定是对称矩阵，因为：</p>
<script type="math/tex; mode=display">
 \left(\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}\right)^{\mathrm{T}}=\left(\boldsymbol{A}^{\mathrm{T}}\right)^{\mathrm{T}} \boldsymbol{A}^{\mathrm{T}}=\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}},\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)^{\mathrm{T}}=\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}</script><p>这也是构造对称矩阵的一种方法。</p>
<h4 id="5-3-向量空间"><a href="#5-3-向量空间" class="headerlink" title="5.3 向量空间"></a>5.3 向量空间</h4><p>向量空间必须对线性组合封闭，主要是“<strong>加法封闭和数乘封闭</strong>”。</p>
<ul>
<li>矩阵的列空间<br>矩阵$\boldsymbol{A}$的列的所有线性组合构成一个线性空间，称为$\boldsymbol{A}$的列空间。</li>
</ul>
<h3 id="6-列空间和零空间"><a href="#6-列空间和零空间" class="headerlink" title="6 列空间和零空间"></a>6 列空间和零空间</h3><h4 id="6-1-子空间-Subspace"><a href="#6-1-子空间-Subspace" class="headerlink" title="6.1 子空间(Subspace)"></a>6.1 子空间(Subspace)</h4><p>设非空集合$\boldsymbol S ⊂ \mathbb{R}^n$，且$\boldsymbol S$中的元素对加法和数乘封闭(即对任意的$\boldsymbol u, \boldsymbol v \in \boldsymbol S$，$\boldsymbol{u+v} \in \boldsymbol S$，$\lambda \boldsymbol u \in \boldsymbol S$，$\lambda$是常数），子空间中必须包含“0向量”，则$\boldsymbol S$是$\mathbb{R}^n$的子空间。</p>
<p>设$\boldsymbol V, \boldsymbol W$是$\mathbb{R}^n$的子空间，则$\boldsymbol V \cap \boldsymbol W$也是$\mathbb{R}^n$的子空间(显然对加法和数乘封闭)，但$\boldsymbol V \cup \boldsymbol W$未必是$\mathbb{R}^n$的子空间，因为$\boldsymbol V \cup \boldsymbol W$中的元素未必对加法和数乘封闭。</p>
<p>“子空间”为包含于向量空间内的一个向量空间。它是原向量空间的一个子集，而且本身也满足向量空间的要求。但是“子空间”和“子集”的概念有区别，所有元素都在原空间之内就可称之为子集，但是要满足对线性运算封闭的子集才能成为子空间。</p>
<h4 id="6-2-列空间-Column-space"><a href="#6-2-列空间-Column-space" class="headerlink" title="6.2 列空间(Column space)"></a>6.2 列空间(Column space)</h4><p>矩阵$\boldsymbol{A}$的所有列向量的线性组合构成一个线性空间，称为$\boldsymbol{A}$的列空间，记为$C(\boldsymbol{A})$。</p>
<p>由此可知，线性方程组$\boldsymbol{Ax = b}$有解当且仅当$\boldsymbol{b}$在$\boldsymbol{A}$的列空间中，也即是当且仅当$\boldsymbol{b}$是$\boldsymbol{A}$的列向量的线性组合。</p>
<p>显然，<strong>列空间是线性空间</strong>。(存疑？？？)</p>
<h4 id="6-3-零空间-Nullspace"><a href="#6-3-零空间-Nullspace" class="headerlink" title="6.3 零空间(Nullspace)"></a>6.3 零空间(Nullspace)</h4><p>方程组$\boldsymbol{Ax = 0}$的所有解$\boldsymbol x$的集合称为$\boldsymbol A$的零空间，记为$N(\boldsymbol{A})$。</p>
<p>零空间也是线性空间，因为若$\boldsymbol{u,v} \in N(\boldsymbol A)$，则$\boldsymbol {A(u+v)=Au+Av=0}$，故$\boldsymbol{u+v} \in N(\boldsymbol A)$，同理可知对数乘也封闭。</p>
<p>方程组$\boldsymbol{Ax = b}$的<strong>解</strong>构成的集合不是线性空间，因为其不含零向量(也可很容易地验证对加法和数乘不封闭)。</p>
<h3 id="7-求解-boldsymbol-Ax-0-：主变量、特解"><a href="#7-求解-boldsymbol-Ax-0-：主变量、特解" class="headerlink" title="7 求解$\boldsymbol{Ax = 0}$：主变量、特解"></a>7 求解$\boldsymbol{Ax = 0}$：主变量、特解</h3><p>以：</p>
<script type="math/tex; mode=display">
\boldsymbol A=\left(\begin{array}{cccc}
1 & 2 & 2 & 2 \\
2 & 4 & 6 & 8 \\
3 & 6 & 8 & 10
\end{array}\right)</script><p>为例，对$\boldsymbol A$进行消元(行变换，消元不改变$\boldsymbol A$的行空间和零空间，改变$\boldsymbol A$的列空间)得：</p>
<script type="math/tex; mode=display">
\left(\begin{array}{cccc}
1 & 2 & 2 & 2 \\
0 & 0 & 2 & 4 \\
0 & 0 & 0 & 0
\end{array}\right) \triangleq \boldsymbol{U}</script><p>其中，1、2为主元(每个非零行的第一个非零元素就是主元)，1、2所在的列第1列、第3列称为<strong>主元列</strong>，第2列、第4列称为<strong>自由列</strong>。主元的个数即为$\boldsymbol A$的秩，即$\text{rank}(\boldsymbol A) = 2$。</p>
<blockquote>
<p>主元列和自由列的一个重要区别就是，自由列可以表示为其左侧所有主元列的线性组合，而主元列则不可以。具体参考：<a href="https://zhuanlan.zhihu.com/p/45815011">https://zhuanlan.zhihu.com/p/45815011</a></p>
</blockquote>
<p>设$\boldsymbol x=\left[\begin{array}{c}x_1 \\ x_2 \\ x_3 \\ x_4\end{array}\right]$，则$x_1, x_3$为主变量，$x_2, x_4$为自由变量。自由变量的个数为未知数的个数减去主元的个数(即减去$\boldsymbol A$的秩)，即若$\boldsymbol A$是$m \times n$维矩阵，则自由变量的个数为$n - \text{rank}(\boldsymbol A)$。</p>
<p>由于消元不改变方程组的解，因此求解$\boldsymbol{Ax = 0}$就等价于求解$\boldsymbol{Ux = 0}$。分别取自由变量$(x_2, x_4) = (1, 0)$、$(x_2, x_4) = (0, 1)$， 可得$\boldsymbol{Ux = 0}$的两个特解：</p>
<script type="math/tex; mode=display">
\boldsymbol{\xi}=\left(\begin{array}{c}
-2 \\
1 \\
0 \\
0
\end{array}\right), \boldsymbol{\eta}=\left(\begin{array}{c}
2 \\
0 \\
-2 \\
1
\end{array}\right)</script><p>因此，零空间中的元素为：$\boldsymbol{x} = a\boldsymbol{\xi} + b\boldsymbol{\eta}$，其中，$a, b$为任意常数。</p>
<p>进一步简化行阶梯形式，在简化行阶梯形式中，主元上下的元素都为0，且主元都为1。下面我们进一步将矩阵$\boldsymbol U$化为简化行阶梯形式$\boldsymbol R$：</p>
<script type="math/tex; mode=display">
\boldsymbol R = \left(\begin{array}{cccc}
1 & 2 & 0 & -2 \\
0 & 0 & 1 & 2 \\
0 & 0 & 0 & 0
\end{array}\right)</script><p>这样，求解$\boldsymbol{Ax = 0}$就等价于$\boldsymbol{Ux = 0}$再等价于$\boldsymbol{Rx = 0}$，从而能够更快地写出方程组的解。</p>
<blockquote>
<p>这一讲有点难以理解，可以多看几遍课程+参考笔记：<a href="https://zhuanlan.zhihu.com/p/45815011">MIT—线性代数笔记07 求解Ax=0：主变量，特解 - 三少爷的键的文章 - 知乎</a></p>
</blockquote>
<h3 id="8-boldsymbol-Ax-b-：可解性及解的结构"><a href="#8-boldsymbol-Ax-b-：可解性及解的结构" class="headerlink" title="8 $\boldsymbol{Ax = b}$：可解性及解的结构"></a>8 $\boldsymbol{Ax = b}$：可解性及解的结构</h3><h4 id="8-1-可解的条件-Solvability-conditions-on-b"><a href="#8-1-可解的条件-Solvability-conditions-on-b" class="headerlink" title="8.1 可解的条件 Solvability conditions on b"></a>8.1 可解的条件 Solvability conditions on b</h4><p>仍取：</p>
<script type="math/tex; mode=display">
\boldsymbol A=\left(\begin{array}{cccc}
1 & 2 & 2 & 2 \\
2 & 4 & 6 & 8 \\
3 & 6 & 8 & 10
\end{array}\right)</script><p>则方程为：</p>
<script type="math/tex; mode=display">
\boldsymbol{A x}=\left(\begin{array}{cccc}
1 & 2 & 2 & 2 \\
2 & 4 & 6 & 8 \\
3 & 6 & 8 & 10
\end{array}\right)\left(\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3} \\
x_{4}
\end{array}\right)=\left(\begin{array}{l}
b_{1} \\
b_{2} \\
b_{3}
\end{array}\right)</script><p>矩阵$\boldsymbol A$的第3行为第1行和第2行的加和，因此$\boldsymbol{Ax = b}$中$\boldsymbol b$的第3个分量也要等于其第1和第2个分量的和。若$\boldsymbol b$不满足$b_3 = b_1+b_2$则方程组无解，下面取$\boldsymbol b = \left(\begin{array}{l} 1 \\ 5 \\ 6 \end{array}\right)$。</p>
<p>检验$\boldsymbol{Ax = b}$是否可解的方法是对增广矩阵进行行消元。如果矩阵$\boldsymbol{A}$的行被完全消去的话，则对应的$\boldsymbol b$的分量也要得0。在本例中，矩阵$\boldsymbol{A}$的第3行被消去。</p>
<p>可解的条件：如果$\boldsymbol{Ax = b}$有解的话，则$\boldsymbol b$应该处于矩阵$\boldsymbol A$的列空间$C(\boldsymbol A)$ 里面。等价的另一种描述方式为：矩阵$\boldsymbol A$的行向量若经过线性组合为零向量时，则对应的$\boldsymbol b$经同样的线性组合后也为0(注意是单个0)。</p>
<h4 id="8-2-特解A-particular-solution"><a href="#8-2-特解A-particular-solution" class="headerlink" title="8.2 特解A particular solution"></a>8.2 特解A particular solution</h4><p>求$\boldsymbol{Ax = b}$特解的方法是将自由变量均赋为0，求解其主变量。本例中，令$x_2 = 0, x_4 = 0$得到方程组：</p>
<script type="math/tex; mode=display">
\begin{array}{r}
x_{1}+2 x_{3}=1 \\
2 x_{2}=3
\end{array}</script><p>可解得$x_1 = -2, x_3 = \dfrac{3}{2}$，即特解为：$\boldsymbol x_p=\left[\begin{array}{c}-2 \\ 0 \\ \dfrac{3}{2} \\ 0 \end{array}\right]$。</p>
<h4 id="8-3-通解Complete-solution"><a href="#8-3-通解Complete-solution" class="headerlink" title="8.3 通解Complete solution"></a>8.3 通解Complete solution</h4><p>为求得$\boldsymbol{Ax = b}$的所有解，我们首先检验方程是否可解，然后找到一个特解。将特解和矩阵零空间的向量相加即为方程的通解。</p>
<ul>
<li>与零空间进行线性组合 Combined with nullspace：</li>
</ul>
<script type="math/tex; mode=display">\boldsymbol{A x_p = b}</script><script type="math/tex; mode=display">\boldsymbol{A x_n = 0}</script><script type="math/tex; mode=display">\boldsymbol{A (x_p + x_n) = b}</script><p>即通解为：$\boldsymbol x_{\text{complete}} = \boldsymbol x_p + \boldsymbol x_n$。</p>
<p>将$\boldsymbol A$转换成rref(行最简)，则结果如下所示：</p>
<script type="math/tex; mode=display">
\boldsymbol{R}=\left(\begin{array}{cccc}
x_{1} & x_{2} & x_{3} & x_{4} \\
- & - & - & - \\
1 & 2 & 0 & -2 \\
0 & 0 & 1 & 2 \\
0 & 0 & 0 & 0
\end{array}\right)</script><p>将$x_2, x_3$进行互换，把主元列、自由列分别放在一起，则为：</p>
<script type="math/tex; mode=display">
\boldsymbol{R}=\left(\begin{array}{cccc}
x_{1} & x_{2} & x_{3} & x_{4} \\
- & - & - & - \\
1 & 0 & 2 & -2 \\
0 & 1 & 0 & 2 \\
0 & 0 & 0 & 0
\end{array}\right)
=\left(\begin{array}{cc}
\boldsymbol I & \boldsymbol F  \\
\boldsymbol 0 & \boldsymbol 0
\end{array}\right)</script><p>互换后的$\boldsymbol{Ax = 0} \stackrel{转变为}{\longrightarrow} \boldsymbol{Rx = 0}$解应该为</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}
-\boldsymbol{F} \\
\boldsymbol{I}
\end{array}\right)=\left(\begin{array}{cc}
-2 & 2 \\
0 & -2 \\
1 & 0 \\
0 & 1
\end{array}\right)</script><p>再将$x_2, x_3$互换回来，即可得到原方程的解：</p>
<script type="math/tex; mode=display">
\boldsymbol{x}=\left(\begin{array}{cc}
-2 & 2 \\
1 & 0 \\
0 & -2 \\
0 & 1
\end{array}\right)</script><p>这是零空间解的一种简便算法，即通过行变换得到(必要时需要交换列，当然最后还要交换回来)得到$\boldsymbol{I, F}$后可直接写出零解。</p>
<h4 id="8-4-秩Rank"><a href="#8-4-秩Rank" class="headerlink" title="8.4 秩Rank"></a>8.4 秩Rank</h4><p>假设矩阵的shape为$m \times n$，如果矩阵的秩为$r$，则必有$r \leq m, r \leq n$。</p>
<h5 id="8-4-1-列满秩-r-n-n-leq-m"><a href="#8-4-1-列满秩-r-n-n-leq-m" class="headerlink" title="8.4.1 列满秩$r = n(n \leq m)$"></a>8.4.1 列满秩$r = n(n \leq m)$</h5><p>零空间$N(\boldsymbol A)$内只有零向量。原因：每列都有主元，即也可以认为每列都是线性无关的，$\boldsymbol x$的每个分量都是主变量，没有自由变量。方程$\boldsymbol{Ax = b}$无解或者有唯一解$\boldsymbol x_p$。</p>
<h5 id="8-4-2-行满秩-r-m-m-leq-n"><a href="#8-4-2-行满秩-r-m-m-leq-n" class="headerlink" title="8.4.2 行满秩$r = m(m \leq n)$"></a>8.4.2 行满秩$r = m(m \leq n)$</h5><p>每行都有主元，无论$\boldsymbol b$取何值，方程$\boldsymbol{Ax = b}$都有解(因为$\boldsymbol A$是行满秩，所以$\boldsymbol A$的列向量能够线性组合充满整个$\mathbb{R}^{m \times m}$空间，而$\boldsymbol b$一定在$\mathbb{R}^{m \times m}$空间中)。主变量$r = m$个，自由变量$n-r = n-m$个，即也一定存在零空间解。</p>
<h5 id="8-4-3-满秩-r-n-m"><a href="#8-4-3-满秩-r-n-m" class="headerlink" title="8.4.3 满秩$r = n = m$"></a>8.4.3 满秩$r = n = m$</h5><p>满秩，矩阵可逆。零空间只有零向量(因为矩阵$\boldsymbol A$的列都是线性无关的，不可能组合出零向量)，无论$\boldsymbol b$取何值，方程$\boldsymbol{Ax = b}$都有唯一解。$\boldsymbol{R = I}$。</p>
<h4 id="8-5-小结"><a href="#8-5-小结" class="headerlink" title="8.5 小结"></a>8.5 小结</h4><p><img src="https://pic.imgdb.cn/item/62ac497509475431293c9580.jpg"></p>
<p>简单来说，$\boldsymbol R$的倒数行是否为零行决定了是否有解。如果没有零行，则一定有解。秩决定了方程组解的数量。</p>
<h3 id="9-线性无关，基和维度"><a href="#9-线性无关，基和维度" class="headerlink" title="9. 线性无关，基和维度"></a>9. 线性无关，基和维度</h3><p>向量的线性无关意味着什么？如何用线性无关的概念来帮助我们描述包括零空间在内的子空间。</p>
<p>首先我们需要注意的是，线性无关是针对向量组而言的，而不是对矩阵而言的。重要概念：线性无关(线性相关)、张成空间、基、维度。</p>
<h4 id="9-1-复习"><a href="#9-1-复习" class="headerlink" title="9.1 复习"></a>9.1 复习</h4><p>假设矩阵$\boldsymbol R$的shape为$m \times n$，并且$m &lt; n$，其中$m$表示的是方程组的个数，而$n$表示的是未知数的个数。那么$\boldsymbol{Ax = 0}$一定包含非零解。其中解存在的原因在于一定存在自由变量，其中自由变量个数最少为$n - m$。</p>
<h4 id="9-2-线性无关-Independence"><a href="#9-2-线性无关-Independence" class="headerlink" title="9.2 线性无关 Independence"></a>9.2 线性无关 Independence</h4><p>若$c_1 \boldsymbol x_1 + c_2 \boldsymbol x_2 + \cdots + c_n \boldsymbol x_n = \boldsymbol 0$仅仅在$c_1 = c_2 = \cdots = c_n = 0$时成立，则称$\boldsymbol x_1, \boldsymbol x_2, \cdots, \boldsymbol x_n$是线性无关的。若这些向量作为列向量构成矩阵$\boldsymbol A$，则方程$\boldsymbol{A x = 0}$只有零解$\boldsymbol x = \boldsymbol 0$，或称矩阵$\boldsymbol A$的零空间只有零向量。换而言之，若存在非零向量$\boldsymbol c$，使得$\boldsymbol{Ac = 0}$，则这个矩阵的列向量线性相关。</p>
<ul>
<li><strong>思考：</strong>零向量和另外一个向量是线性相关还是线性无关呢？<br>答案是线性相关的。也就是说只要向量组中包含一个零向量，那么一定是线性相关的。</li>
</ul>
<p>在$\mathbb{R}^2$中，两个向量只要不共线就是线性无关的。（在$\mathbb{R}^3$中，三个向量线性无关的条件是它们不在一个平面上）若选定空间$\mathbb{R}^2$中的三个向量，则他们必然是线性相关的。例如，如下的三个向量线性相关的：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\begin{array}{lll}
\boldsymbol{v}_{1} & \boldsymbol{v}_{2} & \boldsymbol{v}_{3}
\end{array}\\
&\left(\begin{array}{ccc}
2 & 1 & 2.5 \\
1 & 2 & -1
\end{array}\right)
\end{aligned}</script><p>此矩阵构成的方程$\boldsymbol{Ax = 0}$必有非零解，即三个向量线性相关。</p>
<p>向量组线性无关等价于将该向量组构成的矩阵$\boldsymbol A$的零空间中只有零向量。如果是线性相关的话，存在非零向量$\boldsymbol c$使得$\boldsymbol{Ac=0}$。<br>如果矩阵$\boldsymbol A$的列向量为线性无关，则$\boldsymbol A$所有的列均为主元列，没有自由列，$\text{rank}(\boldsymbol A) = n$。若$\boldsymbol A$的列向量为线性相关，则$\text{rank}(\boldsymbol A) &lt; n$，并且存在自由列。其中自由列的本质是主列的一种组合。</p>
<p>总结：向量组的线性相关性可转换成计算矩阵的零空间。</p>
<h4 id="9-3-张成空间-Span-a-space"><a href="#9-3-张成空间-Span-a-space" class="headerlink" title="9.3 张成空间 Span a space"></a>9.3 张成空间 Span a space</h4><p>当一个空间是由向量$\boldsymbol v_1, \boldsymbol v_2, \cdots, \boldsymbol v_k$的所有线性组合组成时，我们称这些向量张成了这个空间。例如矩阵的列向量张成了该矩阵的列空间。<br>如果向量$\boldsymbol v_1, \boldsymbol v_2, \cdots, \boldsymbol v_k$张成空间$\boldsymbol S$，则$\boldsymbol S$是包含这些向量的最小空间。</p>
<h4 id="9-4-基-Basis"><a href="#9-4-基-Basis" class="headerlink" title="9.4 基 Basis"></a>9.4 基 Basis</h4><p>向量空间的基是具有如下两个性质的一组向量$\boldsymbol v_1, \boldsymbol v_2, \cdots, \boldsymbol v_d$：</p>
<ul>
<li>$\boldsymbol v_1, \boldsymbol v_2, \cdots, \boldsymbol v_d$线性无关；</li>
<li>$\boldsymbol v_1, \boldsymbol v_2, \cdots, \boldsymbol v_d$张成该向量空间；</li>
</ul>
<p>在$\mathbb{R}^3$空间中，其中一组基为：</p>
<script type="math/tex; mode=display">
\left(\begin{array}{l}
1 \\
0 \\
0
\end{array}\right)\left(\begin{array}{l}
0 \\
1 \\
0
\end{array}\right)\left(\begin{array}{l}
0 \\
0 \\
1
\end{array}\right)</script><p>若以$\mathbb{R}^n$空间中的$n$个向量为列向量构成的矩阵为可逆矩阵，则这些向量可以构成$\mathbb{R}^n$空间中的一组基。</p>
<p><strong>(一) 子空间的基 Basis for a subspace</strong></p>
<p>向量$\left[\begin{array}{c}1 \\ 1 \\ 0 \end{array}\right]$、$\left[\begin{array}{c}0 \\ 1 \\ 1 \end{array}\right]$构成$\mathbb{R}^3$中的一个平面，但是它们无法成为$\mathbb{R}^3$空间的一组基。空间中的基并不是唯一的。</p>
<p><strong>(二) 维度 Dimension</strong></p>
<p>空间的每一组基都具有相同的向量数，这个数值就是空间的维度(dimension)。所以$\mathbb{R}^n$空间的每组基都包含$n$个向量。简单来说，向量空间的维度就是基向量的个数。对于一个向量空间而言，基向量可以是不同的，但是维度是相同的。</p>
<p><strong>(三) 列空间的基</strong></p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{cccc}
1 & 2 & 3 & 1 \\
1 & 1 & 2 & 1 \\
1 & 2 & 3 & 1 
\end{array}\right)</script><script type="math/tex; mode=display">
\text{rank} (\boldsymbol A) =  \text{pivot numbers} = \text{dimension } C(\boldsymbol A) = 2</script><p>注意：矩阵具有秩rank而不是维数dimension，而空间有维数而不是秩。当知道了列空间的维数，可以从矩阵列向量中随意选取足够数量的线性无关的向量，它们每一组都可以构成列空间的一组基。其中一组基是第一列与第二列</p>
<p><strong>(四) 零空间的基</strong></p>
<p>本例中矩阵的列向量不是线性无关的，因此其零空间$N(\boldsymbol A)$不止包含零向量。因为可以看出第3列是第1列和第2列的和。所以向量$\left[\begin{array}{c}-1 \\ -1 \\ 1 \\ 0 \end{array}\right]$必然在零空间$N(\boldsymbol A)$之内。由于第1列和第4列相等，从而得到$\left[\begin{array}{c}1 \\ 0 \\ 0 \\ -1 \end{array}\right]$也在零空间之内。它们就是的两个特解。</p>
<p><strong>零空间的维数=自由列的数目=$n - r$(列数减去秩)</strong>，因此本例中$N(\boldsymbol A)$的维数为$4-2=2$。这两个特解就构成了零空间的一组基。</p>
<h3 id="10-四个基本子空间"><a href="#10-四个基本子空间" class="headerlink" title="10 四个基本子空间"></a>10 四个基本子空间</h3><h4 id="10-1-四个子空间-Four-subspaces"><a href="#10-1-四个子空间-Four-subspaces" class="headerlink" title="10.1 四个子空间 Four subspaces"></a>10.1 四个子空间 Four subspaces</h4><p>任意的$m \times n$矩阵$\boldsymbol A$都定义了四个子空间。</p>
<h5 id="10-1-1-列空间Column-space"><a href="#10-1-1-列空间Column-space" class="headerlink" title="10.1.1 列空间Column space"></a>10.1.1 列空间Column space</h5><p>矩阵$\boldsymbol A$的列空间是$\boldsymbol A$的列向量的线性组合在空间中构成的子空间。如何表示列空间中的任意向量呢？很显然是$\boldsymbol{Ax = b}$。</p>
<h5 id="10-1-2-零空间-Nullspace"><a href="#10-1-2-零空间-Nullspace" class="headerlink" title="10.1.2 零空间 Nullspace"></a>10.1.2 零空间 Nullspace</h5><p>矩阵$\boldsymbol A$的零空间$\boldsymbol{Ax = 0}$是的所有解$\boldsymbol x$在$\mathbb{R}^n$空间中成的子空间。</p>
<h5 id="10-1-3-行空间-Row-space"><a href="#10-1-3-行空间-Row-space" class="headerlink" title="10.1.3 行空间 Row space"></a>10.1.3 行空间 Row space</h5><p>矩阵$\boldsymbol A$的行空间是$\boldsymbol A$的行向量的线性组合在$\mathbb{R}^n$空间中构成的子空间，也就是矩阵$\boldsymbol A^{\mathrm T}$的列空间。</p>
<h5 id="10-1-4-左零空间-Left-nullspace"><a href="#10-1-4-左零空间-Left-nullspace" class="headerlink" title="10.1.4 左零空间 Left nullspace"></a>10.1.4 左零空间 Left nullspace</h5><p>我们称矩阵$\boldsymbol A^{\mathrm T}$的零空间为矩阵$\boldsymbol A$的左零空间，它是$\mathbb{R}^m$空间中的子空间。</p>
<h4 id="10-2-基和维度-Basis-amp-Dimension"><a href="#10-2-基和维度-Basis-amp-Dimension" class="headerlink" title="10.2 基和维度 Basis&amp; Dimension"></a>10.2 基和维度 Basis&amp; Dimension</h4><h5 id="10-2-1-列空间"><a href="#10-2-1-列空间" class="headerlink" title="10.2.1 列空间"></a>10.2.1 列空间</h5><p>矩阵$\boldsymbol A$的个主元列(pivot columns)构成了列空间$C(\boldsymbol A)$的一组基。</p>
<script type="math/tex; mode=display">\dim C(\boldsymbol A) = \text{rank}(\boldsymbol A) = r</script><h5 id="10-2-2-零空间"><a href="#10-2-2-零空间" class="headerlink" title="10.2.2 零空间"></a>10.2.2 零空间</h5><p>$\boldsymbol{Ax = 0}$的一组特解对应于矩阵$\boldsymbol A$的$n - r$个自由列，并构成了零空间的一组基。个人理解：自由列分别进行one-hot处理。</p>
<script type="math/tex; mode=display">\dim N(\boldsymbol A) = n - r</script><h5 id="10-2-3-行空间"><a href="#10-2-3-行空间" class="headerlink" title="10.2.3 行空间"></a>10.2.3 行空间</h5><p>我们用矩阵$\boldsymbol A$的化简的行阶梯矩阵$\boldsymbol R$。</p>
<p>矩阵$\boldsymbol A$和矩阵$\boldsymbol R$的列空间不同$C(\boldsymbol A) \neq C(\boldsymbol R)$，但两者行空间相同。$\boldsymbol R$的行向量来自于$\boldsymbol A$的行向量的线性组合，因为<strong>消元操作是可逆的</strong>，所以$\boldsymbol A$的向量也可以表示为$\boldsymbol R$行向量的线性组合。</p>
<p>$\boldsymbol R$的前$r$行向量就是矩阵$\boldsymbol A$行空间$C(\boldsymbol A^{\mathrm T})$的一组基。以$\boldsymbol A$为例，其中一组基是$\boldsymbol R$中的前两行。</p>
<script type="math/tex; mode=display">\dim C(\boldsymbol A^{\mathrm T}) = r</script><p>为什么$\dim C(\boldsymbol A^{\mathrm T}) = r$，可以重点关注中$\boldsymbol R$的$\boldsymbol I$。</p>
<h5 id="10-2-4-左零空间"><a href="#10-2-4-左零空间" class="headerlink" title="10.2.4 左零空间"></a>10.2.4 左零空间</h5><p>左零空间矩阵$\boldsymbol A^{\mathrm T}$有$m$列，而其秩为$r$，因此其自由列数目为$m-r$，故$\dim N(\boldsymbol A^{\mathrm T}) = m - r$，左零矩阵是满足$\boldsymbol A^{\mathrm T} \boldsymbol y = \boldsymbol 0$的所有向量$\boldsymbol y$的集合。称之为左零矩阵是因为该式可写作$\boldsymbol y^{\mathrm T} \boldsymbol A = \boldsymbol 0^{\mathrm T}$此时右边为行向量，而$\boldsymbol y$出现在矩阵$\boldsymbol A$左侧。</p>
<p>为找到左零空间的基，我们应用增广矩阵：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}
\boldsymbol A_{m \times n} & \boldsymbol I_{m \times n}
\end{array}\right] \rightarrow\left[\begin{array}{ll}
\boldsymbol R_{m \times n} & \boldsymbol E_{m \times n}
\end{array}\right]</script><p>我们将$\boldsymbol A$通过消元得到矩阵$\boldsymbol R$，其消元矩阵记为$\boldsymbol E$，即$\boldsymbol{EA = R}$。若为$\boldsymbol A$方阵，且$\boldsymbol{R = I}$，则有$\boldsymbol{E = A}^{-1}$</p>
<p><img src="https://pic.imgdb.cn/item/62ac8c5b0947543129aa9d0d.jpg" style="zoom:80%"></p>
<p>以行操作的观点来看矩阵$\boldsymbol E$和$\boldsymbol A$的乘法，则矩阵$\boldsymbol E$最下面的$m -r$个行向量使得矩阵$\boldsymbol A$的行向量线性组合成为$\boldsymbol 0$，也就是矩阵$\boldsymbol R$最下面的$m-r$个零向量。本例中，$m -r = 1$。</p>
<p>矩阵$\boldsymbol E$的这$m-r$个行向量满足$\boldsymbol{y}^{\mathrm T} \boldsymbol{A} = \boldsymbol{0}^{\mathrm T}$，它组成了矩阵$\boldsymbol A$左零空间的一组基，在本例中的左零空间的一组基为$\left[\begin{array}{c}-1 \\ 0 \\ 1 \end{array}\right]$。即上图中左边的部分所示的内容。</p>
<h4 id="10-3-新向量空间-New-vector-space"><a href="#10-3-新向量空间-New-vector-space" class="headerlink" title="10.3 新向量空间 New vector space"></a>10.3 新向量空间 New vector space</h4><p>所有$3 \times 3$矩阵构成的集合是一个矩阵空间，符合对线性运算封闭，称之为$\boldsymbol M$。$\boldsymbol M$的子空间包括：</p>
<ul>
<li>所有的上三角阵</li>
<li>所有的对称阵</li>
<li>所有的对角阵</li>
</ul>
<p>对角阵是前两个子空间的交集，其维度为3，其中一组基为：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right]\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{array}\right]\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{array}\right]</script><blockquote>
<p>矩阵空间的概念参考链接：<a href="https://blog.csdn.net/suyimin2010/article/details/90338886">矩阵空间、秩1矩阵 - Luckie stone - CSDN</a></p>
<p>讨论：当最初告诉我说，矩阵的列秩等于主元数，并且主元列构成了列空间的一组基时，其实我是拒绝的。主元这个东西不是行变换消元得来的么，消元过程列空间不是已经改变了么，为什么所得出U的主元数和主元列的位置还能够反映出矩阵A列空间的状态呢？</p>
<p>这里需要说明的是两点，其一是关于秩的定义有很多在数学上等价但是描述差异很大的说法，在这里我们把“秩”理解为行（列）向量中最大的线性无关向量组的向量数即可，在矩阵A行变换消元成梯形阵后，很容易看到行空间内极大无关组之一就是主元所在的那前r行，这r个行向量可以张成行空间，因此行空间的维数与主元数相等都是r，并且前r行构成了行空间的一组基。</p>
<p>但是为什么列空间的维数也是r，并且主元列可以构成列空间的一组基呢？这就是要说明的第二点，初等行变换不会改变列向量的线性相关性。为了叙述方便起见，我们假定矩阵A列向量的极大无关组就是A前r’列的向量（若否可以通过列交换而达成，列交换不会改变线性关系）。<a href="https://zhuanlan.zhihu.com/p/45826349">MIT—线性代数笔记10 四个基本子空间 - 三少爷的键的文章 - 知乎</a></p>
</blockquote>
<h3 id="11-矩阵空间、秩1矩阵和小世界图"><a href="#11-矩阵空间、秩1矩阵和小世界图" class="headerlink" title="11 矩阵空间、秩1矩阵和小世界图"></a>11 矩阵空间、秩1矩阵和小世界图</h3><h4 id="11-1-3∗3矩阵空间-3-by-3-matrices"><a href="#11-1-3∗3矩阵空间-3-by-3-matrices" class="headerlink" title="11.1 3∗3矩阵空间 3 by 3 matrices"></a>11.1 3∗3矩阵空间 3 by 3 matrices</h4><p>空间$\boldsymbol M$是所有$3 \times 3$矩阵所构成的空间，$\boldsymbol M$的部分子空间包括：</p>
<ul>
<li>所有的上三角阵 </li>
<li>所有的对称阵 </li>
<li>所有的对角阵</li>
</ul>
<p>对于矩阵空间而言，矩阵空间的维度是基矩阵的个数。它的一组基即为多个基矩阵。空间$\boldsymbol M$的维数为9，与$\mathbb{R}^9$空间很类似。我们可以选定它的一组基：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],
\cdots
,\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 1 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{array}\right]</script><p>对称阵构成的子空间$\boldsymbol S$维数为6，它的一组基为：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{array}\right],\left[\begin{array}{lll}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 1 \\
0 & 0 & 0 \\
1 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0
\end{array}\right]</script><p>上三角阵构成的子空间$\boldsymbol U$维数也为6，它的一组基为：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{array}\right],\left[\begin{array}{lll}
0 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{array}\right]</script><p>对角阵构成的子空间$\boldsymbol D$维数为3，可以选定$\boldsymbol S$和$\boldsymbol U$的基的交集为$\boldsymbol D$的基，具体为：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{array}\right],\left[\begin{array}{lll}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{array}\right]</script><p>$\boldsymbol S$和$\boldsymbol U$的并集，即$3 \times 3$矩阵中或为上三角阵或为对称阵的矩阵，构成$\boldsymbol M$的子空间么？答案是否定的。如下列矩阵加法所示：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}
1 & -1 & 0 \\
-1 & 1 & 0 \\
0 & 0 & 1
\end{array}\right] + \left[\begin{array}{lll}
0 & 3 & 3 \\
0 & 0 & 4 \\
0 & 0 & 0
\end{array}\right] = \left[\begin{array}{lll}
1 & 2 & 3 \\
-1 & 1 & 4 \\
0 & 0 & 1
\end{array}\right]</script><p>显然最终结果并不在两者的并集中。这就如同在$\mathbb{R}^2$空间中找出两条直线，询问它们的并集是否构成一个子空间。如果我们将$\boldsymbol S$和$\boldsymbol U$中所有元素可能构成的加和作为一个集合，可以称为和集$\boldsymbol{S+U}$，它是$\boldsymbol M$的一个子空间。实际上$\boldsymbol{S+U}$就是$\boldsymbol M$本身，其维数为9。</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\operatorname{dim}(S)+\operatorname{dim}(U)=\operatorname{dim}(S \cap U)+\operatorname{dim}(S+U) \\
&\operatorname{dim}(S+U)=\operatorname{dim}(S)+\operatorname{dim}(U)-\operatorname{dim}(S \cap U)
\end{aligned}</script><h4 id="11-2-微分方程-Differential-equations"><a href="#11-2-微分方程-Differential-equations" class="headerlink" title="11.2 微分方程 Differential equations"></a>11.2 微分方程 Differential equations</h4><p>对于给定的微分方程$\dfrac{\text{d}^2y}{\text{d}x^2}$，求解该方程可以视为求它的零空间。可以得到解为：$y = \cos(x), y = \sin(x), y = e^{ix}$，事实上通解为$y = c_1\cos(x) + c_2\sin(x)$，其中$c_1, c_2$可以取任意实数。也将解的线性组合构成的空间称为解空间，其维数为2。$\cos(x), \sin(x)$可以成为解空间的一组基。它们是函数，而不是向量，但是可以对其进行线性运算，在线性代数的范畴内讨论之。</p>
<h4 id="11-3-子空间的交，和与维数定理"><a href="#11-3-子空间的交，和与维数定理" class="headerlink" title="11.3 子空间的交，和与维数定理"></a>11.3 子空间的交，和与维数定理</h4><p>接下来到关键的地方了，建议先阅读<a href="https://zhuanlan.zhihu.com/p/76674347">《线性代数-线性空间的知识梳理3》</a>中子空间的<strong>交</strong>、<strong>和</strong>和<strong>维数定理</strong>等小节。</p>
<p>接下来我们研究矩阵空间$\boldsymbol M$的子空间$\boldsymbol S$和$\boldsymbol U$的交，即$\boldsymbol S \cap \boldsymbol U$，这个比较简单，易知$\boldsymbol S \cap \boldsymbol U$即对角矩阵，其维数明显为3。</p>
<p>但若要直接研究$\boldsymbol S$和$\boldsymbol U$ 的和，即$\boldsymbol S + \boldsymbol U$ ，这个就没有$\boldsymbol S \cap \boldsymbol U$那么直观了：</p>
<p><strong>一种方法</strong>是通过定义，即</p>
<script type="math/tex; mode=display">
\boldsymbol S + \boldsymbol U = \begin{Bmatrix} \boldsymbol \alpha + \boldsymbol \beta \mid \boldsymbol \alpha \in \boldsymbol S, \boldsymbol \beta \in \boldsymbol U \end{Bmatrix}</script><p>那么可以发现，对于任何一个$3 \times 3$矩阵，它是可以表示成一个对称矩阵和一个上三角矩阵的和：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right]=\left[\begin{array}{ccc}
a_{11} & a_{21} & a_{31} \\
a_{21} & a_{22} & a_{32} \\
a_{31} & a_{32} & a_{33}
\end{array}\right]+\left[\begin{array}{ccc}
0 & a_{12}-a_{21} & a_{13}-a_{31} \\
0 & 0 & a_{23}-a_{32} \\
0 & 0 & 0
\end{array}\right]</script><p>所以$\boldsymbol S + \boldsymbol U = \boldsymbol M$ ，因此$\dim(\boldsymbol S + \boldsymbol U) = 9$ 。</p>
<p><strong>另一种方法</strong>，为了确定$\boldsymbol S + \boldsymbol U$的维数，可以利用维数定理，即：</p>
<script type="math/tex; mode=display">
\dim(\boldsymbol S + \boldsymbol U) = \dim(\boldsymbol S) + \dim(\boldsymbol U) - \dim(\boldsymbol S \cap \boldsymbol U)</script><p>这个公式中，$\dim(\boldsymbol S \cap \boldsymbol U) = 3$ ，所以$\dim(\boldsymbol S + \boldsymbol U)  = 9$ ，而由$\boldsymbol S + \boldsymbol U$的定义可知，其本身就是$\boldsymbol M$的一个子空间，且$\dim(\boldsymbol M) = 9$，所以$\boldsymbol S + \boldsymbol U = \boldsymbol M$ ，即$\boldsymbol S$和$\boldsymbol U$的和刚好覆盖了整个矩阵空间$\boldsymbol M$。</p>
<p>这就是维数定理一个很好的应用，有时候直接分析两个子空间的和不容易，那么可以考虑通过维数定理先分别分析这两个子空间，再分析其交，这样就能得到$\dim(\boldsymbol S + \boldsymbol U)$，<strong>一旦我们知道一个子空间的维数</strong>$n$<strong>，那么只要找到这个子空间中</strong>$n$<strong>个线性无关的向量，那么这</strong>$n$<strong>个向量就是子空间的一个基</strong>。</p>
<p>本小节除了介绍子空间的交，和和维数定理，另一方面是给出线性空间中元素一般性的例子，当然课堂中，老师还讲到了线性微分方程的解空间也是一个线性空间，这里不做具体介绍。</p>
<h4 id="11-4-秩1矩阵"><a href="#11-4-秩1矩阵" class="headerlink" title="11.4 秩1矩阵"></a>11.4 秩1矩阵</h4><p>对秩为1的矩阵，也可以进行研究，比如秩为1的矩阵：</p>
<script type="math/tex; mode=display">
\boldsymbol A=\left(\begin{array}{ccc}
1 & 4 & 5 \\
2 & 8 & 10
\end{array}\right)</script><p>我们从行向量的角度分解，可以等价表示为：</p>
<script type="math/tex; mode=display">
\boldsymbol A=\left(\begin{array}{ccc}
1 & 4 & 5 \\
2 & 8 & 10
\end{array}\right)=\left(\begin{array}{l}
1 \\
2
\end{array}\right)\left(\begin{array}{lll}
1 & 4 & 5
\end{array}\right)</script><p>我们有所有<strong>秩为1的矩阵可以表示为一列乘以一行的形式</strong>，即：$\boldsymbol A = \boldsymbol{uv}^{\mathrm T}$。</p>
<p>之后的学习中，我们会认识到秩1矩阵行列式和特征值都会很简单。再讨论一下几个问题：</p>
<h5 id="11-4-1-问题1"><a href="#11-4-1-问题1" class="headerlink" title="11.4.1 问题1"></a>11.4.1 问题1</h5><p>比如，一个$s \times n$ 的矩阵的秩为$r$，我们可以将其表示成$r$个秩1矩阵的组合，所以秩1矩阵很有用，它就像搭建其他矩阵的积木一样。</p>
<p>举个简单例子：</p>
<script type="math/tex; mode=display">
\boldsymbol A=\left[\begin{array}{cccc}
1 & 2 & 2 & 2 \\
2 & 4 & 6 & 8 \\
3 & 6 & 8 & 10
\end{array}\right]=\left(\vec{a}_{1}, \vec{a}_{2}, \vec{a}_{3}, \vec{a}_{4}\right)</script><p>通过初等行变换：</p>
<script type="math/tex; mode=display">
\boldsymbol A=\left[\begin{array}{cccc}
1 & 2 & 2 & 2 \\
2 & 4 & 6 & 8 \\
3 & 6 & 8 & 10
\end{array}\right] \sim\left[\begin{array}{cccc}
1 & 2 & 2 & 2 \\
0 & 0 & 2 & 4 \\
0 & 0 & 0 & 0
\end{array}\right] \sim\left[\begin{array}{cccc}
1 & 2 & 0 & -2 \\
0 & 0 & 1 & 2 \\
0 & 0 & 0 & 0
\end{array}\right]</script><p>所以$\text{rank}(\boldsymbol A) = 2$ ，$\boldsymbol A$的1和3列是列空间的一个基，其零空间：</p>
<script type="math/tex; mode=display">
N_{n \times(n-r)}=\left[\begin{array}{c}
-F_{r \times(n-r)} \\
I_{n-r}
\end{array}\right]=N_{4 \times 2}=\left[\begin{array}{cc}
-2 & 2 \\
1 & 0 \\
0 & -2 \\
0 & 1
\end{array}\right]</script><p>由原始$\boldsymbol A$第一列可知$\vec{a}_{2}=2 \vec{a}_{1}, \vec{a}_{4}=-2 \vec{a}_{1}+2 \vec{a}_{3}$，则：</p>
<script type="math/tex; mode=display">
A=\left(\vec{a}_{1}, 2 \vec{a}_{1}, \vec{a}_{3},-2 \vec{a}_{1}+2 \vec{a}_{3}\right)=\left(\vec{a}_{1}, 2 \vec{a}_{1}, \overrightarrow{0},-2 \vec{a}_{1}\right)+\left(\overrightarrow{0}, \overrightarrow{0}, \vec{a}_{3}, 2 \vec{a}_{3}\right)</script><p>即：</p>
<script type="math/tex; mode=display">
\boldsymbol A=\left[\begin{array}{cccc}
1 & 2 & 0 & -2 \\
2 & 4 & 0 & -4 \\
3 & 6 & 0 & -6
\end{array}\right] + \left[\begin{array}{cccc}
0 & 0 & 2 & 4 \\
0 & 0 & 6 & 12 \\
0 & 0 & 8 & 16
\end{array}\right]</script><p>这就将$\boldsymbol A$ 分解成了两个秩1矩阵的和。</p>
<h5 id="11-4-2-问题2"><a href="#11-4-2-问题2" class="headerlink" title="11.4.2 问题2"></a>11.4.2 问题2</h5><p>但$s \times n$型矩阵所有秩1矩阵所构成的子集显然不是一个子空间。</p>
<h5 id="11-4-3-问题3"><a href="#11-4-3-问题3" class="headerlink" title="11.4.3 问题3"></a>11.4.3 问题3</h5><p>我们来看这样一个定义在数域$\boldsymbol K$上的列向量的集合$\boldsymbol W$：</p>
<script type="math/tex; mode=display">
\left\{\left[v_{1}, v_{2}, v_{3}, v_{4}\right]^{\mathrm T} \mid v_{1}+v_{2}+v_{3}+v_{4}=0, \quad v_{1}, v_{2}, v_{3}, v_{4} \in \boldsymbol K\right\}</script><p>它是$\boldsymbol K^4(\mathbb{K}^4)$的子空间吗？</p>
<p>我们很容易验证它对加法和数乘运算都封闭，所以$\boldsymbol W$是$\boldsymbol K^4$的一个子空间。</p>
<p>那么，这个子空间结构是什么样子，换句话说，它的基和维数又是什么？</p>
<p>观察到$v_{1}+v_{2}+v_{3}+v_{4}=0$，这很像求解$\boldsymbol{Ax = 0}$ 时将化成的列向量组的形式，那么可以构造一个$\boldsymbol A = [1, 1,1, 1]$。这样，求$\boldsymbol W$的维数就变成了求 $\boldsymbol A$的零空间的维数。而$\text{rank}(\boldsymbol A) = 1$所以 $\text{rank }N(\boldsymbol A) = 3$ ，即$\text{rank}(\boldsymbol W) = 3$。同理，我们可以求出$\boldsymbol W$ 的一个基，即$N(\boldsymbol A)$的一个基为：</p>
<script type="math/tex; mode=display">
N_{n \times(n-r)}=\left[\begin{array}{c}
-F_{r \times(n-r)} \\
I_{n-r}
\end{array}\right]=N_{4 \times 3}=\left[\begin{array}{ccc}
-1 & -1& -1 \\
1 & 0 & 0\\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]</script><h4 id="11-5小世界图-Small-world-graphs"><a href="#11-5小世界图-Small-world-graphs" class="headerlink" title="11.5小世界图 Small world graphs"></a>11.5小世界图 <strong>Small world graphs</strong></h4><p>介绍小世界图主要是引出<strong>图论和线性代数</strong>的联系。</p>
<p>在这里，“图”G是结点和边的集合$G = \begin{Bmatrix} \text{node}, \text{edge} \end{Bmatrix}​$ ：                </p>
<p><img src="https://pic.imgdb.cn/item/62ad3a680947543129628128.jpg" style="zoom:30%"></p>
<p>此图包含5个结点和6条边，我们可以利用一个$5 \times 6$矩阵完全描述它。</p>
<p>我们可以用图来描述一个实际问题，如果每个人是一个结点，两个人互相认识为一个边，那么整个美国可以以此构成一张大图。我们可以通过这张图来确认两个人之间的最短距离是多少，即两个人需要通过最少几个朋友才能建立联系。G本人和克林顿之间的距离为2，他的一个朋友是参议员，他认识这个参议员朋友，那个人认识克林顿。班里的学生跟克林顿的距离因此不会大于3。还可以继续算希拉里和莱温斯基之间的距离，哈哈。</p>
<p>所谓“六度分割理论”（six degrees of separation）猜想一个人和陌生人之间间隔的点不会超过六个。因此当陌生的两人聊起这种联系都会感叹：“世界真小啊！”这也是“小世界图”这个名字的由来。</p>
<h3 id="12-图、网络、关联矩阵"><a href="#12-图、网络、关联矩阵" class="headerlink" title="12 图、网络、关联矩阵"></a>12 图、网络、关联矩阵</h3><p>本讲讨论线性代数在物理系统中的应用。可参考链接为：</p>
<h4 id="12-1-图和网络-Graphs-amp-Networks"><a href="#12-1-图和网络-Graphs-amp-Networks" class="headerlink" title="12.1 图和网络 Graphs &amp; Networks"></a>12.1 <strong>图和网络</strong> Graphs &amp; Networks</h4><p>图是结点(node)和边(edge)的一个集合。</p>
<p><img src="https://pic.imgdb.cn/item/62ad3be8094754312964443f.jpg" style="zoom:70%"></p>
<p>边线上的箭头代表从结点流出的正方向。上图里包含4个结点，5条边，我们可以将每条边都指定参考方向用于区分正负，比如一个电路网络。在此例子中，将使用电势、回路、电流之类的词汇（当然这个模型还可以表示为液压系统、建筑结构等）。我们通过构造一个incidence matrix关联矩阵来解析这个图的含义。</p>
<h4 id="12-2-关联矩阵（Incidence-matrices）"><a href="#12-2-关联矩阵（Incidence-matrices）" class="headerlink" title="12.2 关联矩阵（Incidence matrices）"></a>12.2 关联矩阵（Incidence matrices）</h4><p>构造一个矩阵来表示图的内在含义，此矩阵称为关联矩阵，图中每个结点代表一列，每边代表一行。则上图为$5 \times 4$矩阵。反过来从这个矩阵出发我们也能画出图。</p>
<center><img src="https://pic.imgdb.cn/item/62ad3c83094754312964ed92.jpg" style="zoom:40%"></center>

<p>第1行代表边①，从结点1流出记为-1，从结点2流入记为 1。也就是从结点1流向了结点2。</p>
<p>边①、边②和边③构成了一个回路，称为<strong>环</strong>（loop）。反映在矩阵上是这三个行向量线性相关。</p>
<p>源于现实问题的关联矩阵，通常描述了问题的结构。如果我们研究一个很大的图，则会构建一个很大的矩阵，但这个矩阵会是<strong>稀疏矩阵</strong>。</p>
<h5 id="12-2-1-零空间"><a href="#12-2-1-零空间" class="headerlink" title="12.2.1 零空间"></a>12.2.1 零空间</h5><p>考察矩阵的零空间，即求$\boldsymbol{Ax = 0}$的解。零空间告诉我们列向量线性组合的状态。这里$\boldsymbol x$的分量表示的是每个节点。</p>
<script type="math/tex; mode=display">
\boldsymbol{A x}=\left[\begin{array}{l}
x_{2}-x_{1} \\
x_{3}-x_{2} \\
x_{3}-x_{1} \\
x_{4}-x_{1} \\
x_{4}-x_{3}
\end{array}\right]=\left[\begin{array}{l}
0 \\
0 \\
0 \\
0 \\
0
\end{array}\right]</script><p>如果$\boldsymbol x$为结点上的电势，则$\boldsymbol{Ax}$给出了每个边上的电势差。求解可以得到零空间为一维$\dim N(\boldsymbol A) = 1$，它的基就是$\left(\begin{array}{l}1 \\1 \\1 \\1 \end{array}\right)$，解集则是$\boldsymbol x = c \left(\begin{array}{l}1 \\1 \\1 \\1 \end{array}\right)$代表等电势，说明等电势条件下不会有电流产生。常数$c$的确定需要边界条件，比如我们将结点4接地，则$x_4 = 0$ 。</p>
<h5 id="12-2-2-列空间"><a href="#12-2-2-列空间" class="headerlink" title="12.2.2 列空间"></a>12.2.2 列空间</h5><p>若求$\boldsymbol{Ax = b}$的解，则相当于在给定了电压$\boldsymbol b$的情况下，求各点的电势，但实际上我们得不到电势的准确值，因为零空间有常数解$c$ ，各点得到的电势需要加上常数$c$，这很类似于求积分要加上常函数，常数值需要边界条件来确定。</p>
<p>矩阵的列数为4，而其零空间的维数为1，则矩阵的秩为3，矩阵第1列、第2列和第4列的列向量线性无关。</p>
<p>考察矩阵列空间，一个重要的问题就是对于什么样的$\boldsymbol b$，$\boldsymbol{Ax = b}$有解。边①、边②和边③构成了环，这三个行向量线性相关，同样的情况还有边④、边⑤和边③构成的环。</p>
<p>我们沿着第一幅图中的一个环边$(1, 3, -2)$对电势差求和：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\left(x_{2}-x_{1}\right)+\left(x_{3}-x_{2}\right)-\left(x_{3}-x_{1}\right)=0 \\
&x_{2}-x_{1}=b_{1}, x_{3}-x_{2}=b_{3}, x_{3}-x_{1}=b_{2}
\end{aligned}</script><p>所以$\boldsymbol b$的分量满足$b_1+b_3-b_2 = 0$和$b_3-b_4+b_5 = 0$。如果把边①、边②、边④、边⑤构成的大环也表示出来则还可以得到一个等式$b_1-b_2+b_4-b_5 = 0$，但实际上这个等式就是之前这两个等式的组合。这两个等式就是<strong>基尔霍夫电压定律</strong>(Kirchhoff’s Voltage law)，即环路电势差之和为零。</p>
<h5 id="12-2-3-左零空间"><a href="#12-2-3-左零空间" class="headerlink" title="12.2.3 左零空间"></a>12.2.3 左零空间</h5><p>矩阵的左零空间是满足$\boldsymbol A^{\mathrm T} \boldsymbol y = 0$的向量$\boldsymbol y$的集合。其中$\boldsymbol y$的每个分量表示的是每个边。因为矩阵$\boldsymbol A^{\mathrm T} $有5列，且矩阵的秩为3，因此矩阵的左零空间维数为2。这反应了行向量的线性关系，整个“图”中，环数为2。</p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{T} \boldsymbol{y}=\left[\begin{array}{ccccc}
-1 & 0 & -1 & -1 & 0 \\
1 & -1 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & -1 \\
0 & 0 & 0 & 1 & 1
\end{array}\right]\left[\begin{array}{l}
y_1 \\
y_2 \\
y_3 \\
y_4 \\
y_5
\end{array}\right]=\left[\begin{array}{l}
0 \\
0 \\
0 \\
0
\end{array}\right]</script><p>其中$\boldsymbol y$的分量的值为“边”上的电流。在电势差和电流之间建立联系就是<strong>欧姆定律</strong>(Ohm’s Law)。</p>
<p><img src="https://pic.imgdb.cn/item/62ad40a609475431296a5750.jpg" style="zoom:50%"></p>
<p>我们求解$\boldsymbol A^{\mathrm T} \boldsymbol y = 0$就是在求5个满足基尔霍夫电流定律(Kirchhoff’s Law)的电流值。</p>
<p>$\boldsymbol A^{\mathrm T} \boldsymbol y = 0$的方程形式$\left\{\begin{array}{r}-y_{1}-y_{3}-y_{4}=0 \\ y_{1}-y_{2}=0 \\ y_{2}+y_{3}-y_{5}=0 \\ y_{4}+y_{5}=0\end{array}\right.$，每一个方程关于一个结点，方程表示结点电流值为0，即流入等于流出。</p>
<p>从图上解方程，而不是采用消元法解方程。如果我们设定$y_1 = 1$组成的回路的“环流“为0，则有$y_2 = 1, y_3 = -1$可解得$\boldsymbol y =\left[\begin{array}{c} 1 \\ 1 \\ -1 \\ 0 \\ 0\end{array}\right]$。取另一个回路的环流为0，则有$y_3 = 1, y_4 = -1, y_5 = 1$可解得$\boldsymbol y =\left[\begin{array}{c} 0 \\ 0 \\ 1 \\ -1 \\ 1\end{array}\right]$。如果设定$y_1, y_2, y_3, y_4, y_5$组成的大回路环流为0，则可以得到另一个向量$\boldsymbol y$，而该向量在零空间内，是前两个向量的线性组合。 </p>
<h5 id="12-2-4-行空间——对应边"><a href="#12-2-4-行空间——对应边" class="headerlink" title="12.2.4 行空间——对应边"></a>12.2.4 行空间——对应边</h5><p>考察矩阵的行空间，因为矩阵秩$r = 3$，所以存在3个线性无关的向量。第1行、第2行和第4行为线性无关，在“图”中，边①、边②和边④构成了一张小图，这三个边没有形成回路。线性相关问题等价于形成回路。没有回路的小图包含4个结点和3条边，再添加一条边就会产生回路，在矩阵里表现为在第1行、第2行和第4行之上再添加一个行向量就会变为线性相关。<strong>没有回路的图称为“树”</strong>。</p>
<p>思考一下维数公式的在“图”中的意义：</p>
<ul>
<li>左零空间维数$\dim N(\boldsymbol A^{\mathrm T}) = m - r$</li>
<li>等价于“环”数量 = “边”数量-(“结点”数量-1)</li>
</ul>
<p>即<strong><font color="red">Eular公式</font></strong>：</p>
<script type="math/tex; mode=display">
“结点” − “边” + “环” = 1</script><p>对所有图都成立。$矩阵的秩r = 结点 − 1$，因为r表示了线性无关的边的数目，也就是“树”中“边”的数目。</p>
<p><img src="https://pic.imgdb.cn/item/62ad44f209475431296fc86b.jpg" style="zoom:50%"></p>
<p>之前的讨论都是针对于一个无源的电场，如果加入电源则情况又不同，例如加入电流源相当于将基尔霍夫定律的方程变为$\boldsymbol A^{\mathrm T} \boldsymbol y = f$，$f$就是外部流入的电流。将$\boldsymbol{e = Ax}, \boldsymbol y = c \boldsymbol e, \boldsymbol A^{\mathrm T} \boldsymbol y = f$ ，三个等式结合得到应用数学中的基本方程$\boldsymbol A^{\mathrm T}c\boldsymbol A \boldsymbol x = f$。</p>
<p>关于方程$\boldsymbol A^{\mathrm T}c\boldsymbol A \boldsymbol x = f$的更多内容可以阅读GS老先生08的书“Computational science and engineering”的第二章。</p>
<h3 id="13-习题课1"><a href="#13-习题课1" class="headerlink" title="13 习题课1"></a>13 习题课1</h3><p>具体内容参考：<a href="https://zhuanlan.zhihu.com/p/45855579">MIT—线性代数笔记13 复习一 - 三少爷的键的文章 - 知乎</a></p>
<hr>
<p>资料链接：<a href="https://www.zhihu.com/column/c_1029672383375949824">三少爷的MIT笔记</a>，超强数学笔记！！！</p>
<p>资料链接：<a href="https://blog.csdn.net/xhf0374/article/details/63250083">MIT 公开课：Gilbert Strang《线性代数》课程笔记（汇总）</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>数学基础</category>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>本质问题</tag>
        <tag>线代</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数_Part2</title>
    <url>/2022/04/05/LinearAlgebra2/</url>
    <content><![CDATA[<h3 id="第十四讲-正交向量与正交子空间"><a href="#第十四讲-正交向量与正交子空间" class="headerlink" title="第十四讲 正交向量与正交子空间"></a>第十四讲 正交向量与正交子空间</h3><p><img src="https://pic.imgdb.cn/item/625423a7239250f7c5047658.jpg" style="zoom:70%"></p>
<h4 id="14-1-正交向量-Orthogonal-vectors"><a href="#14-1-正交向量-Orthogonal-vectors" class="headerlink" title="14.1 正交向量 Orthogonal vectors"></a>14.1 正交向量 Orthogonal vectors</h4><p><img src="https://pic.imgdb.cn/item/625423c4239250f7c504af7f.jpg"></p>
<p>正交就是垂直(perpendicular)的另一种说法。两向量正交的判据之一是其点积$\boldsymbol x^{\text T} \boldsymbol y = \boldsymbol y^{\text T} \boldsymbol x = 0$。当两个向量的夹角为90度时，按照毕达哥拉斯定理，$\boldsymbol {x,y}$ 满足：</p>
<script type="math/tex; mode=display">
||\boldsymbol x||^2 + ||\boldsymbol y||^2 = ||\boldsymbol{x+y }||^2,其中||\boldsymbol x||^2 = \boldsymbol x^{\text T} \boldsymbol x</script><p>例如<script type="math/tex">\boldsymbol  x=\left(\begin{array}{l} 1 \\ 2 \\ 3 \end{array}\right), \boldsymbol  y=\left(\begin{array}{l} 2 \\ -1 \\ 0 \end{array}\right)</script>，则<script type="math/tex">\boldsymbol  {x+y}=\left(\begin{array}{l} 3 \\ 1 \\ 3 \end{array}\right), ||\boldsymbol x||^2=14,  ||\boldsymbol y||^2=5, ||\boldsymbol{x+y }||^2 = 19</script>。</p>
<p>将勾股定理展开进行计算，则有</p>
<script type="math/tex; mode=display">
\boldsymbol x^{\text T} \boldsymbol x + \boldsymbol y^{\text T} \boldsymbol y = (\boldsymbol x + \boldsymbol y)^{\text T} (\boldsymbol x + \boldsymbol y) = \boldsymbol x^{\text T} \boldsymbol x + \boldsymbol y^{\text T} \boldsymbol y +\boldsymbol x^{\text T} \boldsymbol y + \boldsymbol y^{\text T} \boldsymbol x</script><p>得到$\boldsymbol x^{\text T} \boldsymbol y =0$。</p>
<p>零向量与所有向量都正交。 </p>
<h4 id="14-2-正交子空间-Orthogonal-subspaces"><a href="#14-2-正交子空间-Orthogonal-subspaces" class="headerlink" title="14.2 正交子空间 Orthogonal subspaces"></a>14.2 正交子空间 Orthogonal subspaces</h4><p>子空间$\boldsymbol S$与子空间$\boldsymbol T$正交，则$\boldsymbol S$中的任意一个向量都和$\boldsymbol T$中的任意向量正交。黑板所在的平面和地板所在平面不是正交关系，沿两者的交线方向的向量同时属于两个平面，但并不与自己正交。<font color="green"><strong>所以如果两个平面的交点（交线）如果不是零向量，则它们就不是正交的。</strong></font></p>
<p>如果在平面内(二维空间)讨论正交子空间，平面的子空间包括只包含零向量的0空间、过原点的直线以及整个平面。经过原点的直线不会和整个空间正交(二维空间限制下过原点的直线在平面内，不要想成3维空间了)；0空间和过原点的直线正交(<strong>0空间与任意空间正交</strong>)；经过原点的两条直线若夹角为直角则互相正交。</p>
<h4 id="14-3-零空间与行空间正交-Nullspace-is-perpendicular-to-row-space"><a href="#14-3-零空间与行空间正交-Nullspace-is-perpendicular-to-row-space" class="headerlink" title="14.3 零空间与行空间正交 Nullspace is perpendicular to row space"></a>14.3 零空间与行空间正交 Nullspace is perpendicular to row space</h4><p>矩阵$\boldsymbol  A$的行空间和它的零空间正交。若在零空间内，则有$\boldsymbol {Ax} = \boldsymbol 0$，将$\boldsymbol A$表示为行向量的格式：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
\operatorname{row}_{1} \\
\operatorname{row}_{2} \\
\vdots \\
\text { row }_{m}
\end{array}\right][\boldsymbol x]=\left[\begin{array}{c}
\operatorname{row}_{1} \cdot \boldsymbol x \\
\operatorname{row}_{2} \cdot\boldsymbol  x \\
\vdots \\
\operatorname{row}_{m} \cdot \boldsymbol x
\end{array}\right]=\left[\begin{array}{c}
0 \\
0 \\
\vdots \\
0
\end{array}\right]</script><p>$\boldsymbol x$与矩阵$\boldsymbol A$的行向量点积都等于0，则它和矩阵$\boldsymbol A$行向量的线性组合进行点积也为 0，所以$\boldsymbol x$与矩阵$\boldsymbol A$的行空间正交。$\boldsymbol x$为零空间内的任意向量，所以零空间与行空间正交。（一vs多-&gt;多vs多）</p>
<p>同理可以证明列空间与左零空间正交。</p>
<p>行空间和零空间实际上把$R^n$空间分割成了两个正交的子空间。例如对于矩阵：</p>
<script type="math/tex; mode=display">
A=\left[\begin{array}{lll}
1 & 2 & 5 \\
2 & 4 & 10
\end{array}\right]</script><p>则其行空间是1维的，向量<script type="math/tex">\left(\begin{array}{l} 1 \\ 2 \\ 5 \end{array}\right)</script> 是它的基向量，而其零空间是垂直于<script type="math/tex">\left(\begin{array}{l} 1 \\ 2 \\ 5 \end{array}\right)</script> 并穿过原点的2维平面。其中零空间的法向量为<script type="math/tex">\left(\begin{array}{l} 1 \\ 2 \\ 5 \end{array}\right)</script>。</p>
<h4 id="14-4-正交补Orthogonal-Complements"><a href="#14-4-正交补Orthogonal-Complements" class="headerlink" title="14.4 正交补Orthogonal Complements"></a>14.4 正交补Orthogonal Complements</h4><p>行空间和零空间不仅仅是正交，并且其维数之和等于$n$，我们称行空间和零空间为$R^n$空间内的<font color="red"><strong>正交补(orthogonal complements)</strong></font>。</p>
<script type="math/tex; mode=display">
\dim N(\boldsymbol A) + \dim C(\boldsymbol A^{\text T}) = n</script><p>正交补的概念很重要，它表明$R^n$中的任何一个向量$\boldsymbol x$都可以<font color="blue">拆分为互为正交补的子空间的向量之和</font>。以$\boldsymbol {Ax = b}$的解$\boldsymbol x$为例，$\boldsymbol x$是$R^n$中的向量，$\boldsymbol x$可以拆分为在行空间的分量$\boldsymbol x_r$和零空间中的分量$\boldsymbol x_n$，即$\boldsymbol x = \boldsymbol x_r + \boldsymbol x_n$，$\boldsymbol {Ax=b}$可以进行如下拆解</p>
<script type="math/tex; mode=display">
零空间分量：\boldsymbol {Ax}_n = \boldsymbol 0 \\
行空间分量：\boldsymbol {Ax}_r = \boldsymbol b</script><p>这表示零空间包含所有和行空间正交的向量，反之亦然。 想想我们之前提到的黑板和地板平面不是正交子空间的例子，二者都在 3 维空间中，分别为 2 维空间，因此不可能正交。<font color="red">一个空间中正交子空间的维数之和不可能超过原空间的维数</font>。</p>
<p><strong>我们可以称目前讨论的这部分内容是线性代数基本定理的第二部分。第一部分是给出四个子空间和它们的维数，第二部分说明它们是两两互为正交补，第三部分讨论子空间的正交基。 这些内容都反映在了本讲座开始的那幅图上。</strong></p>
<h4 id="14-5-矩阵-boldsymbol-A-text-T-boldsymbol-A"><a href="#14-5-矩阵-boldsymbol-A-text-T-boldsymbol-A" class="headerlink" title="14.5 矩阵$\boldsymbol A^{\text T} \boldsymbol A$"></a>14.5 矩阵$\boldsymbol A^{\text T} \boldsymbol A$</h4><p>下面讨论如何求解一个<strong>无解</strong>方程组$\boldsymbol {Ax} = \boldsymbol b$的解($\boldsymbol b$不在$\boldsymbol A$的列空间中)。如果$\boldsymbol A$是长方形矩阵，$m$大于$n$(方程数大于未知数)。当左侧方程数特别多的时候，容易混入“坏”数据，方程变得无解。但是对于数据的可信度我们无从判断，线性代数要做的就是在这种条件下求一个方程的“最优解”（把好数据筛选出来）。其中一种求解方法是删掉一些方程，使得矩阵变成可逆的方阵，然后进行求解。但是由于无法判断哪些数据是好数据，哪些是坏数据。希望利用所有测量值求出最优解，从而得到最完整的信息。</p>
<p>矩阵$\boldsymbol A^{\text T} \boldsymbol A$会发挥重要作用，它是一个$n \times n$方阵，并且是对称阵，$(\boldsymbol A^{\text T} \boldsymbol A)^{\text T} = \boldsymbol A^{\text T} \boldsymbol A$。本章的核心内容就是当$\boldsymbol {Ax} = \boldsymbol b$无解的时候，求解$\boldsymbol A^{\text T}\boldsymbol A \hat {\boldsymbol x}= \boldsymbol A^{\text T} \boldsymbol b$得到<strong>最优解</strong>。<br><img src="https://pic.imgdb.cn/item/6254d8cd239250f7c5e5984d.jpg"></p>
<p>实际上$N(\boldsymbol A^{\text T} \boldsymbol A) = N(\boldsymbol A)$，并且矩阵$\boldsymbol A^{\text T} \boldsymbol A$的秩等于$\boldsymbol A$的秩。因此矩阵$\boldsymbol A^{\text T} \boldsymbol A$可逆要求$\boldsymbol A$的零空间只有零向量，即$\boldsymbol A$的列向量线性无关(或者说$\boldsymbol A$列满秩)。</p>
<h3 id="第十五讲-子空间投影"><a href="#第十五讲-子空间投影" class="headerlink" title="第十五讲 子空间投影"></a>第十五讲 子空间投影</h3><h4 id="15-1-投影-射影-Projections"><a href="#15-1-投影-射影-Projections" class="headerlink" title="15.1 投影(射影)Projections"></a>15.1 投影(射影)Projections</h4><p>简单来说，两个向量之间的投影是将其中一个向量转换到另外一个向量的方向上，然后求内积。 </p>
<p><img src="https://pic.imgdb.cn/item/6254ea45239250f7c5fe5d00.jpg" style="zoom:70%"></p>
<p><strong>投影问题的几何解释就是：如何在向量$\boldsymbol a$的方向上寻找与向量$\boldsymbol b$距离最近的一点</strong>。从图中可以看出，这个距离最近的点$\boldsymbol p$就位于穿过$\boldsymbol b$点并与向量$\boldsymbol a$正交的直线与向量$\boldsymbol a$所在直线的交点上。这就是$\boldsymbol b$在$\boldsymbol a$上的投影。如果我们将向量$\boldsymbol p$视为$\boldsymbol b$的一种近似，则长度$\boldsymbol {e = b-p}$就是这一近似的误差。</p>
<p>因为在向量$\boldsymbol a$的方向上，因此可以令$\boldsymbol p = x \boldsymbol a$，而因为它和$\boldsymbol e$正交，我们可以得到方程：$\boldsymbol a^{\text T}(\boldsymbol b - x \boldsymbol a) = 0$。（该结果是把$\boldsymbol p$与$\boldsymbol e$正交转换成了$\boldsymbol a$与$\boldsymbol e$正交，本质上只是两边同除了个数）。</p>
<p>解得： $x = \dfrac{\boldsymbol a^{\text T} \boldsymbol b}{\boldsymbol a^{\text T} \boldsymbol a}$，$\boldsymbol p = x \boldsymbol a = \boldsymbol a x = \boldsymbol a \dfrac{\boldsymbol a^{\text T} \boldsymbol b}{\boldsymbol a^{\text T} \boldsymbol a}$</p>
<p>如果$\boldsymbol b$变为原来的2倍，则$\boldsymbol  p$也变为原来的2倍。 而如果$\boldsymbol  a$变为原来的2倍，$\boldsymbol p$不发生变化。从几何上和计算中都会得到验证。</p>
<h4 id="15-2-投影矩阵"><a href="#15-2-投影矩阵" class="headerlink" title="15.2 投影矩阵"></a>15.2 投影矩阵</h4><p>我们将投影问题用投影矩阵的方式进行描述，即为$\boldsymbol p = \boldsymbol {Pb}$，其中$\boldsymbol P$为投影矩阵。</p>
<p>Q：为什么是$\boldsymbol p = \boldsymbol {Pb}$，而不是$\boldsymbol p = \boldsymbol {bP}$呢？   </p>
<p>A：一来是由于上述的推导，二来可以通过反证法，$\boldsymbol p = \boldsymbol {Pb}$很显然的是$\boldsymbol p$的维度和$\boldsymbol b$的是一致的（维度相同的列向量）。而反之不成立。 </p>
<p>$\boldsymbol p = \boldsymbol a x = \boldsymbol a \dfrac{\boldsymbol a^{\text T} \boldsymbol b}{\boldsymbol a^{\text T} \boldsymbol a}$，则有$\boldsymbol P = \dfrac{\boldsymbol a \boldsymbol a^{\text T} }{\boldsymbol a^{\text T} \boldsymbol a}$，其分子$\boldsymbol a  \boldsymbol a^{\text T}$ 是一个矩阵，而分母是一个数。</p>
<p>观察这个矩阵可知，矩阵$\boldsymbol P$的列空间就是向量$\boldsymbol a$所在的直线，矩阵的秩是1。投影矩阵$\boldsymbol P$是一个对称矩阵。另一方面，如果做两次投影则有第二次投影还在原来的位置。因此矩阵$\boldsymbol P$有如下性质：</p>
<ul>
<li>$\boldsymbol P^{\text T} = \boldsymbol P$</li>
<li>$\boldsymbol P^2 = \boldsymbol P$</li>
</ul>
<h4 id="15-3-为什么要投影-Why-Project"><a href="#15-3-为什么要投影-Why-Project" class="headerlink" title="15.3 为什么要投影 Why Project"></a>15.3 为什么要投影 Why Project</h4><p>如前所述，方程$\boldsymbol {Ax} = \boldsymbol b$有可能无解，我们需要得到方程的“最优解”。这里的题在于向量$\boldsymbol {Ax}$一定在矩阵$\boldsymbol b$的列空间之内，但是$\boldsymbol b$不一定，因此我们希望将$\boldsymbol b$投影到$\boldsymbol A$的列空间得到$\boldsymbol p$，将问题转化为求解$\boldsymbol {A \hat x} = \boldsymbol p$。</p>
<h4 id="15-4-在高维投影-Projection-in-higher-dimensions"><a href="#15-4-在高维投影-Projection-in-higher-dimensions" class="headerlink" title="15.4 在高维投影 Projection in higher dimensions"></a>15.4 在高维投影 Projection in higher dimensions</h4><p>在$R^3$空间内，如何将向量$\boldsymbol b$投影到它距离平面最近的一点$\boldsymbol p$？<br><img src="https://pic.imgdb.cn/item/6257cb08239250f7c5c15905.jpg"></p>
<p>如果$\boldsymbol a_1$和$\boldsymbol a_2$构成了平面的一组基，则平面就是矩阵$\boldsymbol A = [\boldsymbol a_1; \boldsymbol a_2]$的列空间。</p>
<p>已知向量$\boldsymbol p$在平面内，则有$\boldsymbol p = \hat x_1 \boldsymbol a_1 + \hat x_2 \boldsymbol a_2 = \boldsymbol {A \hat x}$。$\boldsymbol e = \boldsymbol{b-p}$与投影平面正交（<strong>重点</strong>），因此 $\boldsymbol e$与$\boldsymbol {a_1,a_2}$均正交，因此可以得到： $\boldsymbol a_1^{\text T}(\boldsymbol b - \boldsymbol {A \hat x})=0$并且$\boldsymbol a_2^{\text T}(\boldsymbol b - \boldsymbol {A \hat x})=0$。因为$\boldsymbol a_1$和$\boldsymbol a_2$分别为矩阵$\boldsymbol A$的列向量，即$\boldsymbol a_1^{\text T}$和$\boldsymbol a_2^{\text T}$分别为矩阵$\boldsymbol A^{\text T}$的行向量，所以将两个方程式写成矩阵形式即为 $\boldsymbol A^{\text T }(\boldsymbol b-\boldsymbol {A \hat x}) = \boldsymbol 0$。这与一维投影的方程形式相同。 </p>
<p>向量 $\boldsymbol e = \boldsymbol b - \boldsymbol {A \hat x}$存在于矩阵 $\boldsymbol A^{\text T}$的零空间$N(\boldsymbol A^{\text T})$里，从上一讲讨论子空间的正交性可知，向量$\boldsymbol e$与矩阵$\boldsymbol A$的列空间正交，这也正是方程的意义。</p>
<p>将方程$\boldsymbol A^{\text T}(\boldsymbol b - \boldsymbol{A \hat x})= \boldsymbol 0$改写，可得$\boldsymbol A^{\text T} \boldsymbol{A \hat x} = \boldsymbol A^{\text T} \boldsymbol b$</p>
<script type="math/tex; mode=display">
\begin{gathered}
\hat{\boldsymbol x}=\left(\boldsymbol A^{\text T} \boldsymbol A\right)^{-1} \boldsymbol A^{\text T} \boldsymbol b \\
\boldsymbol p=\boldsymbol A \hat{\boldsymbol x}=\boldsymbol A\left(\boldsymbol A^{\text T} \boldsymbol A\right)^{-1} \boldsymbol A^{\text T} \boldsymbol b \\
\boldsymbol P=\boldsymbol A\left( \boldsymbol A^{\text T}  \boldsymbol A\right)^{-1} \boldsymbol A^{\text T}
\end{gathered}</script><blockquote>
<p>注意：这里由于考虑矩阵$\boldsymbol A$是列满秩的，因此$\boldsymbol A^{\text T} \boldsymbol A$一定是方阵可逆。</p>
</blockquote>
<p>因为矩阵$\boldsymbol A$不是方阵，无法简单的用$\left(\boldsymbol A^{\text T} \boldsymbol A\right)^{-1}=\boldsymbol A^{-1}\left(\boldsymbol A^{\text T}\right)^{-1}$对投影矩阵公式进行化简。若$\boldsymbol A$是可逆方阵，则化简得到$\boldsymbol{P = I}$。此时$\boldsymbol A$的列空间就是整个$R^n$ 空间，$\boldsymbol b$到这个空间的投影就是其本身，投影矩阵等于单位阵。</p>
<p>对$\boldsymbol P=\boldsymbol A\left( \boldsymbol A^{\text T}  \boldsymbol A\right)^{-1} \boldsymbol A^{\text T}$用矩阵乘法的结合律和矩阵乘积的转置公式，可以证明投影矩阵的性质： </p>
<ul>
<li>$\boldsymbol P^{\text T} = \boldsymbol P$</li>
<li>$\boldsymbol P^2 = \boldsymbol P$</li>
</ul>
<h4 id="15-5-最小二乘法"><a href="#15-5-最小二乘法" class="headerlink" title="15.5 最小二乘法"></a>15.5 最小二乘法</h4><p><img src="https://pic.imgdb.cn/item/6257d401239250f7c5cf756d.jpg"></p>
<p>应用投影矩阵求方程组最优解的方法，最常用于“最小二乘法”拟合曲线。 </p>
<p>有三个数据点$(1,1), (2,2), (3,2)$，求直线方程$b = C + Dt $，要求直线尽量接近于三个点。把三个点的数据代入方程则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&C+D=1 \\
&C+2 D=2 \\
&C+3 D=2
\end{aligned}</script><p>矩阵形式为</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}
1 & 1 \\
1 & 2 \\
1 & 3
\end{array}\right]\left[\begin{array}{l}
C \\
D
\end{array}\right]=\left[\begin{array}{l}
1 \\
2 \\
2
\end{array}\right]</script><p>这个的方程$\boldsymbol {Ax=b}$是无解的，解决办法就是求其最优解，即方程$\boldsymbol A^{\text T} \boldsymbol {Ax=A^{\text T}b}$的解。</p>
<h3 id="第十六讲-投影矩阵和最小二乘法"><a href="#第十六讲-投影矩阵和最小二乘法" class="headerlink" title="第十六讲 投影矩阵和最小二乘法"></a>第十六讲 投影矩阵和最小二乘法</h3><h4 id="16-1-投影-射影-Projections"><a href="#16-1-投影-射影-Projections" class="headerlink" title="16.1 投影(射影)Projections"></a>16.1 投影(射影)Projections</h4><p>上一讲介绍了投影矩阵$\boldsymbol P = \boldsymbol{A(A^{\text T}A)^{-1}A^{\text T}}$，当它作用于向量$\boldsymbol b$，相当于把$\boldsymbol b$投影到矩阵$\boldsymbol A$的列空间中。其中$\boldsymbol p = \boldsymbol{Pb}$，$\boldsymbol p$为投影后的向量。</p>
<p>如果向量$\boldsymbol b$本身就在$\boldsymbol A$列空间之内，即存在$\boldsymbol x$使得$\boldsymbol {Ax = b}$，则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{P} \boldsymbol{b} &=\boldsymbol{A}\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{b}(\text { 将 } \boldsymbol{A} \boldsymbol{x}=\boldsymbol{b} \text { 代入 }) \\
&=\boldsymbol{A}\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x} \\
&=\boldsymbol{A}\left(\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)^{-1}\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)\right) \boldsymbol{x} \\
&=\boldsymbol{A} \boldsymbol{x}
\end{aligned}</script><p>如果向量$\boldsymbol b$与$\boldsymbol A$的列空间正交，即向量$\boldsymbol b$在矩阵$\boldsymbol A$的左零空间$N(\boldsymbol A^{\text T})$中，则有</p>
<script type="math/tex; mode=display">
\boldsymbol {Pb}=\boldsymbol{A}\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{b} = \boldsymbol{A}\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)^{-1} (\boldsymbol{A}^{\mathrm{T}} \boldsymbol{b}) = \boldsymbol{A}\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)^{-1} \boldsymbol 0 = \boldsymbol 0</script><p>向量之间的关系图如下： </p>
<p><img src="https://pic.imgdb.cn/item/62593a42239250f7c5c2f83d.jpg"></p>
<p>从图我们可以总结出：</p>
<ul>
<li>$\boldsymbol {p+e=b}$，说明$\boldsymbol b$由两部分组成；</li>
<li>$\boldsymbol {p=Pb}$为$\boldsymbol {A}$的列空间的部分；</li>
<li>$\boldsymbol {e=(I-P)b}$为$\boldsymbol {A}$的左零空间中的部分；</li>
<li>$\boldsymbol{I-P}$为左零空间的投影矩阵，可以验证$\boldsymbol{(I-P)}^{\mathrm T} = (\boldsymbol {I-P})$，并且$\boldsymbol {(I-P)}^2 = (\boldsymbol {I-P})$。</li>
</ul>
<h4 id="16-2-最小二乘法-Least-Squares"><a href="#16-2-最小二乘法-Least-Squares" class="headerlink" title="16.2 最小二乘法 Least Squares"></a>16.2 最小二乘法 Least Squares</h4><p>应用投影矩阵求方程组最优解的方法，最常用于“最小二乘法”拟合曲线。 </p>
<p><img src="https://pic.imgdb.cn/item/62593c43239250f7c5c64e48.jpg" style="zoom:80%"></p>
<p>有三个数据点$(1,1), (2,2), (3,2)$，求直线方程$b = C + Dt $，要求直线尽量接近于三个点。把三个点的数据代入方程则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&C+D=1 \\
&C+2 D=2 \\
&C+3 D=2
\end{aligned}</script><p>矩阵形式为</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}
1 & 1 \\
1 & 2 \\
1 & 3
\end{array}\right]\left[\begin{array}{l}
C \\
D
\end{array}\right]=\left[\begin{array}{l}
1 \\
2 \\
2
\end{array}\right]</script><p>这个的方程$\boldsymbol {Ax=b}$是无解的，解决办法就是求其最优解，即误差最小，这里误差是指每个方程误差值的平方和$||\boldsymbol e||^2 = ||\boldsymbol {Ax-b}||^2$（误差平方相加），因此就是寻找具有最小误差平方和的解$\boldsymbol x$，这就是所谓的“最小二乘”问题。其中最接近的解为将$\boldsymbol b$投影到$\boldsymbol A$矩阵的系数$\boldsymbol x$。</p>
<script type="math/tex; mode=display">
\min \|\boldsymbol{A x-b}\|^{2}=\|\boldsymbol e\|^{2}=\left\|e_{1}\right\|^{2}+\left\|e_{2}\right\|^{2}+\left\|e_{3}\right\|^{2}</script><p>误差即为数据点到直线距离的平方和。这部分工作可称为线性回归，在数据点中没有“<strong>离群值</strong>” 时，这是非常有用的方法。 </p>
<p><img src="https://pic.imgdb.cn/item/62593d85239250f7c5c871e3.jpg"></p>
<p>从几何上讨论求解过程，就是试图寻找数据点到直线距离的平方和$e_1^2+e_2^2+e_3^2$最小的情况，此时得到的$C+Dt $分别为$p_1,p_2,p_3$ ，它们是满足方程并最接近于$\boldsymbol b$的结果。另一种看法是，对于$R^3$空间上的向量$\boldsymbol b$，它投影到矩阵$\boldsymbol A$的列空间中会得到向量$\boldsymbol p = [p_1,p_2,p_3]^{\mathrm T}$，投影到矩阵$\boldsymbol A$的零空间中则为$\boldsymbol e$。</p>
<p>现在求解$\hat {\boldsymbol x} =  \left(\begin{array}{l} \hat C \\ \hat D \end{array}\right)$和$\boldsymbol p$。</p>
<p>老师称为统计上最重要的方程式： </p>
<script type="math/tex; mode=display">
\boldsymbol {A^{\mathrm T}A \hat x} = \boldsymbol {A^{\mathrm T} b}</script><script type="math/tex; mode=display">
\left[\begin{array}{lll}
1 & 1 & 1 \\
1 & 2 & 3
\end{array}\right]\left[\begin{array}{ll:l}
1 & 1 & 1 \\
1 & 2 & 2 \\
1 & 3 & 2
\end{array}\right]=\left[\begin{array}{rr:r}
3 & 6 & 5 \\
6 & 14 & 11
\end{array}\right] \text {,则有 }\left[\begin{array}{cc}
3 & 6 \\
6 & 14
\end{array}\right]\left[\begin{array}{l}
\hat{C} \\
\hat{D}
\end{array}\right]=\left[\begin{array}{c}
5 \\
11
\end{array}\right]</script><p>得到解为$\hat C = \dfrac{2}{3}, \hat D = \dfrac{1}{2}$。</p>
<p>还可以从误差最小的角度出发求解： </p>
<script type="math/tex; mode=display">
e_{1}^{2}+e_{2}^{2}+e_{3}^{2}=(C+D-1)^{2}+(C+2 D-2)^{2}+(C+3 D-2)^{2}</script><p>对等号右边的表达式求偏导数，极值出现在偏导数为0的位置。<font color="red"><strong>求偏导最终会得到相同的线性方程组和相同的解</strong></font>。展开结果为</p>
<script type="math/tex; mode=display">
\|e\|^{2}=3 C^{2}+14 D^{2}+9-10 C-22 D+12 C D</script><p>求偏导可得到和$\boldsymbol {A^{\mathrm T}A \hat x} = \boldsymbol {A^{\mathrm T} b}$的结果相同。</p>
<p>得到直线表达式$y=2/3+t/2$。将$t=1、2、3$分别代入可得：</p>
<script type="math/tex; mode=display">
\boldsymbol{p}=\left(\begin{array}{c}
\dfrac{7}{6} \\
\dfrac{5}{3} \\
\dfrac{13}{6}
\end{array}\right), \boldsymbol{e}=\left(\begin{array}{c}
-\dfrac{1}{6} \\
\dfrac{1}{3} \\
-\dfrac{1}{6}
\end{array}\right)</script><p>投影向量$\boldsymbol p$与误差向量$\boldsymbol e$是正交的：</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}
\boldsymbol {A^{T} A \hat{x}=A^{T} b \\
p=A \hat{x}}
\end{array}\right.</script><h4 id="16-3-矩阵-boldsymbol-A-mathrm-T-A"><a href="#16-3-矩阵-boldsymbol-A-mathrm-T-A" class="headerlink" title="16.3 矩阵$\boldsymbol {A^{\mathrm T}A}$"></a>16.3 矩阵$\boldsymbol {A^{\mathrm T}A}$</h4><p>证明：若$\boldsymbol A$的列向量线性无关时，矩阵$\boldsymbol {A^{\mathrm T}A}$为可逆矩阵。$\boldsymbol A$的各列线性无关，是最小二乘法成立的前提。</p>
<p>假设存在$\boldsymbol x$使得$\boldsymbol {A^{\mathrm T}Ax = 0}$。则有$\boldsymbol {x^{\mathrm T}A^{\mathrm T}Ax = 0}$(两边同乘以$\boldsymbol x^{\mathrm T}$)，则有</p>
<script type="math/tex; mode=display">
\boldsymbol {x^{\mathrm T}A^{\mathrm T}Ax = (Ax)^{\text T}(Ax)=0}</script><p>因此$\boldsymbol {Ax = 0}$，因为$\boldsymbol A$的列向量线性无关，所以只有当$\boldsymbol {x = 0}$时有$\boldsymbol{Ax = 0}$，因此只有当$\boldsymbol {x = 0}$时有$\boldsymbol {A^{\mathrm T}Ax = 0}$。即矩阵$\boldsymbol {A^{\mathrm T}A = 0}$为可逆矩阵。</p>
<p>如果矩阵的列向量是互相垂直的单位向量，则它们一定是线性无关的。我们将这种向量称之为标准正交组（orthonormal）。 </p>
<script type="math/tex; mode=display">
\text { 例如 : }\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right],\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right],\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right] \text { ;  还有 }\left[\begin{array}{c}
\cos \theta \\
\sin \theta
\end{array}\right] \text { 和 }\left[\begin{array}{c}
\cos \theta \\
\sin \theta
\end{array}\right]</script><blockquote>
<p>本小节我还有一个问题就是最小二乘法中误差$\boldsymbol e$为什么不是做垂直线而是竖直线，不是很明白。【MIT线性代数—-投影矩阵和最小二乘 - August的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/45351889">https://zhuanlan.zhihu.com/p/45351889</a> 】【线性代数视角下的最小二乘法 - 孙孙的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/62694878">https://zhuanlan.zhihu.com/p/62694878</a> 】这两篇文章有评论里面有一个解释，可以好好理解一下。</p>
</blockquote>
<h3 id="第十七讲-正交矩阵和施密特正交化"><a href="#第十七讲-正交矩阵和施密特正交化" class="headerlink" title="第十七讲 正交矩阵和施密特正交化"></a>第十七讲 正交矩阵和施密特正交化</h3><p>本节是“正交”部分的最后内容。Gram-Schmidt 过程可以将原空间的一组基转变为标准正交基。 </p>
<h4 id="17-1-标准正交向量-Orthonormal-vectors"><a href="#17-1-标准正交向量-Orthonormal-vectors" class="headerlink" title="17.1 标准正交向量 Orthonormal vectors"></a>17.1 标准正交向量 Orthonormal vectors</h4><p>满足如下条件的向量$\boldsymbol{q_1, q_2, \dots, q_n}$为标准正交向量：</p>
<script type="math/tex; mode=display">
\boldsymbol{q_{i}^{T} q_{j}}=\left\{\begin{array}{l}
0, i \neq j \\
1, i=j
\end{array}\right.</script><p>换而言之，它们都具有单位长度 1，并且彼此正交。标准正交向量是线性无关的。很多线性代数的计算都建立在标准正交基础上，它让一切变得简单可控(从来不上溢或者下溢)。 </p>
<h4 id="17-2-标准正交矩阵-Orthonormal-matrix"><a href="#17-2-标准正交矩阵-Orthonormal-matrix" class="headerlink" title="17.2 标准正交矩阵 Orthonormal matrix"></a>17.2 标准正交矩阵 Orthonormal matrix</h4><p>如果矩阵$\boldsymbol Q$的列向量为标准正交向量，则$\boldsymbol{Q^{\mathrm T} Q = I}$为单位阵。</p>
<p><img src="https://pic.imgdb.cn/item/6269dc7b239250f7c52e5f8e.jpg" style="zoom:70%"></p>
<p>注意这里的矩阵$\boldsymbol Q$可以不是方阵。我们已经学过了一系列矩阵，包括三角阵、对角阵、置换矩阵、对称矩阵、行最简梯形矩阵、投影矩阵等等，现在有了“标准正交”矩阵。</p>
<p>一个<strong>标准正交的方阵</strong>称之为“正交矩阵”（orthogonal matrix）。如果$\boldsymbol Q$为方阵，因为$\boldsymbol{Q^{\mathrm T} Q = I}$，所以$\boldsymbol{Q^{\mathrm T} = Q^{-1}}$ 。 注意必须是方阵，必须是标准正交，而不只是正交。</p>
<p>例如，置换矩阵 <script type="math/tex">\boldsymbol Q=\left[\begin{array}{lll} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0\end{array}\right]</script>，则有<script type="math/tex">\boldsymbol Q^T=\left[\begin{array}{lll} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0\end{array}\right]</script>，两者皆为正交矩阵，并且两者乘积为单位阵。 </p>
<p>例如， <script type="math/tex">\boldsymbol Q=\left[\begin{array}{lll} \cos x & -\sin x  \\ \sin x & \cos x \end{array}\right]</script>为正交矩阵。 </p>
<p>例如， <script type="math/tex">\boldsymbol Q=\dfrac{1}{2}\left[\begin{array}{cccc} 1 & 1 & 1 & 1 \\ 1 & -1 & 1 & -1 \\ 1 & 1 & -1 & -1 \\ 1 & -1 & -1 & 1\end{array}\right]</script>是由-1 和+1 组成的正交矩阵，这种类型的矩阵称之为阿达玛Hadamard矩阵，不同阶数矩阵性质不同并且没有规律，无从判断几阶的阿达玛矩阵为正交阵。</p>
<h4 id="17-3-标准正交列向量的优势-Orthonormal-columns-are-good"><a href="#17-3-标准正交列向量的优势-Orthonormal-columns-are-good" class="headerlink" title="17.3 标准正交列向量的优势 Orthonormal columns are good"></a>17.3 标准正交列向量的优势 Orthonormal columns are good</h4><p>若$\boldsymbol Q$的列向量为标准正交向量，且我们已知投影到$\boldsymbol Q$的列空间的投影矩阵为：</p>
<script type="math/tex; mode=display">
\boldsymbol {P = Q(Q^{\mathrm T}Q)^{-1}Q^{\mathrm T}}</script><p>证明过程略，可以参考笔记。此时根据$\boldsymbol{Q^{\mathrm T} Q = I}$可得：$\boldsymbol{P = QQ^{\mathrm T}}$，这会降低很多运算量。 </p>
<p>更特殊一点，若$\boldsymbol Q$为方阵，则$\boldsymbol {P=I}$，因为$\boldsymbol Q$的列向量线性无关，$\boldsymbol Q$的列向量张成了整个空间，投影过程不会对向量有任何改变。当$\boldsymbol Q$为方阵时，$\boldsymbol {Q^{-1} = Q^{\mathrm T}}$ 。</p>
<p>投影矩阵的性质为：投影矩阵为对称矩阵$\boldsymbol {P^{\mathrm T} = P}$；如果投影两次，还会在原有的位置$\boldsymbol {P^{2} = P}$，代入即为$\boldsymbol {(QQ^{\mathrm T})(QQ^{\mathrm T}) = (QQ^{\mathrm T})}$。这是由于$\boldsymbol{Q^{\mathrm T} Q = I}$。</p>
<p>在很多复杂问题中使用标准正交向量之后都变得简单。如果基为标准正交，则方程</p>
<script type="math/tex; mode=display">
\boldsymbol{A^{\mathrm T}A \hat x = A^{\mathrm T}b} {\longrightarrow} \boldsymbol{Q^{\mathrm T}Q \hat x = Q^{\mathrm T}b}</script><p>的解变为$\boldsymbol {\hat x = Q^{\mathrm T}b}$，$\boldsymbol {\hat x}$的分量$\hat x_i = \boldsymbol {q^{\mathrm T}_i b}$</p>
<h4 id="17-4-施密特正交化-Gram-Schmidt-A-gt-Q"><a href="#17-4-施密特正交化-Gram-Schmidt-A-gt-Q" class="headerlink" title="17.4 施密特正交化 Gram-Schmidt(A-&gt;Q)"></a>17.4 施密特正交化 Gram-Schmidt(A-&gt;Q)</h4><p>从两个线性无关的向量$\boldsymbol a$和$\boldsymbol b$开始，它们张成了一个空间，我们的目标是希望找到两个标准正交的向量$\boldsymbol {q_1, q_2}$能张成同样的空间。Schmidt 给出的结论是如果我们有一组正交基$\boldsymbol A$和$\boldsymbol B$，那么令它们除以自己的长度就得到标准正交基：</p>
<script type="math/tex; mode=display">
\boldsymbol{q_{1}=\frac{A}{\|A\|}, q_{2}=\frac{B}{\|B\|}}</script><p>Gram做了重要的工作，令$\boldsymbol {A = a}$，在向量$\boldsymbol a$和$\boldsymbol b$张成的空间中，取与$\boldsymbol A$正交向量做成标准正交基，方法就是将$\boldsymbol b$投影到$\boldsymbol a$的方向，然后取$\boldsymbol {B=b-p}$($\boldsymbol B$就是之前谈论过的误差$\boldsymbol e$的方向）<br><img src="https://pic.imgdb.cn/item/6269e662239250f7c53b67b8.jpg"></p>
<script type="math/tex; mode=display">
\boldsymbol{B=b-\frac{A^{T} b}{A^{T} A} A}</script><p>如果从等式两端左乘$\boldsymbol A^{\mathrm T}$，可以得到$\boldsymbol{A^{\mathrm T}B = 0}$。</p>
<p>从三个线性无关的向量$\boldsymbol {a, b, c}$出发，则可以通过从$\boldsymbol c$中减去其在$\boldsymbol A$和$\boldsymbol B$两个方向的投影来得到$\boldsymbol C$。 </p>
<script type="math/tex; mode=display">
\boldsymbol {C=c-\frac{A^{T} c}{A^{T} A} A-\frac{B^{T} c}{B^{T} B} B}</script><p>例如：<script type="math/tex">\boldsymbol a = \left(\begin{array}{l} 1 \\ 1 \\ 1 \end{array}\right)</script>，<script type="math/tex">\boldsymbol b = \left(\begin{array}{l} 1 \\ 0 \\ 2 \end{array}\right)</script> ，则有$\boldsymbol {A = a}$，<script type="math/tex">\boldsymbol B=\left[\begin{array}{l} 1 \\ 0 \\ 2 \end{array}\right]-\dfrac{3}{3}\left[\begin{array}{l} 1 \\  1 \\ 1 \end{array}\right]=\left[\begin{array}{l} 0 \\ 1 \\ -1\end{array}\right]</script> ，验证计算得到$\boldsymbol{A^{\mathrm T}B = 0}$。</p>
<p>写出$\boldsymbol q_1, \boldsymbol q_2$ 所组成的矩阵为：</p>
<script type="math/tex; mode=display">
\boldsymbol Q=\left[\begin{array}{ll}
\boldsymbol q_{1} & \boldsymbol q_{2}
\end{array}\right]=\left[\begin{array}{cc}
\frac{1}{\sqrt{3}} & 0 \\
\frac{1}{\sqrt{3}} & \frac{-1}{\sqrt{2}} \\
\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{2}}
\end{array}\right]</script><p>$\boldsymbol Q$列向量的空间就是向量$\boldsymbol a$和$\boldsymbol b$张成的空间。 因此矩阵$\boldsymbol Q$和矩阵<script type="math/tex">\boldsymbol W = \boldsymbol{[a,b]} = \left[\begin{array}{ll} 1 & 1 \\ 1 & 0 \\ 1 & 2\end{array}\right]</script>有相同的列空间。</p>
<p>在消元过程中，可以对矩阵进行分解得到$\boldsymbol {W = LU}$，而在对$\boldsymbol  W$做施密特正交化的过程也可以用矩阵运算的方式表示为$\boldsymbol {W = QR}$。此处$\boldsymbol R$为上三角阵。</p>
<p><img src="https://pic.imgdb.cn/item/6269f252239250f7c551223b.jpg"></p>
<p>$\boldsymbol R$为上三角阵，则$\boldsymbol {a^{\mathrm T}_1q_2} = 0$。这是因为$\boldsymbol a^{\mathrm T}_1$就是$\boldsymbol q_1$的方向，而$\boldsymbol q_1$和$\boldsymbol q_2$为标准正交向量，因此$\boldsymbol q_2$的方向与$\boldsymbol a_1$ 垂直，因此内积为 0。$\boldsymbol R$在$\boldsymbol Q$右侧相当于对$\boldsymbol Q$做列操作，即$\boldsymbol W$的列向量是$\boldsymbol Q$列向量的线性组合，而$\boldsymbol Q$为列空间的$\boldsymbol W$一组标准正交基，则$\boldsymbol R$的元素实际上是$\boldsymbol W$的列向量基于$\boldsymbol Q$这组标准正交基的权。</p>
<p>左下角为0，但需要注意的是右上角并不一定为0。</p>
<ul>
<li>采用矩阵的QR分解来帮助求解$\boldsymbol{Ax = b}$的问题，最大的优势是提高了数值的稳定性。<ul>
<li>QR分解可以改善矩阵条件数，从而提高数值稳定</li>
<li>参考1：<a href="https://zhuanlan.zhihu.com/p/84415000">https://zhuanlan.zhihu.com/p/84415000</a></li>
</ul>
</li>
</ul>
<h3 id="第十八讲-行列式及其性质"><a href="#第十八讲-行列式及其性质" class="headerlink" title="第十八讲 行列式及其性质"></a>第十八讲 行列式及其性质</h3><p>之前学习了大量长方形矩阵的性质，现在我们集中讨论方阵的性质，行列式和特征值将我们的又一个重点，求行列式则与特征值息息相关。 </p>
<h4 id="18-1-行列式-Determinants"><a href="#18-1-行列式-Determinants" class="headerlink" title="18.1 行列式 Determinants"></a>18.1 行列式 Determinants</h4><p>行列式是一个每个方阵都具有的数值，我们将矩阵$\boldsymbol A$的行列式记作$\det(\boldsymbol A) = |\boldsymbol A|$。它将尽可能多的矩阵信息压缩在这一个数里。例如矩阵不可逆或称奇异与矩阵的行列式等于0等价，因此可以用行列式来判定矩阵是否可逆。</p>
<h4 id="18-2-性质-Properties"><a href="#18-2-性质-Properties" class="headerlink" title="18.2 性质 Properties"></a>18.2 性质 Properties</h4><p>直接给出$n$阶行列式的公式，则一下子代入了大量信息，并不利于接受这个概念，我们从行列式的三个基本性质开始讲起。</p>
<ul>
<li><p><strong>性质1：</strong> $|\boldsymbol I| = 1$</p>
</li>
<li><p><strong>性质2：</strong> 如果交换行列式的两行，则行列式的数值会反号。从前两条可以推知置换矩阵的行列式是+1 或者-1（交换的次数是奇数还是偶数决定了正负号）。</p>
</li>
<li><p><strong>性质3：</strong></p>
<ul>
<li>a. 如果在矩阵的一行乘上$t$，则行列式的值就要乘上$t$，<script type="math/tex">\left|\begin{array}{cc} t a & t b \\ c & d\end{array}\right|=t\left|\begin{array}{ll} a & b \\ c & d \end{array}\right|</script>；</li>
<li>b. 行列式是“矩阵的行”的线性函数，<script type="math/tex">\left|\begin{array}{cc} a+a^{\prime} & b+b^{\prime} \\ c & d \end{array}\right|=\left|\begin{array}{cc} a & b \\ c & d \end{array}\right|+\left|\begin{array}{cc} a^{\prime} & b^{\prime} \\ c & d \end{array}\right|</script></li>
</ul>
<blockquote>
<ol>
<li>行列式本身是有显式的，但是直接给出显式真的无益于理解行列式，我觉得G.Strang这里做了一个类似公理化的办法来给出行列式，是一个比较高明的办法。如他所言，很少有人用公式进行行列式计算，计算机也是用<strong>消元法</strong>来求解的。窃以为把大量时间花在“逆序”这一概念上对学习线代帮助并不大。 </li>
<li>通过高斯消元法(初等行变换)可以计算矩阵的行列式：<ul>
<li>第一类初等变换（交换矩阵的两行）：行列式值变号；</li>
<li>第二类初等变换（以一个非零数k乘矩阵的某一行所有元素）：行列式值变k倍；</li>
<li>第三类初等变换（把矩阵的某一行所有元素乘以一个数k后加到另一行对应的元素）：行列式值不变。</li>
</ul>
</li>
<li>为什么行列式的某一行的各元素乘以同一个数，然后加到另一行元素上，行列式的值不变？<ul>
<li>根据行列式按行裂项性质，而裂项根据乘法对加法分配律 </li>
<li>即利用了行列式的性质2和性质3.a和性质3.b，也就是<strong>性质5</strong>。</li>
</ul>
</li>
</ol>
</blockquote>
</li>
</ul>
<p><strong>更多的性质可以从以上的三条性质中推导出来：</strong></p>
<ul>
<li><p><strong>性质4：</strong>如果矩阵的两行是完全相同的，则它的行列式为0。</p>
<blockquote>
<p>这可以从第二条性质推导出来，因为交换这个相同的两行，行列式应该变号；但是新生成的矩阵跟原矩阵没有区别，因此行列式应该不变，所以有det=-det，所以det等于0。 </p>
</blockquote>
</li>
<li><p><strong>性质5：</strong>从矩阵的某行$k$减去另一行$i$的倍数，并不改变行列式的数值，也就是说行/列变换不改变行列式的值，以二阶为例： </p>
</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/626a023d239250f7c5751f1d.jpg"></p>
<ul>
<li><p><strong>性质6：</strong>如矩阵$\boldsymbol A$的某一行都是0，则其行列式为0。可以应用<strong>性质3.a</strong>，取$t=0$证明。</p>
</li>
<li><p><strong>性质7：</strong>三角阵的行列式的值等于其对角线上数值（主元）的乘积。 </p>
<p><img src="https://pic.imgdb.cn/item/626a02bb239250f7c576089b.jpg"></p>
<blockquote>
<p>性质5告诉我们三角阵通过行消元法得到对角阵的过程中，行列式的数值没有发生变化。性质3(a)告诉我们对角阵的行列式等于其主元的乘积再乘以单位阵的行列式。而性质1表明单位阵行列式为1。 </p>
</blockquote>
</li>
<li><p><strong>性质8：</strong>当且仅当矩阵$\boldsymbol A$为奇异矩阵时，其行列式为0。 </p>
<blockquote>
<p>如果矩阵$\boldsymbol A$为奇异阵，则必可通过消元法使得矩阵的某行全等于零，则按照性质6，$\boldsymbol A$的行列式为0。</p>
<p>如果其不是奇异阵，则通过消元可以得到一个上三角矩阵，且其主元均不为0，则按照性质7，行列式的数值等于主元的乘积也不等于0。</p>
<p>计算非奇异矩阵的行列式有确切的公式，但通常计算机是靠消元的方法来转化为三角阵，然后将主元相乘来进行计算的。</p>
</blockquote>
</li>
<li><p><strong>性质9：</strong>$\det(\boldsymbol {AB})=\det(\boldsymbol A) \det(\boldsymbol B) $</p>
<blockquote>
<p>尽管矩阵的和的行列式不等于行列式的和，但矩阵乘积的行列式等于矩阵行列式的乘积。 </p>
<p>如果$\boldsymbol A$为可逆矩阵，则$\boldsymbol {A^{-1}A = I}$ ，所以有$\det(\boldsymbol A^{-1}) = \dfrac{1}{\det(\boldsymbol A)}$</p>
<p>此外，$\det(\boldsymbol A^2) = \det(\boldsymbol A)^2$并且有。$\det(\boldsymbol 2A) = 2^n\det(\boldsymbol A)$后一个公式让我们容易联想到体积，当长宽高都倍增之后，体积变成了原来的$2^3=8$倍。</p>
</blockquote>
</li>
<li><p><strong>性质10</strong>：$\det(\boldsymbol {A^{\mathrm T}})=\det(\boldsymbol A) $</p>
<blockquote>
<p>证明： 矩阵消元可得$\boldsymbol {A = LU}$，则$\boldsymbol {A^T = L^TU^T}$，由性质9可知$\det(\boldsymbol {A})=\det(\boldsymbol L) \det(\boldsymbol U)$，$\det(\boldsymbol {A^T})=\det(\boldsymbol L^T) \det(\boldsymbol U^T)$，根据性质7可知 $\det(\boldsymbol {L^T})=\det(\boldsymbol L)$，$\det(\boldsymbol {U^T})=\det(\boldsymbol U)$，则二者乘积相等。</p>
<p>因为性质10成立，所以性质2,3,4,5,6可以用在行列式的列性质上。</p>
<p>行列式的性质2中隐藏着一个内容，这就是置换隐藏着奇偶性，一个矩阵不可能经过奇数次置换得到和偶数次置换相同的方阵。</p>
</blockquote>
</li>
</ul>
<h3 id="第十九讲-行列式公式和代数余子式与伴随矩阵"><a href="#第十九讲-行列式公式和代数余子式与伴随矩阵" class="headerlink" title="第十九讲 行列式公式和代数余子式与伴随矩阵"></a>第十九讲 行列式公式和代数余子式与伴随矩阵</h3><p>我们已经认识到了行列式的性质，应该推导出其公式了。</p>
<h4 id="19-1-行列式公式-Formula-for-the-determinant"><a href="#19-1-行列式公式-Formula-for-the-determinant" class="headerlink" title="19.1 行列式公式 Formula for the determinant"></a>19.1 行列式公式 Formula for the determinant</h4><p>根据上节可知，由三条基本性质可以推导出后续的七条性质，从这十个性质出发可以得到二阶方阵的行列式公式： </p>
<script type="math/tex; mode=display">
\begin{aligned}
\left|\begin{array}{ll}
a & b \\
c & d
\end{array}\right| &=\left|\begin{array}{ll}
a & 0 \\
c & d
\end{array}\right|+\left|\begin{array}{ll}
0 & b \\
c & d
\end{array}\right| \\
&=\left|\begin{array}{ll}
a & 0 \\
c & 0
\end{array}\right|+\left|\begin{array}{ll}
a & 0 \\
0 & d
\end{array}\right|+\left|\begin{array}{ll}
0 & b \\
c & 0
\end{array}\right|+\left|\begin{array}{ll}
0 & b \\
0 & d
\end{array}\right| \\
&=0+a d-b c+0 \\
&=a d-b c
\end{aligned}</script><p>通过性质3对n阶矩阵的行列式进行拆分，我们可以得到所有只包含n个非零元素的行列式，对于二阶行列式我们从1个拆分为2个，然后拆分成4个。而对于三阶矩阵我们从1个拆分成3个，然后拆分成9个，最后要拆分成27个。但最终这些行列式中有很大一部分等于0。 </p>
<script type="math/tex; mode=display">
\begin{aligned}
\left|\begin{array}{lll}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{array}\right| &=\left|\begin{array}{ccc}
a_{11} & 0 & 0 \\
0 & a_{22} & 0 \\
0 & 0 & a_{33}
\end{array}\right|+\left|\begin{array}{ccc}
a_{11} & 0 & 0 \\
0 & 0 & a_{23} \\
0 & a_{32} & 0
\end{array}\right|+\left|\begin{array}{ccc}
0 & a_{12} & 0 \\
a_{21} & 0 & 0 \\
0 & 0 & a_{33}
\end{array}\right| \\
&+\left|\begin{array}{ccc}
0 & a_{12} & 0 \\
0 & 0 & a_{23} \\
a_{31} & 0 & 0
\end{array}\right|+\left|\begin{array}{ccc}
0 & 0 & a_{13} \\
a_{21} & 0 & 0 \\
0 & a_{32} & 0
\end{array}\right|+\left|\begin{array}{ccc}
0 & 0 & a_{13} \\
0 & a_{22} & 0 \\
a_{31} & 0 & 0
\end{array}\right| \\
&=a_{11} a_{22} a_{33}-a_{11} a_{23} a_{33}-a_{12} a_{21} a_{33} \\
&+a_{12} a_{23} a_{31}+a_{13} a_{21} a_{32}-a_{13} a_{22} a_{31}
\end{aligned}</script><p>每一个拆分出来的非0行列式都是在每行每列都有且只有一个元素，就如同置换矩阵的元素分布。应用性质3可以将元素从行列式中提出来，而置换矩阵的行列式值为+1或者-1，因此可以给出行列式的公式。n阶拆分矩阵非0行列式的个数的计算方法就如同计算置换矩阵的个数一样，第一行放置一个非0元素的位置有n个选择，第二行为n-1个……。最后得到共n!个矩阵。</p>
<p>对于拆分得到的三阶矩阵，元素从上至下朝向右侧方向的，其行列式的数值为正，朝向左侧方向的则为负。但是这个规律只适用于三阶矩阵，不适用于高阶矩阵。</p>
<p><img src="https://pic.imgdb.cn/item/626a0d14239250f7c58d3ad9.jpg"></p>
<p>行列式的公式： </p>
<script type="math/tex; mode=display">
\operatorname{det}(\boldsymbol{A})=\sum_{n !} \pm a_{1 \alpha} a_{2 \beta} a_{3 \gamma} \cdots a_{n \omega}</script><p>其中列标号$(\alpha, \beta, \gamma, \dots)$是列标号$(1, 2, 3, \dots, n)$的某个排列。比如说对于单位阵而言，只有$\alpha = 1, \beta = 2, \dots, \omega = n$所得到的行列式为+1，其它都为零，所以单位阵的行列式为1。 </p>
<p>例如： </p>
<script type="math/tex; mode=display">
\left|\begin{array}{llll}
0 & 0 & 1 & 1 \\
0 & 1 & 1 & 0 \\
1 & 1 & 0 & 0 \\
1 & 0 & 0 & 1
\end{array}\right|=\left|\begin{array}{llll}
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0
\end{array}\right|+\left|\begin{array}{llll}
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1
\end{array}\right|</script><p>列标号取$(4,3,2,1)$得到第一个拆分行列式，符号为正，因为只要经两次交换就能变为$(1,2,3,4)$。第二个为$(3,2,1,4)$，因为只需交换一次就可变为正序，所以符号为负。因此本行列式为0。 </p>
<h4 id="19-2-代数余子式-Cofactor-formula"><a href="#19-2-代数余子式-Cofactor-formula" class="headerlink" title="19.2 代数余子式 Cofactor formula"></a>19.2 代数余子式 Cofactor formula</h4><p> <strong>代数余子式是用较小的矩阵的行列式来写出$n$阶行列式的公式</strong>。 </p>
<script type="math/tex; mode=display">
\begin{aligned}
&\operatorname{det}(\boldsymbol{A})=a_{11}\left(a_{22} a_{33}-a_{23} a_{32}\right)+a_{12}\left(-a_{21} a_{33}+a_{23} a_{31}\right)+a_{13}\left( a_{21} a_{32}-a_{22} a_{31}\right) \\
&=\left|\begin{array}{ccc}
a_{11} & 0 & 0 \\
0 & a_{22} & a_{23} \\
0 & a_{32} & a_{33}
\end{array}\right|+\left|\begin{array}{ccc}
0 & a_{12} & 0 \\
a_{21} & 0 & a_{23} \\
a_{31} & 0 & a_{33}
\end{array}\right|+\left|\begin{array}{ccc}
0 & 0 & a_{13} \\
a_{21} & a_{22} & 0 \\
a_{31} & a_{32} & 0
\end{array}\right|
\end{aligned}</script><p>将原公式中属于矩阵第一行的$a_{1j}$提出来，其系数即为代数余子式，是一个低阶行列式的值。这个低阶行列式是由原矩阵去掉$a_{1j}$所在的行和列组成的。</p>
<p><strong><font color="red">对矩阵中任意元素$a_{ij}$而言，其代数余子式$C_{ij}$就是矩阵的行列式的公式中$a_{ij}$的系数。$C_{ij}$等于原矩阵移除第$i$行和第$j$列后剩余元素组成的$n-1$阶矩阵的行列式数值乘以$(-1)^{i+j}$。（$C_{ij}$在$i+j$为偶数时为正，奇数时为负数。）</font></strong></p>
<p>对于$n$阶方阵，其行列式的代数余子式公式为：</p>
<script type="math/tex; mode=display">
\operatorname{det}(\boldsymbol{A})=a_{11} \mathrm{C}_{11}+a_{12} \mathrm{C}_{12}+\cdots+a_{1 \mathrm{n}} \mathrm{C}_{1 \mathrm{n}}</script><p>对于二阶矩阵，按照代数余子式公式则有： <script type="math/tex">\left|\begin{array}{ll} a & b \\ c & d\end{array}\right|=a d+b(-c)</script></p>
<p>对于矩阵行列式的计算，消元的得到主元是一个很好的方法，与之相比行列式的展开公式较为复杂，而代数余子式的方法介于两者之间，它的核心想法是通过降阶来将原来的行列式展开成更简单的行列式。</p>
<p>举”三对角阵”（tridiagonal matrix）为例，它除了对角线和对角线两侧相邻的元素之外，其它元素均为0。例如由1组成的4阶三对角阵为： <script type="math/tex">\boldsymbol{A}_{4}=\left[\begin{array}{llll} 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 1\end{array}\right]</script></p>
<p>由1组成的$n$阶三对角阵的行列式等于多少？</p>
<p>从矩阵的特殊结构我们可以得到： <script type="math/tex">\left|\boldsymbol{A}_{\mathrm{n}}\right|=\left|\boldsymbol{A}_{\mathrm{n}-1}\right|-1\left|\boldsymbol{A}_{\mathrm{n}-2}\right|</script></p>
<p>由1组成的$n$阶三对角阵的行列式从1阶开始按照$1，0，-1，-1，0，1$进行循环。 </p>
<h4 id="19-3-伴随矩阵-adjoint-matrix"><a href="#19-3-伴随矩阵-adjoint-matrix" class="headerlink" title="19.3 伴随矩阵 adjoint matrix"></a>19.3 伴随矩阵 adjoint matrix</h4><p><img src="https://pic.imgdb.cn/item/627e0a0709475431296a2889.jpg" style="zoom:50%"></p>
<p>上图中，$\boldsymbol A^<em>$即为矩阵$\boldsymbol A$的伴随矩阵，伴随矩阵$\boldsymbol A^</em>$的元素计算方法为：</p>
<ul>
<li>$a_{11}$的代数余子式为$\boldsymbol A_{11}$，$a_{12}$的代数余子式为$\boldsymbol A_{12}$，$a_{1n}$的代数余子式为$\boldsymbol A_{1n}$；</li>
<li>原矩阵的行对应伴随矩阵的列。</li>
</ul>
<h3 id="第二十讲-行列式应用：克莱默法则、逆矩阵、体积"><a href="#第二十讲-行列式应用：克莱默法则、逆矩阵、体积" class="headerlink" title="第二十讲 行列式应用：克莱默法则、逆矩阵、体积"></a>第二十讲 行列式应用：克莱默法则、逆矩阵、体积</h3><h4 id="20-1-逆矩阵的公式"><a href="#20-1-逆矩阵的公式" class="headerlink" title="20.1 逆矩阵的公式"></a>20.1 逆矩阵的公式</h4><p>我们已经知道二阶矩阵的逆矩阵公式为：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}
a & b \\
c & d
\end{array}\right] \stackrel{逆矩阵}\Longrightarrow \frac{1}{a d-b c}\left[\begin{array}{cc}
d & -b \\
-c & a
\end{array}\right]</script><p>那么我们能写出三阶甚至高阶的公式么？通过观察二阶矩阵逆矩阵的公式，我们可以用同样的策略来构造高阶矩阵的求逆公式，为： </p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{-1}=\frac{1}{\operatorname{det}(\boldsymbol{A})} \boldsymbol{C}^{T}</script><p>等式右侧矩阵外的因子，其分母是矩阵的行列式的值，而矩阵$\boldsymbol C$是“代数余子式矩阵”的转置矩阵，又称<strong>伴随矩阵</strong>($\boldsymbol C$矩阵的值是由行列式组成的)。</p>
<p>矩阵$\boldsymbol A$的行列式的计算中包含的都是$n$个元素的乘积： </p>
<script type="math/tex; mode=display">
\operatorname{det} \boldsymbol{A}=\sum_{n} \pm a_{1 \alpha} a_{2 \beta} a_{3 \gamma} \cdots a_{n \omega}</script><p>而伴随矩阵中的元素都是$n-1$阶行列式，它的运算中出现的是$n-1$个矩阵$\boldsymbol A$中元素的乘积。所以矩阵$\boldsymbol A$与两者相乘才能完全消去，而得到单位矩阵。下面我们就用矩阵$\boldsymbol A$与矩阵$\boldsymbol C^{\mathrm T}$相乘来验证$\boldsymbol {AC}^{\mathrm T} = \det(\boldsymbol A) \boldsymbol I$ ，并且理解逆矩阵的构造策略。 </p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{C}^{T}=\left[\begin{array}{ccc}
a_{11} & \ldots & a_{1 n} \\
\vdots & \ddots & \vdots \\
a_{n 1} & \cdots & a_{n n}
\end{array}\right]\left[\begin{array}{ccc}
C_{11} & \ldots & C_{n 1} \\
\vdots & \ddots & \vdots \\
C_{1 n} & \cdots & C_{n n}
\end{array}\right]</script><p>矩阵$\boldsymbol {AC}^{\mathrm T}$第一行第一列的元素等于矩阵$\boldsymbol A$第一行和矩阵$\boldsymbol {C}^{\mathrm T} $第一列进行点积，计算可得： </p>
<script type="math/tex; mode=display">
\sum_{j=1}^{n} a_{1 j} C_{1 j}=\operatorname{det}(\boldsymbol{A})</script><p>点积的算式本身就是矩阵$\boldsymbol A$的计算公式，因此结果为$\boldsymbol A$行列式的值。而矩阵$\boldsymbol {AC}^{\mathrm T}$对角线上所有的元素都是如此，因此其对角戏上的元素都等于$\det(\boldsymbol A)$，即：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{llll}
\operatorname{det}(A) & & & \\
& \operatorname{det}(A) & \\
& & \ddots & \\
& & & \operatorname{det}(A)
\end{array}\right]</script><p>而对于非对角线元素，我们以第二行第一列的元素为例，其计算公式为：</p>
<script type="math/tex; mode=display">
\sum_{j=1}^{n} a_{2 j} C_{1 j}=\operatorname{det}\left(\boldsymbol{A}_{s}\right)</script><p>这可以视为矩阵$\boldsymbol A_s$的行列式数值，各个代数余子式的形式不变，但是与代数余子式相乘的变为了矩阵$\boldsymbol A$第二行第$j$列元素。因此$\boldsymbol A_s$的形式相当于用矩阵$\boldsymbol A$第二行的元素替代第一行的元素得到的矩阵。因为该矩阵中前两行完全相同，因此按照行列式性质4，$\det(\boldsymbol A_s) = 0$（G.Strang在课堂上是用二阶矩阵为例），其他列类似，所以有：</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{C}^{T}=\left[\begin{array}{rrrrrr}
\operatorname{det} \boldsymbol{A} & 0 & 0 & \cdots & 0 \\
0 & \operatorname{det} \boldsymbol{A} & 0 & \cdots & 0 \\
0 & 0 & \ddots & & 0 \\
\vdots & & & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \operatorname{det} \boldsymbol{A}
\end{array}\right]=\operatorname{det}(\boldsymbol{A}) \boldsymbol{I}</script><p>即<script type="math/tex">\boldsymbol{A} \dfrac{1}{\operatorname{det}(\boldsymbol{A})} \boldsymbol{C}^{T}=\boldsymbol{I}</script> ，因此有 </p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{-1}=\frac{1}{\operatorname{det}(\boldsymbol{A})} \boldsymbol{C}^{T}</script><p>逆矩阵公式的一个好处就是，我们从中可以看到，当改变原矩阵中的一个元素时，给逆矩阵带来了怎样的变化。</p>
<h4 id="20-2-克莱默法则"><a href="#20-2-克莱默法则" class="headerlink" title="20.2 克莱默法则"></a>20.2 克莱默法则</h4><p>对于可逆矩阵$\boldsymbol A$，方程$\boldsymbol {Ax = b}$必然有解<script type="math/tex">\boldsymbol {x = A^{-1}b}</script>，将逆矩阵的公式带入其中，则有： </p>
<script type="math/tex; mode=display">
\boldsymbol {x}=\boldsymbol{A}^{-1} \boldsymbol {b}=\frac{1}{\operatorname{det}(\boldsymbol{A})} \boldsymbol{C}^{T} \boldsymbol{b}</script><p>克莱姆法则是从另一个角度来看待这个公式。实际上$\boldsymbol x$的分量<script type="math/tex">x_{j}=\dfrac{\operatorname{det}\left(\boldsymbol{B}_{j}\right)}{\operatorname{det}(\boldsymbol{A})}</script></p>
<p>其中我们需要思考一下矩阵<script type="math/tex">\boldsymbol{B}_{j}</script>是什么，由于<script type="math/tex">\boldsymbol{C}^{\mathrm T}</script>由<script type="math/tex">\boldsymbol{A}</script>的代数余子式构成，那么<script type="math/tex">\boldsymbol{C}^{\mathrm T}*\boldsymbol b</script>可以视为用向量<script type="math/tex">\boldsymbol{b}</script>替换矩阵<script type="math/tex">\boldsymbol A</script>的第<script type="math/tex">j</script>列所得到的新矩阵，这就是<strong>克莱默发现的规律</strong>——矩阵<script type="math/tex">\boldsymbol{B}_{j}</script>的构成规律，例如： </p>
<script type="math/tex; mode=display">
\boldsymbol{B}_{1}=\left[\begin{array}{ccccc}
b_{1} & a_{12} & \cdots & \cdots & a_{1 n} \\
b_{2} & a_{22} & \cdots & \cdots & a_{2 n} \\
b_{3} & a_{32} & \ddots & & a_{3 n} \\
\vdots & & & \ddots & \vdots \\
b_{n} & a_{n 2} & \cdots & \cdots & a_{n n}
\end{array}\right]</script><script type="math/tex; mode=display">
\boldsymbol{B}_{n}=\left[\begin{array}{rrrrr}
a_{11} & \cdots & \cdots & a_{1 n-1} & b_{1} \\
a_{21} & \cdots & \cdots & a_{2 n-1} & b_{2} \\
\vdots & & \ddots & \vdots & \vdots \\
\vdots & & & a_{n-1 n-1} & b_{n-1} \\
a_{n 1} & a_{n 2} & \cdots & a_{n n-1} & b_{n}
\end{array}\right]</script><p>矩阵$\boldsymbol{B}_{j}$的行列式的数值从第$j$列用代数余子式进行展开计算，正好是伴随矩阵$\boldsymbol{C}^{\mathrm T}$的第$j$行与向量$\boldsymbol{b}$点积的结果。此处我们用到了行列式的<strong>性质10</strong>。</p>
<p>相比于消元法，采用克莱姆法则计算方程的解，效率较低。</p>
<h4 id="20-3-体积-面积"><a href="#20-3-体积-面积" class="headerlink" title="20.3  体积/面积"></a>20.3  体积/面积</h4><p>为什么行列式等价于空间“箱子”的体积，老师的证明思路是证明空间“箱子”体积满足行列式的三条基本性质。</p>
<p>矩阵$\boldsymbol A$行列式的绝对值等于以矩阵$\boldsymbol A$行（列）向量为边所构成的平行六面体的体积。行列式的正负对应左手系和右手系。</p>
<ul>
<li>如果矩阵$\boldsymbol A$是单位矩阵，则其构成的是三个边长均为1且互相垂直的立方体，其体积为1，这与上面的结论相符。这也是行列式的性质1。</li>
<li>而如果矩阵$\boldsymbol A$为正交矩阵$\boldsymbol Q$，则其构成的也是三个边边长为1且三边互相垂直的立方体，其体积也为1只是取向与单位阵不同。这也与上面的结论相符，因为$\boldsymbol{Q^{\mathrm T} Q = I}$，且$\operatorname{det}(\boldsymbol{Q})=\operatorname{det}\left(\boldsymbol{Q}^{T}\right)$，所以$\det(\boldsymbol Q)=1$。 </li>
</ul>
<p>交换矩阵$\boldsymbol A$中的行并不会改变其行列式的绝对值，显然也不会改变向量围成的体积，因此这也和体积理论相符。这是行列式的性质2。 </p>
<p><img src="https://pic.imgdb.cn/item/626a544b239250f7c53692a2.jpg"></p>
<p>对于长方体，也非常直观，当你将其中一条边的边长增加2倍时，平行六面体的体积也会增加2倍，这相当于性质3a。</p>
<p>对于性质3b，实际上是要求体积理论摆脱角度的限制（之前几条完全都是在直角的背景下讨论得），我们可以在二维条件下简单证明。</p>
<ul>
<li><strong>证明二阶行列式是平行四边形的面积</strong>：</li>
</ul>
<p>如图所示，向量$\boldsymbol a$和$\boldsymbol b$围成的平行四边形的面积，就等于向量$\boldsymbol a$和高$\boldsymbol h$的长度之积，就等于两个互相垂直的向量$\boldsymbol a$和高$\boldsymbol h$围成的长方形的面积，按照前面所述即为 <script type="math/tex">\left|\begin{array}{ll} a_{1} & a_{2} \\ h_{1} & h_{2}\end{array}\right|</script> 。根据行列式运算法则有：</p>
<script type="math/tex; mode=display">
\left|\begin{array}{ll}
a_{1} & a_{2} \\
h_{1} & h_{2}
\end{array}\right|=\left|\begin{array}{ll}
a_{1} & a_{2} \\
b_{1} & b_{2}
\end{array}\right|-\left|\begin{array}{ll}
a_{1} & a_{2} \\
p_{1} & p_{2}
\end{array}\right|</script><p>而$\boldsymbol p$和$\boldsymbol a$同方向因此第二项为0，因此平行四边形的面积等于<script type="math/tex">\left|\begin{array}{ll} a_{1} & a_{2} \\ b_{1} & b_{2}\end{array}\right|</script>结果可以推广到高维。 </p>
<p>二阶行列式的值等于平行四边形的面积，同时其一半也是{(a,b),(c,d),(0,0)}围成的三角形的面积。</p>
<p>若三角形不过原点，为<script type="math/tex">{(x_1,y_1),(x_2,y_2),(x_3,y_3)}</script>围成，则其面积等于<script type="math/tex">\dfrac{1}{2}\left|\begin{array}{lll} x_{1} & y_{1} & 1 \\ x_{2} & y_{2} & 1 \\ x_{3} & y_{3} & 1\end{array}\right|</script></p>
<p>这个公式可以从第三列用代数余子式展开，所得结果可以看做是从一个过原点的大三角形中减去两个过原点的小三角形。 </p>
<p>除以上各条之外性质4也比较直观，当有两条边重合时，平行六面体或平行四边形被压扁，体积或者面积为0。 </p>
<h3 id="第二十一讲-特征值与特征向量"><a href="#第二十一讲-特征值与特征向量" class="headerlink" title="第二十一讲 特征值与特征向量"></a>第二十一讲 特征值与特征向量</h3><p>本单元后面的课程主要围绕特征值和特征向量。在这个议题下讨论得都是<font color="red"><strong>方阵</strong></font>。 </p>
<h4 id="21-1-特征向量和特征值-Eigenvectors-and-eigenvalues"><a href="#21-1-特征向量和特征值-Eigenvectors-and-eigenvalues" class="headerlink" title="21.1 特征向量和特征值 Eigenvectors and eigenvalues"></a>21.1 特征向量和特征值 Eigenvectors and eigenvalues</h4><p>将矩阵$\boldsymbol A$与向量$\boldsymbol x$相乘当做是对向量的一种操作或者函数，输入$\boldsymbol x$而输出$\boldsymbol {Ax}$。特征向量即在特定的向量$\boldsymbol x$ 方向上输出的$\boldsymbol {Ax}$平行于$\boldsymbol x$，即为： </p>
<script type="math/tex; mode=display">
\boldsymbol {Ax} = \lambda \boldsymbol x</script><p>其中$\boldsymbol x$为矩阵$\boldsymbol A$的特征向量，而$\lambda$为$\boldsymbol A$的特征值。 </p>
<p>如果0是矩阵的特征值，则有$\boldsymbol{Ax = 0x = 0}$。特征值0所对应的向量生成了矩阵的零空间。如果矩阵为$\boldsymbol A$不可逆矩阵，则0是其特征值之一。 </p>
<p><strong>例1：</strong>矩阵$\boldsymbol P$是朝向一个平面的投影矩阵。对于这个平面之内的$\boldsymbol x$，均有$\boldsymbol{Px=x}$，因此$\boldsymbol x$是特征向量而1为特征值。垂直于该平面的向量$\boldsymbol x$经投影得到$\boldsymbol {Px=0}$，这个$\boldsymbol x$也是矩阵的特征向量而0为特征值。矩阵$\boldsymbol P$的所有特征向量张成了整个空间。 </p>
<p><strong>例2：</strong>矩阵<script type="math/tex">\boldsymbol A=\left[\begin{array}{ll} 0 & 1 \\ 1 & 0\end{array}\right]</script>，具有特征向量 <script type="math/tex">\boldsymbol  x=\left[\begin{array}{l} 1 \\ 1\end{array}\right]</script> ，对应的特征向量为1；另一个特征向量为<script type="math/tex">\boldsymbol  x=\left[\begin{array}{l} 1 \\ -1\end{array}\right]</script>，对应的特征向量为-1。这些特征向量张成了整个空间。<strong>因为是<font color="red">对称矩阵</font>，其特征向量互相<font color="red">垂直</font></strong> </p>
<h4 id="21-2-特征值计算与性质"><a href="#21-2-特征值计算与性质" class="headerlink" title="21.2 特征值计算与性质"></a>21.2 特征值计算与性质</h4><p>方程$\boldsymbol {Ax} = \lambda \boldsymbol x$中特征值和特征向量均未知，没法直接求解。因此我们做如下数学处理： $\boldsymbol {Ax} - \lambda \boldsymbol x = (\boldsymbol A - \lambda \boldsymbol I)\boldsymbol x = 0$，此时可知$(\boldsymbol A - \lambda \boldsymbol I)$为奇异阵，也就是</p>
<script type="math/tex; mode=display">
\det(\boldsymbol A - \lambda \boldsymbol I) = 0</script><p>在这个没有$\boldsymbol x$的方程中，可以解得$n$个特征值，但是有可能方程有重根，则会得到重复的特征值。 </p>
<p>得到特征值之后，可以用消元法解$(\boldsymbol A - \lambda \boldsymbol I)$这一矩阵零空间中的向量为矩阵的$\boldsymbol A$特征向量。 </p>
<ul>
<li><strong>性质1:</strong></li>
</ul>
<p><strong><font color="green"> 任意$n \times n$矩阵具$\boldsymbol A$有$n$个特征值，并且它们的和等于矩阵对角线上的元素之和，这个数值为矩阵的迹(trace) </font></strong>。对于二阶矩阵，在已知一个特征值的条件下，可以据此得到另一个特征值。 </p>
<script type="math/tex; mode=display">
\text{trace}(\boldsymbol A) = 对角元素和 = \lambda_1+ \lambda_2+ \dots +\lambda_n</script><ul>
<li><strong>性质2：</strong></li>
</ul>
<p><strong><font color="green"> 矩阵$\boldsymbol A$的特征值的乘积等于矩阵的行列式 </font></strong></p>
<script type="math/tex; mode=display">
\det(\boldsymbol A) = \lambda_1 \lambda_2 \dots \lambda_n</script><ul>
<li><strong>性质3：</strong></li>
</ul>
<p><strong><font color="green"> 对称矩阵的特征向量正交  </font></strong></p>
<p><img src="https://pic.imgdb.cn/item/626c8dc7239250f7c590bdca.jpg" style="zoom:70%"></p>
<blockquote>
<p>证明：<a href="https://zhuanlan.zhihu.com/p/83669028">https://zhuanlan.zhihu.com/p/83669028</a></p>
</blockquote>
<ul>
<li><strong>性质4：</strong></li>
</ul>
<p><strong><font color="green">对称矩阵永远具有实数的特征值，而反对称矩阵（antisymmetric matrices），即满足$\boldsymbol A^{\mathrm T} = - \boldsymbol A$的矩阵，具有纯虚数的特征值</font></strong>。 <em>实数特征值让特征向量<strong>伸缩</strong>而虚数让其<strong>旋转</strong></em>。</p>
<ul>
<li><strong>性质5：</strong></li>
</ul>
<p><strong><font color="green">三角阵特征值就是矩阵对角线上的元素。 </font></strong>重特征值 Triangular matrices and repeated eigenvalues</p>
<ul>
<li><strong>性质6：</strong></li>
</ul>
<p><strong><font color="green">矩阵$\boldsymbol A^2$的特征向量与$\boldsymbol A$的相同，但是特征值变为$\lambda^2$ </font></strong></p>
<script type="math/tex; mode=display">
\boldsymbol {A^2x} = \boldsymbol A(\boldsymbol {Ax}) = \boldsymbol A (\lambda \boldsymbol x) = \lambda \boldsymbol{Ax} = \lambda ( \lambda\boldsymbol x ) = \lambda^2 \boldsymbol x</script><p>这个性质可以进一步推广到$\boldsymbol A^n$。这个性质是计算矩阵幂的一种方式。</p>
<blockquote>
<p>需要注意的是，两个矩阵的和的特征值不是两特征值直接相加之和，因为特征向量并不相同。 </p>
</blockquote>
<h3 id="第二十二讲-对角化和矩阵乘幂"><a href="#第二十二讲-对角化和矩阵乘幂" class="headerlink" title="第二十二讲 对角化和矩阵乘幂"></a>第二十二讲 对角化和矩阵乘幂</h3><p>本讲中我们学习如何对角化含有n个线性无关特征向量的矩阵，以及对角化是怎样简化计算的。 </p>
<h4 id="22-1-对角化矩阵-Diagonalizing-a-matrix-boldsymbol-S-1-boldsymbol-A-boldsymbol-S-boldsymbol-Lambda"><a href="#22-1-对角化矩阵-Diagonalizing-a-matrix-boldsymbol-S-1-boldsymbol-A-boldsymbol-S-boldsymbol-Lambda" class="headerlink" title="22.1 对角化矩阵 Diagonalizing a matrix $\boldsymbol{S}^{-1} \boldsymbol{A} \boldsymbol{S}=\boldsymbol{\Lambda}$"></a>22.1 对角化矩阵 Diagonalizing a matrix $\boldsymbol{S}^{-1} \boldsymbol{A} \boldsymbol{S}=\boldsymbol{\Lambda}$</h4><p>如果矩阵$\boldsymbol A$具有$n$个线性无关的特征向量，将它们作为列向量可以组成一个可逆方阵$\boldsymbol S$(这是前提，因为特征向量组成的方阵不一定是可逆的)，并且有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{A} \boldsymbol{S} &=\boldsymbol{A}\left[\begin{array}{llll}
\boldsymbol{x}_{1} & \boldsymbol{x}_{2} & \cdots & \boldsymbol{x}_{\mathrm{n}}
\end{array}\right] \\
&=\left[\begin{array}{lllll}
\lambda_{1} \boldsymbol{x}_{1} & \lambda_{2} \boldsymbol{x}_{2} & \cdots & \lambda_{\mathrm{n}} \boldsymbol{x}_{\mathrm{n}}
\end{array}\right] \\
&=\boldsymbol{S}\left[\begin{array}{cccc}
\lambda_{1} & 0 & \cdots & 0 \\
0 & \lambda_{2} & & 0 \\
\vdots & & \ddots & \vdots \\
0 & \cdots & 0 & \lambda_{\mathrm{n}}
\end{array}\right] \\
&=\boldsymbol{S} \boldsymbol{\Lambda}
\end{aligned}</script><p>这里的矩阵$\boldsymbol{\Lambda}$为对角阵，它的非零元素就是矩阵$\boldsymbol{A}$的特征值。因为矩阵$\boldsymbol{S}$中的列向量线性无关，因此逆矩阵$\boldsymbol{S}^{-1}$存在。在等式两侧左乘逆矩阵，得到$\boldsymbol{S}^{-1} \boldsymbol{A} \boldsymbol{S}=\boldsymbol{\Lambda}$，同样地， $\boldsymbol{A}=\boldsymbol{S} \boldsymbol{\Lambda} \boldsymbol{S}^{-1}$。</p>
<p>对于消元法而言，矩阵有$\boldsymbol{LU}$分解，对于施密特正交法，矩阵有$\boldsymbol{QR}$分解，而上面的推导是一种新的矩阵分解。 </p>
<blockquote>
<p>之前曾经提到过消元进行行操作和列操作最后会得到“相抵标准型”。现在我们得到的是矩阵的“相似标准形”,它还保有矩阵操作的基本性质——特征值，而相抵标准型只剩下最内核的秩信息还保留着。 </p>
</blockquote>
<h4 id="22-2-矩阵的幂-Powers-of-boldsymbol-A"><a href="#22-2-矩阵的幂-Powers-of-boldsymbol-A" class="headerlink" title="22.2 矩阵的幂 Powers of $\boldsymbol{A}$"></a>22.2 矩阵的幂 Powers of $\boldsymbol{A}$</h4><p>参考第二十一讲<strong>性质6</strong>。</p>
<p>如果矩阵$\boldsymbol{A}$具有$n$个线性无关的特征向量，如果所有的特征值均满足$|\lambda_i|&lt;1$，则当$k \to \infty$ 时，$\boldsymbol{A}^k \to \boldsymbol 0$。</p>
<h4 id="22-3-重特征值-Repeated-eigenvalues"><a href="#22-3-重特征值-Repeated-eigenvalues" class="headerlink" title="22.3 重特征值 Repeated eigenvalues"></a>22.3 重特征值 Repeated eigenvalues</h4><p> 如果矩阵$\boldsymbol{A}$没有重特征值，则其一定具有$n$个线性无关的特征向量。</p>
<p>如果矩阵$\boldsymbol{A}$有重特征值，它有可能具有$n$个线性无关的特征向量，也可能没有。比如单位阵的特征值为重特征值1，但是其具有$n$个线性无关的特征向量。</p>
<p>对于如 <script type="math/tex">\boldsymbol{A}=\left[\begin{array}{ll} 2 & 1 \\ 0 & 2\end{array}\right]</script>的三角矩阵，特征值就是矩阵对角线上的元素2。其特征向量在$(\boldsymbol A - \lambda \boldsymbol I)$的零空间中，满足</p>
<script type="math/tex; mode=display">
(\boldsymbol{A}-\lambda \boldsymbol{I}) \boldsymbol{x}=\left[\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right] \mathbf{x}=\mathbf{0}</script><p>求解可得<script type="math/tex">\boldsymbol x=\left[\begin{array}{l} 1 \\ 0\end{array}\right]</script>，而没有第二个特征向量。 </p>
<h4 id="22-4-差分方程-Difference-equations"><a href="#22-4-差分方程-Difference-equations" class="headerlink" title="22.4 差分方程 Difference equations"></a>22.4 差分方程 Difference equations</h4><p>从给定的一个向量<strong><em>u</em></strong>0出发，我们可以通过对前一项乘以矩阵得到下一项的方式，得到一个向量序列：</p>
<script type="math/tex; mode=display">
\boldsymbol u_{k+1}=\boldsymbol{Au}_k</script><p>上式为一个一阶差分方程，易知</p>
<script type="math/tex; mode=display">
\boldsymbol u_{k+1}=\boldsymbol{A}^k {\boldsymbol u}_0</script><p>是方程的解。但这种简洁形式并没有给出足够的信息，我们需要通过特征向量和矩阵的幂运算给出真实解的结构。 </p>
<p>将<strong><em>u</em></strong>0写成特征向量的线性组合： </p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{u}_{0}=c_{1} \boldsymbol{x}_{1}+c_{2} \boldsymbol{x}_{2}+\ldots+c_{n} \boldsymbol{x}_{n}=\boldsymbol{S c} \\
&\boldsymbol{A} \boldsymbol{u}_{0}=c_{1} \lambda_{1} \boldsymbol{x}_{1}+c_{2} \lambda_{2} \boldsymbol{x}_{2}+\ldots+c_{n} \lambda_{n} \boldsymbol{x}_{n} \\
&\boldsymbol{u}_{k}=\boldsymbol{A}^{k} \boldsymbol{u}_{0}=c_{1} \lambda_{1}^{k} \boldsymbol{x}_{1}+c_{2} \lambda_{2}^{k} \boldsymbol{x}_{2}+\ldots+c_{n} \lambda_{n}^{k} \boldsymbol{x}_{n}= \boldsymbol{S} \boldsymbol{\Lambda}^{k} \boldsymbol{c}
\end{aligned}</script><blockquote>
<p>很多人会问矩阵的特征值特征向量为什么这么神奇，可以把矩阵的操作变成一个简单的参数。还有人会问道为什么特征值在物理中出现非常频繁。对此我只能简单解释一下，物理中常见的被研究物体都有一个自身的内禀结构，这个内在结构的方向往往和观察者也就是外场的坐标有区别。当我们给物体施加一个外场刺激的时候，比如说外力或者电场极化等等，物体沿着其内在结构的取向来响应外场，但是观察者从外场坐标下采集反馈。实际上矩阵在不同坐标之间实现变换，特征向量显示了物体内结构的方向，特征值则是在这个主方向上物体对外场的响应参数。在有的领域直接将特征值称为伸缩系数，实际上它反应了在其所对应的特征向量方向上，内结构与外场之间的相互关系。 特征值还有一个应用是作为降维的判据，比如在图像压缩过程中，极小的特征值会被赋值为0，以此节省存储空间，也便于其它操作。反应在图像上，降维后的图像基本轮廓依旧清晰，图像细节有所牺牲。 </p>
</blockquote>
<h3 id="第二十三讲-微分方程和-e-boldsymbol-A-t"><a href="#第二十三讲-微分方程和-e-boldsymbol-A-t" class="headerlink" title="第二十三讲 微分方程和$e^{\boldsymbol A t}$"></a>第二十三讲 微分方程和$e^{\boldsymbol A t}$</h3><p>本讲将面对微分方程，将一阶常系数微分方程转化为线性代数问题进行处理。主要思路基于常系数线性方程的解是指数形式，而寻找其指数和系数就是线代主要研究的问题。这里会涉及到矩阵型指数的运算$e^{\boldsymbol A t}$。</p>
<h4 id="23-1-微分方程-Differential-equations"><a href="#23-1-微分方程-Differential-equations" class="headerlink" title="23.1 微分方程 Differential equations"></a>23.1 微分方程 Differential equations</h4><p>例： <script type="math/tex">\begin{aligned} &\frac{\mathrm d u_{1}}{\mathrm d t}=-u_{1}+2 u_{2} \\ &\frac{\mathrm d u_{2}}{\mathrm d t}=u_{1}-2 u_{2}\end{aligned}</script>，初值条件$u_1(0)=1，u_2(0)=0$</p>
<p>则有<script type="math/tex">\dfrac{\mathrm d \mathbf{u}}{\mathrm d t}=\boldsymbol{A} \boldsymbol{u}</script>，<script type="math/tex">\boldsymbol{A}=\left[\begin{array}{cc} -1 & 2 \\ 1 & -2\end{array}\right], \boldsymbol{u}(0)=\left[\begin{array}{l} 1 \\ 0\end{array}\right]</script></p>
<p>分析矩阵$\boldsymbol{A}$的目的是要追踪$\boldsymbol{u}$随时间的变化，而首先要做的是找到矩阵的特征值和特征向量。矩阵$\boldsymbol{A}$为奇异矩阵，因此存在一个特征值$\lambda_1 = 0$ ，而矩阵的迹为-3，因此还有一个特征值为$\lambda_2 = -3$。</p>
<p>一阶线性微分方程的解的形式是$e^{\boldsymbol \lambda t}$。两个特征值中，0会使结果达到稳态，而-3所对应的$e^{\boldsymbol -3 t}$会随时间增大而变小。 </p>
<p>方程的通解为<script type="math/tex">\boldsymbol{u}(\mathrm{t})=c_{1} e^{\lambda_{1} t} \boldsymbol{x}_{1}+c_{2} e^{\lambda_{2} t} \boldsymbol{x}_{2}</script></p>
<blockquote>
<p>尽管直接代入可以验证。。。</p>
</blockquote>
<p>将<script type="math/tex">\lambda_{1}=0, \quad \lambda_{2}=-3</script>代入<script type="math/tex">(\boldsymbol{A}-\lambda \boldsymbol{I}) \boldsymbol{x}=\boldsymbol{0}</script>，分别求得对应的特征向量<script type="math/tex">\boldsymbol x_1=\left[\begin{array}{l} 2 \\ 1\end{array}\right], \boldsymbol x_2=\left[\begin{array}{l} 1 \\ -1\end{array}\right]</script>，即解为：</p>
<script type="math/tex; mode=display">
\mathbf{u}(\boldsymbol{t})=c_{1} e^{\lambda_{1} t} \boldsymbol {x}_{1}+c_{2} e^{\lambda_{2} t} \boldsymbol {x}_{2}=c_{1} e^{0}\left[\begin{array}{l}
2 \\
1
\end{array}\right]+c_{2} e^{-3 t}\left[\begin{array}{c}
1 \\
-1
\end{array}\right]</script><p>由初始条件得</p>
<script type="math/tex; mode=display">
\boldsymbol{u}(0)=c_{1}\left[\begin{array}{l}
2 \\
1
\end{array}\right]+c_{2}\left[\begin{array}{c}
1 \\
-1
\end{array}\right]=\left[\begin{array}{l}
1 \\
0
\end{array}\right]</script><p> 解为$c_1 = c_2 = \dfrac{1}{3}$，因此 </p>
<script type="math/tex; mode=display">
\boldsymbol{u}(\mathrm{t})=\frac{1}{3}\left[\begin{array}{l}
2 \\
1
\end{array}\right]+\frac{1}{3} e^{-3 t}\left[\begin{array}{c}
1 \\
-1
\end{array}\right]</script><p>前一项为稳态状态，后一项随着时间衰减。 </p>
<p>稳定性：并不是所有的系统都会达到稳态，矩阵的特征值会告诉我们$\boldsymbol u(t)$的发展趋势。</p>
<ul>
<li><strong>Stability(稳定性)：</strong>$\mathrm{Re}(\lambda) &lt; 0$，则有$\boldsymbol u(t) \to 0$（支配稳定性的是实部，虚部的作用是在单位圆上转圈。）</li>
<li><strong>Steady State(稳态)：</strong>有一个特征值为0，并且其它所有的特征值实部为负数$\mathrm{Re}(\lambda) &lt; 0$。</li>
<li><strong>Bolw Up(不收敛)：</strong>至少有一个特征值满足$\mathrm{Re}(\lambda) &gt; 0$，则发散。</li>
</ul>
<p>如果二阶矩阵<script type="math/tex">\boldsymbol{A}=\left[\begin{array}{ll} a & b \\ c & d\end{array}\right]</script>的两个特征值实部为负数，则矩阵的迹$a+d$也是负数。反之并不一定成立，例如矩阵<script type="math/tex">\left[\begin{array}{cc} -2 & 0 \\ 0 & 1\end{array}\right]</script>的迹为-1，但是一个特征值为1。如果二阶矩阵的行列式为正而迹为负，则解为收敛的。 </p>
<p>在方程<script type="math/tex">\dfrac{\mathbf d \boldsymbol {u}}{\mathbf d \boldsymbol t}=\boldsymbol{A} \boldsymbol {u}</script>中，是矩阵$\boldsymbol A$使得不同分量之间相互耦合(<font color="red">我们的方程表明两个变量相互耦合，特征值和特征向量的作用就在于可以实现<strong>解耦</strong>（又称对角化）</font> )。令$\boldsymbol {u=Sv}$其中$\boldsymbol S$是由矩阵$\boldsymbol A$的特征向量组成。则有： </p>
<script type="math/tex; mode=display">
\boldsymbol{S} \frac{\mathrm d \boldsymbol{v}}{\mathrm d t}=\boldsymbol{A} \boldsymbol{S} \boldsymbol{v} \Rightarrow \frac{\mathrm d \boldsymbol{v}}{\mathrm d t}=\boldsymbol{S}^{-1} \boldsymbol{A} \boldsymbol{S} \boldsymbol{v}=\boldsymbol{\Lambda} \boldsymbol{v}</script><p>新的方程不再耦合，则方程组的对角线为： <script type="math/tex">\dfrac{\mathrm d v_{i}}{\mathrm d t}=\lambda_{i} v_{i}</script>，方程组的通解为： </p>
<script type="math/tex; mode=display">
\boldsymbol{v}(t)=e^{\boldsymbol{\Lambda} t} \boldsymbol{v}(0), \quad \boldsymbol{u}(t)=\boldsymbol{S} e^{\boldsymbol{\Lambda} t} \boldsymbol{S}^{-1} \boldsymbol{u}(0)=e^{\boldsymbol{A} t} \boldsymbol{u}(0)</script><h4 id="23-2-矩阵指数函数-Matrix-exponential-e-boldsymbol-At"><a href="#23-2-矩阵指数函数-Matrix-exponential-e-boldsymbol-At" class="headerlink" title="23.2 矩阵指数函数 Matrix exponential $e^{\boldsymbol At}$"></a>23.2 矩阵指数函数 Matrix exponential $e^{\boldsymbol At}$</h4><p>我们可以用幂级数的公式： </p>
<script type="math/tex; mode=display">
e^{x}=\sum_{n=0}^{\infty} \frac{x^{n}}{n !}=1+x+\frac{x^{2}}{2}+\frac{x^{3}}{6}+\cdots</script><p>来定义矩阵型指数运算$e^{\boldsymbol At}$：</p>
<script type="math/tex; mode=display">
e^{\boldsymbol{A} t}=I+\boldsymbol{A} t+\frac{(\boldsymbol{A} t)^{2}}{2}+\frac{(\boldsymbol{A} t)^{3}}{6}+\cdots</script><p>如果$\boldsymbol At$的特征值很小，满足收敛条件$|\lambda({\boldsymbol At})|&lt;1$，则可以用几何级数来定义矩阵型指数： </p>
<script type="math/tex; mode=display">
\frac{1}{1-x}=\sum_{n=0}^{\infty} x^{n} \rightarrow(I-\boldsymbol{A} t)^{-1}=I+\boldsymbol{A} t+(\boldsymbol{A} t)^{2}+(\boldsymbol{A} t)^{3}+\cdots</script><p>前文中我们已经写出了矩阵指数函数的公式$e^{\boldsymbol At} = \boldsymbol {Se^{\Lambda t}S^{-1}}$。如果矩阵$\boldsymbol A$具有$n$个线性无关的特征向量，我们可以从幂级数定义的矩阵指数公式来再次验证： </p>
<script type="math/tex; mode=display">
\begin{aligned}
e^{\boldsymbol{A} t} &=I+\boldsymbol{A} t+\frac{(\boldsymbol{A} t)^{2}}{2}+\frac{(\boldsymbol{A} t)^{3}}{6}+\cdots \\
&=\boldsymbol{S} \boldsymbol{S}^{-1}+\boldsymbol{S} \boldsymbol{\Lambda} \boldsymbol{S}^{-1} t+\frac{\boldsymbol{S \Lambda}^{2} \boldsymbol{S}^{-1}}{2} t^{2}+\frac{\boldsymbol{S} \boldsymbol{\Lambda}^{3} \boldsymbol{S}^{-1}}{6} t^{3}+\cdots \\
&=\boldsymbol{S}\left(I+\boldsymbol{\Lambda} t+\frac{\Lambda^{2}}{2} t^{2}+\frac{\Lambda^{3}}{6} t^{3}+\cdots\right) \boldsymbol{S}^{-1} \\
&=\boldsymbol{S} e^{\boldsymbol{\Lambda} t} \boldsymbol{S}^{-1}
\end{aligned}</script><p>能够对角化的矩阵都可以表示为上式。 </p>
<script type="math/tex; mode=display">
e^{\boldsymbol{\Lambda} t}=\left[\begin{array}{cccc}
e^{\lambda_{1} t} & 0 & \cdots & 0 \\
0 & e^{\lambda_{2} t} & & 0 \\
\vdots & & \ddots & \vdots \\
0 & \cdots & 0 & e^{\lambda_{n} t}
\end{array}\right]</script><p><img src="https://pic.imgdb.cn/item/62709bcb239250f7c5026d84.jpg" style="zoom:50%"></p>
<p><img src="https://pic.imgdb.cn/item/62709c1d239250f7c503214b.jpg" style="zoom:55%"></p>
<h4 id="23-3-二阶微分方程-Second-order-differential-equations"><a href="#23-3-二阶微分方程-Second-order-differential-equations" class="headerlink" title="23.3 二阶微分方程 Second order differential equations"></a>23.3 二阶微分方程 Second order differential equations</h4><p>我们可以将二阶微分方程$y’’ + by’ + ky = 0$转化为$2\times2$的一阶问题进行处理，构造方法类似于我们对斐波那契数列的处理方法。 </p>
<p> 令<script type="math/tex">\boldsymbol {u}=\left[\begin{array}{l} y^{\prime} \\ y\end{array}\right]</script>，则有</p>
<script type="math/tex; mode=display">
\boldsymbol{u}^{\prime}=\left[\begin{array}{l}
y^{\prime \prime} \\
y^{\prime}
\end{array}\right]=\left[\begin{array}{cc}
-b & -k \\
1 & 0
\end{array}\right]\left[\begin{array}{l}
y^{\prime} \\
y
\end{array}\right]</script><p>如果是$k$阶微分方程，那么需要一个$k \times k$矩阵，除了第一行和对角线下面一排斜线上的元素之外，这个系数矩阵其它元素均为0。 </p>
<h3 id="第二十四-马尔可夫矩阵和傅里叶级数"><a href="#第二十四-马尔可夫矩阵和傅里叶级数" class="headerlink" title="第二十四 马尔可夫矩阵和傅里叶级数"></a>第二十四 马尔可夫矩阵和傅里叶级数</h3><p><font color="red"><strong>两者是关于特征值和投影矩阵的应用</strong></font>。</p>
<h4 id="24-1-马尔可夫矩阵-Markov-matrices"><a href="#24-1-马尔可夫矩阵-Markov-matrices" class="headerlink" title="24.1 马尔可夫矩阵 Markov matrices"></a>24.1 马尔可夫矩阵 Markov matrices</h4><h5 id="一-定义"><a href="#一-定义" class="headerlink" title="(一) 定义"></a>(一) 定义</h5><p>若某一矩阵$\boldsymbol A$，其所有元素非负，且每列的元素加和为1的矩阵被称为马尔可夫矩阵。马尔可夫矩阵主要应用在概率领域。将一个马尔可夫矩阵进行方幂运算得到的矩阵仍旧是马尔可夫矩阵。例如：</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left[\begin{array}{ccc}
0.1 & 0.01 & 0.3 \\
0.2 & 0.99 & 0.3 \\
0.7 & 0 & 0.4
\end{array}\right]</script><p>由23讲可知：当处理一个微分方程问题时，特征值0意味着得到一个稳态。但当进行矩阵的方幂运算时，特征值给出稳态的条件包括： </p>
<ul>
<li><strong>条件1：</strong>$\lambda_1 = 1$是矩阵的特征值之一</li>
<li><strong>条件2：</strong>$|\lambda_i| &lt; 1$其它特征值的绝对值都比1小 </li>
</ul>
<p><strong>条件1和条件2 是马尔可夫矩阵的特征值的特点。</strong></p>
<p>如果矩阵具有$n$个线性无关的特征向量，则有： </p>
<script type="math/tex; mode=display">
\boldsymbol{u}_{k}=\boldsymbol{A}^{k} \boldsymbol{u}_{0}=c_{1} \lambda_{1}{ }^{k} \boldsymbol{x}_{1}+c_{2} \lambda_{2}{ }^{k} \boldsymbol{x}_{2}+\ldots+c_{n} \lambda_{n}{ }^{k} \boldsymbol{x}_{n}</script><p>如果$\lambda_1 = 1$并且其他的特征值都小于1，则系统在$k$增大过程中趋近于$\boldsymbol u_0$的分量$c_1 \boldsymbol x_1$，即给出了一个稳态状况。这里特征向量$\boldsymbol x_1$的每一分量都是正的，因此若初始值为正，则最终的稳态也是正的。</p>
<p>Markov矩阵每一列的元素加和为1这个条件，保证了矩阵具有1这个特征值。</p>
<script type="math/tex; mode=display">
\boldsymbol{A}-\boldsymbol{I}=\left[\begin{array}{rrr}
-0.9 & 0.01 & 0.3 \\
0.2 & -0.01 & 0.3 \\
0.7 & 0 & -0.6
\end{array}\right]</script><p>从每一列减去1，则每列的加和都从1变为0。这时候行向量相加的结构就是$\boldsymbol 0$向量，因此行向量线性相关，矩阵为奇异矩阵。矩阵$\boldsymbol A$有特征向量在$\boldsymbol{A-I}$的零空间中，其对应的特征值为1。回代计算可得<script type="math/tex">\boldsymbol {x}_1=\left[\begin{array}{l} 0.6 \\ 33 \\ 0.7\end{array}\right]</script>。</p>
<p><strong>转置矩阵的特征值：</strong>转置矩阵$\boldsymbol A^{\mathrm T}$具有和原矩阵$\boldsymbol A$相同的特征值。 </p>
<script type="math/tex; mode=display">
(\boldsymbol A - \lambda \boldsymbol I)^{\mathrm T} = \boldsymbol A^{\mathrm T} -  \lambda \boldsymbol I</script><p>则由行列式的性质10可知：$\det(\boldsymbol A - \lambda \boldsymbol I) = \det(\boldsymbol A^{\mathrm T} -  \lambda \boldsymbol I)$。如果$\lambda$是矩阵$\boldsymbol A$的特征值，则其满足$\det(\boldsymbol A^{\mathrm T} -  \lambda \boldsymbol I) = 0$，因此它也是转置矩阵$\boldsymbol A^{\mathrm T}$的特征值。但是两者特征向量有所区别，<strong>零空间不等同于左零空间</strong>。</p>
<h5 id="二-例子——人口流动"><a href="#二-例子——人口流动" class="headerlink" title="(二) 例子——人口流动"></a>(二) 例子——人口流动</h5><p>用马尔可夫矩阵来研究人口流动问题：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
u_{\text {Cal }} \\
u_{\text {Mass }}
\end{array}\right]_{t=k+1}=\left[\begin{array}{cc}
0.9 & 0.2 \\
0.1 & 0.8
\end{array}\right]\left[\begin{array}{c}
u_{\text {Cal }} \\
u_{\text {Mass }}
\end{array}\right]_{t=k}</script><p>方程中$\boldsymbol u$的分量分别代表加州和麻省的人口，矩阵中的每一列中元素代表着人口去留比例，比如第一列$0.9$表示留在加州的人口占加州人口的$90\%$，而$10\%$进入麻省，第二列中由麻省进入加州的人口占麻省的$20\%$，而$80\%$选择留在麻省。如果取初值<script type="math/tex">\left[\begin{array}{c} u_{\text {Cal }} \\ u_{\text {Mass }} \end{array}\right]_{0}=\left[\begin{array}{c} 0 \\ 1000\end{array}\right]</script>，则经过一次迁徙<script type="math/tex">\left[\begin{array}{c} u_{\text {Cal }} \\ u_{\text {Mass }} \end{array}\right]_{1}=\left[\begin{array}{cc} 0.9 & 0.2 \\ 0.1 & 0.8\end{array}\right]\left[\begin{array}{c} 0 \\ 1000\end{array}\right] = \left[\begin{array}{c} 200 \\ 800\end{array}\right]</script>。</p>
<p>为了获取长时间后的人口分布，我们需要了解矩阵的特征值和特征向量。因为这是马尔可夫矩阵，所以有一个特征值1，则另一个特征值为$0.9+0.8-1=0.7$。可以求得<script type="math/tex">\boldsymbol {x}_1=\left[\begin{array}{l} 2 \\ 1 \end{array}\right], \boldsymbol {x}_2=\left[\begin{array}{l} 1 \\ -1 \end{array}\right]</script>，从$\boldsymbol x_1$可知最后的稳态为加州人口$2/3$，麻省人口$1/3$。 通解为： </p>
<script type="math/tex; mode=display">
\boldsymbol{u}_{k}=c_{1}\left[\begin{array}{l}
2 \\
1
\end{array}\right]+c_{2}(0.7)^{k}\left[\begin{array}{c}
-1 \\
1
\end{array}\right]</script><p>可以从$\boldsymbol u_0$解得$c1=1000/3$、$c2=2000/3$。 </p>
<h4 id="24-2-傅里叶级数和投影矩阵-Fourier-series-amp-Projections"><a href="#24-2-傅里叶级数和投影矩阵-Fourier-series-amp-Projections" class="headerlink" title="24.2 傅里叶级数和投影矩阵 Fourier series &amp; Projections"></a>24.2 傅里叶级数和投影矩阵 Fourier series &amp; Projections</h4><p>如果有一组标准正交基为$\boldsymbol q_1，\boldsymbol q_2, \dots, \boldsymbol q_n$，则任意向量$\boldsymbol v$可以写成： </p>
<script type="math/tex; mode=display">
\boldsymbol{v}=x_{1} \boldsymbol{q}_{1}+x_{2} \boldsymbol{q}_{2}+\ldots+x_{n} \boldsymbol{q}_{n}</script><p>因为当$i，j$不相等时有$\boldsymbol q_1^{\mathrm T}\boldsymbol q_2$。因此有</p>
<script type="math/tex; mode=display">
\boldsymbol{q}_{i}^{T} \boldsymbol{v}=x_{1} \boldsymbol{q}_{i}^{T} \boldsymbol{q}_{1}+x_{2} \boldsymbol{q}_{i}^{T} \boldsymbol{q}_{2}+\ldots+x_{n} \boldsymbol{q}_{i}^{T} \boldsymbol{q}_{n}=x_{i}</script><p>我们得到了分量$x_i$的公式：$x_i = \boldsymbol q_i^{\mathrm T} \boldsymbol v$。</p>
<p>因为<script type="math/tex">\boldsymbol{v}=\left[\begin{array}{lll} \boldsymbol{q}_{1} & \cdots & \boldsymbol{q}_{n} \end{array}\right]\left[\begin{array}{c} x_{1} \\ \vdots \\x_{n}\end{array}\right]</script>，即$\boldsymbol v = \boldsymbol {Qx}$，所以$\boldsymbol x = \boldsymbol Q^{-1} \boldsymbol v$，<strong>因为$\boldsymbol Q$为<font color="red">正交矩阵</font>，所以有</strong>$\boldsymbol Q^{-1} = \boldsymbol Q^{\mathrm T}$，可得$\boldsymbol x = \boldsymbol Q^{\mathrm T} \boldsymbol v$ 。这与我们之前得到的$x_i = \boldsymbol q_i^{\mathrm T} \boldsymbol v$完全相同。这里给出了求分量的思路就是用空间的一组标准正交基去点乘目标向量，利用其标准正交的性质得到所求。</p>
<p>标准正交是此处的核心概念。而傅里叶级数也是在这个概念上构建的。我们可以对任意函数做傅里叶展开，得到表达式： </p>
<script type="math/tex; mode=display">
f(x)=a_{0}+a_{1} \cos x+b_{1} \sin x+a_{2} \cos 2 x+b_{2} \sin 2 x+\cdots</script><p>与之前的有限个标准正交向量组成的正交矩阵不同，这个空间是无限维，它的一组基是：</p>
<script type="math/tex; mode=display">
1, \cos x, \sin x, \cos 2x, \sin 2x, \dots</script><p>此处的正交概念与$R^n$空间不同，点积的概念也不同。 </p>
<ul>
<li>向量正交：<script type="math/tex">\boldsymbol{v}^{T} \boldsymbol{w}=v_{1} w_{1}+v_{2} w_{2}+\ldots+v_{n} w_{n}</script></li>
<li>函数正交：$f^{T} g=\int_{0}^{2 \pi} f(x) g(x) d x$</li>
</ul>
<p>计算基$\sin x$和$\cos x$的点积可以验证其正交性。 </p>
<p>采用和标准正交基相同的策略可以得到傅里叶变换的参数。 </p>
<script type="math/tex; mode=display">
\begin{aligned}
\int_{0}^{2 \pi} f(x) \cos x d x &=\int_{0}^{2 \pi}\left(a_{0}+a_{1} \cos x+b_{1} \sin x+a_{2} \cos 2 x+\cdots\right) \cos x d x \\
&=0+\int_{0}^{2 \pi} a_{1} \cos ^{2} x d x+0+0 \cdots \\
&=a_{1} \pi
\end{aligned}</script><p>可以得到：</p>
<script type="math/tex; mode=display">
a_{1}=\frac{1}{\pi} \int_{0}^{2 \pi} f(x) \cos x d x</script><p>同理可以求得其它参数。 </p>
<hr>
<p>参考笔记：<a href="https://blog.csdn.net/herosunly?type=blog">https://blog.csdn.net/herosunly?type=blog</a></p>
<p>笔记链接：<a href="https://blog.csdn.net/qq_16829085/article/details/102908029">https://blog.csdn.net/qq_16829085/article/details/102908029</a></p>
<p>MIT 公开课：Gilbert Strang《线性代数》课程笔记（汇总）<a href="https://blog.csdn.net/xhf0374/article/details/63250083">https://blog.csdn.net/xhf0374/article/details/63250083</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>数学基础</category>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>本质问题</tag>
        <tag>线代</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数_Part3</title>
    <url>/2022/05/04/LinearAlgebra3/</url>
    <content><![CDATA[<h3 id="第二十五讲-正交向量与正交子空间"><a href="#第二十五讲-正交向量与正交子空间" class="headerlink" title="第二十五讲 正交向量与正交子空间"></a>第二十五讲 正交向量与正交子空间</h3><p>进入第三单元学习，主题是正定矩阵及其应用。<strong>对称矩阵是其中最重要的矩阵之一，对称矩阵的特征值为实数并且拥有一套正交特征向量</strong>。正定矩阵的性质则比对称矩阵更好更好。 </p>
<h4 id="25-1-实对称矩阵-Symmetric-matrices-boldsymbol-A-mathrm-T-A"><a href="#25-1-实对称矩阵-Symmetric-matrices-boldsymbol-A-mathrm-T-A" class="headerlink" title="25.1 实对称矩阵 Symmetric matrices $\boldsymbol{A^{\mathrm T} = A}$"></a>25.1 实对称矩阵 Symmetric matrices $\boldsymbol{A^{\mathrm T} = A}$</h4><p>包含特殊性质的矩阵，例如Markov矩阵，其特征值和特征向量往往拥有一定特性。对于对称矩阵来说有两个待证明性质：</p>
<ul>
<li>实对称矩阵的特征值也是实数 </li>
<li>有完全正交的特征向量<ul>
<li>如果特征值互不相同，那么每个特征值的特征向量是在单独的一条线上，那些线是垂直正交的；如果特征值重复，那就有一整个平面的特征向量，在那个平面上可以选择垂直的向量</li>
<li>可以将这组特征向量转化为标准正交向量。</li>
</ul>
</li>
</ul>
<p>例如：<strong>单位矩阵</strong>，单位矩阵是对称矩阵，特征值都为1，每一个向量都是特征向量。</p>
<p>如果$\boldsymbol A$具有$n$个线性无关的特征向量，可以对角化得到：</p>
<script type="math/tex; mode=display">
\boldsymbol A = \boldsymbol{S \Lambda S}^{-1}</script><p>而对于对称矩阵，由于其特征向量是正交的，所以有：</p>
<script type="math/tex; mode=display">
\boldsymbol A = \boldsymbol{Q \Lambda Q}^{-1} = \boldsymbol{Q \Lambda Q}^{\mathrm T}</script><p>上式$\boldsymbol Q$表示标准正交矩阵，这里是方阵，因为对称矩阵的$\boldsymbol S$是垂直正交的，所以可转化为$\boldsymbol Q$，同时标准正交矩阵$\boldsymbol Q$有：$\boldsymbol Q^{-1} = \boldsymbol Q^{\mathrm T}$ ，所以以上式子是对称矩阵的分解形式，分解成特征向量和特征值的组合。等式右边取转置又得到自己，所以A是对称矩阵。 </p>
<ul>
<li>数学上叫这个为<font color="blue"><strong>谱定理</strong></font>，谱就是指矩阵的特征值集合，一些纯东西组合。</li>
<li>力学上叫这个为<font color="blue"><strong>主轴定理</strong></font>，从几何图形上看，它意味着如果给定某种材料，在合适的轴上来看，它就变成对角化的，方向就不会重复。</li>
</ul>
<p><strong>问题1：为什么实对称矩阵的特征值是实数</strong> </p>
<p>先假设特征值中存在复数。矩阵$\boldsymbol A$具有特征值$\lambda$和特征向量$\boldsymbol x$，则有：</p>
<script type="math/tex; mode=display">
\boldsymbol {Ax} = \lambda \boldsymbol x {\Longrightarrow} \overline{\boldsymbol {Ax}} = \overline{ \lambda \boldsymbol {x}} \Longrightarrow \overline{\boldsymbol A} \overline{\boldsymbol x} = \overline{\lambda}\overline{\boldsymbol x} \Longrightarrow  \boldsymbol A \overline{\boldsymbol x} = \overline{\lambda}\overline{\boldsymbol x}</script><p>上式说明实对称矩阵有特征值$\lambda$和特征向量$\boldsymbol x$，那该矩阵必然有另一特征值$\overline \lambda$和特征向量$\overline{\boldsymbol x}$。将上式最后部分$\boldsymbol A \overline{\boldsymbol x} = \overline{\lambda}\overline{\boldsymbol x}$转置可得：</p>
<script type="math/tex; mode=display">
\overline{\boldsymbol x}^{\mathrm T} \boldsymbol A^{\mathrm T} = \overline{\lambda}\overline{\boldsymbol x}^{\mathrm T}</script><p>将上式从右边乘$\boldsymbol x$取内积：</p>
<script type="math/tex; mode=display">
\overline{\boldsymbol x}^{\mathrm T} \boldsymbol A^{\mathrm T}{\boldsymbol x} = \overline{\lambda}\overline{\boldsymbol x}^{\mathrm T}{\boldsymbol x}</script><p>再对上述长式子中前面部分$\boldsymbol {Ax} = \lambda \boldsymbol x$左边乘$\overline{\boldsymbol x}^{\mathrm T}$取内积：</p>
<script type="math/tex; mode=display">
\overline{\boldsymbol x}^{\mathrm T} \boldsymbol {Ax} = \lambda \overline{\boldsymbol x}^{\mathrm T}\boldsymbol x</script><p>上面两个式子一对比可知：等式左侧相同，因此右端也相等：</p>
<script type="math/tex; mode=display">
\overline{\lambda}\overline{\boldsymbol x}^{\mathrm T}{\boldsymbol x} = \lambda \overline{\boldsymbol x}^{\mathrm T}\boldsymbol x</script><p>所以有$\overline{\lambda} = \lambda$，所以实对称阵的特征值为实数。</p>
<blockquote>
<p>注意：上面推导的最后一步要保证$\overline{\boldsymbol x}^{\mathrm T}\boldsymbol x$不等于0。这里我们简单带入数据可知$\overline{\boldsymbol x}^{\mathrm T}\boldsymbol x$不等于0且为正数。( 一个向量为复向量，那么它乘以其共轭复向量得到实部的平方加上虚部的平方，为其长度平方 )</p>
</blockquote>
<p><strong>问题1延伸：性质好矩阵</strong></p>
<p>本节中性质好矩阵是指<font color="red">“特征值为实数”、”特征向量正交垂直”</font>。</p>
<p>那么什么矩阵是好的呢？</p>
<ul>
<li>如果一个矩阵是实数矩阵，那么<strong>“性质好”</strong>的矩阵就是<strong>“实对称矩阵”</strong>，$\boldsymbol{A^{\mathrm T} = A}$</li>
<li>如果一个矩阵是复数矩阵，那么<strong>“性质好”</strong>的矩阵就是<strong>“复对称矩阵”</strong>，$\overline{\boldsymbol A}^{\mathrm T} = \boldsymbol A$<ul>
<li>因为可以类比实矩阵的推导过程进行推导，这时可以发现对复数矩阵来说，只有当$\overline{\boldsymbol A}^{\mathrm T} = \boldsymbol A$成立时才可以推导出该复数矩阵”特征值为实数”。也就是说假设矩阵原来是复数矩阵，对称的等价条件是$\overline{\boldsymbol A}^{\mathrm T} = \boldsymbol A$；</li>
<li>这里就是想说：若给定一复矩阵，想让它和实对称矩阵有一样的性质，那该复矩阵就需要满足$\overline{\boldsymbol A}^{\mathrm T} = \boldsymbol A$。</li>
</ul>
</li>
</ul>
<p>对于对称矩阵，$\boldsymbol A = \boldsymbol{Q \Lambda Q}^{-1} = \boldsymbol{Q \Lambda Q}^{\mathrm T}$，可以写作： </p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{A}=\boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{-1}=\left[\begin{array}{llll}
\boldsymbol{q}_{1} & \boldsymbol{q}_{2} & \cdots & \boldsymbol{q}_{\mathrm{n}}
\end{array}\right]\left[\begin{array}{llll}
\lambda_{1} & & & \\
& \lambda_{2} & & \\
& & \ddots & \\
& & & \lambda_{\mathrm{n}}
\end{array}\right]\left[\begin{array}{c}
\boldsymbol{q}_{1}^{T} \\
\boldsymbol{q}_{2}^{T} \\
\vdots \\
\boldsymbol{q}_{\mathbf{n}}
\end{array}\right]\\
&=\lambda_{1} \boldsymbol{q}_{1} \boldsymbol{q}_{1}^{T}+\lambda_{2} \boldsymbol{q}_{2} \boldsymbol{q}_{2}{ }^{T}+\cdots+\lambda_{\mathrm{n}} \boldsymbol{q}_{\mathrm{n}} \boldsymbol{q}_{\mathrm{n}}{ }^{T}
\end{aligned}</script><p>矩阵<script type="math/tex">\boldsymbol q_k \boldsymbol q_k^{\mathrm T}</script>朝向向量<script type="math/tex">\boldsymbol q_k</script>的投影矩阵，所以<strong>每一个对称矩阵都是正交投影矩阵的线性组合(每一个对称矩阵都是一些相互垂直的投影矩阵的组合)</strong>。这是理解谱定理的另一种方法。</p>
<p>当确认矩阵特征值为实数后，下一个要考虑的问题就是它是正还是负数，因为这影响着微分方程中体系的稳定与否。但是对于大型矩阵通过计算$|(\boldsymbol A -\lambda \boldsymbol I)| = 0$ 得到特征值进行判定难以实现，即使用MATLAB求解结果也不一定可靠，但MATLAB可以得到矩阵的主元，而<font color="green"><strong>对称阵的主元中正负数的个数与特征值相同</strong></font>，即正主元的数目等于正特征值的数目。 </p>
<blockquote>
<p>注意：课程中老师并没有说“主元正负个数等于特征值正负个数”这个性质是如何证明的，可以不做深究。</p>
<p>但是简要证明可以参考：<a href="https://zhuanlan.zhihu.com/p/46023045">https://zhuanlan.zhihu.com/p/46023045</a></p>
<p>(对于对称阵主元与特征值符号相匹配这件事情，通常是用合同矩阵的惯性定理加以证明)</p>
</blockquote>
<p>矩阵$\boldsymbol A + b \boldsymbol I$的特征值比矩阵的$\boldsymbol A$特征值大$b$，可以通过$\boldsymbol A + b \boldsymbol I$的主元来了解矩阵$\boldsymbol A$的特征值与$b$的大小关系，因此利用这个性质可以估计特征值的状态。 </p>
<h4 id="25-2-正定矩阵-Positive-definite-matrices"><a href="#25-2-正定矩阵-Positive-definite-matrices" class="headerlink" title="25.2 正定矩阵 Positive definite matrices"></a>25.2 正定矩阵 Positive definite matrices</h4><p>正定矩阵是对称阵的一个子类，如果一个实对称矩阵的特征值都是正数，那么它是正定矩阵。其性质比普通对称阵更好一点，即其性质为：</p>
<ul>
<li>① 特征值都为正数的对称矩阵，它的主元也均为正数。 </li>
<li>② 正定矩阵的所有子行列式都是正数 </li>
</ul>
<p><strong>正定矩阵将方阵特征值，主元，行列式融为一体</strong>。 </p>
<p>若将行列式作为正定的判据，则要求$n$阶矩阵左上角的所有$k \times k,(1 \leq k \leq n)$子行列式（subdeterminant）数值均为正，矩阵确定为正定矩阵。</p>
<p>本讲的内容将之前教授的主元、行列式和特征值的概念结合在了一起，对于正定矩阵这些都是正的，当完全掌握了它们的性质后会推广到非对称矩阵，甚至非方阵。</p>
<h3 id="第二十六讲-复数矩阵和快速傅里叶变换"><a href="#第二十六讲-复数矩阵和快速傅里叶变换" class="headerlink" title="第二十六讲 复数矩阵和快速傅里叶变换"></a>第二十六讲 复数矩阵和快速傅里叶变换</h3><p>实矩阵也可能有复特征值，因此无法避免在矩阵运算中碰到复数，本讲学习处理复数矩阵和复向量。</p>
<p>最重要的复矩阵是傅里叶矩阵，它用于傅里叶变换。而对于大数据处理快速傅里叶变换（FFT）显得更为重要，它将矩阵乘法的运算次数从$n^2$降到$n \log_2 n$次。 </p>
<h4 id="26-1-复向量-Complex-vectors"><a href="#26-1-复向量-Complex-vectors" class="headerlink" title="26.1 复向量 Complex vectors"></a>26.1 复向量 Complex vectors</h4><p>对于给定的复向量<script type="math/tex">\boldsymbol{z}=\left[\begin{array}{c} z_{1} \\ z_{2} \\ \vdots \\ z_{\mathrm{n}}\end{array}\right] \in \mathbf{C}^{n}</script>，其元素中有复数，因此$\boldsymbol z^{\mathrm T}\boldsymbol z$无法给出向量的长度。例如：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}
1 & i
\end{array}\right]\left[\begin{array}{l}
1 \\
i
\end{array}\right]=0</script><p>因此对于复向量而言，需要定义其模长为：</p>
<script type="math/tex; mode=display">
|\boldsymbol{z}|^{2}=\overline{\boldsymbol{z}}^{T} \boldsymbol{z}=\left|z_{1}\right|^{2}+\left|z_{1}\right|^{2}+\cdots+\left|z_{1}\right|^{2}</script><p>将复矩阵的<strong>“共轭转置”</strong>记为$\overline{\boldsymbol{z}}^{\mathrm T} = {\boldsymbol{z}}^{\mathrm H}$(H来自于“Hermite”)。</p>
<p>与之相似，内积的定义也变为：</p>
<script type="math/tex; mode=display">
\boldsymbol{y}^{\mathrm H} \boldsymbol{x}=\overline{\boldsymbol{y}}^{\mathrm T} \boldsymbol{x}=\bar{y}_{1} x_{1}+\bar{y}_{2} x_{2}+\cdots+\bar{y}_{n} x_{n}</script><h4 id="26-2-复矩阵-Complex-matrices"><a href="#26-2-复矩阵-Complex-matrices" class="headerlink" title="26.2 复矩阵 Complex matrices"></a>26.2 复矩阵 Complex matrices</h4><p> 上一讲中讲到了对于复矩阵$\boldsymbol A$，若有$\overline{\boldsymbol A}^{\mathrm T} = \boldsymbol A$则复矩阵$\boldsymbol A$的特征值为实数。这种复矩阵被称为埃尔米特矩阵(Hermitian matrixes)。转置共轭记作$\overline{\boldsymbol{A}}^{\mathrm T} = {\boldsymbol{A}}^{\mathrm H}$</p>
<p> 例如矩阵<script type="math/tex">\left[\begin{array}{cc} 2 & 3+i \\ 3-i & 5\end{array}\right]</script> 为埃尔米特矩阵。它具有实数特征值和正交的特征向量。由性质可知埃尔米特矩阵对角线均为实数。 </p>
<p>此处向量标准正交的意思是：</p>
<script type="math/tex; mode=display">
\overline{\boldsymbol{q}}_{j}^{T} \boldsymbol{q}_{k}= \begin{cases}0 & j \neq k \\ 1 & j=k\end{cases}</script><p>用$n$个标准正交的复向量作为列向量可以构造一个矩阵$\boldsymbol Q$，则有</p>
<script type="math/tex; mode=display">
\boldsymbol Q^{\mathrm H}\boldsymbol Q  = \boldsymbol Q \boldsymbol Q^{\mathrm H} = \boldsymbol I</script><script type="math/tex; mode=display">
\boldsymbol Q^{\mathrm T}\boldsymbol Q = \boldsymbol I = \boldsymbol Q^{\mathrm H}\boldsymbol Q</script><p>这个复空间的正交矩阵$\boldsymbol Q$称为<font color="red"><strong>酉矩阵</strong>（unitary matrix）</font>，换句话说，酉矩阵的逆矩阵，就是其共轭转置： </p>
<script type="math/tex; mode=display">
\boldsymbol Q^{-1} = \boldsymbol Q^{\mathrm H}</script><p>酉矩阵是实数上的正交矩阵在复数的推广。 </p>
<h4 id="26-3-酉矩阵的性质"><a href="#26-3-酉矩阵的性质" class="headerlink" title="26.3 酉矩阵的性质"></a>26.3 酉矩阵的性质</h4><ul>
<li><strong>性质1：</strong>$\boldsymbol Q^{\mathrm H}\boldsymbol Q  = \boldsymbol Q \boldsymbol Q^{\mathrm H} = \boldsymbol I$</li>
<li><strong>性质2：</strong>酉矩阵亦必定可逆，且逆矩阵等于其共轭转置：$\boldsymbol Q^{-1} = \boldsymbol Q^{\mathrm H}$</li>
<li><strong>性质3：</strong>酉矩阵的所有特征值，都是绝对值等于1的复数：$|\lambda_i| = 1$</li>
<li><strong>性质4：</strong>由性质3可知，酉矩阵行列式的绝对值也是 1： $\det(\boldsymbol Q) = 1$</li>
<li><strong>性质5：</strong>酉矩阵不会改变两个复向量$\boldsymbol x$和$\boldsymbol y$的点积：$(\boldsymbol{Qx}) \cdot (\boldsymbol{Qy}) = \boldsymbol{x \cdot y}$ </li>
</ul>
<h4 id="26-3-傅里叶变换Fourier-transform"><a href="#26-3-傅里叶变换Fourier-transform" class="headerlink" title="26.3 傅里叶变换Fourier transform"></a>26.3 傅里叶变换Fourier transform</h4><p><strong>傅里叶矩阵</strong>：最著名的“酉矩阵”。 </p>
<p>傅里叶级数是将周期函数或者信号变换为不同频率的三角函数的和函数。 </p>
<script type="math/tex; mode=display">
f(x)=a_{0}+a_{1} \cos x+b_{1} \sin x+a_{2} \cos 2 x+b_{2} \sin 2 x+\cdots</script><p>在电子工程或者计算机科学中，矩阵的行和列从第0行和第0列开始计数，最后到第$n-1$行和第$n-1$列。我们在讨论傅里叶矩阵的时候遵从这种习惯。 $n$阶傅里叶矩阵：全矩阵，是一个酉矩阵：</p>
<script type="math/tex; mode=display">
\boldsymbol{F}_{n}=\left[\begin{array}{rrrrr}
1 & 1 & 1 & \cdots & 1 \\
1 & \omega & \omega^{2} & & \omega^{(n-1)} \\
1 & \omega^{2} & \omega^{2} & & \omega^{2(n-1)} \\
\vdots & & & \ddots & \\
1 & \omega^{n-1} & \omega^{2(n-1)} & & \omega^{(n-1)^{2}}
\end{array}\right]</script><p>也就是<script type="math/tex">(\boldsymbol F_n)_{jk} = \omega^{jk}</script>，且傅里叶矩阵为对称矩阵<script type="math/tex">\boldsymbol F_n \boldsymbol F_n^{\mathrm T}</script> 。矩阵中的元素是<script type="math/tex">\omega</script>的幂，且<script type="math/tex">\omega^n=1</script>($n$是矩阵阶数)，在复平面内，<script type="math/tex">\omega</script>落在单位圆上：</p>
<script type="math/tex; mode=display">
\omega = e^{i \frac{2\pi}{n}}</script><p>矩阵的列向量正交。的方次分布在复平面的单位元上，只是幅角不同。当$n=4$时有</p>
<script type="math/tex; mode=display">
\omega^4=1, \omega = e^{i \frac{2\pi}{4}} = i</script><script type="math/tex; mode=display">
\boldsymbol{F}_{4}=\left[\begin{array}{cccc}
1 & 1 & 1 & 1 \\
1 & i & i^{2} & i^{3} \\
1 & i^{2} & i^{4} & i^{6} \\
1 & i^{3} & i^{6} & i^{9}
\end{array}\right]=\left[\begin{array}{rrrr}
1 & 1 & 1 & 1 \\
1 & i & -1 & -i \\
1 & -1 & 1 & -1 \\
1 & -i & -1 & i
\end{array}\right]</script><p>从矩阵可以得到一个四点(离散的)傅里叶变换，它的逆矩阵就是反傅里叶变换。逆矩阵很容易计算，因为傅里叶矩阵列向量正交。实际上这个矩阵可以分解成一系列稀疏矩阵，并且它们的逆矩阵都很容易得到。 </p>
<p>计算可知列向量的模不是1，矩阵除以2之后，向量标准正交： </p>
<script type="math/tex; mode=display">
\dfrac{1}{4}\boldsymbol F_n^{\mathrm H}\boldsymbol F_n =\boldsymbol I</script><p>它的逆矩阵就是共轭转置。 一个很好的性质：可以把傅里叶矩阵分解为一些列“稀疏矩阵”。 </p>
<h4 id="26-4-快速傅里叶变换-Fast-Fourier-transform"><a href="#26-4-快速傅里叶变换-Fast-Fourier-transform" class="headerlink" title="26.4 快速傅里叶变换 Fast Fourier transform"></a>26.4 快速傅里叶变换 Fast Fourier transform</h4><p>对于64阶傅里叶矩阵<script type="math/tex">\boldsymbol F_{64}</script>中的<script type="math/tex">\omega_{64}</script>与32阶傅里叶矩阵<script type="math/tex">\boldsymbol F_{32}</script>的元素<script type="math/tex">\omega_{32}</script>相比，幅角是其一半，<script type="math/tex">\omega_{64}^2 = \omega_{32}</script>。可以从分块矩阵运算找到两者的联系：</p>
<script type="math/tex; mode=display">
\left[\boldsymbol{F}_{64}\right]=\left[\begin{array}{rr}
\boldsymbol{I} & \boldsymbol{D} \\
\boldsymbol{I} & -\boldsymbol{D}
\end{array}\right]\left[\begin{array}{rr}
\boldsymbol{F}_{32} & 0 \\
0 & \boldsymbol{F}_{32}
\end{array}\right][\boldsymbol{P}]</script><p> 其中$\boldsymbol P​$是置换矩阵，而$\boldsymbol D​$为对角矩阵：</p>
<script type="math/tex; mode=display">
\boldsymbol{P}=\left[\begin{array}{cccccccc}
1 & 0 & 0 & 0 & \cdots & \cdots & 0 & 0 \\
0 & 0 & 1 & 0 & & & 0 & 0 \\
& & & \vdots & & & & \\
0 & 0 & 0 & 0 & \cdots & \cdots & 1 & 0 \\
0 & 1 & 0 & 0 & \cdots & \cdots & 0 & 0 \\
0 & 0 & 0 & 1 & \cdots & \cdots & 0 & 0 \\
& & & \vdots & & & & \\
0 & 0 & 0 & 0 & \cdots & \cdots & 0 & 1
\end{array}\right]</script><script type="math/tex; mode=display">
\boldsymbol{D}=\left[\begin{array}{lllll}
1 & & & & \\
& \omega & & & \\
& & \omega^{2} & & \\
& & & \ddots & \\
& & & & \\
& & & & \omega^{31}
\end{array}\right]</script><p>$\boldsymbol P$的效果是使得所乘的向量$\boldsymbol x$序数为奇数的分量如$x_1，x_3，x_5$等提到前面，而偶数分量$x_2，x_4$等放到后面。</p>
<p>计算$64$阶傅里叶变换（傅里叶矩阵乘以向量）的计算量是$64 \times 64$，而等式右侧的计算量是$2 \times 32 \times 32$（两个$32$阶）再加上一些修正项，修正项主要来自于与对角矩阵$\boldsymbol D$的乘法，大约为$32$次。继续对$\boldsymbol F_{32}$进行分解，计算的运算量再一次下降变为$2 (2 \times 16 \times 16+16)+32$。分解到最后，仅剩修正项的运算，$32 \times \log_2 64$次。对于$n$阶矩阵，即将$n^2$次计算降至$\dfrac{n}{2}\log_2 n$次。例如对于$1024$阶矩阵，运算量从$1024 \times 1024$降至$5 \times 1024$。 </p>
<h3 id="第二十七讲-正定矩阵和最小值"><a href="#第二十七讲-正定矩阵和最小值" class="headerlink" title="第二十七讲 正定矩阵和最小值"></a>第二十七讲 正定矩阵和最小值</h3><p>本讲学习正定矩阵，这部分内容将本课程之前的知识点：主元、行列式、特征值以及方程的稳定性融为一体。本讲介绍如何判定一个矩阵是否正定矩阵，以及当一个矩阵是正定矩阵时，其内涵和矩阵操作的效果有何特别之处。此外还有正定矩阵与几何的关系：椭圆和正定有关，双曲线与正定无关。 </p>
<h4 id="27-1-正定矩阵-Positive-definite-matrices"><a href="#27-1-正定矩阵-Positive-definite-matrices" class="headerlink" title="27.1 正定矩阵 Positive definite matrices"></a>27.1 正定矩阵 Positive definite matrices</h4><p>给定一个$2\times 2$对称矩阵<script type="math/tex">\boldsymbol A=\left[\begin{array}{ll} a & b \\ b & c\end{array}\right]</script>，有四个途径判定矩阵是否正定矩阵： </p>
<ul>
<li>特征值： 所有特征值均大于0——$\lambda_1 &gt; 0$，$\lambda_2 &gt; 0$</li>
<li>行列式：所有子行列式均大于0——$a&gt;0$，$ac-b^2&gt;0$</li>
<li>主元： 主元要均大于0——$a&gt;0$，$\dfrac{ac-b^2}{a} &gt;0$</li>
<li>表达式 ：$\boldsymbol x^{\mathrm T} \boldsymbol A \boldsymbol x &gt; 0, (\boldsymbol x 为任意向量，但是\boldsymbol x \neq  0)$——<strong>通常这就是正定的定义，而前三条是用来验证正定性的条件</strong></li>
</ul>
<p>给定矩阵<script type="math/tex">\boldsymbol A=\left[\begin{array}{ll} 2 & 6 \\ 6 & c\end{array}\right]</script>，从判据(行列式)可知矩阵为正定阵的条件是$2c-36&gt;0$，即$c&gt;18$。 </p>
<ul>
<li><strong>当$c = 18$时：</strong></li>
</ul>
<p>矩阵<script type="math/tex">\left[\begin{array}{ll} 2 & 6 \\ 6 & 18\end{array}\right]</script>正好处在判定为正定矩阵的临界点上，称之为<strong>半正定(positive semidefinite)矩阵</strong>，它具有一个特征值0，是奇异矩阵，只有一个主元，而行列式为0。<strong>半正定矩阵特征值大于等于0。</strong> </p>
<p>再观察$\boldsymbol x^{\mathrm T} \boldsymbol A \boldsymbol x &gt; 0$判据： </p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{x}^{\mathrm T} \boldsymbol{A} \boldsymbol{x} &=\left[\begin{array}{ll}
x_{1} & x_{2}
\end{array}\right]\left[\begin{array}{cc}
2 & 6 \\
6 & 18
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2}
\end{array}\right] \\
&=2 x_{1}^{2}+12 x_{1} x_{2}+18 x_{2}^{2}
\end{aligned}</script><p>之前讨论得都是线性方程$\boldsymbol {Ax}$(展开方程次数为一阶)，现在引入$\boldsymbol x^{\mathrm T}$变成二次，如果对于任意$x,y$，这种二次型(quadratic form)$ax^2+2bxy+cy^2$均大于零，则矩阵为正定矩阵。</p>
<p>在本例的半正定矩阵中，当$x_1=3, x_2=-1$时，带入可知：<script type="math/tex">\boldsymbol{x}^{\mathrm T} \boldsymbol{A} \boldsymbol{x} = 0</script></p>
<ul>
<li><strong>当$c = 7$时：</strong></li>
</ul>
<p>矩阵变为<script type="math/tex">\left[\begin{array}{ll} 2 & 6 \\ 6 & 7\end{array}\right]</script>，其二次型为<script type="math/tex">f(x,y) = \boldsymbol{x}^{\mathrm T} \boldsymbol{A} \boldsymbol{x} = 2 x_{1}^{2}+12 x_{1} x_{2}+ 7 x_{2}^{2}</script>，从图像上看无最小值点，在原点处有一<strong>鞍点</strong>。鞍点在某个方向上看是极大值点，在另一方向上是极小值点，实际上最佳观测角度是<strong>特征向量的方向</strong>。 </p>
<ul>
<li><strong>当$c = 20$时：</strong></li>
</ul>
<p>矩阵变为<script type="math/tex">\left[\begin{array}{ll} 2 & 6 \\ 6 & 20\end{array}\right]</script>，主元为正；特征值之积为行列式的值4，特征值和为矩阵的迹22，因此特征值为正；子行列式均为正。矩阵为正定矩阵。 </p>
<p>其二次型<script type="math/tex">f(x,y) = \boldsymbol{x}^{\mathrm T} \boldsymbol{A} \boldsymbol{x} = 2 x_{1}^{2}+12 x_{1} x_{2}+ 20 x_{2}^{2}</script>，其图像最小值点为原点，一阶偏导数为0，二阶偏导数大于0。 </p>
<p><img src="https://pic.imgdb.cn/item/62731bf60947543129a804f5.jpg" style="zoom:80%"></p>
<blockquote>
<p>当年上课时候老师讲双曲面的鞍点，画的图不好，有个篮球队的哥们理解不了，于是下了课跑去问老师，老师指着他的热身篮球裤说，就你这种裤腿两侧能撕开的篮球裤，全部打开，拎起来，裤裆地方就是鞍点……算了，反正GS画图真心不行，看看Lay的吧： </p>
<p><img src="https://pic.imgdb.cn/item/62731c620947543129a8977a.jpg" style="zoom:80%"></p>
</blockquote>
<p>微积分中判定最小值点的判据【海森矩阵（Hessian Matrix）？】：</p>
<ul>
<li>一阶导数等于零：$\dfrac{\mathrm du}{\mathrm d x} = 0$</li>
<li>二阶导数为正：$\dfrac{\mathrm d^2u}{\mathrm d x^2} &gt; 0$</li>
</ul>
<p>线性代数中判据为二阶导数矩阵正定。</p>
<p>对于二次型我们可以用配方的办法来验证其是否具有最小值：</p>
<script type="math/tex; mode=display">
f(x,y) = \boldsymbol{x}^{\mathrm T} \boldsymbol{A} \boldsymbol{x} = 2 x^{2}+12 xy+ 20 y^{2} = 2(x+3y^2)+2y^2</script><p>配方使得$x^2$的系数和交叉项$xy$的系数配合形成完全平方的形式，这个时候用到的$y^2$的系数正好是18，即判定正定的临界点。在此例中如果实际的系数$c&gt;18$，则还剩余$(c-18)y^2$，二次型在原点之外一定大于零，若$c&lt;18$则二次型可以小于等于0。 </p>
<p>对于$f(x,y) = \boldsymbol{x}^{\mathrm T} \boldsymbol{A} \boldsymbol{x} = 2 x^{2}+12 xy+ 20 y^{2} = 2(x+3y^2)+2y^2$，其几何图像为碗型的曲面，如果我们用$f=1$的截面横截曲面，得到的就是$2(x+3y^2)+2y^2 = 1$的椭圆。而对于双曲面进行切割就得到双曲线。</p>
<p>配方法其实就是消元：</p>
<p><img src="https://pic.imgdb.cn/item/62731ff60947543129ad7ba2.jpg"></p>
<p>主元就是平方项系数，$\boldsymbol L$矩阵中的行操作数$l_{21}$ 就是配方项内$y$的系数。因为主元是每一个完全平方项的系数，因此这就是为什么矩阵的主元为正则该矩阵为正定矩阵。本例中二次型表达式的配方说明了二维的情形，而线代的理论可以将之推广到$n$维。 </p>
<blockquote>
<p>PS：虽然知道配方和消元数字上是的对上的，但我还是不太明白消元和配方过程上有啥联系…… </p>
<p>消元和配方都是在去耦合吧，调配成最简略的关系 </p>
</blockquote>
<p>二阶导数的矩阵记为<script type="math/tex">\left[\begin{array}{ll} f_{x x} & f_{x y} \\ f_{y x} & f_{y y}\end{array}\right]</script>【海森矩阵（Hessian Matrix）？】，矩阵对称代表交叉二阶偏导数与求导顺序无关，即<script type="math/tex">f _ {xy} = f _ {yx}</script>。在微积分中我们学到的判据<script type="math/tex">f _ {xx}f _ {yy} > f _ {xy}^2</script>，和<strong>二阶矩阵判定正定是等价的</strong>，并且线代可以推广到$n$维。 </p>
<script type="math/tex; mode=display">
f_{xx}f_{yy}>f_{xy}^2 \stackrel{等价于}\Longleftrightarrow 二阶矩阵判定正定</script><p>3阶矩阵<script type="math/tex">\boldsymbol A=\left(\begin{array}{ccc} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2\end{array}\right)</script>，它是正定矩阵。计算子行列式得到</p>
<script type="math/tex; mode=display">
|2| = 2, \left|\begin{array}{rr}
2 & -1 \\
-1 & 2
\end{array}\right|=3, 
\left|\begin{array}{rrr}
1 & 2 & 1 \\ 0 & 2 & -2 \\ 0 & 4 & 1
\end{array}\right|=4</script><p>主元是$2，3/2，4/3$。特征值是$2-\sqrt2, 2, 2+\sqrt2$。</p>
<blockquote>
<p>PS：这是G. Strang最爱的矩阵之一，可以用来把二阶微分方程变成离散问题，因为它每一行都是差分方程 <script type="math/tex">f_{n+1}-2f_n+f_{n-1}</script></p>
</blockquote>
<p>其二次型为：</p>
<script type="math/tex; mode=display">
f(x,y) = \boldsymbol{x}^{\mathrm T} \boldsymbol{A} \boldsymbol{x} = 2 x_{1}^{2}+2 x_{2}^{2}+2 x_{3}^{2}-2 x_{1} x_{2}-2 x_{2} x_{3}</script><p>是一四维图像，三个维度<script type="math/tex">x_1，x_2，x_3</script>加函数<script type="math/tex">f</script>，若用<script type="math/tex">f=1</script>切割，则有<script type="math/tex">2 x_{1}^{2}+2 x_{2}^{2}+2 x_{3}^{2}-2 x_{1} x_{2}-2 x_{2} x_{3} = 1</script> 。这是一个椭球体，<font color="green"><strong>三个特征值不同，因此椭球的三个长轴长度不同。三个轴的方向就是特征向量的方向，轴长度就是特征值</strong></font>，矩阵的分解<script type="math/tex">\boldsymbol{A = Q \Lambda Q}^{\mathrm T}</script>很好的说明了这件事，这就是所谓的“主轴定理”。 </p>
<h4 id="27-2-小说明"><a href="#27-2-小说明" class="headerlink" title="27.2 小说明"></a>27.2 小说明</h4><p>对于三条判据可以判定正定：$\boldsymbol x^{\mathrm T} \boldsymbol A \boldsymbol x &gt; 0, (\boldsymbol x 为任意向量，但是\boldsymbol x \neq  0)$。已经分析了判据3主元要大于零的原因，因为它是配方后的参数，只有都大于零才能保证正定。以下对于判据1和2做简要说明。  </p>
<ul>
<li><strong>判据1：所有特征值大于0</strong></li>
</ul>
<p>对称矩阵$\boldsymbol A$，其正交的特征向量可以张成整个空间，因此任意向量$\boldsymbol x$均可表示成特征向量的线性组合</p>
<script type="math/tex; mode=display">
\boldsymbol x = c_1\boldsymbol x_1+c_2\boldsymbol x_2+\dots + c_n\boldsymbol x_n</script><p>代入得：</p>
<script type="math/tex; mode=display">
\boldsymbol x^{\mathrm T} \boldsymbol A \boldsymbol x = c_1^2 \lambda_1 + c_2^2 \lambda_2 + \dots + c_n^2 \lambda_n</script><p>当特征值都大于零且$\boldsymbol x \neq  0)$时，才能保证$\boldsymbol x^{\mathrm T} \boldsymbol A \boldsymbol x &gt; 0$。因此条件1与正定性等价。 </p>
<ul>
<li><strong>判据2：所有子行列式大于0</strong></li>
</ul>
<p>记$\boldsymbol A_k$ 为矩阵$\boldsymbol A$左上角$k$阶方块，取特殊向量<script type="math/tex">x=\left[\begin{array}{l} \boldsymbol x_k \\ \boldsymbol 0 \end{array}\right]</script>即后$n-k$个元素为0，则有：</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{T} \boldsymbol{A} \boldsymbol{x}=\left[\begin{array}{ll}
\boldsymbol x_{k} & \boldsymbol 0
\end{array}\right]\left[\begin{array}{cc}
\boldsymbol A_{k} & * \\
* & *
\end{array}\right]\left[\begin{array}{c}
\boldsymbol x_{k} \\
\boldsymbol 0
\end{array}\right]=\boldsymbol{x}_{k}^{T} \boldsymbol{A}_{k} \boldsymbol{x}_{k}</script><p>若矩阵$\boldsymbol A$满足正定性，则所有$\boldsymbol A_k$均满足正定性。已证明正定性等价于特征值均为正，而矩阵行列式等于特征值之积，因此可知子行列式均大于零。反之亦成立，两命题等价。 </p>
<h3 id="第二十八讲-相似矩阵和若尔当标准型"><a href="#第二十八讲-相似矩阵和若尔当标准型" class="headerlink" title="第二十八讲 相似矩阵和若尔当标准型"></a>第二十八讲 相似矩阵和若尔当标准型</h3><p>本讲介绍相似矩阵，这些内容以及奇异值分解是线性代数最核心的概念。 </p>
<blockquote>
<p>Strang 老师说了, Jordan标准型不再是线性代数的核心，现代线性代数尤其是AI也不怎么用到他，所以此节略去Jordan型的内容。这一讲重点讲解相似矩阵 </p>
</blockquote>
<h4 id="28-1-正定矩阵-boldsymbol-A-mathrm-T-boldsymbol-A"><a href="#28-1-正定矩阵-boldsymbol-A-mathrm-T-boldsymbol-A" class="headerlink" title="28.1 正定矩阵 $\boldsymbol A^{\mathrm T} \boldsymbol A$"></a>28.1 正定矩阵 $\boldsymbol A^{\mathrm T} \boldsymbol A$</h4><p>若矩阵$\boldsymbol A$满足对任意向量$\boldsymbol x_0$均有$\boldsymbol x^{\mathrm T} \boldsymbol A \boldsymbol x &gt; 0$，则称矩阵为正定矩阵，可以通过特征值、主元和行列式的办法来判断矩阵的正定性。</p>
<p><strong>正定矩阵来自于最小二乘问题</strong>。有大量的实际问题用到了长方形矩阵，而最小二乘问题中用到了长方形矩阵的积$\boldsymbol A^{\mathrm T} \boldsymbol A$，它是正定矩阵。</p>
<p>正定矩阵$\boldsymbol A$是对称矩阵，它的逆矩阵$\boldsymbol A^{-1}$也是正定矩阵，逆矩阵的特征值是原矩阵的倒数，因此也都是正数。若矩阵$\boldsymbol A$和$\boldsymbol B$都是正定矩阵，则$\boldsymbol {A+B}$也是正定矩阵：$\boldsymbol x^{\mathrm T} \boldsymbol A \boldsymbol x &gt; 0$，$\boldsymbol x^{\mathrm T} \boldsymbol B \boldsymbol x &gt; 0$，则有$\boldsymbol x^{\mathrm T} (\boldsymbol {A+B}) \boldsymbol x &gt; 0$ 。</p>
<p>如果$\boldsymbol A$是一个$m \times n$长方形矩阵，则$\boldsymbol A^{\mathrm T} \boldsymbol A$是对称方阵。通过讨论$\boldsymbol x^{\mathrm T} (\boldsymbol A^{\mathrm T} \boldsymbol A) \boldsymbol x &gt; 0$的正负可以确认它是正定矩阵：</p>
<script type="math/tex; mode=display">
\boldsymbol x^{\mathrm T} (\boldsymbol A^{\mathrm T} \boldsymbol A) \boldsymbol x = (\boldsymbol A \boldsymbol x)^{\mathrm T} (\boldsymbol A \boldsymbol x) = |\boldsymbol A \boldsymbol x|^2 \geq 0</script><p>当且仅当$\boldsymbol A \boldsymbol x = \boldsymbol 0$时，上面表达式取等号为0。当矩阵$\boldsymbol A$的各列线性无关时，即矩阵为列满秩$r=n$，$\boldsymbol A$的零空间只有零向量，即此条件下仅有零向量，满足$\boldsymbol x^{\mathrm T} (\boldsymbol A^{\mathrm T} \boldsymbol A) \boldsymbol x = 0$。因此矩阵列满秩时，$\boldsymbol A^{\mathrm T} \boldsymbol A$是正定矩阵。正定矩阵将之前的知识点串联起来。</p>
<h4 id="28-2-相似矩阵-Similar-matrices"><a href="#28-2-相似矩阵-Similar-matrices" class="headerlink" title="28.2 相似矩阵 Similar matrices"></a>28.2 相似矩阵 Similar matrices</h4><p>$\boldsymbol A$和$\boldsymbol B$均是$n \times n$方阵，若存在可逆矩阵$\boldsymbol M$，使得$\boldsymbol B = \boldsymbol {M^{-1}AM}$，则$\boldsymbol A$和$\boldsymbol B$为相似矩阵。 </p>
<h5 id="一-特征值互不相同-Distinct-eigenvalues"><a href="#一-特征值互不相同-Distinct-eigenvalues" class="headerlink" title="(一) 特征值互不相同 Distinct eigenvalues"></a>(一) 特征值互不相同 Distinct eigenvalues</h5><p>若矩阵$\boldsymbol A$具有$n$个线性无关的特征向量，可以对角化得到$\boldsymbol {S^{-1}AS} = \boldsymbol \Lambda$，则$\boldsymbol A$相似于$\boldsymbol \Lambda$，这里的$\boldsymbol M$是特征向量矩阵$\boldsymbol S$。如果将$\boldsymbol M$取其它可逆矩阵，可以得到和$\boldsymbol A$相似的另一矩阵$\boldsymbol B$，实际上这样可以定义一类矩阵，$\boldsymbol \Lambda$是其中最简洁的一个。 </p>
<p>例： <script type="math/tex">\boldsymbol A=\left[\begin{array}{ll} 2 & 1 \\ 1 & 2\end{array}\right]</script>，则<script type="math/tex">\boldsymbol \Lambda = \left[\begin{array}{ll} 3 & 0 \\ 0 & 1\end{array}\right]</script>，而取另一$\boldsymbol M$，则有：</p>
<script type="math/tex; mode=display">
\boldsymbol B = \boldsymbol {M^{-1}AM} =
\left[\begin{array}{cc}
1 & -4 \\
0 & 1
\end{array}\right]\left[\begin{array}{ll}
2 & 1 \\
1 & 2
\end{array}\right]\left[\begin{array}{ll}
1 & 4 \\
0 & 1
\end{array}\right]=\left[\begin{array}{cc}
-2 & -15 \\
1 & 6
\end{array}\right]</script><p>相似矩阵最重要的特性是：<font color="red"><strong>相似矩阵具有相同的特征值</strong></font>。事实上，所有特征值为3和1的二阶矩阵都是$\boldsymbol A$的相似矩阵。</p>
<p><strong>证明</strong>：矩阵$\boldsymbol A$的相似矩阵$\boldsymbol B = \boldsymbol {M^{-1}AM}$具有和矩阵$\boldsymbol A$相同的特征值：</p>
<p>矩阵$\boldsymbol A$具有的特征值$\lambda$ ，即存在特征向量$\boldsymbol x$满足$\boldsymbol {Ax} = \lambda \boldsymbol x$ 。则有： </p>
<script type="math/tex; mode=display">
\boldsymbol{A M M}^{-1} \boldsymbol{x} =\lambda \boldsymbol{x} \\
\Downarrow  \\
\boldsymbol M^{-1}\boldsymbol{A M M}^{-1} \boldsymbol{x} =\lambda \boldsymbol M^{-1} \boldsymbol{x} \\
\Downarrow \\
\boldsymbol{B} (\boldsymbol M^{-1} \boldsymbol{x}) =\lambda (\boldsymbol M^{-1} \boldsymbol{x})</script><p>即矩阵$\boldsymbol B$具有特征值$\lambda$，且特征向量为$\boldsymbol M^{-1} \boldsymbol{x}$。</p>
<p>因此，相似矩阵具有相同的特征值，并且线性无关的特征向量的个数相同，但是特征向量往往不同。如果矩阵$\boldsymbol A$的特征值互不相等$\lambda_1 \neq \lambda_2 \neq \dots \neq \lambda_n$，且与另一个矩阵$\boldsymbol B$的特征值完全相同，则它们与相同的对角矩阵$\boldsymbol \Lambda$相似。</p>
<h5 id="二-重特征值-Repeated-eigenvalues"><a href="#二-重特征值-Repeated-eigenvalues" class="headerlink" title="(二) 重特征值 Repeated eigenvalues"></a>(二) 重特征值 Repeated eigenvalues</h5><p> 如果矩阵有重特征值，则可能无法进行对角化。 </p>
<p>例：二阶矩阵有重特征值$\lambda_1 = \lambda_2 = 4$</p>
<ul>
<li>第一类： 只与自己相似——<script type="math/tex">\left[\begin{array}{cc} 4 & 0 \\ 0 & 4\end{array}\right]</script></li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol{M}^{-1}\left[\begin{array}{ll}
4 & 0 \\
0 & 4
\end{array}\right] \boldsymbol{M}=4 \boldsymbol{M}^{-1} \boldsymbol{I} \boldsymbol{M}=\left[\begin{array}{ll}
4 & 0 \\
0 & 4
\end{array}\right]</script><p>这个系列的相似矩阵仅包含其自身。 </p>
<ul>
<li>第二类：包含其它所有的重特征值为4的矩阵，其中最简洁的是 —— <script type="math/tex">\left[\begin{array}{cc} 4 & 1 \\ 0 & 4\end{array}\right]</script></li>
</ul>
<p>元素1的位置换上其它数值仍然是相似矩阵。这个最优形式称为<strong>若尔当(Jordan form)标准型</strong>。有了这个理论，就可以处理不可对角化的矩阵，完成近似的“对角化”转化为若尔当标准型进行处理。 </p>
<p>与<script type="math/tex">\left[\begin{array}{cc} 4 & 1 \\ 0 & 4\end{array}\right]</script>相似的矩阵，迹为8，行列式为16，因此我们可以构造出很多相似矩阵：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{cc} 5 & 1 \\ -1 & 3\end{array}\right], \left[\begin{array}{cc} 4 & 0 \\ 17 & 4\end{array}\right], \left[\begin{array}{cc} a & * \\ * & 8-a\end{array}\right]</script><p>它们都不能对角化(因为若可以对角化则按照特征值可知结果为$4 \boldsymbol I$，而只$4 \boldsymbol I$与自己相似）。 </p>
<h5 id="三-若尔当标准型-Jordan-form"><a href="#三-若尔当标准型-Jordan-form" class="headerlink" title="(三) 若尔当标准型 Jordan form"></a>(三) 若尔当标准型 Jordan form</h5><p>更复杂的情况，一个四阶矩阵具有重特征值0：$\lambda_1 = \lambda_2 = \lambda_3 = \lambda_4 = 0$。如下面的例子所示。</p>
<p>矩阵<script type="math/tex">\boldsymbol A=\left(\begin{array}{cccc} 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{array}\right)</script>，它的秩为2，因此其零空间的维数为$4-2=2$，而零空间的向量就是矩阵的特征向量，满足$\boldsymbol{Ax} = 0\boldsymbol{x} $，所以矩阵$\boldsymbol{A}$只有两个特征向量。若尔当指出上对角线每增加一个1，矩阵就减掉一个特征向量，本例中特征向量数为$4-2=2$。 </p>
<p> 矩阵<script type="math/tex">\boldsymbol B=\left(\begin{array}{cccc} 0 & 1 & 7 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{array}\right)</script>与矩阵<script type="math/tex">\boldsymbol A=\left(\begin{array}{cccc} 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{array}\right)</script>为相似矩阵。 </p>
<p>但矩阵<script type="math/tex">\boldsymbol C=\left(\begin{array}{cccc} 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0\end{array}\right)</script>与矩阵<script type="math/tex">\boldsymbol A=\left(\begin{array}{cccc} 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0\end{array}\right)</script>并不是相似矩阵，两者具有不同的若尔当块。 </p>
<p>若尔当块形如：</p>
<script type="math/tex; mode=display">
\boldsymbol J_i=\left[\begin{array}{ccccc}
\lambda_{i} & 1 & 0 & \cdots & 0 \\
0 & \lambda_{i} & 1 & \ddots & \vdots \\
0 & 0 & \ddots & \ddots & 0 \\
\vdots & \vdots & \ddots & \ddots & 1 \\
0 & 0 & \cdots & 0 & \lambda_{i}
\end{array}\right]</script><p>对角线上为重特征值$\lambda_i$，上对角线为1，其它位置的元素均为0，每个若尔当块只有1个特征向量。若干个若尔当块可以拼成一个若尔当矩阵。</p>
<script type="math/tex; mode=display">
\boldsymbol{J}=\left[\begin{array}{cccc}
\boldsymbol{J}_{1} & 0 & \cdots & 0 \\
0 & \boldsymbol{J}_{2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \boldsymbol{J}_{d}
\end{array}\right]</script><p>两个矩阵具有相同的特征值和特征向量个数，但是其若尔当块的尺寸不同，两者也并不是相似矩阵。如前述矩阵$\boldsymbol A$与并$\boldsymbol C$不相似。</p>
<p>若尔当理论：任意$n$阶矩阵$\boldsymbol A$都与一个若尔当矩阵$\boldsymbol J$相似。若尔当矩阵中的每一个若尔当块对应一个特征向量。若矩阵具有$n$个不同的特征向量，则可以对角化，此时其若尔当标准型$\boldsymbol J$就是对角矩阵$\boldsymbol \Lambda$。若出现重特征值，则特征向量个数变少。</p>
<blockquote>
<p>说到了$\boldsymbol A^{\mathrm T} \boldsymbol A$和最小二乘问题就要解释一下G.Strang举的曲线拟合的例子，都是线性公式$y=ax+b$，但实际上最小二乘法也处理非线性方程，因为这里所谓的非线性是对$x$而言，而只要对于所求的参数是线性方程就可以。比如下面的例子中$x$的方幂组成的矩阵$\boldsymbol X$只是一个系数矩阵，对于所求的参数$β$这仍是个线性方程组。</p>
</blockquote>
<p><img src="https://pic.imgdb.cn/item/627378b509475431296c80fa.jpg"></p>
<p><img src="https://pic.imgdb.cn/item/627378c809475431296ca597.jpg"></p>
<ul>
<li><strong>相似矩阵拥有相同的特征值，但是特向量一般不相等(如果特征向量也相等那估计就是同一个矩阵了)</strong></li>
<li><strong>如果两个矩阵拥有n个相同的特征值，那么他们必然相似于某个对角矩阵。</strong></li>
</ul>
<h3 id="第二十九讲-奇异值分解SVD"><a href="#第二十九讲-奇异值分解SVD" class="headerlink" title="第二十九讲 奇异值分解SVD"></a>第二十九讲 奇异值分解SVD</h3><p><img src="https://pic.imgdb.cn/item/6279aad50947543129d3b40d.jpg" style="zoom:70%"></p>
<h4 id="29-1-简介"><a href="#29-1-简介" class="headerlink" title="29.1 简介"></a>29.1 简介</h4><p>本讲介绍奇异值分解（Singular value decomposition），简称SVD。这是矩阵最终也是最好的分解，任意矩阵可分解为$\boldsymbol {A = U Σ V}^{\mathrm T}$，分解结果为正交矩阵$\boldsymbol U$，对角阵$\boldsymbol Σ$和正交矩阵$\boldsymbol V$ 。</p>
<p>如果矩阵$\boldsymbol A$是正定矩阵，它的奇异值分解就是$\boldsymbol {A = Q \Lambda Q}^{\mathrm T}$，一个正交矩阵$\boldsymbol Q$就可以满足分解，而不需要两个。而对于可对角化的矩阵有$\boldsymbol {A = S \Lambda S}^{-1}$，但特征向量矩阵$\boldsymbol S$并不是正交矩阵，而SVD中的$\boldsymbol U$和$\boldsymbol V$都是正交矩阵。</p>
<h4 id="29-2-对矩阵进行SVD分解"><a href="#29-2-对矩阵进行SVD分解" class="headerlink" title="29.2 对矩阵进行SVD分解"></a>29.2 对矩阵进行SVD分解</h4><p>可将矩阵$\boldsymbol A$视为一种线性变换操作，将其行空间的一个向量$\boldsymbol v_1$,变为其列空间中的向量$\boldsymbol u_1 = \boldsymbol{Av}_1$ 。奇异值分解就是要在行空间中寻找一组正交基，将其通过矩阵$\boldsymbol A$线性变换生成列空间中的一组正交基$\boldsymbol{Av}_i = \sigma_i \boldsymbol u_i$。 </p>
<p><img src="https://pic.imgdb.cn/item/6279ad7e0947543129d7d307.jpg" style="zoom:70%"></p>
<p>找出矩阵$\boldsymbol A$行空间中的正交基很容易，Gram-Schmidt正交化过程就可以做到，但是随便的一组正交基经过矩阵矩阵$\boldsymbol A$变换得到的向量并不一定正交，因此满足此要求的行空间的正交基非常特殊。而矩阵$\boldsymbol A$零空间的向量所对应的是矩阵$\boldsymbol Σ$对角线上的0元素，因此很容易处理。 </p>
<ul>
<li>问题的核心就是找到行空间中一组特殊的正交基： </li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{A}\left[\begin{array}{llll}
\boldsymbol{v}_{1} & \boldsymbol{v}_{2} & \cdots & \boldsymbol{v}_{r}
\end{array}\right] &=\left[\begin{array}{lllll}
\sigma_{1} \boldsymbol{u}_{1} & \sigma_{2} \boldsymbol{u}_{2} & \cdots & \sigma_{r} \boldsymbol{u}_{r}
\end{array}\right] \\
&=\left[\begin{array}{lllll}
\boldsymbol{u}_{1} & \boldsymbol{u}_{2} & \cdots & \boldsymbol{u}_{r}
\end{array}\right]\left[\begin{array}{ccc}
\sigma_{1} & & \\
& \sigma_{2} & & \\
& & \ddots & \\
& & & \sigma_{r}
\end{array}\right]
\end{aligned}</script><p>如果加入零空间的部分，等式就变为$\boldsymbol {AV = U Σ}$，其中<script type="math/tex">\boldsymbol V = \boldsymbol V_行 + \boldsymbol V_零</script>。</p>
<ul>
<li><p>选取的行空间的一组基向量为：<script type="math/tex">\boldsymbol V_行 = \boldsymbol v_1, \boldsymbol v_2,  \cdots  ,\boldsymbol{v}_{r}</script></p>
</li>
<li><p>零空间对应的正交基<script type="math/tex">\boldsymbol V_零 = \boldsymbol v_{r+1}, \dots ,\boldsymbol v_{n}</script>，经过线性变换得到<script type="math/tex">\boldsymbol {AV_零 = 0}</script>，对应矩阵$\boldsymbol Σ$中对角线最后的特征值<script type="math/tex">\sigma_{r+1} = \sigma_{r+2} = \dots = \sigma_{n} = 0</script>。</p>
</li>
</ul>
<p>在等式<script type="math/tex">\boldsymbol {AV = U Σ}</script>两侧右乘<script type="math/tex">\boldsymbol V^{-1}</script>得到<script type="math/tex">\boldsymbol{A = U Σ V}^{-1} = \boldsymbol{U Σ V}^{\mathrm T}</script>。</p>
<p>现在的问题就是怎么找到符合要求的向量$\boldsymbol v_i, \boldsymbol u_i$。</p>
<p>为了得到这两个正交矩阵，考虑首先解决其中的一个，在等式$\boldsymbol {A = U Σ V}^{\mathrm T}$两侧分别乘上等式$\boldsymbol A^{\mathrm T} = \boldsymbol{VΣ}^{\mathrm T}\boldsymbol{U}^{\mathrm T}$ 两侧的项：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{A}^{T} \boldsymbol{A} &=\boldsymbol{V} \boldsymbol{\Sigma}^{T} \boldsymbol{U}^{T} \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{T} \\
&=\boldsymbol{V} \boldsymbol{\Sigma}^{T} \boldsymbol{\Sigma} \boldsymbol{V}^{T} \\
&=\boldsymbol{V}\left[\begin{array}{llll}
\sigma_{1}{ }^{2} & & & \\
& \sigma_{2}{ }^{2} & & \\
& & \ddots & \\
& & & \sigma_{r}{ }^{2}
\end{array}\right] \boldsymbol{V}^{T}
\end{aligned}</script><p>注意到$\boldsymbol {A}^{\mathrm T} \boldsymbol {A}$是一个对称方阵，并且符合正定矩阵正交分解的形式，上式其实就是正定矩阵$\boldsymbol {A}^{\mathrm T} \boldsymbol {A}$的正交分解，$\boldsymbol v_i$就是矩阵$\boldsymbol {A}^{\mathrm T} \boldsymbol {A}$的特征向量，$\sigma_i^2$就是矩阵$\boldsymbol {A}^{\mathrm T} \boldsymbol {A}$的特征值，奇异值$\sigma_i$要取正平方根。用同样的办法也可以求得$\boldsymbol U$，它的列向量就是矩阵$\boldsymbol {A} \boldsymbol {A}^{\mathrm T}$的的特征向量。 </p>
<blockquote>
<p>注意：上述求$\boldsymbol V$或者$\boldsymbol U$即——两侧分别乘上等式$\boldsymbol A^{\mathrm T} = \boldsymbol{VΣ}^{\mathrm T}\boldsymbol{U}^{\mathrm T}$ 的方法只能用于$\boldsymbol V$或者$\boldsymbol U$求解某一个，不能将$\boldsymbol V$和$\boldsymbol U$都用这种方法求解，这是因为确定特征向量的过程中，特征向量反向仍然符合要求，通过现在的方法无法确认向量的符号，但是一旦我们确认的$\boldsymbol V$方向之后，$\boldsymbol U$的方向也就随之确定，将$\boldsymbol V$代入<script type="math/tex">\boldsymbol {AV = U Σ}</script>计算$\boldsymbol U$可以避免这种问题。$\boldsymbol V$和$\boldsymbol U$之间的符号联系在进行$\boldsymbol {A}^{\mathrm T} \boldsymbol {A}$的计算时被切断了，而用<script type="math/tex">\boldsymbol {AV = U Σ}</script>计算可以避免此问题。 </p>
<p>我们注意到，如果$\boldsymbol U$的一个向量反向，$\boldsymbol U$中的两个向量仍然是正交向量，且此时可以完成奇异值分解，这是因为在奇异值分解的过程中，一旦确认了$\boldsymbol V$中向量的方向之后，$\boldsymbol U$中向量的方向也一起被限定了，而在本例中，我们用一种和$\boldsymbol V$无关的方式求得了$\boldsymbol U$，而这种方法并不一定总是有效，需要尝试改变向量的方向。通过将$\boldsymbol V$代入<script type="math/tex">\boldsymbol {AV = U Σ}</script>求得$\boldsymbol U$，可以避免这个问题。  </p>
</blockquote>
<h4 id="29-3-小结语"><a href="#29-3-小结语" class="headerlink" title="29.3 小结语"></a>29.3 小结语</h4><p>做奇异值分解就是在矩阵的四个子空间中寻找到合适的基：</p>
<ul>
<li>$\boldsymbol v_1, \boldsymbol v_2, \dots, \boldsymbol v_r$为行空间的标准正交基</li>
<li>$\boldsymbol u_1, \boldsymbol u_2, \dots, \boldsymbol u_r$为列空间的标准正交基</li>
<li><script type="math/tex">\boldsymbol v_{r+1}, \boldsymbol v_{r+2}, \dots, \boldsymbol v_n</script>为零空间的标准正交基</li>
<li><script type="math/tex">\boldsymbol u_{r+1}, \boldsymbol u_{r+2}, \dots, \boldsymbol u_m</script>为左零空间的标准正交基</li>
</ul>
<blockquote>
<p>奇异值分解在最小二乘法问题中有重要应用，因为在实际问题中常碰到矩阵$\boldsymbol A$不是列满秩的状态，因此$\boldsymbol {A}^{\mathrm T} \boldsymbol {A}$不可逆，无法用之前的方法求最优解。即使是列满秩的情况当矩阵是超大型矩阵时，$\boldsymbol {A}^{\mathrm T} \boldsymbol {A}$的计算量太大，用奇异值分解的办法会降低计算量。</p>
</blockquote>
<p><img src="https://pic.imgdb.cn/item/6279b8280947543129f1ef20.jpg"></p>
<blockquote>
<p>图为G.Strang给出的二阶方阵SVD的几何意义</p>
<p>关于该图的解释、奇异阵的意义、以及奇异阵为什么能丢掉请看一下链接：</p>
<p>【SVD的数学涵义及其应用实例 - 钱默吟的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/57803955">https://zhuanlan.zhihu.com/p/57803955</a> 】</p>
<p>【奇异值的物理意义是什么？ - 知乎 <a href="https://www.zhihu.com/question/22237507">https://www.zhihu.com/question/22237507</a> 】</p>
<p>【为什么PCA中奇异值分解（SVD）和特征分解可以降低数据维度 <a href="https://blog.csdn.net/qq_41987033/article/details/81211574】">https://blog.csdn.net/qq_41987033/article/details/81211574】</a></p>
</blockquote>
<h3 id="第三十讲-线性变换及对应矩阵"><a href="#第三十讲-线性变换及对应矩阵" class="headerlink" title="第三十讲 线性变换及对应矩阵"></a>第三十讲 线性变换及对应矩阵</h3><p>本讲介绍线性变换。在传统的线代课程中，线性变换会出现在的比矩阵还要早的位置，实际上可以在脱离坐标和具体数值的情况下讨论线性变换。但是面对科学计算的课题，还是要回到坐标上来。每个线性变换都对应于一个矩阵，矩阵概念的背后就是线性变换的概念。 </p>
<blockquote>
<p>无论是“线性变换”还是“矩阵”，对于初学者来说都是陌生而且突兀的概念。矩阵看起来直观，但是其运算规则总让人摸不着头脑；线性变换的概念显得抽象一些，但其实也可以描述得很朴素。G.Strang是从矩阵的概念出发，往求解线性方程组的方向引导大家，他最成功的地方是引入了列图像以及矩阵乘法的行操作、列操作,将矩阵运算变得不那么“没头没脑”。而之前推荐过的《线性代数应该这样学》是从线性变换的角度出发，用朴素的描述在更高的抽象层面讨论，很多原理与法则就变得比较简洁。个人认为两者都优于从行列式出发进入线性代数的路线。在《理解矩阵》中，作者写到矩阵的背后就是线性变换，相似矩阵描述的实际上是同一种线性变换。G.Strang说理解线性变换的本质就是找到它背后的矩阵。这实际上是一个意思，只是出发点和看问题的角度不同。当具体的矩阵和抽象的线性变换在大脑中合二为一的时候，才是对线代最本质的理解。 线性变换和矩阵的关系还可以参考以下课程，在2D空间中这个关系更容易描述： </p>
</blockquote>
<h4 id="30-1-几个小例子"><a href="#30-1-几个小例子" class="headerlink" title="30.1 几个小例子"></a>30.1 几个小例子</h4><ul>
<li><strong>正例1：投影 Projection</strong></li>
</ul>
<p>抛开矩阵，从线性变换的概念来讨论“投影”。通过线性变换使得平面内的一个向量变为平面内的另一个向量，这种变换关系通常称之为“映射”（mapping）。</p>
<p>$T：R^2→R^2$</p>
<p>$T(\boldsymbol v)$就像一个函数，给一个”输入”，进行线性变换后得到一个”输出”。比如将二维平面的向量投影到一条直线上。</p>
<p>线性代数只讨论线性变换，而线性变换符合如下规则：</p>
<p>对于任意向量$\boldsymbol v，\boldsymbol w$和标量$c$，有</p>
<script type="math/tex; mode=display">
T(\boldsymbol{v+w})=T(\boldsymbol v)+T(\boldsymbol w); T(c\boldsymbol v)=cT(\boldsymbol v)</script><p>将两者结合就得到：</p>
<script type="math/tex; mode=display">
T(c\boldsymbol v+d\boldsymbol w)=cT(\boldsymbol v)+dT(\boldsymbol w)</script><ul>
<li><strong>正例2：旋转45度 Rotation by 45</strong></li>
</ul>
<p>这个变换$T：R^2→R^2$，是一个线性变换，数乘和加法均符合。</p>
<ul>
<li><strong>反例1：平面平移 Shift whole plane</strong></li>
</ul>
<p>沿着某方向$\boldsymbol v_0$平移一个平面。这就不是一个线性变换。两条法则都违反。更简单的验证方法是线性运算规则的特例$T(\boldsymbol 0) = \boldsymbol 0$，“平移”不符合这个规则特例。</p>
<ul>
<li><strong>反例2：求长度$T(\boldsymbol v) = ||\boldsymbol v||$</strong></li>
</ul>
<p>这个变换，输入一个三维向量，得到一个数值(或者说一维向量)，即$T：R^3→R^1$。该变换可以满足$T(\boldsymbol 0) = \boldsymbol 0$，但是数乘负数就不符合规则。</p>
<h4 id="30-2-线性变换"><a href="#30-2-线性变换" class="headerlink" title="30.2 线性变换"></a>30.2 线性变换</h4><p>对二维平面的图像做线性变换的操作示意图。 </p>
<p><img src="https://pic.imgdb.cn/item/627a09500947543129e7ee9f.jpg"></p>
<p>矩阵$T(\boldsymbol v) = \boldsymbol{Av}$，矩阵乘法是一个线性变换：</p>
<script type="math/tex; mode=display">
\boldsymbol {A(v+w)} = \boldsymbol {Av} + \boldsymbol {Aw}; \boldsymbol A c \boldsymbol v = c \boldsymbol {Av}</script><p>整个平面可以通过矩阵的乘法完成变换。</p>
<p>例如，对上图的房子图像施加矩阵<script type="math/tex">\boldsymbol A=\left(\begin{array}{cc} 1 & 0 \\ 0 & -1\end{array}\right)</script> ，则输出的是上下颠倒的房子。</p>
<p><strong><font color="red">理解线性变换的本质就是确定它背后的矩阵</font></strong>。</p>
<p><strong>例：</strong>对某一线性变换$T：R^3→R^2$，输入一个三维向量而输出是一个二维向量。变成矩阵的形式$T(\boldsymbol v) = \boldsymbol{Av}$，则矩阵$\boldsymbol A$是一个$2 \times 3$矩阵。</p>
<h4 id="30-3-描述线性变换-Describing-T-v"><a href="#30-3-描述线性变换-Describing-T-v" class="headerlink" title="30.3 描述线性变换 Describing T(v)"></a>30.3 描述线性变换 Describing T<strong>(</strong>v)</h4><p>在平面内，如果我们已经了解两个线性无关的向量$\boldsymbol v_1$和$\boldsymbol v_2$经过线性变换的结果$T(\boldsymbol v_1)$和$T(\boldsymbol v_1)$，我们实际上可以通过其线性组合，了解平面内所有的向量线性变换的结果。因此如果我们想了解线性变换对整个输入空间的影响，只需要确定它的一组基$\boldsymbol v_1, \boldsymbol v_2, \dots, \boldsymbol v_n$线性变换的结果。</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{v}=c_{1} \boldsymbol{v}_{1}+c_{2} \boldsymbol{v}_{2}+\ldots+c_{n} \boldsymbol{v}_{n} \\
&\boldsymbol{T}(\boldsymbol{v})=c_{1} \boldsymbol{T}\left(\boldsymbol{v}_{1}\right)+c_{2} \boldsymbol{T}\left(\boldsymbol{v}_{2}\right)+\ldots+c_{n} \boldsymbol{T}\left(\boldsymbol{v}_{n}\right)
\end{aligned}</script><p><strong><font color="green">线性变换与坐标无关，而矩阵是与坐标有关的</font></strong>。选定一组基，则对于一个向量而言$c_1, c_2$等等就是一组坐标值，给定了将向量表示为基向量线性组合的唯一的表达式。因此可以说坐标源自于一组基，$c_1, c_2, \dots, c_n$就是向量的一组坐标值。通常给出空间的坐标是标准坐标，即一组标准基。例如：</p>
<script type="math/tex; mode=display">
\boldsymbol{v}=\left[\begin{array}{l}
3 \\
2 \\
4
\end{array}\right]=3\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right]+2\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right]+4\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]</script><p>如果用矩阵$\boldsymbol A$来表示线性变换$T：R^n→R^m$。则需要两组基，即输入空间的一组基和输出空间的一组基，来分别确认输入向量与输出向量的坐标。设$\boldsymbol v_1，\boldsymbol v_2, \dots ,\boldsymbol v_n$为输入空间的基，而为输出$\boldsymbol w_1，\boldsymbol w_2, \dots ,\boldsymbol w_n$空间的基，用矩阵来表示线性变换就是将向量的坐标乘以矩阵得到它在输出空间的坐标。</p>
<ul>
<li><strong>例：投影</strong></li>
</ul>
<p>将二维平面内的向量投影到一条直线，选择输入空间基向量为$\boldsymbol v_1$和$\boldsymbol v_2$，其中$\boldsymbol v_1$沿着投影方向，$\boldsymbol v_2$垂直于投影方向，输出空间基选择$\boldsymbol w_1 = \boldsymbol v_1, \boldsymbol w_2 = \boldsymbol v_2$。对于输入空间中任意向量$\boldsymbol v$有$\boldsymbol v = c_1 \boldsymbol v_1 + c_2 \boldsymbol v_2$，输出为$T(\boldsymbol v) = c_1 \boldsymbol v_1$。 </p>
<p>因此这个线性变换的矩阵就是<script type="math/tex">\boldsymbol A=\left(\begin{array}{cc} 1 & 0 \\ 0 & 0\end{array}\right)</script>，输入<script type="math/tex">\left[\begin{array}{l} c_1 \\ c_2\end{array}\right]</script>得到<script type="math/tex">\left[\begin{array}{cc} 4 & 0 \\ 0 & 4\end{array}\right] \left[\begin{array}{l} c_1 \\ c_2\end{array}\right] = \left[\begin{array}{l} c_1 \\ 0 \end{array}\right]</script>。这组例子中实际上我们以投影矩阵的特征向量为基，因此得到的矩阵$\boldsymbol A$是对角阵。对角线上就是投影矩阵的特征值1和0。</p>
<p>而如果我们以标准坐标为基，即<script type="math/tex">\boldsymbol w_1 = \boldsymbol v_1 = \left[\begin{array}{l} 1 \\ 0\end{array}\right]</script>，<script type="math/tex">\boldsymbol w_2 = \boldsymbol v_2 = \left[\begin{array}{l} 0 \\ 1\end{array}\right]</script> 。对应的投影矩阵就是之前学到的投影矩阵$\boldsymbol P = \dfrac{\boldsymbol {aa}^{\mathrm T}}{\boldsymbol a^{\mathrm T} \boldsymbol a}$对于投影到斜率45度的直线，<script type="math/tex">\boldsymbol P=\left(\begin{array}{cc} \dfrac{1}{2} & \dfrac{1}{2} \\ \dfrac{1}{2} & \dfrac{1}{2}\end{array}\right)</script></p>
<h4 id="30-4-如何确定矩阵A-Rule-to-find-A"><a href="#30-4-如何确定矩阵A-Rule-to-find-A" class="headerlink" title="30.4 如何确定矩阵A Rule to find A"></a>30.4 如何确定矩阵A Rule to find A</h4><p>矩阵$\boldsymbol A$的列实际上是描述输入原空间的基向量得到的列空间线性组合的系数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{T}\left(\boldsymbol{v}_{1}\right)=a_{11} \boldsymbol{w}_{1}+a_{21} \boldsymbol{w}_{2}+\ldots+a_{m 1} \boldsymbol{w}_{m} \\
&\boldsymbol{T}\left(\boldsymbol{v}_{2}\right)=a_{12} \boldsymbol{w}_{1}+a_{22} \boldsymbol{w}_{2}+\ldots+a_{m 2} \boldsymbol{w}_{m} \\
& \dots
\end{aligned}</script><p>这样矩阵$\boldsymbol A$就满足“$\boldsymbol A [输入向量的坐标]=[输出空间的坐标]$”。</p>
<p>例如：<script type="math/tex">\boldsymbol{A}\left[\begin{array}{c} 1 \\ 0 \\ \vdots \\ 0 \end{array}\right]=\left[\begin{array}{c}a_{11} \\ a_{21} \\ \vdots \\ a_{m 1} \end{array}\right]</script>，所得结果就是输出空间的坐标，也就是输出空间的基进行线性组合所需要的系数。</p>
<ul>
<li>介绍一个特别的线性变换——求导，$T = \dfrac{\mathrm{d}}{\mathrm d x}$<ul>
<li>输入：$c_1 + c_2 x + c_3 x^2$ ，基：$1, x, x^2$</li>
<li>输出：$c_2 + 2c_3 x$ ，基：$1, x$</li>
</ul>
</li>
</ul>
<p>这是一个$T:R^3 \to R^2$的线性变换。</p>
<p>矩阵$\boldsymbol A$满足：<script type="math/tex">\boldsymbol{A}\left[\begin{array}{c} c_1 \\ c_2 \\ c_3 \end{array}\right]=\left[\begin{array}{c} c_2 \\ 2c_3 \end{array}\right]</script>，可求得矩阵<script type="math/tex">\boldsymbol A=\left(\begin{array}{ccc} 0& 1 & 0 \\ 0 & 0 & 2\end{array}\right)</script> 。</p>
<p><strong>更普遍的来讲，矩阵的逆矩阵就是线性变换的逆变换，矩阵的乘积就是线性变换的乘积，矩阵乘法源自于线性变换。</strong></p>
<h3 id="第三十一讲-基变换和图像压缩"><a href="#第三十一讲-基变换和图像压缩" class="headerlink" title="第三十一讲 基变换和图像压缩"></a>第三十一讲 基变换和图像压缩</h3><p>本讲介绍基变换。选择合适的基向量会给计算制造便利。基变换的一个重要应用就是压缩，图像、影像、音频和其它一些数据都会因为基变换而得到更高效的压缩储存。本讲的主题仍旧是线性变换和矩阵的关联。 </p>
<h4 id="31-1-图像压缩-Compression-of-images"><a href="#31-1-图像压缩-Compression-of-images" class="headerlink" title="31.1 图像压缩 Compression of images"></a>31.1 图像压缩 Compression of images</h4><p>本讲涉及的压缩过程是有损压缩。例如一幅像素是$512 \times 512$的静态黑白图像，图像用一个向量来表示，向量的分量xi表示像素的灰度，变化范围$0 \leq x_i \leq 255$，占8bits。该向量属于$R^n$空间，$n=(512) \times (512)$。彩色图像描述每个点的像素需要三个数据，向量长度是黑白图像的3倍。</p>
<p>图像的标准压缩方式为<strong>JPEG</strong>(联合图像专家组 Joint Photographic Experts Group)。<strong>图像压缩的本质就是基变换</strong>。</p>
<p>压缩前图像采用的基向量是标准基。但是在图像中离得很近的区域，颜色是非常接近的，比如教学视频中黑板的一个区域，这些区域像素的灰度值很接近，但是用标准基来存储并没有利用上这一特点，这就给了我们压缩的空间。</p>
<p>标准基就是：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
1 \\
0 \\
\vdots \\
0 \\
0
\end{array}\right],\left[\begin{array}{c}
0 \\
1 \\
\vdots \\
0 \\
0
\end{array}\right], \cdots , \left[\begin{array}{c}
0 \\
0 \\
\vdots \\
1 \\
0
\end{array}\right], \left[\begin{array}{c}
0 \\
0 \\
\vdots \\
0 \\
1
\end{array}\right]</script><p>而显然对于灰度很接近的情况(即一幅图像的灰度基本都是一个值)，<script type="math/tex">\left[\begin{array}{c} 1 \\ 1 \\ \vdots \\ 1 \\1 \end{array}\right]</script>是一个很好的基（代表低频信号，频率为0，平滑。在图像压缩后经常会存在，因为其系数通常很大），在一组基中有一个这样的向量能解决很大的问题，可以处理像素灰度接近一致的情况。但图像不是灰度完全一致的，因此接下来的问题是跟它相配合的基要选择哪些。极端的情况包括选择<script type="math/tex">\left[\begin{array}{c} +1 \\ -1 \\ +1 \\ -1 \\ \vdots \end{array}\right]</script>，它可以给出类似国际象棋盘那种黑白相间的状态（最高频信号，噪音、扰动……在图像压缩后很少存在），还有图像一半图像暗一半图像亮，可以选择<script type="math/tex">\left[\begin{array}{c} +1 \\ \vdots \\ +1 \\ -1 \\ \vdots \\ -1 \end{array}\right]</script>。</p>
<ul>
<li><strong>傅里叶基 Fourier basis</strong></li>
</ul>
<p>在JPEG中，将$512 \times 512$的区域划分为$8 \times 8$的区块进行处理，所用到的基是傅里叶基向量。</p>
<p>以$R^8$中的基举例：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{l}
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1
\end{array}\right],\left[\begin{array}{c}
1 \\
\omega \\
\omega^{2} \\
\omega^{3} \\
\omega^{4} \\
\omega^{5} \\
\omega^{6} \\
\omega^{7}
\end{array}\right],\left[\begin{array}{c}
1 \\
\omega^{2} \\
\omega^{4} \\
\omega^{6} \\
\omega^{8} \\
\omega^{10} \\
\omega^{12} \\
\omega^{14}
\end{array}\right], \cdots, \left[\begin{array}{c}
1 \\
\omega^{7} \\
\omega^{14} \\
\omega^{21} \\
\omega^{28} \\
\omega^{35} \\
\omega^{42} \\
\omega^{49}
\end{array}\right]</script><p><img src="https://pic.imgdb.cn/item/627a1f5c0947543129253f77.jpg"></p>
<p>傅里叶基（Fourier basis）就是之前讲过的傅里叶矩阵的列向量，每个元素为复数的幂。在8x8区块中有64个系数，64个基向量，在这个64维空间中做基变换。</p>
<script type="math/tex; mode=display">
信号\boldsymbol x \stackrel{无损}\Longrightarrow 系数c \stackrel{有损}\Longrightarrow 系数 \hat c(很多0) \Longrightarrow \hat{\boldsymbol x} = \sum \hat c_i \boldsymbol v_i</script><p>首先对输入的信号$\boldsymbol x$，从标准基变换为傅里叶基，得到系数$c$，这一步是无损的过程。之后设置阀值进行压缩，超过阀值的认定为肉眼无法分辨的信号。在数学上表现为某些基向量的系数很小，这部分可以丢弃，随之得到新的系数$\hat c$。将新的系数赋值在傅里叶基上求和得到$\hat{\boldsymbol x} = \sum \hat c_i \boldsymbol v_i$。此时求和项已经不是64项，可能只剩下两三项，这就是压缩。</p>
<p>视频文件可以视为图像的序列，一幅一幅进行图像压缩即可。但这样做没有利用好视频的性质，因为视频是连续的，一幅图像和下一幅图像非常接近，因此可以存储一幅基础图像，随后只存储下一幅图像对它的修正部分。</p>
<ul>
<li><strong>小波 Wavelets</strong></li>
</ul>
<p>下面介绍另一组和傅里叶竞争的基向量——小波。</p>
<p>以$R^8$中的基举例：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{l}
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1 \\
1
\end{array}\right],\left[\begin{array}{c}
1 \\
1 \\
1 \\
1 \\
-1 \\
-1 \\
-1 \\
-1
\end{array}\right],\left[\begin{array}{c}
1 \\
1 \\
-1 \\
-1 \\
0 \\
0 \\
0 \\
0
\end{array}\right],\left[\begin{array}{c}
0 \\
0 \\
0 \\
0 \\
1 \\
1 \\
-1 \\
-1
\end{array}\right],\left[\begin{array}{c}
1 \\
-1 \\
0 \\
0 \\
0 \\
0 \\
0 \\
0
\end{array}\right], \cdots, \left[\begin{array}{c}
0 \\
0 \\
0 \\
0 \\
0 \\
0 \\
1 \\
-1
\end{array}\right]</script><p>这个只是一个小波选择，还有很多更精细的选择，这一组基中有太多从+1跳转到-1的变化。线性代数要做的基变换，就是将标准基下的向量$\boldsymbol p = [p_1, p_2, \cdots, p_8]$表示为小波基的线性组合，求出线性组合的参数$\boldsymbol c$满足$\boldsymbol p = c_1 \boldsymbol w_1 + c_2 \boldsymbol w_2+ \cdots + c_8 \boldsymbol w_8$，即$\boldsymbol{p = Wc}$。$\boldsymbol W$即以小波基向量为列向量的小波矩阵。因此有$\boldsymbol c = \boldsymbol W^{-1} \boldsymbol p$ 。</p>
<p>好的基向量组要求：第一，可以快速求逆矩阵，例如快速傅里叶变换，这里也存在快速小波变换，因为小波矩阵列向量正交，因此可以转置得到逆矩阵；第二，要少量基向量就可以近似信号，可压缩的比例就比较高。</p>
<h4 id="31-2-基变换-Change-of-basis"><a href="#31-2-基变换-Change-of-basis" class="headerlink" title="31.2 基变换 Change of basis"></a>31.2 基变换 Change of basis</h4><p>$\boldsymbol W$的列向量是一组新的基向量。在旧基向量体系下的向量$\boldsymbol x$和新基向量下的$\boldsymbol c$的关系是：$\boldsymbol {x = Wc}$。</p>
<ul>
<li><strong>变换矩阵</strong></li>
</ul>
<p>已知一个线性变换$T：R^8 \to R^8$。当使用空间的一组基时$\boldsymbol v_1，\boldsymbol v_2, \dots ,\boldsymbol v_8$，线性变换对应的矩阵为$\boldsymbol A$；当使用一组新的基$\boldsymbol u_1，\boldsymbol u_2, \dots ,\boldsymbol u_8$时，线性变换对应的矩阵为矩阵$\boldsymbol B$。两个矩阵对应的是同一个线性变换，只是使用了不同的基向量。那么$\boldsymbol A$和$\boldsymbol B$为<strong>相似矩阵</strong>——$\boldsymbol B = \boldsymbol{M^{-1} A M}$，$\boldsymbol M$是基变换矩阵（相似矩阵就是通过不同的基表示同一个向量）。</p>
<p>复习一下线性变换的内容：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{x}=c_{1} \boldsymbol{v}_{1}+c_{2} \boldsymbol{v}_{2}+\ldots+c_{8} \boldsymbol{v}_{8} \\
&\boldsymbol{T}(\boldsymbol{x})=c_{1} \boldsymbol{T}\left(\boldsymbol{v}_{1}\right)+c_{2} \boldsymbol{T}\left(\boldsymbol{v}_{2}\right)+\ldots+c_{8} \boldsymbol{T}\left(\boldsymbol{v}_{8}\right) \\
&\boldsymbol{T}\left(\boldsymbol{v}_{1}\right)=a_{11} \boldsymbol{v}_{1}+a_{21} \boldsymbol{v}_{2}+\ldots+a_{81} \boldsymbol{v}_{8} \\
&\boldsymbol{T}\left(\boldsymbol{v}_{2}\right)=a_{12} \boldsymbol{v}_{1}+a_{22} \boldsymbol{v}_{2}+\ldots+a_{82} \boldsymbol{v}_{8} \\
& \cdots
\end{aligned}</script><p>矩阵$\boldsymbol A$就是$a_{ij}$组成的矩阵。</p>
<p>如果我们使用的基向量就是特征向量$T(\boldsymbol v_I = \lambda_i \boldsymbol v_i)$，矩阵$\boldsymbol A$就变成对角阵$\boldsymbol \Lambda$。找出特征向量是压缩最理想的结果，但是找出图像的特征向量代价太大，因此我们找到代价小但是接近理想状态的基向量（例如小波基）进行基变换，完成压缩过程。</p>
<blockquote>
<p>之前我在线性变换和基变换这两讲后面留了很多具体文字，希望帮助大家理解和区分这些概念，但是有的小伙伴貌似反而被误导，所以我把另一门课程的几何描述放在这里，希望大家看了之后可以完全明白，基变换是在同一个空间中，改变对一个东西的描述方式而已。</p>
</blockquote>
<h4 id="31-3-线性变换与基变换的区别与联系"><a href="#31-3-线性变换与基变换的区别与联系" class="headerlink" title="31.3 线性变换与基变换的区别与联系"></a>31.3 线性变换与基变换的区别与联系</h4><p>我们可以将基变换理解为特殊的线性变换，因为基变换其实是可逆线性变换，也就是说，始终是可逆矩阵。 </p>
<ul>
<li>线性变换：</li>
</ul>
<p>将一个向量变换为另一个向量，是在同一个坐标系(基向量)下的两个不同向量，其中矩阵的列可以看为标准基变换后的基，从这个变换中我们能看出这个矩阵对应什么变换。</p>
<ul>
<li>基变换：</li>
</ul>
<p>基变换在变换前后描述的都是一个向量，它表示一个向量在原来的基下的表示(系数)变换为另一组基下的表示(系数)。其中基变换矩阵的列表示原来的基在新的基下的坐标。</p>
<p>。但是变换前的向量是在原来的基下的表示，变换后的向量的表示在新的基下的表示</p>
<p>线性代数的本质09 基变换 - 三少爷的键的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/110975625">https://zhuanlan.zhihu.com/p/110975625</a> </p>
<h3 id="第三十二讲-第三部分小结"><a href="#第三十二讲-第三部分小结" class="headerlink" title="第三十二讲 第三部分小结"></a>第三十二讲 第三部分小结</h3><p><strong>主要内容</strong></p>
<ul>
<li>特征值和特征向量$\boldsymbol{Ax} = \lambda \boldsymbol x$</li>
<li>微分方程$\dfrac{\mathrm d \boldsymbol u}{\mathrm d t} = \boldsymbol{Au}$和矩阵指数形式$e^{\boldsymbol At}$</li>
<li>对称矩阵$\boldsymbol A = \boldsymbol A^{\mathrm T}$的特征值永远为实数，总有足够的特征向量实现矩阵的对角化$\boldsymbol{A = Q \Lambda Q}^{\mathrm T}$。</li>
<li>正定矩阵</li>
<li>相似矩阵$\boldsymbol B = \boldsymbol M^{-1} \boldsymbol{AM}$，两矩阵具有相同的特征值，但是特征向量不同，矩阵的乘方形式很接近$\boldsymbol B = \boldsymbol M^{-1} \boldsymbol A^k \boldsymbol M$。</li>
<li>奇异值分解SVD$\boldsymbol A = \boldsymbol{U Σ V}^{\mathrm T}$。</li>
</ul>
<p>几个知识点：</p>
<p>矩阵$\boldsymbol A$的特征向量相互正交的条件：</p>
<script type="math/tex; mode=display">
\boldsymbol A \boldsymbol A^{\mathrm T} = \boldsymbol A^{\mathrm T} \boldsymbol A</script><p>对称阵、反对称阵、正交阵都满足上面条件。</p>
<p>投影矩阵的特征值等于0或者1，因为$\boldsymbol P^2 = \boldsymbol P \Longrightarrow \lambda^2 = \lambda$。</p>
<p>例题参考：<a href="https://zhuanlan.zhihu.com/p/46124814">https://zhuanlan.zhihu.com/p/46124814</a></p>
<h3 id="第三十三讲-左右逆和伪逆"><a href="#第三十三讲-左右逆和伪逆" class="headerlink" title="第三十三讲 左右逆和伪逆"></a>第三十三讲 左右逆和伪逆</h3><p>【MIT—线性代数笔记33 左右逆和伪逆 - 三少爷的键的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/46124893">https://zhuanlan.zhihu.com/p/46124893</a> 】</p>
<hr>
<p>参考笔记：<a href="https://zhuanlan.zhihu.com/p/355960334">https://zhuanlan.zhihu.com/p/355960334</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>数学基础</category>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>本质问题</tag>
        <tag>线代</tag>
      </tags>
  </entry>
  <entry>
    <title>我的阅读</title>
    <url>/2023/01/15/MyBook/</url>
    <content><![CDATA[<h3 id="一、《南京大屠杀》-【2023-01-09-】"><a href="#一、《南京大屠杀》-【2023-01-09-】" class="headerlink" title="一、《南京大屠杀》 【2023/01/09 】"></a>一、《南京大屠杀》 【2023/01/09 】</h3><p><a target="_blank" href="https://book.douban.com/subject/26545308/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">南京大屠杀 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://book.douban.com/subject/26545308/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1ET411E7Ln" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i1.hdslb.com/bfs/archive/79b31626509f8d67d7a966e974f89250e238ef05.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">01:43:16</span>
            </div>
            <div class="bvideo-info">
                <p class="title">【完整版】《张纯如 南京大屠杀》中英双语字幕【1080P】</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>44.4万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>2577</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">夏末TnT-纪实影像录</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p><strong>转载书评</strong></p>
<p>The Rape of Nanking ，张纯如女士的这本书虽然存在瑕疵（张纯如不懂得中文和日语，所以她得到的材料既有限又有很多瑕疵。该书中有一百多处比如人名地名时间之类史料上的错误。藤原章等日本左派学者向她提出这些错误，日本右翼也因此攻击她。日本出版商“柏书房”让她修改这些错误，但她拒绝更正，其原因是南京大屠杀已从学术上的讨论变得被政治化了——王选），但是它对中国的南京大屠杀史实研究具有极大的贡献，它们体现在：</p>
<ul>
<li>1、历史研究的态度必须严谨，必须运用现代历史研究的方法和手段，因为历史是记叙给全人类了，应该获得最广泛的认同；</li>
<li>2、历史研究的意义是获得共同记忆，必须还原历史绝大部分的真实面貌，才能使后代获得真实的记忆，而这种记忆就是一个民族或国家赖以延续的根本；</li>
<li>3、书名的深刻含义，rape，直译为“强奸”，意译为“浩劫”“梦魇”“暴行”，更多的是将 the rape of nanking 整译为“南京大屠杀”。</li>
</ul>
<p>我个人理解，张纯如使用“rape”一词，体现了她对南京事件的两个认识：第一，南京事件不是单纯的屠杀，而是涉及到对整个城市和人民的各个层面进行了强制的毁坏，是绝对丰富的犯罪；第二，“强奸”作为人类社会中的一种强制侵犯行为，它的发生是比较隐蔽的，通常只有加害人和被害人才了解整个真相，而往往是双方都会向第三人否认并尽量遗忘此事，所以副标题是“The Forgotten Holocaust of World War II”，指出它是被整个世界遗忘的，与犹太人被屠杀同等重要的人类记忆。</p>
<p>忘记就是第二次被施以暴行，这是张纯如的核心观点，也应该是我们的共识。 　　 </p>
<p>在新千年里，中国人在南京大屠杀事件上至少需要进行三个方面的努力：</p>
<ul>
<li>1、事件本身的深入研究，发掘、收集、考证史实，使对该事件的表述更接近真相。</li>
<li>2、重视日本国内对该事件的研究，系统的整理已出版的学术层面的日文专著、论文，侵华日军日记、回忆录等一手历史资料；以严谨和开放的史学态度，增加与日本研究人员的交流，无论其抱有承认或者否定的观点，中日之间这种以史实证据为基础的学术激辩，将促进更多史料被举证和公开，会有力推动中国的研究；不主观排斥日本的任何观点，并依据中日两国都无争议的史实证据进行逐一的验证和辩驳，从而修正中国方面的史料瑕疵。以求同存异的原则，中日共同出版南京大屠杀史实研究专著，贡献给世界史学界。</li>
<li>3、第三，让世界上更多的人知道南京大屠杀事件，正如世人知道德国人曾对犹太人所做的，他们也应该知道日本人曾对中国人做过什么。这样的人类记忆，才是第二次世界大战的完整历史。</li>
</ul>
<p>纪念南京大屠杀，纪念张纯如。 </p>
<blockquote>
<p>作者：豆瓣用户——<a href="https://www.douban.com/people/1112943/">Sweet M</a>  </p>
<p>原文链接：<a href="https://book.douban.com/review/1258849/">https://book.douban.com/review/1258849/</a></p>
<p>侵删</p>
</blockquote>
<h3 id="二、《尼罗河上的惨案》-【2023-01-09-】"><a href="#二、《尼罗河上的惨案》-【2023-01-09-】" class="headerlink" title="二、《尼罗河上的惨案》 【2023/01/09 】"></a>二、《尼罗河上的惨案》 【2023/01/09 】</h3><p><a target="_blank" href="https://book.douban.com/subject/25697546/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">尼罗河上的惨案 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://book.douban.com/subject/25697546/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1FW411P7ae" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i1.hdslb.com/bfs/archive/499d48432159e1b92b99cd4035d02ff55d81a6df.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:31:23</span>
            </div>
            <div class="bvideo-info">
                <p class="title">【木鱼微剧场】《尼罗河上的惨案》阿加莎克里斯蒂名篇</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>192.5万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>9207</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">木鱼水心</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p><strong>转载书评</strong></p>
<p>看阿加莎总会觉得不太过瘾，一是人物多而杂，有点晕晕乎乎，直到后面才慢慢搞清楚谁是谁，这时候书的一半篇幅已经过去了；二是波洛总是走后门，能掌握读者不知道的许多幕后信息，灵机一动就了然于胸，而你就算每个字都用放大镜来照，前前后后地读上他好几遍，想破了脑袋也找不到破案的线索，这种感觉就像看柯南一样，不好，非常不好。每次闪电劈过柯南的时候，我总是还一愣一愣的，这小子咋又想通了捏。。难道人与人的智商差距真的那么大么。。不能不说是一种折磨。看《尼罗河》是这样，看《无人生还》也是这样，后者的结局更是牵强。就是《罗杰疑案》让人称道，结局真正出乎意料之外又合情合理，你想不出来是因为没好好看书，受惯性思维所囿，作者真是探案故事的个中高手。 </p>
<blockquote>
<p>作者：豆瓣用户——<a href="https://www.douban.com/people/1756537/">petitspois</a> </p>
<p>原文链接：<a href="https://book.douban.com/review/5421091/#comments">看阿加莎总会觉得不太过瘾（尼罗河上的惨案）书评 (douban.com)</a> </p>
<p>侵删</p>
</blockquote>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>生活休闲</category>
        <category>书-音-影</category>
      </categories>
      <tags>
        <tag>阅读</tag>
        <tag>观影</tag>
        <tag>听音乐</tag>
        <tag>思考</tag>
      </tags>
  </entry>
  <entry>
    <title>我的观影</title>
    <url>/2022/02/12/MyFilm/</url>
    <content><![CDATA[<h3 id="一、鬼子来了【22-02-11】"><a href="#一、鬼子来了【22-02-11】" class="headerlink" title="一、鬼子来了【22-02-11】"></a>一、鬼子来了【22-02-11】</h3><p><a target="_blank" href="https://movie.douban.com/subject/1291858/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">鬼子来了 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://movie.douban.com/subject/1291858/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1Wx41157LP" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i0.hdslb.com/bfs/archive/69b07d9131be3f5dcd1fc1ea0bf33e44ce3e4613.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:20:54</span>
            </div>
            <div class="bvideo-info">
                <p class="title">细读经典 20: 真正的抗日神片《鬼子来了》</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>125.6万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>7654</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">电影最TOP</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p><strong>转载影评：</strong></p>
<p>终于看了姜文被禁止公映的《鬼子来了》，很多人都将这部片子的禁止公映解说为该片中绝口不提共产党抗日，自始至终未出现一个党员形象，因此而引起不满。但看了这部片子才知道决不是因为这个原因。</p>
<p>《鬼子来了》讲的是川北抗战的故事，在那个叫挂甲台的小村落里老百姓无知的固守着自己的善良，却悲惨的被鬼子集体屠杀。而且这场大屠杀发生在日本天皇发表投降诏书之后！中国的农民是淳朴而且安于现状的，他们被诸多的势力欺压了两千年，形成的秉性却温顺贤良。可以想见，当小鬼子们没有进行烧杀戮掠的时候，老百姓虽然会心有不甘，但真的是尚不至于奋起以命相搏（并不是说老百姓就不爱国，而是在那样一个自给自足、封闭且未开化的时期里，“爱国”还是一个虚幻的理念）。片中有大量的这样的痕迹，在那个被日军牢牢把持的小地方，因为没有国军和共军的争夺而显得相对安宁，小日本对当地人进行着和平的奴役。习惯了被压迫的老百姓已经对于尊严的事情麻木了，为了生计这样的眼前利益而过分乐观的活着。小镇上唱曲的在集市上宣扬着皇道乐土，教育众人中日“八百年前是一家”，村落里老百姓见了鬼子一口一个“森塞”的叫着。固执的不肯杀人的村民们为了结果被俘的鬼子，费尽周折请刀斧手，最终得到的结论是这鬼子命不该绝。并最终同意了鬼子用人换粮的建议。</p>
<p>烧杀掳掠外加奸淫妇女，这是鬼子留在中国人心目中固有的形象。但在这部片中的开始，虽然老百姓见了鬼子依然是战战兢兢有着被杀的危险，但小鬼子们仅是打骂并未随意杀人。不过，小鬼子们的武士道精神和对中国人的残忍施暴的一面却被生动形象的刻画出来。两个场景就可以进行完全的阐释： </p>
<ul>
<li>一、炮楼里的两个鬼子溜出来想吃鸡，老兵教新兵如何粗暴的对待中国人，如何利用自己手中的刺刀，并说“对付支那猪就只有用这种办法”； </li>
<li>二、被囚半年兽性冷却后的小三郎在所谓的鬼子＋百姓的联欢中得意忘形，对中国人大声说着“大哥大嫂过年好，你是我的爷，我是你的儿”，但明显喝高了的他在队长一番激励下，兽性大发对养活了他半年的村民们举起了屠刀。</li>
</ul>
<p>凡中国人与外族的殊死争斗，敌阵中必有我国人！这简直是个千古不变的道理！汉武帝征讨匈奴，有前宫人在匈奴王帐中出谋划策；抗日战争中更是伪军比日寇数量都多！这部片子中着重刻画了翻译这个角色，这个懂日本话的东北人几乎没有一句是在“直译”，国人对国人的欺瞒才是最可悲的事情。而且这个翻译在被村民释放后居然给鬼子队长出主意让鬼子爽约，结果招来一句奚落：“我们日本人不像你们中国人那样不讲信用。”这句话对我来说真的是太刺耳了。</p>
<p>悲剧就是把美好的东西打碎给人看。这部片子的结局是挂甲台整个村落在大火中消失在鬼子的屠刀下，而国军的抗战将领又让斧劈日本战俘的三儿受戮于已经被俘的小鬼子。这就是中国人的悲哀。为什么这样一个国土辽阔人口众多的民族被一个岛国践踏了整整八年，这部片子给了我们一定的启示。很多时候，不是别人的凶残和强大，而是我们自己的愚昧和自残。</p>
<p>从联合早报网上看到消息，东京举行了一个反华游行，理由是中国还没有对去年4月份爆发的反日游行向日本人道歉。不明白为什么这个世界上还有如此不讲道理的人种，但是毕竟有这样的邻居，有惨痛的教训，我们只能用《霍元甲》中的那句震撼的台词一样——“自强不息”！ </p>
<blockquote>
<p>作者：豆瓣用户——俩飞</p>
<p>原文链接：<a href="https://movie.douban.com/review/1045537/">https://movie.douban.com/review/1045537/</a></p>
<p>侵删</p>
</blockquote>
<h3 id="二、爆裂鼓手【22-02-15】"><a href="#二、爆裂鼓手【22-02-15】" class="headerlink" title="二、爆裂鼓手【22-02-15】"></a>二、爆裂鼓手【22-02-15】</h3><p><a target="_blank" href="https://movie.douban.com/subject/25773932/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">爆裂鼓手 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://movie.douban.com/subject/25773932/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1XK4y1a7gC" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i2.hdslb.com/bfs/archive/d383ead9dc036edb5b562e10ae2bd7b1ab0d8147.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:31:21</span>
            </div>
            <div class="bvideo-info">
                <p class="title">细读经典 103: 豆瓣8.7，暗黑励志神作《爆裂鼓手》</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>122.8万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>1.0万</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">电影最TOP</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p><strong>转载影评：</strong></p>
<p>作为今年圣丹斯的获奖影片和奥斯卡热门，影片的上佳口碑主要来自于紧凑的剪辑、优质的表演和对于师徒关系非常规的刻画。诚然，影片的剪辑凌厉而准确，除结尾处有少许松懈，整部电影更像是一部动作悬疑片，节奏一再加快，冲突一再升级，影片结束，107分钟不知不觉中过去。影片对于音乐的处理也相当不马虎，体现了对于细节的重视和专业精神。</p>
<p>然而，如果将电影解读为描绘师徒关系，以及少年经历魔鬼训练，超越极限，从而实现自我的励志电影，大概是个误会。片中鼓手和指挥从始至终是纯粹的互相利用关系。鼓手其实从未得到特殊待遇——他始终是乐队中和其他乐手一样微不足道的一员，随时被指挥予取予求，玩弄于鼓掌之间。在鼓手坦陈指挥对于乐队的虐待，间接导致指挥离职之后，酒吧相遇，看似一番肺腑之言，实际上指挥已暗暗设计陷害，如若没有鼓手最后的破釜沉舟，一个乐手的生涯就要万劫不复（影片一再强调在卡内基演出对于音乐生涯的决定性作用）。即使仇人之间，若有一丁点原则和不忍，做事尚不会如此残忍，何况师徒。</p>
<p>鼓手本身也并非传统的善良天才少年。一定的才华和逼迫自己到极限的决心也许天才少年皆有，但是本片主角个性中的偏执和阴暗才是他一直被指挥吸引，也是指挥最后透露出欣赏的主因——鼓手一定程度是年轻版本的指挥，同样为达目的不择手段。影片中有诸多巧合，其中鼓手丢失首席鼓手让他代为保管的乐谱一节，其实颇值得思量。鼓手是否真的如观众期待那样无辜？未必。台词给出许多暗示：首席为自己辩解时说自己因身体有恙不能记谱，指挥早就知晓（那么其他乐手自然也知道）。鼓手在事发之后毫无愧疚，也从未想过补偿。键盘手似开玩笑让鼓手不要动自己的谱子，未必不是防人之心，等等。鼓手在争夺首席位置、与女友分手和佯作无辜答应指挥帮助演出时也同样毫无愧怍。因为同样认同追求卓越就要牺牲自我、他人，以至阻碍道路的一切，鼓手才会被指挥一番话吸引从而上钩，因为指挥的人生哲学实际道出了鼓手的心声。整部影片细述了鼓手从安德鲁到弗莱彻的渐变，最后一幕父亲在后台观看鼓手的独奏，眼神中惊恐多于欣慰，因为眼前的少年早已不是曾经一起吃爆米花开玩笑的儿子，技术上的提升和人性的嬗变相比已经显得次要。</p>
<p>本片导演说结尾实际是悲剧，主要媒体的影评也多少在这一点达到共识，大概正是为此。片中固然有励志的成分，但是成功的代价是少年纯真的丧失，如果本来尚有纯真的话。结尾高潮处影院中的观众畅然拊掌，其实是庆祝这种扭曲哲学的胜利——鼓手大概经此洗礼，从此更加坚定目标，不顾旁人，不择手段，从而一帆风顺。更可怕的是，等他自己成为别人的老师，恐怕也要把这样的哲学传递下去了。相对于严师是否定出高徒、学艺之路如何逼迫自我突破极限这类常规主题，影片提出的问题更集中于追求极致（无论目标是艺术的完美还是名利的丰收，也无论是否关于音乐）和葆有灵魂的平衡取舍。</p>
<blockquote>
<p>作者：豆瓣用户——Z</p>
<p>原文链接：<a href="https://movie.douban.com/review/7265093/">https://movie.douban.com/review/7265093/</a></p>
<p>侵删</p>
</blockquote>
<h3 id="三、寻梦环游记【22-02-17】"><a href="#三、寻梦环游记【22-02-17】" class="headerlink" title="三、寻梦环游记【22-02-17】"></a>三、寻梦环游记【22-02-17】</h3><p><a target="_blank" href="https://movie.douban.com/subject/20495023/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">寻梦环游记 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://movie.douban.com/subject/20495023/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1pW411e7oF" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i2.hdslb.com/bfs/archive/a4840b6079f876e0e31075471d2f9c7e3fca0148.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:15:49</span>
            </div>
            <div class="bvideo-info">
                <p class="title">【木鱼微剧场】《寻梦环游记》</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>99.7万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>4474</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">木鱼水心</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p><strong>转载影评：</strong></p>
<p>5星推荐，片子真的好看，适合全年龄层。孩子们可以看到梦想，看到童趣，看到动画乐园一般的亡灵世界。大人可以看到亲情，看到爱情，看到梦想与责任，看到人性善恶。老年人会更感动吧，家人就是要生活在一起，要互相体谅，互相扶持。</p>
<p>最令我感动的一幕是，小孩回到家，他的太奶奶要去世了，忘记了亲人。小孩弹吉他唱起了“请记住我”，歌声响起，我的眼泪跟着落下。老奶奶眼睛亮起，跟着唱出来，亲人还在我们心里。</p>
<p>电影构建的死后世界很华丽，美轮美奂，就象一个巨大的游乐场。亡灵们很萌，白骨间的随意组合带来各种意料之外的笑点，不恐怖反倒很有趣。这个世界建立在另一个活人世界的记忆上，凡是在世的人记得的有照片的人，就可以在亡灵世界存在，反之就彻底消亡。所以一个家族代代传承的记念和供奉，会让家族中死去的人在另一个世界快乐的永生。</p>
<p>鲁迅说，亲人或余悲，他人亦已歌。确实，当人们不再记得一个人，他就真的消失了。如果世界的规律真是如此，我们活着的时候就应该更善意，更有责任感，更关爱家庭，关爱亲人，大家更团结，影响更多的人，这样当我们死去时，才会被人纪念，才会一直活在亲人和世人的心里。</p>
<p>看了电影，很希望亡灵世界就是这样的，我们的亲人也快乐的在。</p>
<p>另：这个逻辑基础下，还是当名人划算，你看电影里那个歌手，明明是个坏人，死后几十年了，还有无数的歌迷送无数的礼物，可以在亡灵世界风生水起。按这个设想一下，大魔头类的，会不会也因为被无数人记住而永生呢。</p>
<blockquote>
<p>作者：豆瓣用户——雪</p>
<p>原文链接：<a href="https://movie.douban.com/review/8999006/">https://movie.douban.com/review/8999006/</a></p>
<p>侵删</p>
</blockquote>
<h3 id="四、催眠大师【22-02-23】"><a href="#四、催眠大师【22-02-23】" class="headerlink" title="四、催眠大师【22-02-23】"></a>四、催眠大师【22-02-23】</h3><p><a target="_blank" href="https://movie.douban.com/subject/24743711/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">催眠大师 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://movie.douban.com/subject/24743711/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1ut411X7cR" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i0.hdslb.com/bfs/archive/14aea911bef6e21f58ee7a71b81da29234ccc59e.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:25:21</span>
            </div>
            <div class="bvideo-info">
                <p class="title">【木鱼微剧场】优秀国产悬疑电影《催眠大师》</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>122.7万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>7604</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">木鱼水心</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p><strong>转载影评：</strong></p>
<p>今天电影催眠大师上映了，凑了个热闹，第一时间看到这部电影的公映，以国产电影的水准，应该算得上一部‘好电影’了，悬疑+惊悚，不仔细推敲的话，应该是勉强达到了观众的期待值。同时，电影里运用了大量的催眠理论和催眠技术，并且做了艺术的夸张，相信一定让很多观众迷惑，怀疑，不解，让人感觉玄之又玄，虚之又虚。本文，笔者就根据所学的有限的催眠知识，免费给这部电影做一次答疑解惑，也尽自己的一份力解除电影对于催眠术的误导。解释过程尽量做到不剧透吧，免得被喷！</p>
<p>首先科普一下催眠的几个深度，最广泛接受的划分方法是将催眠程度分为六个等级：</p>
<p>第一级催眠深度：小肌群受到暗示被操控，如暗示被催眠对象眼皮睁不开或嘴唇张不开，被催眠者受到暗示无法睁开，被催眠者不感觉被催眠，觉得完全清醒，这阶段已经可以做到减肥、戒烟等心理治疗了。</p>
<p>第二级催眠深度：大的肌群受到暗示，如手臂无法抬起或无法落下。此时被催眠者更加放松，也更加专注与催眠师的指令。</p>
<p>第三级催眠深度：可以完全控制所有的肌肉系统，产生数字阻滞。如无法从椅子上站起来，无法走路，无法说出某个数字，但并不是忘了那个数字，临床催眠师的大部分工作，一般在这前三个等级之内。</p>
<p>第四级催眠深度：开始有失忆现象，被催眠者可以遗忘掉自己的名字，地址，某个数字，并可以产生痛觉阻断，可以做一些手术，拔牙、开刀等。</p>
<p>第五级催眠深度：开始产生梦游状态，产生正性幻觉，可以看到不存在的东西（是真的看到，而不是想象出来的）。 </p>
<p>第六级催眠深度：更新的梦游状态，可以产生负性幻觉，看不见，听不见，确实存在的事物或声音，如暗示被催眠者看不见别人的身体，只能看到头部，那么他就只能看到一个的头。</p>
<p>在影片《催眠大师》中到底达到那个深度，也有人在争议，有人说三级，有人说四级；我认为，至少达到了第五级催眠深度，甚至是第六级，因为被催眠者已经完全不认识自己很熟悉的人（具体情节电影中会交待，此处不详细说的话要剧透），用自己的幻觉代替了现实，这已经完全超出了想像的范畴了。但大多情节中出现的现象达到前三个等级就可以完成，如回忆起以前自己想不起来的事情，跟想象中的人进行对话交流（这在催眠治疗中十分常用，实质是跟自己的潜意识在交流）等，另外，演员在在催眠状态下的表现也符合催眠的状态，如，莫文蔚在催眠状态下产生的眼皮跳动，快速动眼活动，肢体放松，泪水增加，眼白部分变红等等，说明剧组确实注意到了催眠状态下的细节，大家在观看的时候可以留意一下。</p>
<p>最意思是影片中使用的各种催眠技术，种类繁多，甚至可以作为了解催眠技术的一个入门教学了。</p>
<p>先说直接催眠法的技术，首先是眼睛凝视法，影片中使用的道具是怀表，用它的好处是让被催眠者不仅可以凝视重复的摆动，还可以听到单调重复的滴答声， 从而快速导入催眠。影片中的用的凝视法并不限于盯着看怀表，还包括催眠师的眼睛，烟头，笔尘（沈医生在试图催眠徐瑞宁时所用到的）等等，让观众感觉处处有催眠，防不胜防。  </p>
<p>其次是联想法导入催眠的运用，通过让被催眠对象在头脑中产生想象的画面，进而引导他进入更深的催眠状态，这个想像的画面，既可以是对象凭空创造的一个形象或场景，也可以是过去记忆中的的一个场景，这种方法在影片中使用的次数也非常多，如果有兴趣，大家在观影过程中也可以注意一下。</p>
<p>再次是反复加深技术，这种技术的做法就是不断引发被催眠对象唤醒，再不断地导入催眠，从而让对象进入更深的催眠状态，影片中徐峥和莫文蔚不断的互相催眠，又不断的唤醒，再不断的进入催眠……实质上确实使用了这种反复加深技术，不知道是剧情的需要，还是编剧越催眠技术的特意安排。这也是我很喜欢用的一个技术，效果很好，在催眠中，把对象唤醒调整下身体的姿势，反馈一下感受，接着再导入催眠，可以避免被催眠对象长时间保持一个姿势造成的肌肉疲劳。</p>
<p>然后再说一下间接催眠，这是影片中让观众感觉比较玄乎的地方，在影片中叫做清醒催眠，也有人叫做隐性催眠；就是让人在清醒状态下，通过交流谈话或者环境暗示，让对象在不知不觉中进入催眠状态。整个影片框架也是在这种理论的基础上搭建的，为了让观众了解这种催眠技术，影片还特意让主人公对这种理论做了一个讲解，给剧情的勾画做了一个铺垫。如，影片中多次用到了环境的暗示，通过水，声音、光线等，或某一个词语（如影片中用到的“船长”），让对象不知不觉中产生联想和回忆，从而再进一步导入催眠。</p>
<p>另外，影片中有这样的一个比较的高级催眠技术是：混乱技术。该技术的原理是，通过一些异乎寻常的语言，动作，表情等，让被催眠对象短时间内接受大量的信息，造成信息过载，引发神经失衡，触发战斗或逃跑反应（Fight-or-flight response），绕过对象的意识评判区，从而导入催眠。仔细分析一下，影片中一直在使用这种技术，如，影片中心理咨询室故意布置得非常怪异，无形中给予被催眠对象一些混乱；催眠师故意把怀表掉在地上，也是一种混乱；来访者任小妍怪异的心理行为本身就是一种混乱，突然去拨弄钟表指针，突然把水泼出来等都是一种混乱技术的应用，以及从其他咨询师传来的大量信息，也加强了这种混乱，所以，对象没有理由不进入催眠状态了。</p>
<p>当然，能把这种隐性催眠技术运用的如此娴熟，绝非等闲之辈所能及，我想，即使催眠大师艾瑞克森还在世的话，也未必能做到。</p>
<p>现在问题来了，现实中，到底有没有可能做到影片中那样的催眠？我的答案是，有可能。但前期必须做大量的铺垫工作，我不可能在一次咨询过程中做到这样的效果。说到这里，不得不提及另外一种心理技术技术，这就是“心锚”的建立。所谓“心锚”，就是人的内心某一心理状态或行与另外某一动作或表情之间的锚定，它的基础是条件反射。比如开车时看到红灯就不自觉的踩刹车，这就是一种“心锚”。影片中，被催眠对象，一看到有节律的“一，二，三”运动形式就会被催眠，如听到数数123，看烟头有节律的3次一熄一灭，三次晃动水杯，拨动三下钟表针等都能迅速地导入催眠状态，要实现这种简单的瞬间催眠，一般就需要“心锚”的建立，而建立这样的“心锚” 一般还得在催眠状态下，所以这样的“心锚”又称为“催眠后暗示”。</p>
<p>看完影片后，观众还可能有一个担忧，担心自己会不会像影片中那样，某一天被不知不觉的被催眠、被控制，从而说出自己不愿说出的事情，作出自己不愿做的事。其实，大可不必担心，首先，能做到这样快速隐性催眠的催眠师少之又少，凤毛麟角，即使有，也早忙着去某个商业机构都赚大钱去了，恐怕也没有时间闲的蛋疼催眠你玩吧；其次，人在催眠状态下，潜意识的自我保护能力是非常强的，一旦有人要求你做违背个人意愿的事情，很快就能从催眠状态下次苏醒过来。</p>
<p>调查显示，很多普通大众对催眠有很深的误解，认为催眠可以控制人或者让人失去意识进入昏睡状态。因为催眠术早期多用于巫术和宗教等，让它蒙上了一层迷信和神秘的色彩，影片的播出，也可能让许多人加深了这种误解，所以普及和了解一下催眠知识，还是非常有必要的。其实，任何一种催眠状态在平常生活中都出现过，如在睡觉即将清醒或快睡着的时候，突然身体无法动弹，出现‘鬼压床’的现象，就是一种催眠状态；人在某个极度的悲伤或恐惧情绪状态下，眼前突然浮现某个人，就是一种正性幻觉；又如某小美女穿着超短裙去玩，回到家发现青一块，紫一块，什么时候碰的都不知道，这就是一种催眠麻醉状态；有时候时候拿着钥匙找钥匙，拿着电话找手机，或某个东西明明就在桌子上就是找不到，看不见，这就是催眠状态下的一种负性幻觉；所以说催眠一点儿也不神奇。  　　最后再回答一个问题，催眠术可不可能被用于犯罪？答案是可能的！任何一种技术都是一把双刃剑，历史上也确实出现过催眠犯罪的案例，如着名的“海德堡事件”，所以不要轻易找不熟悉的催眠师进行催眠，进行催眠之前最好对催眠师进行一个调查了解，既有利于对催眠师产生信任更好地进入催眠状态，也是一种自我保护。</p>
<p>写这篇文章的过程中，我被催眠了，不知不觉中时间都这么晚了，哈哈哈！有些东西可能说的还不是太清楚，大家有什么问题可以在下面留言，也可以加我QQ：315678525。（吕艳朋，2014年4月29日，于济南宝华苑。） </p>
<blockquote>
<p>作者：豆瓣用户——吕艳朋</p>
<p>原文链接：<a href="https://movie.douban.com/review/6653031/">https://movie.douban.com/review/6653031/</a></p>
<p>侵删</p>
</blockquote>
<h3 id="五、记忆大师【22-02-24】"><a href="#五、记忆大师【22-02-24】" class="headerlink" title="五、记忆大师【22-02-24】"></a>五、记忆大师【22-02-24】</h3><p><a target="_blank" href="https://movie.douban.com/subject/25884801/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">记忆大师 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://movie.douban.com/subject/25884801/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1Rx411n7He" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i1.hdslb.com/bfs/archive/b1c145efca0651dbf279a3ee081504e0d1ea14a6.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:05:00</span>
            </div>
            <div class="bvideo-info">
                <p class="title">【片片】五分钟看懂《记忆大师》，影帝黄渤的“杀人回忆”</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>36.6万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>570</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">小片片说大片</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p><strong>转载影评：</strong></p>
<p>故事一开始的设定就是和现实不太关联的另外一个国度，我感觉抽取记忆出来的技术就像未来一段时间或许可以发生的，但是后面很多场景又让自己感觉这个又是很接地气的画面，可能是交错未来与现在的交错吧，一些场景选的还是很不错的。</p>
<p>在电影里，可以感觉一个人的记忆对一个人性格影响太大了，在江丰记忆弄错之后，一些举动就变得怪异许多，一个植入的记忆对于一个人的行为的影响是否过大？这个需要推敲一下。朋友们有的说这部电影是比较烧脑的，一路猜测，最后回归到自己第一感觉，沈警官。</p>
<p>故事从零碎的记忆开始，穿插了两段故事，在现实中江丰与妻子有矛盾起冲突，慢慢铺展开来，这一部电影感觉又带有对家暴现象的评价，沈警官对于保护那些避免到家暴女性的态度太过于偏执，最开始怀疑他的原因，来自于去警察局找他的男子他一直不愿意去面对，另外就是去找那个女孩调查时，女孩对他的态度和对他徒弟的态度有点差异。</p>
<p>高潮大概是从被设计之后的江丰怀疑陈姗姗开始，故事开始紧张起来，这里相对来讲精彩了，将所有人的目光吸引到这个凶手可能是女性，又有一点疑问，就是录像中从背影看挺像一个女子，但是从电话到最后，都未再提及这个。</p>
<p>最后是一个美好结局，算是一部烧脑的但又不能细究的悬疑电影吧，还是推荐看看的。对于记忆是否影响人的行为，通过记忆找出凶手，还是保持一个探究之心。 </p>
<blockquote>
<p>作者：豆瓣用户——不懂相负</p>
<p>原文链接：<a href="https://movie.douban.com/review/8536325/">https://movie.douban.com/review/8536325/</a></p>
<p>侵删</p>
</blockquote>
<p> <strong>从狼人杀角度分析记忆大师</strong></p>
<p>先盘狼坑：沈sir金刚狼，拿了警徽，最后时刻发言爆炸；沈sir他爸深水狼；李航发言爆炸的冲锋狼，被沈sir卖了做自己身份。 陈姗姗平民钻了狼坑，又被狼队作为完美抗推位，最后没被推出去晚上被狼人给刀了；李惠兰，首夜吃刀的平民；张代晨划水民但最后跳出来帮女巫挡刀。 江丰女巫，吃夜里信息但是逻辑不行没盘出狼坑，差点把毒撒到被狼人污了一手的陈姗姗身上，还好最后时刻抓住沈sir的发言漏洞又站对了边；徒弟原本是铁站边陈sir的猎人，最后因为沈sir他爸的发言漏洞看出了沈sir是只狼一枪把沈sir给崩了；沈sir他妈，离线预言家。。。</p>
<blockquote>
<p>作者：豆瓣用户——Turkey is done</p>
<p>原文链接：<a href="https://movie.douban.com/review/8510215/">https://movie.douban.com/review/8510215/</a></p>
<p>侵删</p>
</blockquote>
<h3 id="六、我是山姆【22-02-27】"><a href="#六、我是山姆【22-02-27】" class="headerlink" title="六、我是山姆【22-02-27】"></a>六、我是山姆【22-02-27】</h3><p><a target="_blank" href="https://movie.douban.com/subject/1306861/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">我是山姆 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://movie.douban.com/subject/1306861/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1pA411P7wh" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i1.hdslb.com/bfs/archive/7a5f1453a3d4948a8ba5f099886956caa63d7665.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:11:18</span>
            </div>
            <div class="bvideo-info">
                <p class="title">这个父亲只有7岁智商，却治愈了无数正常人，泪点低的请一个人看</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>604.2万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>669</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">贰哥电影</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p>大概是深夜了，我从手中挑了挑盘，打算安静的找个剧情片看，我挑了一会。衡量再三，选了一部讲述弱智，不健全、低能父亲的这部《我是山姆》放进了DVD机当中。</p>
<p>山姆是一个快乐的人。生活在美国，工作在星巴克，他是一个小时工，拿时薪。那天突然来了一个电话，告诉他马上就快有了，速来医院认领。于是他快乐的飞奔而去。</p>
<p>是一个女孩，他如获至宝，他想这比什么都珍贵，那应该是天上掉下来的钻石，于是他给孩子取名“露西”（《天空中拥有钻石的露西》：披头四的一首曲子）。办完手续，他携子带母出了医院的大门，孩子他妈对山姆说，快看，外星人。山姆一愣，那孩子他妈仓皇的飞奔而去。</p>
<p> 山姆独自拉扯女孩的日子就这么开始了，这日子很惨，惨过1939年的《飘》，但一步步的，他也明白了很多东西：例如婴儿两个小时要喝一次奶；每周例行的电影会，谈心会将要改成不定期的了；苏斯博士的《绿蛋和火腿》是不能给孩子读一辈子的。同时他也获得了很多疑惑，例如为什么有的男人是光头，天空的尽头在哪，夏时令中少的那一个小时去了哪里，露西长的到底是像他还是像孩子他妈？</p>
<p>有一天露西问山姆，我妈还回来吗？山姆冥思后带着苦相答，保罗麦卡特尼（披头四成员）从小就没妈，约翰列侬（同样是披头四成员）也是，所以你注定是一个不平凡的小孩。露西又问，爸，那你是不平凡的吗？是从小就这样？山姆很茫然，可能明白了露西的一语双关。他带着愧意对露西说，对不起。而露西说，这没什么，我们都是幸运的。</p>
<p>当露西7岁的时候，她已经比山姆显得成熟和聪明了，她会读different，而山姆不能。露西要上小学，同时也快过生日了，山姆想，要给她举办一个聚会，在这个聚会他想给露西一个礼物：一张披头四的唱片。</p>
<p>但惊喜终究是没有到来，那场聚会搞砸了，露西看到慌里慌张的山姆瘫在地上，伤心的飞奔而去。</p>
<p>这个家庭面临困境，露西要被别人托管，美国政府的维权组织切入了进来，法庭也来了，他们要剥夺山姆的抚养权，山姆没见过这阵势，又一次陷入了慌张。</p>
<p>还好，他有一群智囊班子，由一群和他一样有一些缺陷的人员组成的智囊班子。他们合伙给山姆推荐了一个本市最牛逼的律师，是600多页的大黄页中筛选的结果，他们甚至细心的告诉了山姆那律师的地址，这让山姆为之一振，兴奋的飞奔而去。</p>
<p>看到这里我想，如果这个故事没有这一刻的神奇，那这就不是戏剧，而是人生了。</p>
<p>这个牛逼律师是一名女性，招牌服饰是高档职业装，高档眼镜压在高鼻梁上面，是极品女强人，当然她个子也不矮。</p>
<p>电影在这一阶段显示出了诡异的蓝，连那个律师的杯子都是蓝蓝的，让人冷峻不已。这个女律师显得很忙，时间就是金钱，一天办八个案子（都收费），甚至忙的亲儿子的电话都必须排在第二线，山姆看到这个情景，不仅一阵慌张，他想，钱怎么办？我不能再当小时工了，我要学个技术，我要学会调卡布奇诺。</p>
<p>在这期间，山姆又飞奔了很多次（在去律师楼和去看露西的路上），有一次看他去看露西的时候，他没有飞好，滑倒了，那手中的蛋糕塌陷了一地，但露西躺在山姆身边，丝毫不会在意，笑得很甜。</p>
<p>一次次山姆的到来，让女律师被同事的白眼和自己的势利心打动了，他对山姆说，好吧 我义务（pro bono）一次，当你律师。山姆很茫然的样子。女律师又换了一个简单词汇，免费（free）。山姆大喜。 接下来就是很诡异地开了很多次庭，电影的底色还是蓝蓝的，也十分冷峻。镜头中大量使用了近角的特写，照在山姆那茫然的额头上，但是山姆挺住了，没有看到汗水顺着他的额头滴落下来，这应该是一种抗争，但天真的山姆应该不能体会到生活何常不是一种抗争呢？</p>
<p>又一次开庭结束，山姆慷慨邀请女律师吃了一顿便饭，他说这顿饭，我来，我义务（pro bono）一次，看山姆学的多快。</p>
<p>人心是肉长的，女律师在一次次与山姆的交流中也学会了很多，例如耐性和聆听。 她发现在这次义务当中，她得到的比山姆还多，他送给了山姆一套西服和一条领带，山姆很开心，要知道当山姆在法庭上看到他的智囊团中的傻小伙A带着一条红领巾一样的蹩脚领带是多么神往。女律师发现当她和山姆在一起的时候，他确实放下了什么叫成功和高档，甚至有一次她哭着告诉山姆，他丈夫泡上了一个比她还要高档完美的女人，她很痛苦。她当着山姆的面，真的是什么都敢说了。</p>
<p>就这么着，日子过去了，露西暂时被一家人收养。在这期间，女律师还是在和不公正的法律抗争，同时也办了离婚；而山姆同时也是在和生活抗争。但好的方向是，山姆升了职，去调卡布奇诺了。</p>
<p>山姆还找了两份兼职，一份是溜狗，这是为了能够离露西近一些。另一份是去必胜客薄饼店当店员，为了多练习一下算数的实际应用，因为最终在法庭上，法官会考山姆，他要证明山姆脑子够用，是一个能精打细算过日子的人。</p>
<p>山姆开始背很多数据，然后一一串联起来，他知道露西暂时住在九号屋，而约翰.列侬是10月9日生的，约翰.列侬的儿子也是10月9号生的，约翰.列侬他妈住在利物浦纽卡斯尔大街9号。他知道这一切不是巧合，他和露西的相遇也不是巧合，天上掉下的钻石就是他的。    </p>
<p>电影就在此刻伴随着山姆的坚持也慢慢的融化了，那冷峻的蓝色调变成了橙色的暖色调，阳光大量的照射在了草地上，木吉他轻松的和弦带出了一场愉快的足球比赛，而最终的胜利者是山姆，露西回到了他的身边。</p>
<p>最后要说的是，1电影原声很好听，很多大牌乐队或歌手翻唱了披头四的经典歌曲作为这个电影的配乐。我个人最喜欢“across the universe”这首。  2 星巴克和必胜客给我的印象彻底改变，我终于知道，其实那只是美国的牛肉面馆子 </p>
<blockquote>
<p>作者：豆瓣用户——饭忒稀</p>
<p>原文链接：<a href="https://movie.douban.com/review/1631458/">https://movie.douban.com/review/1631458/</a></p>
<p>侵删</p>
</blockquote>
<h3 id="七、禁闭岛【22-03-09】"><a href="#七、禁闭岛【22-03-09】" class="headerlink" title="七、禁闭岛【22-03-09】"></a>七、禁闭岛【22-03-09】</h3><p><a target="_blank" href="https://movie.douban.com/subject/2334904/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">禁闭岛 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://movie.douban.com/subject/2334904/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1zx411879" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration"></span>
            </div>
            <div class="bvideo-info">
                <p class="title">出错了！</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span></span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span></span></span></p>
                <div class="partition">
                    <label class="card-label"></label>
                    <label class="up-label"></label>
                    <label class="up-name"></label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p>一个相同的事情，两个完全不同的故事。</p>
<p>故事一： Marshal Teddy Daniels 的妻子和孩子因为一场由Andrew Laeddis引起的火灾而致死，他深爱着自己的妻子，深爱着自己的孩子。通过对Andrew Laeddis的调查，他了解到一个叫Ashecliff的精神病院，在shutter island上。随着调查的深入Teddy开始怀疑在这个岛上有不可告人的秘密。  </p>
<p>Teddy 的直觉是对的，shutter island 是一个秘密的思维控制的实验室，他们利用有严重精神问题的病人，企图试验对他们的大脑进行手术，让他们没有感觉，没有记忆，在军事上利用他们。  </p>
<p>实验室需要一个新的病人，他们通过调查发现Teddy Daniels心中有trauma. 这正是Teddy的精神弱点。 他们制造了一个假的病人Rachel的失踪，以调查的名义让Teddy来这个精神病院。 他身边还跟着假装Teddy调查助手的心理学家，Dr. Sheehan，目的是观察他的行为，倾听他的描述，了解他，然后和岛上的主精神分析师Dr. John Cawley从心里上摧毁他。  </p>
<p>上了岛以后，岛上的人让Teddy吃谎称阿斯匹林的药丸，吃他们的烟，从生理上加剧Teddy的幻觉问题。  </p>
<p>Dr Cawley 说Rachel是个精神病人，他淹死了自己的三个孩子，却始终觉得他们还活着。通过仔细的调查，Teddy的失踪根本是不可能的，他开始认识到Rachel的失踪也许是什么阴谋。当他审问一个较“清醒”的病人时，这个女病人故意让Dr. Sheehan，那个假装Teddy助手的帮凶心理学家，去倒水。然后趁这个机会马上写给他“快跑”。 …慢慢的他开始怀疑起自己的助手。  </p>
<p>在牢房里，他偶遇到了自己曾经认识的人George Noyce，他告诉Teddy在灯塔里正进行可怕的脑部实验，告诉他要摆脱自己的幻觉，摆脱trauma, 不能相信自己的助手，不然就永远走不出这个岛～  </p>
<p>从牢房里出来后，Teddy第一时间想去那个Geoge提到的灯塔，Dr. Sheehan以助手的理由跟随。去灯塔的路很有危险，Dr. Sheehan借口离开Teddy跑回去告诉全部人Teddy要发现他们的阴谋了。Teddy 没有去成灯塔，却在一个隐蔽的山洞里遇到了真正的Rachel, 她以前是这里的医生，那些心理学家也曾经想把她变成精神病人，然后进行手术。Rachel告诉了Teddy所有的真相，Teddy意识到他们的阴谋就让自己成为第67号精神病人，可是他仍然相信自己的助手。  </p>
<p>回到精神病院，Teddy的助手消失了，Teddy怀疑岛上的人把他带到灯塔里做脑部手术。Teddy要去救助手。当Teddy到灯塔的时候，Dr Cawley 早以在那里等候他。 故事到达了高潮。  </p>
<p>Dr. Sheehan现身和 Dr. Cawley用各种伪造文件，理性分析说服Teddy他自己本来就是精神病人，已经在这里两年了，在这里不断重复自己来这里是调查Rachel的幻觉。 诱导Teddy相信自己杀害了妻子。因为药物的作用，Rachel的故事结合Dr. Cawley的捏造，Teddy产生了幻觉，自己的妻子杀害了三个孩子，然后自己杀了心爱的妻子。  Teddy为了避免立刻的被lobotomize，暂时承认自己是凶手，是精神病人。 可是最后他无法让自己像一个怪物一样活着，还是决定了一死… 电影以象征脑手术的灯塔作为结束…   </p>
<p>故事二： Teddy其实是第66号病人， 他曾经是一个Marshal, 是的他很爱自己的妻子。 可是周六的一天当他发现得了忧郁症的妻子淹死了自己的孩子时，他心里产生了极大的痛苦，一枪把他的妻子杀死了。  </p>
<p>可是正如一个像弗洛伊德的心理学家Dr. Naehring所说的，Teddy心里有很强的defense mechanism (弗洛伊德术语语境下). 于是Marshal 疯了，被送到了shutter island.  </p>
<p>Marshal 是如此爱自己的妻子，他天天带着那个她送的tie，他经常梦见自己的妻子，和她拥抱在一起。他内心无法接受自己就是杀害心爱妻子的人，于是他幻想出Andrew Laeddis这么个凶手，要追寻他报仇，把自己从一个杀人犯变成了一个追查凶手的英雄。</p>
<p> [对应于 弗洛伊德语境下的 reaction formation]  </p>
<p>Teddy 还将妻子杀害三个孩子的记忆transform到一个幻想出来的病人Rachel身上，其实不是Rachel不想承认自己杀孩子的事实而是Teddy本身就不想承认是妻子淹死了自己的孩子。 </p>
<p>[对应于 弗洛伊德语境下的 projection]  </p>
<p>Teddy还经常梦到原来的纳粹集中营，梦见了杀人，梦见了痛苦，梦见了死亡～ </p>
<p>Teddy 通过对 Rachel和Laeddis的追寻和调查去缓和对妻子的爱，不可杀人的道德，和自己杀妻之间的强烈的矛盾。爱，否定，追忆，痛苦在一个失去理智的心中不断的纠缠和升级。构成电影最感人，怜悯而细腻的一面。  </p>
<p>在这期间Teddy还产生了各种幻觉让自己的追寻和调查合理化。  </p>
<p>Dr. Sheehan两年来一直Teddy的心理医师，希望通过role play慢慢的帮助Teddy 从幻觉中走出来。  </p>
<p>当Teddy 的追寻和调查伤害到岛上的人安全时，在灯塔上，Dr. Sheehan和 Dr Cawley试图把他曾杀妻的真相告诉他。 Teddy被说服了，回忆起了那痛苦的杀妻的一幕。  不久后, Teddy又重新回到自己的幻觉当中，岛上的精神分析师怕他的defense mechanism再对别人造成伤害，于是带他去lobotomize…    </p>
<p>故事一是一个充满了阴谋的惊悚片，Teddy以一个刚强执着的探员的形象，一步步探险。故事的高潮发生在对深刻问题的追问：当所有的理性的分析和证据都导向自己是疯子时，你是相信自己的记忆还是相信“现实”的证据？记忆是什么？判断证据是否应该依靠自己的记忆？  </p>
<p>故事二是一个充满悲悯和柔情的故事。对妻子深深的爱，残酷的杀妻事实，令Teddy在幻觉和现实中苦苦的挣扎。 故事二的美在于用精心设计的梦境去深刻分析和表达Teddy的内心，去诠释弗洛伊德关于defense mechanism，dream，hallucination,trauma的理论。    </p>
<p>而shutter island电影最美的地方在于一个电影，根据不同的“相信”，可以是两个完全不同的冲突的故事。 电影对两个故事平衡的表述，用如此多的事实去支持每个故事使得没有哪个故事才是真正的“真相”。  或者两个故事原本都是“真相”，通过观众的主观介入，才使两个真相绝对成一个故事。  而这一切，双向的，绝对的相对只能发生在一个孤立的岛上，任何局外人都能使“真相”暴露。 以shutter island作为电影的题目，可以说再贴切不过。  </p>
<p>我一直觉得如何讲故事才是电影的灵魂，而shutter island这种双向引导故事的叙事方法就好象电影史的一个奇迹，通过创造“两个故事”间的冲突和矛盾重新开拓出新的电影的美  </p>
<p>远远不仅如此，电影中竟然多次出现Mahler的”Quartet for Strings and Piano in A minor”来描述弗洛伊德式的“回忆”或“幻觉”，Mahler和弗洛伊德同是20世纪初奥地利人，音乐不仅仅在内容上和叙事内容吻合(Mahler的七个姐妹死于童年，且从未从自己失去四岁爱女儿Maria的痛苦中恢复过来)，而且时间和地点相吻合， 电影的原创不仅对故事的表述，心理学有认识，而且对古典音乐也有很深入的理解，真是让人赞叹。  </p>
<p>还有电影中的几幕都非常经典： </p>
<p>Teddy梦见自己的妻子在家中，忘出窗外，“妻子死于火灾”（故事一的视角），他紧紧抱着妻子，天上掉着灰，而妻子像一张纸一样，慢慢的烧为灰烬。不舍，爱恋，温情，烧死的信息暗示以一种唯美的方式表达。  </p>
<p>Teddy 幻想到在Dr. Naehring的office, Rachel满身是血，杀害三个孩子前的一幕。 </p>
<p>Teddy周围弥漫的烟雾突然凝固，不仅创造了一种静止的对比，而且还有空间上深度的对比。  …  </p>
<p>还有Leonardo DiCaprio 出色的表演… </p>
<p>还有Dr. Naehring 的办公室那充满十九世纪维也纳风格的室内装饰…  </p>
<p>这个电影越是回味，越有味道。    </p>
<p> 最后我想追问的是，是的正如shakespeare所说 All the world ‘s a stage, and all the men and women merely players. They have their exits and their entrances; And one man in his time plays many parts 即使一个人在生命中的一个时期只有一个角色，那他是否仅仅在上演一个故事呢？  一个相同的事情，两个完全不同的故事。 </p>
<blockquote>
<p>作者：豆瓣用户——resurrection</p>
<p>原文链接：<a href="https://movie.douban.com/review/3043968/">https://movie.douban.com/review/3043968/</a></p>
<p>侵删</p>
</blockquote>
<h3 id="八、蜘蛛侠：英雄无归【22-04-01】"><a href="#八、蜘蛛侠：英雄无归【22-04-01】" class="headerlink" title="八、蜘蛛侠：英雄无归【22-04-01】"></a>八、蜘蛛侠：英雄无归【22-04-01】</h3><p><a target="_blank" href="https://movie.douban.com/subject/26933210/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">蜘蛛侠：英雄无归 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://movie.douban.com/subject/26933210/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV13Y411p7sf" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i0.hdslb.com/bfs/archive/bb5f0713766f18055c5b852271392fe77037b30a.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:09:47</span>
            </div>
            <div class="bvideo-info">
                <p class="title">吐槽《蜘蛛侠：英雄无归》：系列最降智！除了三蛛同框几乎一无是处</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>104.8万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>5427</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">阿德LBG</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p>这篇影评我就自己随便简单写一两句吧，一方面是因为漫威电影这个价值不是很大，更多的是看一下特效剧情，二一方面是因为这一步感觉也太差了，没有什么好的看点，除了一些情怀包括三代蜘蛛侠同现、前几代的怪物以及奇异博士和一些特效。</p>
<p>然后想吐槽一下剧情，这剧情设计的太无聊无趣而且强行降智，蜘蛛侠还想要拯救这些人，想要逆天改命，无语了，本来就是每个人的命运，他这一改感觉造成了很多物理以及哲学上的矛盾。</p>
<p>大概就写着点吧，反正满是槽点。</p>
<h3 id="九、叫我第一名【22-04-06】"><a href="#九、叫我第一名【22-04-06】" class="headerlink" title="九、叫我第一名【22-04-06】"></a>九、叫我第一名【22-04-06】</h3><p><a target="_blank" href="https://movie.douban.com/subject/4798888/" style="position: relative; display: -webkit-box; display: -webkit-flex; display: -ms-flexbox; display: flex; box-sizing: border-box; -webkit-flex-direction: row; -ms-flex-direction: row; flex-direction: row; -webkit-align-items: center; -webkit-box-align: center; -ms-flex-align: center; align-items: center; width: 390px; min-height: 84px; border-radius: 8px; max-width: 100%; overflow: hidden; margin: 16px auto; padding: 12px 12px 9px 12px; background-color: #F6F6F6;"><span class="LinkCard-contents"><span style="display: -webkit-box; -webkit-line-clamp: 2; -webkit-box-orient: vertical; overflow: hidden; text-overflow: ellipsis; max-height: 40px; line-height: 1.25; color: #121212;">叫我第一名 - 豆瓣链接</span><span style="display: -webkit-box; font-size: 13px; height: 18px; line-height: 18px; color: #999; word-break: break-all; text-overflow: ellipsis; overflow: hidden; -webkit-line-clamp: 1; -webkit-box-orient: vertical;"><span style="display: inline-flex; align-items: center;"><svg class="Zi Zi--InsertLink" fill="currentColor" viewbox="0 0 24 24" width="14" height="14"><path d="M13.414 4.222a4.5 4.5 0 1 1 6.364 6.364l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005a2.5 2.5 0 1 0-3.536-3.536l-3.005 3.005a.5.5 0 0 1-.707 0l-.707-.707a.5.5 0 0 1 0-.707l3.005-3.005zm-6.187 6.187a.5.5 0 0 1 .638-.058l.07.058.706.707a.5.5 0 0 1 .058.638l-.058.07-3.005 3.004a2.5 2.5 0 0 0 3.405 3.658l.13-.122 3.006-3.005a.5.5 0 0 1 .638-.058l.069.058.707.707a.5.5 0 0 1 .058.638l-.058.069-3.005 3.005a4.5 4.5 0 0 1-6.524-6.196l.16-.168 3.005-3.005zm8.132-3.182a.25.25 0 0 1 .353 0l1.061 1.06a.25.25 0 0 1 0 .354l-8.132 8.132a.25.25 0 0 1-.353 0l-1.061-1.06a.25.25 0 0 1 0-.354l8.132-8.132z"/></svg></span><span>https://movie.douban.com/subject/4798888/</span>&lt;/span&gt;&lt;/span&gt;</span></span></a></p>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1Nv411A7BH" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i0.hdslb.com/bfs/archive/8380cd7928b413e11e707816dfe0e5a271af9204.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:06:59</span>
            </div>
            <div class="bvideo-info">
                <p class="title">经典高分励志电影！数十万人无一差评！——《叫我第一名》</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>115.4万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>423</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">大本有剧</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<p>这部影片改编自真实事件，男主布莱德是一名妥瑞氏症患者，这是一种非常严重的痉挛疾病，患者会不由自主且无法控制抽搐、做鬼脸、发出怪叫。 通过电影，我的感受是男主一路成长来肯定是不容易，毕竟发出怪叫总会引起别人的注意，而且日常生活肯定十分不方便，不论是上课还是听音乐、甚至是交谈都有极大的不变；此外更重要的会引起别人的嘲讽、歧视等，心里需要承受极大的压力（要我估计早放弃了。。。），但是男主不仅没有放弃，反而却要克服这该死的疾病，确实佩服（我做不到）。此外幸亏男主一路上还遇到许多好人，给了他很多帮助和勇气，这其中最重要的是他的妈妈——一直没有放弃他，另外就是他的小学校长——给了他自信，并成为了他的人生目标，这也是他为什么将来要做一名教师的原因。</p>
<p>总之吧，给我的感受就是——努力、奋进可以完成很多看似完不成的事情，可能做不到天赋异禀那样的高度，但是至少能完成最基本，甚至如果你做到足够努力，也可以比大部分人做的足够好。</p>
<hr>
<h3 id="2023年观影记录"><a href="#2023年观影记录" class="headerlink" title="2023年观影记录"></a>2023年观影记录</h3><link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>生活休闲</category>
        <category>书-音-影</category>
      </categories>
      <tags>
        <tag>阅读</tag>
        <tag>观影</tag>
        <tag>听音乐</tag>
        <tag>思考</tag>
      </tags>
  </entry>
  <entry>
    <title>我的音乐</title>
    <url>/2022/06/26/MyMusic/</url>
    <content><![CDATA[<h4 id="1-中文歌曲"><a href="#1-中文歌曲" class="headerlink" title="1 中文歌曲"></a>1 中文歌曲</h4><p>《北京东路的日子》《化身孤岛的鲸》《明天，你好》《演员》《悟空》《一次就好》《小小》《有点甜》《小星星》《追光者》《万有引力》《专属味道》《不分手的恋爱》《巴赫旧约》《海阔天空》《十年》《后来》《蓝莲花》《千千阕歌》《刀剑如梦》《吻别》《我们的明天》《甜蜜蜜》《上海滩》《懂你》《我相信》《西海情歌》《爱是你我》《粉红色的回忆》《爱拼才会赢》《烟火里的尘埃》《南屏晚钟》《画船记》《手心里的温柔》《情人》《2002年第一场雪》《喀什噶尔胡杨》《我们都一样》《他不懂》《逆战》《逆态度》《看月亮爬上来》《天下》《最美的太阳》《明天过后》《这就是爱》《爱》《青苹果乐园》《蝴蝶飞呀》《红蜻蜓》《孤勇者》《好久不见》《浮夸》《富士山下》《因为爱情》《单车》《最佳损友》《夜曲》《青花瓷》《稻香》《七里香》《花海》《夜的第七章》《大鱼》《雪落下的声音》《起风了》《玫瑰与小鹿》《生活总该迎着光亮》《钟无艳》《最初的梦想》《走着走着就散了》《刚好遇见你》《我的天空》《勋章》《生如夏花》《偏爱》《此生不换》《成都》《阿刁》《水中花》《以后的以后》《遇见》《逆光》《作曲家》《年轮》《渡红尘》《如果一切都没有发生过》《南山南》《你的样子》《求佛》《大海》《夜空中最亮的星》《去年夏天》《光辉岁月》《漂洋过海来看你》《可他爱着这个世界》《来不及勇敢》《倒数》《飘向远方》《风筝误》《你好不好》《醉千年》《等一分钟》《回忆总想哭》《说散就散》《怀念青春》《白月光》《野子》《荣耀》《成全》《冲动的惩罚》《暗里着迷》《丑八怪》《爱如潮水》《信仰》《爱就一个字》《时间都去哪了》《都选C》《断桥残雪》《有何不可》《如果当时》《千百度》《弹指一挥间》《灰色头像》《你若成风》《清明雨上》《城府》《国王与乞丐》《红尘来去一场空》《黄昏》《胡广生》《江湖再见》《老街》《老男孩》《木兰》《和你一样》《蜀绣》《平凡之路》《千古》《千年游》《情人》《囚鸟》《最美的期待》《笔记》《岁月神偷》《路过人间》《离人》《若不是那次夜空》《十七岁》《童话》《同桌的你》《我的楼兰》《无期》《喜欢你》《笑看风云》《一百万个可能》《樱花草》《映山红》《一万次悲伤》《直到世界尽头》《着魔》《祝你一路顺风》《小情歌》《曹操》《修炼爱情》《背对背拥抱》《江南》《飞云之下》《小酒窝》《醉赤壁》《可惜没如果》《将故事写成我们》《讲不出再见》《像我这样的人》《牧马城市》《消愁》《沉默是金》《棉花糖》《怒放的生命》《大约在冬季》《不让我的眼泪陪我过夜》《恰好》《卷珠帘》《水手》《万疆》《翱翔》《最后一次的温柔》《难念的经》《月亮代表我的心》《车上的人》《再见只是陌生人》《特别的爱给特别的你》《爱要怎么说出口》《我终于失去了你》《我很丑可是我很温柔》《勇敢一点》《雨蝶》《花火》《洋葱》《光年之外》《来自天堂的魔鬼》《你不是真正的快乐》《红豆》《秋天不回来》《伤心太平洋》《心太软》《在我的歌声里》《真的爱你》《鼓楼》《曾经的你》《单身情歌》《如愿》《匆匆那年》《一剪梅》《男儿当自强》《小苹果》《沧海一声笑》《时间煮雨》《知否知否》《花田错》《需要人陪》《大城小爱》《缘分一道桥》《我的未来不是梦》《在他乡》《千千万万》《寂寞沙洲冷》《很受伤》《甘心情愿》《一路生花》《如果云知道》《那些年》《处处吻》《胆小鬼》《无名之辈》《慢慢喜欢你》《凉凉》《体面》《说好不哭》《给我一首歌的时间》《舞女泪》《小幸运》《天使的翅膀》《笑红尘》《年少有为》《老鼠爱大米》《丁香花》《月亮惹的祸》《单恋一枝花》《给你们》《趁早》《死了都要爱》《隐形的翅膀》《Super Star》《花心》《往事只能回味》《有多少爱可以重来》《可惜不是你》《魔鬼中的天使》《再回首》《该死的温柔》《小薇》《微微》《只是没有如果》《终于等到你》《唯一》《爱江山更爱美人》《水星记》《为你写诗》《爱转角》《最近》《痴心绝对》《独家记忆》《过火》《当你孤单你会想起谁》《白狐》《春泥》《纪念》《窗外》《暗香》《向全世界宣布爱你》《远走高飞》《情人咒》《我的少女时代》《爱你AINI》《情花》《那女孩对我说》《日不落》《欧若拉》《为情所伤》《不是因为寂寞才想你》《至少还有你》《再度重相逢》《天堂》《后来我们会怎样》《为你我受冷风吹》《让我欢喜让我忧》《这世界那么多人》《盛夏的果实》《如果没有你》《真的好想你》《梦里水乡》《口是心非》《空城》《羞答答的玫瑰静悄悄地开》《当》《遇上你是我的缘》《我曾》《这一生关于你的风景》《梦醒时分》《你怎么舍得我难过》《是不是这样的夜晚你才会这样的想起我》《愿得一人心》《飘向北方》《纸短情长》《地球上最浪漫的一首歌》《守着你到永久》《想见你想见你想见你》《再见烟火》《缺口》《不过失去了一点点》《独角戏》《风雨无阻》《明天会更好》《直到遇见了你我只喜欢你》《一剪梅》《醉千年》《燕归巢》《泡沫》《其实都沒有》《太多》《问》《余香》《有一种爱叫做放手》《浪子回头》《答案》《再见》《再也没有》《夏天的风》《我们的纪念》《落在生命里的光》《黄梅戏》《血腥爱情故事》《不再犹豫》《曲终人散》《太想爱你》《红日》《这条街》《勇气》《爱的哲学》《你是人间四月天》《飞舞》《菊花台》《无赖》《撕夜》《手放开》《外滩十八号》《老人与海》《爱死了昨天》《七月七日晴》《千年泪》《花香》《纷飞》《忘记你我做不到》《慢慢》《值得》《友情岁月》《掌心》《我的心太乱》《挪威的森林》《爱的故事上集》《一万个理由》《犯错》《不要在我寂寞的时候说爱我》《心醉》《丁香花》《不要再来伤害我》《香水有毒》《错错错》《你到底你爱谁》《只对你有感觉》《一个像夏天一个像秋天》《三国恋》《像风一样自由》《认真的雪》《放生》《为爱停留》《千里之外》《第一次》《分飞》《一直很安静》《下一个天亮》《雪人》《爱情错觉》《关不上的窗》《千年之恋》《青花》《铁血丹心》《敢爱敢做》《谁明浪子心》《只想一生跟你走》《偏偏喜欢你》《记事本》《出卖》《画心》《类似爱情》《猜不透》《爱丫爱丫》《越长大越孤单》《痴情换情深》《一千年以后》《不得不爱》《美丽的神话》《伤心1999》《雨一直下》《最熟悉的陌生人》《月牙湾》《谢谢你的温柔》《亲爱的那不是爱情》《感动天感动地》《会呼吸的痛》《我怀念的》《挥着翅膀的女孩》《当你》《青春纪念册》《孤单北半球》《断点》《下一站天后》《离歌》《加速度》《开始懂了》《温柔》《暖暖》《如果爱下去》《桃花朵朵开》《梁山伯与茱丽叶》《奔跑》《感谢你曾经来过》《情非得已》《我很快乐》《一生有你》《FLY》《星星点灯》《不要怕》《桥边姑娘》《珊瑚海》《最美的瞬间》《护花使者》《有没有那么一首歌会让你突然想起我》《一笑而过》《天亮了》《第一时间》《绿光》《流星雨》《鸽子情缘》《不浪漫罪名》《k歌之王》《下个路口见》《冬天的秘密》《心痛2009》《没那么简单》《我的好兄弟》《爱情买卖》《我爱他》《红色高跟鞋》《心墙》《说谎》《相思》《风雨无阻》《让我欢喜让我忧》《神话情话》 《是非题》《我想更懂你》 《守护着我的光》《空》《须尽欢》《恋人心》《晚风心里吹》《听闻远方有你》《爸爸妈妈》《夜航星》《把悲伤留给自己》《天上人间》《海浪》《逍遥叹》 </p>
<h4 id="2-英文歌曲"><a href="#2-英文歌曲" class="headerlink" title="2 英文歌曲"></a>2 英文歌曲</h4><p>《Yesterday Once More》《Let Her Go》《See You Again》《We Don’t Talk Any More》《Rain On Me》《Take Me Hand》《That Girl》《Stronger》《Dream It Possible》《Fight Song》《Try》《Lonely》《Stop!Stop!Stop》《Axel f》《Faded》《Lean on》《Closer》《Rockabye》《This is what you came for》</p>
<h4 id="3-日韩歌曲"><a href="#3-日韩歌曲" class="headerlink" title="3 日韩歌曲"></a>3 日韩歌曲</h4><p>《曾经我也想过一了百了》《Planet》《有彩虹》《清新的小女孩》《Trap》</p>
<h4 id="4-纯音乐"><a href="#4-纯音乐" class="headerlink" title="4 纯音乐"></a>4 纯音乐</h4><p>《天空之城》《和兰花在一起》《城南花已开》《Senbonzakura》</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>生活休闲</category>
        <category>书-音-影</category>
      </categories>
      <tags>
        <tag>阅读</tag>
        <tag>观影</tag>
        <tag>听音乐</tag>
        <tag>思考</tag>
      </tags>
  </entry>
  <entry>
    <title>最优化方法_Part1</title>
    <url>/2022/05/15/Optimization-method1/</url>
    <content><![CDATA[<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1m7411u72b" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i2.hdslb.com/bfs/archive/1c52998c22ddf8d0088e8d58f0eb97f5553d5c16.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">01:20:50</span>
            </div>
            <div class="bvideo-info">
                <p class="title">最优化理论与方法-第一讲：最优化问题概述</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>14.6万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>792</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">superfatseven</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<h3 id="第一讲-最优化问题概述"><a href="#第一讲-最优化问题概述" class="headerlink" title="第一讲 最优化问题概述"></a>第一讲 最优化问题概述</h3><h4 id="1-1-什么是最优化问题"><a href="#1-1-什么是最优化问题" class="headerlink" title="1.1 什么是最优化问题"></a>1.1 什么是最优化问题</h4><p>最优化问题是<strong>决策问题</strong>，选择一些可以执行的策略来使得目标最优，一个最优化问题包括：</p>
<ul>
<li>决策变量</li>
<li>一个或多个目标函数</li>
<li>一个由可行策略纽成的集合，可由等式或者不等式刻画</li>
</ul>
<h4 id="1-2-基本形式"><a href="#1-2-基本形式" class="headerlink" title="1.2 基本形式"></a>1.2 基本形式</h4><h5 id="一-视频中的形式"><a href="#一-视频中的形式" class="headerlink" title="(一) 视频中的形式"></a>(一) 视频中的形式</h5><script type="math/tex; mode=display">
\begin{cases}\text { min or max } & f(\boldsymbol x) \\ 
\text{ s.t.} & g_{i}(\boldsymbol x) \leq 0, \text{  } i=1, \cdots, m \\ 
& h_{i}(\boldsymbol x) = 0, \text{  } i=1, \cdots, l\end{cases}</script><p>其中，<script type="math/tex">\boldsymbol x = (x_1, x_2, \cdots, x_n)^{\mathrm T} \in \boldsymbol X</script>是<strong>决策变量</strong>，$\boldsymbol X$表示给定的集合，比如<script type="math/tex">\mathbb{R}_+^n、 \mathbb{Z}n</script>等。</p>
<ul>
<li>$f(\boldsymbol x)$即目标函数，$g_i(\boldsymbol x) \leq 0, h_i(\boldsymbol x)=0$分别为不等式约束和等式约束</li>
<li>集合<script type="math/tex">\boldsymbol S = \begin{Bmatrix} \boldsymbol x \in X \mid g_{i}(\boldsymbol x) \leq 0, \text{  } i=1, \cdots, m,  h_{i}(\boldsymbol x) = 0, \text{  } i=1, \cdots, l \end{Bmatrix}</script>称为最优化问题的<strong>可行集(可行域)</strong></li>
</ul>
<h5 id="二-参考其他文献的形式"><a href="#二-参考其他文献的形式" class="headerlink" title="(二) 参考其他文献的形式"></a>(二) 参考其他文献的形式</h5><script type="math/tex; mode=display">
\begin{cases}\text { min } & f(\boldsymbol x) \\ 
\text{ s.t.} & \boldsymbol x \in \mathcal{X} \end{cases}</script><p>其中，$\boldsymbol x = (x_1, x_2, ··· , x_n)^{\mathrm T} ∈ \mathbb{R}^n ​$是<strong>决策变量</strong>，$f : \mathbb{R}^n → \mathbb{R}​$ 是<strong>目标函数</strong>，$\mathcal{X} ⊆ \mathbb{R}^n​$是<strong>约束集合或可行域</strong>，可行域包含的点称为<strong>可行解或可行点</strong>。记号 <code>s.t.</code>是 “subject to”的缩写，专指<strong>约束条件</strong>。当$\mathcal{X} = \mathbb{R}^n​$时，上式称为<strong>无约束优化问题</strong>。集合$\mathcal{X}​$通常可以由约束函数$c_i( \boldsymbol x): \mathbb{R}^n → \mathbb{R} ,i = 1, 2,··· ,m + l ​$表达为如下具体形式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{X}=\left\{x \in \mathbb{R}^{n} \mid c_{i}(x) \leqslant 0,\right.& i=1,2, \cdots, m \\
c_{i}(x)=0, &i=m+1, m+2 \cdots, m+l\}
\end{aligned}</script><blockquote>
<p> 参考资料1：<a href="https://bicmr.pku.edu.cn/~wenzw/optbook/opt1.pdf">课本——最优化：建模、算法与理论</a></p>
<p> 参考资料2：<a href="https://blog.csdn.net/jbb0523/article/details/50596387">最优化问题简介 - jbb0523 - CSDN</a></p>
</blockquote>
<h4 id="1-3-最优化问题分类"><a href="#1-3-最优化问题分类" class="headerlink" title="1.3 最优化问题分类"></a>1.3 最优化问题分类</h4><h5 id="一-无约束优化-约束优化"><a href="#一-无约束优化-约束优化" class="headerlink" title="(一) 无约束优化/约束优化"></a>(一) 无约束优化/约束优化</h5><ul>
<li>无约束优化问题：没有约束条件<ul>
<li>常见求解方法：梯度下降法(最速下降法)、牛顿法、共轭梯度法等等</li>
</ul>
</li>
<li>约束优化问题：在一定约束条件下求解目标函数的<code>min OR max</code></li>
</ul>
<blockquote>
<p>易知，实际中最碰见的是约束优化问题，但是我们在课程中却要大量研究无约束优化问题，这个有两方面的原因：一是无约束优化问题求解相对简单；二是可以通过一定的变换将有约束转变为无约束优化。</p>
</blockquote>
<h5 id="二-线性-非线性优化"><a href="#二-线性-非线性优化" class="headerlink" title="(二) 线性/非线性优化"></a>(二) 线性/非线性优化</h5><ul>
<li>线性优化问题的一般形式</li>
</ul>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & \boldsymbol{Cx} \\ 
\text{ s.t.} &  \boldsymbol{Ax = b}\end{cases}</script><p><strong>单纯形方法：</strong>在数学优化领域中常用于<strong>线性规划问题</strong>的数值求解。由线性规划问题的可行集的特征来决定的(线性规划最优解存在时，那么最优解一定存在与问题可行集的<strong>顶点位置</strong>)，单纯性方法就是关注这些顶点。</p>
<blockquote>
<p>参考资料：<a href="https://oi-wiki.org/math/simplex/">单纯形算法</a></p>
</blockquote>
<ul>
<li>非线性优化(例子说明)：<strong>均值-方差模型</strong></li>
</ul>
<h5 id="三-连续优化-离散优化"><a href="#三-连续优化-离散优化" class="headerlink" title="(三) 连续优化/离散优化"></a>(三) 连续优化/离散优化</h5><ul>
<li>连续优化：$\boldsymbol x = (x_1, x_2, \cdots, x_n)^{\mathrm T} \in \boldsymbol X$中每一个$x_n$的取值是连续的</li>
<li>离散优化：$\boldsymbol x = (x_1, x_2, \cdots, x_n)^{\mathrm T} \in \boldsymbol X$中每一个$x_n$的取值是离散的，比如$0-1$规划</li>
</ul>
<h5 id="四-单目标优化-多目标优化"><a href="#四-单目标优化-多目标优化" class="headerlink" title="(四) 单目标优化/多目标优化"></a>(四) 单目标优化/多目标优化</h5><ul>
<li>单目标优化</li>
<li>多目标优化(均值-方差模型)常采用的办法：<ul>
<li>法1：转化为单目标，即可以将某些目标经过一定形式转化到约束条件中去。</li>
<li>法2：想办法将多个目标融合为单个目标</li>
</ul>
</li>
</ul>
<h5 id="五-动态规划"><a href="#五-动态规划" class="headerlink" title="(五) 动态规划"></a>(五) 动态规划</h5><h5 id="六-随机规划"><a href="#六-随机规划" class="headerlink" title="(六) 随机规划"></a>(六) 随机规划</h5><p>参数中有不确定系数的优化问题。</p>
<h5 id="七-鲁棒优化"><a href="#七-鲁棒优化" class="headerlink" title="(七) 鲁棒优化"></a>(七) 鲁棒优化</h5><h4 id="1-4-最优化方法主要讲解内容"><a href="#1-4-最优化方法主要讲解内容" class="headerlink" title="1.4 最优化方法主要讲解内容"></a>1.4 最优化方法主要讲解内容</h4><ul>
<li>凸优化理论：凸集，凸函数，凸优化问题；</li>
<li>无约束优化问题的算法；</li>
<li>约束优化的最优性条件及对偶理论；</li>
<li>线性规划、二次规划算法；</li>
<li>约束优化的罚函数方法；</li>
<li>优化软件：CVX，CPLEX</li>
</ul>
<h4 id="1-5-预备知识"><a href="#1-5-预备知识" class="headerlink" title="1.5 预备知识"></a>1.5 预备知识</h4><ul>
<li>向量、矩阵、二次型等知识；</li>
<li>微积分知识；</li>
<li>简单的概率知识。</li>
</ul>
<p>课后小问题：</p>
<ul>
<li>$n$元二次函数$f(\boldsymbol x)=\boldsymbol x^{\mathrm T} \boldsymbol H \boldsymbol x ＋\boldsymbol c^{\mathrm T} \boldsymbol x$,其中$\boldsymbol H$为$n$阶对称矩阵，$\boldsymbol c$为$n$维向量；给出$f(\boldsymbol x)$的梯度和hesse炬阵?</li>
<li>设随机向量服从联合正态分布：$\epsilon = (\epsilon_1, \epsilon_2, \cdots, \epsilon_n) \sim N(\mu, \sigma^2)$，令$η = x_1\epsilon_1 + \cdots + x_n\epsilon_n$，给出$η$的$c$分位数。</li>
</ul>
<h3 id="第二讲-凸集"><a href="#第二讲-凸集" class="headerlink" title="第二讲 凸集"></a>第二讲 凸集</h3><p>在最优化范畴里面，凸优化问题是一类常见的且性质很好的(很多时候可以帮助我们解决非凸优化问题)。</p>
<p>如果目标函数<script type="math/tex">f(\boldsymbol x)</script>是一个凸函数，可行集<script type="math/tex">\boldsymbol S</script>或者<script type="math/tex">\mathcal{X}</script>是凸集，通常来讲就是凸优化问题。</p>
<ul>
<li>为什么凸优化问题具有比较好的性质呢？<ul>
<li>例子1：一元最小化问题：<script type="math/tex">\text{min } f(x), \text{  s.t.  } x\in[a, b]</script></li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">凸函数</th>
<th style="text-align:center">非凸函数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://pic.imgdb.cn/item/6281062209475431293471f6.jpg"></td>
<td style="text-align:center"><img src="https://pic.imgdb.cn/item/6281064f0947543129351608.jpg"></td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><ul>
<li>例子2：二元最小化问题：<script type="math/tex">\text{min } f(x_1, x_2) = x_1^2 + x_2^2, \text{  s.t.  } x\in S</script></li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">凸集</th>
<th>非凸集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://pic.imgdb.cn/item/6281115109475431295d3252.jpg" style="zoom:40%"></td>
<td><img src="https://pic.imgdb.cn/item/6281153d09475431296e05df.jpg"></td>
</tr>
<tr>
<td style="text-align:center">梯度：<script type="math/tex">- \bigtriangledown f(\boldsymbol x^*)</script></td>
<td>梯度：<script type="math/tex">- \bigtriangledown f(\boldsymbol x^*)</script></td>
</tr>
<tr>
<td style="text-align:center">优化问题等价为：<script type="math/tex">\theta > 90° \Leftrightarrow - \bigtriangledown f(\boldsymbol x^*) (\boldsymbol{x-x^*}) \leq 0, \forall \boldsymbol x \in S</script></td>
<td>优化问题等价为：<script type="math/tex">\theta > 90° \Leftrightarrow - \bigtriangledown f(\boldsymbol x^*) (\boldsymbol{x-x^*}) \leq 0, \forall \boldsymbol x \in S</script></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>凸集：</strong>集合中的任意两点连线的点都在该集合中，则称该集合为凸集；凹集为非凸集。 </p>
</blockquote>
<h4 id="2-1-凸集的定义"><a href="#2-1-凸集的定义" class="headerlink" title="2.1 凸集的定义"></a>2.1 凸集的定义</h4><h5 id="一-凸集定义的数学表示"><a href="#一-凸集定义的数学表示" class="headerlink" title="(一) 凸集定义的数学表示"></a>(一) 凸集定义的数学表示</h5><p>凸集：对于任意的$x, y \in \boldsymbol C$与任意$\lambda \in [0,1]$，有</p>
<script type="math/tex; mode=display">
\lambda x + (1-\lambda)y \in \boldsymbol C</script><p>那么$\boldsymbol C$为凸集。凸集中任意两点连线的点都在该集合中。</p>
<p><img src="https://pic.imgdb.cn/item/62819d5909475431296cc326.jpg" style="zoom:60%"></p>
<ul>
<li><strong>推广表示方法：</strong></li>
</ul>
<script type="math/tex; mode=display">
\lambda_1 x_1 + \cdots + \lambda_k x_k \in  \boldsymbol C, \text{  }\forall x_1, \cdots, x_k \in \boldsymbol C</script><p>其中，$\lambda_i \geq 0, \lambda_1 + \cdots + \lambda_k = 1$。</p>
<h5 id="二-凸组合："><a href="#二-凸组合：" class="headerlink" title="(二) 凸组合："></a>(二) 凸组合：</h5><p>凸组合是指，假设$x_1, x_2, \cdots , x_n$是一组对象（要根据讨论问题的背景来确定），$\lambda_1, \lambda_2, \cdots, \lambda_k$是$k$个常数，并且满足$\lambda_i \geq 0, \lambda_1 + \cdots + \lambda_k = 1$，那么这些点的凸组合即一个这样的点：</p>
<script type="math/tex; mode=display">
\lambda_1 x_1 + \cdots + \lambda_k x_k \</script><p>就称为$x_1, x_2, \cdots , x_n$的凸组合。注意区分<a href="https://zhuanlan.zhihu.com/p/348388306">线性组合,仿射组合,非负组合,凸组合</a>。</p>
<blockquote>
<p>通常你看到凸组合只意味着一个意思：<strong>线性组合，系数和为一</strong> </p>
<p>任意两个点的凸组合都在它们之间的线段上。</p>
</blockquote>
<h5 id="三-凸包："><a href="#三-凸包：" class="headerlink" title="(三) 凸包："></a>(三) 凸包：</h5><p>凸包是针对一个集合来说的。任意一个集合(不一定是凸集)$\boldsymbol C$，其中的点的凸组合构成的集合就称为凸包。</p>
<blockquote>
<p>点集的<a href="https://zh.m.wikipedia.org/wiki/%E5%87%B8%E5%8C%85">凸包</a>等价于该点集的所有凸组合。 </p>
<p>凸包一定是凸集合，通过凸包操作能够将非凸集合转变为凸集合。</p>
</blockquote>
<h4 id="2-2-常见凸集"><a href="#2-2-常见凸集" class="headerlink" title="2.2 常见凸集"></a>2.2 常见凸集</h4><h5 id="一-超平面"><a href="#一-超平面" class="headerlink" title="(一) 超平面"></a>(一) 超平面</h5><ul>
<li><p><strong>定义：</strong>$n$维线性空间中维度为$n−1$的子空间，它可以把线性空间分割为不相交的两部分。 </p>
<blockquote>
<p>这里的$n$必须大于$3$，其子空间才能称之为超平面。</p>
<p>更直观理解超平面：其实就是平面中的直线、空间中的平面的推广。在三维坐标系，XoY平面把三维坐标系”分割”成两个空间，这个分割平面引申到一维，二维，四维空间…来，他就是一个超平面。</p>
</blockquote>
</li>
<li><p><strong>超平面方程推导：</strong> </p>
</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol H = \begin{Bmatrix} \boldsymbol x |\boldsymbol a^{\mathrm T} \boldsymbol x = \boldsymbol b ,(\boldsymbol a \neq \boldsymbol 0) \end{Bmatrix}</script><p>式中，$\boldsymbol a^{\mathrm T}$垂直于超平面$\boldsymbol x$。</p>
<blockquote>
<p>参考资料1：<a href="https://blog.csdn.net/qiyedetianfu/article/details/113113703?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-113113703-blog-53074100.pc_relevant_default&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4">机器学习01-超平面理解</a></p>
<p>参考资料2：<a href="https://blog.csdn.net/dengheCSDN/article/details/77313758?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3-77313758-blog-113113703.pc_relevant_antiscanv2&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3-77313758-blog-113113703.pc_relevant_antiscanv2&amp;utm_relevant_index=6">超平面是什么？——理解超平面（SVM开篇之超平面详解）</a></p>
</blockquote>
<h5 id="二-半空间："><a href="#二-半空间：" class="headerlink" title="(二) 半空间："></a>(二) 半空间：</h5><script type="math/tex; mode=display">
\boldsymbol H^+ = \begin{Bmatrix} \boldsymbol x |\boldsymbol a^{\mathrm T} \boldsymbol x \geq \boldsymbol b ,(\boldsymbol a \neq \boldsymbol 0) \end{Bmatrix}</script><script type="math/tex; mode=display">
\boldsymbol H^- = \begin{Bmatrix} \boldsymbol x |\boldsymbol a^{\mathrm T} \boldsymbol x \leq \boldsymbol b ,(\boldsymbol a \neq \boldsymbol 0) \end{Bmatrix}</script><p><img src="https://pic.imgdb.cn/item/6281b42509475431299ca586.jpg"></p>
<h5 id="三-多面体："><a href="#三-多面体：" class="headerlink" title="(三) 多面体："></a>(三) 多面体：</h5><ul>
<li><strong>定义：</strong>多个线性不等式所刻画的集合</li>
</ul>
<script type="math/tex; mode=display">
\begin{Bmatrix} \boldsymbol x \mid \boldsymbol a_i^{\mathrm T} \boldsymbol x \leq \boldsymbol b_i , i = 1, \cdots, m \end{Bmatrix}</script><blockquote>
<p>注意：线性等式刻画的集合也是多面体，因为等式约束可以转换为不等式约束，如</p>
<script type="math/tex; mode=display">
a_{i}^{T} x=b_{i} \Leftrightarrow\left\{\begin{array}{l}
a_{i}^{T} x \leq b_{i} \\
-a_{i}^{T} x \leq-b_{i}
\end{array}\right.</script></blockquote>
<h5 id="四-球体：（Euclidean）ball-with-center-boldsymbol-x-c-and-radius-r"><a href="#四-球体：（Euclidean）ball-with-center-boldsymbol-x-c-and-radius-r" class="headerlink" title="(四) 球体：（Euclidean）ball with center $\boldsymbol x_c$and radius r"></a>(四) 球体：（Euclidean）ball with center $\boldsymbol x_c$and radius r</h5><script type="math/tex; mode=display">
B\left(\boldsymbol x_{c}, r\right)=\left\{ \boldsymbol x \mid \left\| \boldsymbol x - \boldsymbol x_{c}\right\|_{2} \leq r\right\}=\left\{\boldsymbol x_{c}+r\boldsymbol u  \mid \| \boldsymbol u \|_{2} \leq 1\right\}</script><p>式中，$\boldsymbol x_c$为球心，$r$为半径，$\boldsymbol u$表示某一距离长度范围。</p>
<h5 id="五-椭球（Ellipsoid）"><a href="#五-椭球（Ellipsoid）" class="headerlink" title="(五) 椭球（Ellipsoid）"></a>(五) 椭球（Ellipsoid）</h5><script type="math/tex; mode=display">
\left\{\boldsymbol x \mid\left(\boldsymbol x - \boldsymbol x_{c}\right)^{T} \boldsymbol P^{-1}\left(\boldsymbol x- \boldsymbol x_{c}\right) \leq 1\right\}</script><p>其中$\boldsymbol P$为<strong>正定矩阵</strong>；椭球的半轴长为$\sqrt \lambda_i$($\lambda_i$为正定矩阵的特征值)。</p>
<h5 id="六-二阶-次-锥-：Second-order-cone-ice-cream-cone"><a href="#六-二阶-次-锥-：Second-order-cone-ice-cream-cone" class="headerlink" title="(六) 二阶(次)锥 ：Second-order cone, ice-cream cone"></a>(六) 二阶(次)锥 ：Second-order cone, ice-cream cone</h5><p>假设有一个$n+1$维的锥，其前面$n$维写为$\boldsymbol x$，最后第$n+1$维分量记为$t$。</p>
<script type="math/tex; mode=display">
\left\{(\boldsymbol x, t) \mid ||\boldsymbol x||_2 \leq t\right\}</script><p><img src="https://pic.imgdb.cn/item/6281c0db0947543129bd2b92.jpg" style="zoom:50%"></p>
<blockquote>
<p>什么是锥：</p>
<p>如果<script type="math/tex">\boldsymbol x \in \boldsymbol C</script>，那么若有<script type="math/tex">\lambda \boldsymbol x \in \boldsymbol C, \forall \lambda \geq 0</script>，此时就称为锥。</p>
</blockquote>
<h5 id="七-半定矩阵锥"><a href="#七-半定矩阵锥" class="headerlink" title="(七) 半定矩阵锥"></a>(七) 半定矩阵锥</h5><ol>
<li><script type="math/tex">\boldsymbol S^n</script>：所有$n$阶<strong>对称矩阵</strong>组成的集合；</li>
<li><script type="math/tex; mode=display">\boldsymbol  S _ +^n = \begin{Bmatrix} \boldsymbol X \in \boldsymbol S^n \mid \boldsymbol X \geq  0\end{Bmatrix} $$：所有**半正定矩阵**组成的集合，其中：$$\boldsymbol X \geq  0 \Leftrightarrow \boldsymbol z^T \boldsymbol X \boldsymbol z \geq 0, \forall \boldsymbol z</script></li>
<li><script type="math/tex; mode=display">\boldsymbol S _ {++}^n = \begin{Bmatrix} \boldsymbol X \in \boldsymbol S^n \mid \boldsymbol X >  0\end{Bmatrix}$$：所有**正定矩阵**的集合，其中：$$\boldsymbol X >  0 \Leftrightarrow \boldsymbol z^T \boldsymbol X \boldsymbol z > 0, \forall \boldsymbol z</script></li>
</ol>
<p>半定矩阵锥常用于半定规划里面(一种常见的凸优化问题)。</p>
<ul>
<li>例子：<script type="math/tex">\left[\begin{array}{cc} x & y \\ y & x\end{array}\right] \in \boldsymbol S_+^2</script>，如下图所示。</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/628200f009475431297bfaed.jpg" style="zoom:50%"></p>
<blockquote>
<p>小问题：</p>
<p>线性规划</p>
<script type="math/tex; mode=display">
\min \begin{Bmatrix} \boldsymbol{c^Tx} \mid \boldsymbol A \boldsymbol x = \boldsymbol b , \boldsymbol x \geq 0 \end{Bmatrix}</script><p>的最优解组成的集合为<script type="math/tex">\boldsymbol S</script>，<script type="math/tex">\boldsymbol S</script>是凸集合吗？【答案——<script type="math/tex">\boldsymbol S</script>是凸集合】 </p>
<p>证明：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\forall \boldsymbol x_{1}, \boldsymbol x_{2} \in\boldsymbol  S,有\boldsymbol c^{T} \boldsymbol x_{1}=\boldsymbol c^{T} \boldsymbol x_{2}=\boldsymbol v^* \\
\\
&\forall \lambda \in[0,1], 验证\lambda \boldsymbol x_{1}+(1-\lambda) \boldsymbol x_{2} 是否属于\boldsymbol  S\\
\\
&c^{T}\left(\lambda \boldsymbol x_{1}+(1-\lambda) \boldsymbol x_{2} \right) \\
&=\lambda c^{T}\boldsymbol x_{1}+(1-\lambda) c^{T} \boldsymbol x_{2} \\
&=\boldsymbol v^* \in \boldsymbol  S
\end{aligned}</script></blockquote>
<h4 id="2-3-保持集合凸性的运算"><a href="#2-3-保持集合凸性的运算" class="headerlink" title="2.3 保持集合凸性的运算"></a>2.3 保持集合凸性的运算</h4><h5 id="一-第一种"><a href="#一-第一种" class="headerlink" title="(一) 第一种"></a>(一) 第一种</h5><p>设$\boldsymbol C_1, \boldsymbol C_2 ⊂  \mathbb{R}^n$是凸集，$a \in \mathbb{R}$则</p>
<ul>
<li>（1）<script type="math/tex">\boldsymbol C_{1} \cap \boldsymbol C_{2}=\left\{\boldsymbol x \mid \boldsymbol x \in \boldsymbol C_{1}, \boldsymbol x \in \boldsymbol C_{2}\right\}</script>是凸集</li>
<li>（2）<script type="math/tex">\boldsymbol C_{1} \pm \boldsymbol C_{2}=\left\{\boldsymbol{x \pm y} \mid \boldsymbol x \in \boldsymbol C_{1}, \boldsymbol y \in \boldsymbol C_{2}\right\}</script>是凸集</li>
</ul>
<blockquote>
<p>问题：<script type="math/tex">\boldsymbol S=\left\{\boldsymbol x \in \mathbb{R}^n \mid |p(t)|\leq 1,| t \mid \leq \pi / 3\right\}</script>，其中<script type="math/tex">p(t) = \sum_{i=1}^{n}x_i \cos it</script>是否为凸集？</p>
<p>答：带入<script type="math/tex">p(t)</script>得到：<script type="math/tex">-1 \leq x_{1} \cos t+x_{2} \cos 2 t+\cdots+x_{n} \cos n t \leq 1</script></p>
<p>上不等式表示两个半空间交集。如果$t$只有两种取值，那么就是两个上不等式刻画的集合的交；事实上$t$无穷多取值，那么就是无穷多个上不等式刻画的集合的交，即结果仍然是凸集。</p>
<p><img src="https://pic.imgdb.cn/item/6282072a09475431298dfd61.jpg" style="zoom:50%"></p>
</blockquote>
<h5 id="二-仿射变换"><a href="#二-仿射变换" class="headerlink" title="(二) 仿射变换"></a>(二) 仿射变换</h5><p>线性变换是特殊的仿射变换。或者说<strong><font color="red">仿射变换就是线性变换加平移</font></strong>。 </p>
<p>假设$f: \mathbb R^n \to \mathbb R^m$是仿射函数，即$f(\boldsymbol x) = \boldsymbol{Ax +b}, \boldsymbol A \in \mathbb R^{m \times n}, \boldsymbol b \in \mathbb R^m $</p>
<ul>
<li><script type="math/tex">\boldsymbol C</script>为<strong>凸集</strong><script type="math/tex">\Rightarrow f(\boldsymbol C) = \left\{f(\boldsymbol x) \mid \boldsymbol x \in \boldsymbol C \right\}</script>是<strong>凸集</strong></li>
<li><script type="math/tex">\boldsymbol C</script>为<strong>凸集</strong><script type="math/tex">\Rightarrow f^{-1}(\boldsymbol C) = \left\{\boldsymbol x \mid f(\boldsymbol x) \in \boldsymbol C \right\}</script>是<strong>凸集</strong></li>
</ul>
<blockquote>
<p><strong>特殊仿射变换：</strong></p>
<ul>
<li>放缩scaling：<script type="math/tex">\alpha \boldsymbol C = \left\{\alpha \boldsymbol x \mid \boldsymbol x \in \boldsymbol C \right\}</script></li>
<li>平移translation：<script type="math/tex">\boldsymbol x_0 + \boldsymbol C = \left\{\boldsymbol x_0 + \boldsymbol x \mid \boldsymbol x \in \boldsymbol C \right\}</script></li>
<li>投影projection：<script type="math/tex">\left\{x^{1} \mid\left(\begin{array}{l} x^{1} \\ x^{2} \end{array}\right) \in \boldsymbol C\right\}</script></li>
</ul>
</blockquote>
<h4 id="2-4-凸集的基本性质——投影定理"><a href="#2-4-凸集的基本性质——投影定理" class="headerlink" title="2.4 凸集的基本性质——投影定理"></a>2.4 凸集的基本性质——投影定理</h4><h5 id="一-投影定理概念"><a href="#一-投影定理概念" class="headerlink" title="(一) 投影定理概念"></a>(一) 投影定理概念</h5><p>设$\boldsymbol C ⊂  \mathbb{R}^n$是一个非空闭凸集，$\boldsymbol y \in  \mathbb{R}^n, \boldsymbol y \notin \boldsymbol C$，则：</p>
<p>（1）存在唯一的一点$\overline{\boldsymbol x} \in \boldsymbol C$，使得$\overline{\boldsymbol x}$是$\boldsymbol y$到$\boldsymbol C$的距离最小的点(即<strong>投影点</strong>)，即有</p>
<script type="math/tex; mode=display">
|| \overline{\boldsymbol x} - \boldsymbol y || = \inf \begin{Bmatrix} ||\boldsymbol x - \boldsymbol y|| \mid \boldsymbol x \in \boldsymbol C \end{Bmatrix} > 0</script><p>（2）$\overline{\boldsymbol x}$是$\boldsymbol y$到$\boldsymbol C$的投影点(最小距离点)的充要条件是：</p>
<script type="math/tex; mode=display">
(\boldsymbol x - \overline{\boldsymbol x})^{\mathrm T} (\boldsymbol y - \overline{\boldsymbol x}) \leq 0 , \forall \boldsymbol x \in \boldsymbol C</script><h5 id="二-投影定理的证明"><a href="#二-投影定理的证明" class="headerlink" title="(二) 投影定理的证明"></a>(二) 投影定理的证明</h5><p>不妨设，$\overline{\boldsymbol x}、\boldsymbol x’$都是投影点，则：$|| \overline{\boldsymbol x} - \boldsymbol y || = || \boldsymbol x’ - \boldsymbol y ||$</p>
<p>存在$\hat{\boldsymbol x}$在$\overline{\boldsymbol x}、\boldsymbol x’$两点之间，并作为连接三角形的中垂线，而小于其他两条边，从而小于投影点距离，矛盾！</p>
<p>因此投影点是唯一的。</p>
<p><img src="https://pic.imgdb.cn/item/628211980947543129abeb1c.jpg" style="zoom:50%"></p>
<h5 id="三-点与凸集的分离"><a href="#三-点与凸集的分离" class="headerlink" title="(三) 点与凸集的分离"></a>(三) 点与凸集的分离</h5><ul>
<li><strong>分离</strong></li>
</ul>
<p>设$\boldsymbol C_1, \boldsymbol C_2 ⊂  \mathbb{R}^n$是两个非空凸集，若非零$\boldsymbol a \in \mathbb{R}^n$和$\boldsymbol b$使得：</p>
<script type="math/tex; mode=display">
\boldsymbol{a^{\mathrm T}x} \geq \boldsymbol b, \forall \boldsymbol x \in \boldsymbol C_1, \boldsymbol{a^{\mathrm T}z} \leq \boldsymbol b, \forall \boldsymbol z \in \boldsymbol C_2 \\
\Updownarrow \\
\boldsymbol{a^{\mathrm T}x} \geq \boldsymbol{a^{\mathrm T}z}, \forall \boldsymbol x \in \boldsymbol C_1,\forall \boldsymbol z \in \boldsymbol C_2</script><p>则称超平面$\boldsymbol H = \begin{Bmatrix} \boldsymbol x \mid \boldsymbol{a^{\mathrm T}} \boldsymbol x = \boldsymbol b \end{Bmatrix}$<strong>分离</strong>集合$\boldsymbol C_1, \boldsymbol C_2$。</p>
<ul>
<li><strong>严格分离</strong></li>
</ul>
<p>设$\boldsymbol C_1, \boldsymbol C_2 ⊂  \mathbb{R}^n$是两个非空凸集，若非零$\boldsymbol a \in \mathbb{R}^n$和$\boldsymbol b$使得：</p>
<script type="math/tex; mode=display">
\boldsymbol{a^{\mathrm T}x} > \boldsymbol b, \forall \boldsymbol x \in \boldsymbol C_1, \boldsymbol{a^{\mathrm T}z} < \boldsymbol b, \forall \boldsymbol z \in \boldsymbol C_2 \\
\Updownarrow \\
\boldsymbol{a^{\mathrm T}x} > \boldsymbol{a^{\mathrm T}z}, \forall \boldsymbol x \in \boldsymbol C_1,\forall \boldsymbol z \in \boldsymbol C_2</script><p>则称超平面$\boldsymbol H = \begin{Bmatrix} \boldsymbol x \mid \boldsymbol{a^{\mathrm T}} \boldsymbol x = \boldsymbol b \end{Bmatrix}$<strong>严格分离</strong>集合$\boldsymbol C_1, \boldsymbol C_2$。</p>
<blockquote>
<p>小问题1：两个不相交的非空凸集一定能分离吗？</p>
<p>答：一定能分离。</p>
<p>小问题2：设$\boldsymbol C ⊂  \mathbb{R}^n$是两个非空闭凸集，$\boldsymbol y \notin \boldsymbol C$，是否存在超平面分离$\boldsymbol y$ 和$\boldsymbol C$？</p>
<p>答：存在</p>
</blockquote>
<h4 id="2-5-支撑超平面定理"><a href="#2-5-支撑超平面定理" class="headerlink" title="2.5 支撑超平面定理"></a>2.5 支撑超平面定理</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">符号</th>
<th style="text-align:left">意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\partial \boldsymbol C$</td>
<td style="text-align:left">集合<script type="math/tex">\boldsymbol C</script>的边界</td>
</tr>
<tr>
<td style="text-align:center">$\text{int } \boldsymbol C$</td>
<td style="text-align:left">集合<script type="math/tex">\boldsymbol C</script>所有内点表示的集合</td>
</tr>
<tr>
<td style="text-align:center">$\boldsymbol {c</td>
<td style="text-align:left">C}$</td>
<td>闭包，集合<script type="math/tex">\boldsymbol C</script>内点、边界点放在一起组成的集合(有界闭集合)</td>
</tr>
</tbody>
</table>
</div>
<p>设$\boldsymbol C ⊂  \mathbb{R}^n$是非空凸集，$\overline{\boldsymbol x} \in \partial \boldsymbol C $(意思是$\overline{\boldsymbol x}$是$\boldsymbol C$的边界点)，则存在非零向量$\boldsymbol a \in \mathbb{R}^n$使得：</p>
<script type="math/tex; mode=display">
\boldsymbol{a^{\mathrm T}x} \leq \boldsymbol{a^{\mathrm T}} \overline{\boldsymbol x}, \forall \boldsymbol x \in \boldsymbol {c|C}</script><p>其中，$\boldsymbol {c|C}$是凸集$\boldsymbol C$的全域，称为<strong>闭包</strong>，包括内部和边界。当$\boldsymbol C$不是紧的的时候，$\boldsymbol {c|C}$也是指$\boldsymbol {C}$的全域，即包括$\boldsymbol {C}$不包括的边界。</p>
<p>此时，也称超平面$\boldsymbol H = \begin{Bmatrix} \boldsymbol x \in \mathbb{R}^n \mid \boldsymbol{a^{\mathrm T}} \boldsymbol x =\boldsymbol{a^{\mathrm T}} \overline{\boldsymbol x} \end{Bmatrix}$是集合$\boldsymbol C$在$\overline{\boldsymbol x}$处的<strong>支撑超平面</strong>。</p>
<p><img src="https://pic.imgdb.cn/item/628218a80947543129bfd828.jpg" style="zoom:50%"></p>
<blockquote>
<p>通俗来说，该定理描述为：在凸集$\boldsymbol C$的边界一点$\overline{\boldsymbol x}$寻找一个超平面$\boldsymbol H$，将凸集$\boldsymbol C$放在超平面<strong>负半空间</strong>中。</p>
</blockquote>
<ul>
<li><strong>证明：</strong></li>
</ul>
<p>由于$\overline{\boldsymbol x} \in \partial \boldsymbol C $，则$\boldsymbol x_k \to \overline{\boldsymbol x}$(点列，<strong>收敛</strong>到$\overline{\boldsymbol x}$)，且$\boldsymbol x_k \notin \boldsymbol {c|C}$</p>
<p>因为$\boldsymbol x_k \notin \boldsymbol {c|C}​$，则存在$\boldsymbol a_k(\neq \boldsymbol 0)​$（边界点法向量），使得：</p>
<script type="math/tex; mode=display">
\boldsymbol{a_k^{\mathrm T}x} \leq \boldsymbol{a_k^{\mathrm T}} \boldsymbol x_k, \forall \boldsymbol x \in \boldsymbol {c|C}</script><p>不妨设，$||\boldsymbol a_k|| = 1$，则$\begin{Bmatrix} \boldsymbol a_k \end{Bmatrix}$存在收敛子列。</p>
<p>令上式中$k \to \infty$得：</p>
<script type="math/tex; mode=display">
\boldsymbol{a^{\mathrm T}x} \leq \boldsymbol{a^{\mathrm T}} \overline{\boldsymbol x}, \forall \boldsymbol x \in \boldsymbol {c|C}</script><blockquote>
<p>参考链接：<a href="https://blog.csdn.net/qq_42518956/article/details/113943943">最优化理论与方法-第二讲-凸集 - skycrygg - CSDN</a></p>
</blockquote>
<h4 id="2-6-凸集之Farkas引理"><a href="#2-6-凸集之Farkas引理" class="headerlink" title="2.6 凸集之Farkas引理"></a>2.6 凸集之Farkas引理</h4><p><strong>Farkas引理好像可以用于<font color="red">升维度</font>。</strong></p>
<h5 id="一-Farkas引理定义"><a href="#一-Farkas引理定义" class="headerlink" title="(一) Farkas引理定义"></a>(一) Farkas引理定义</h5><p>给定矩阵$\boldsymbol A_{m \times n}$以及$n$维向量$\boldsymbol c$，则以下两个问题<strong>有且只有一个有解</strong>：</p>
<ul>
<li>问题1——<script type="math/tex">\boldsymbol{Ax \leq 0}, \boldsymbol{c^Tx > 0}</script></li>
<li>问题2——<script type="math/tex">\boldsymbol{A^Ty = c}, \boldsymbol{y \geq 0}</script></li>
</ul>
<p>Farkas引理在几何上讨论矩阵$\boldsymbol A​$的行向量与向量$\boldsymbol c​$的<strong>位置关系</strong>。</p>
<p>矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left[\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{array}\right] 
=\left[\begin{array}{c} \boldsymbol a_{1}^{\mathrm T} \\ \boldsymbol a_{2}^{\mathrm T} \\ \vdots \\ \boldsymbol a_{m}^{\mathrm T} \end{array}\right]</script><p>其中，$\boldsymbol a_i$为$n$维列向量，将其带入问题1、2可得：</p>
<ul>
<li>问题1——<script type="math/tex">\boldsymbol{a_i^Tx \leq 0}(i = 1,2, \cdots, m), \boldsymbol{c^Tx >0}</script></li>
<li>问题2——<script type="math/tex">\boldsymbol{A^Ty} = (\boldsymbol a_1, \boldsymbol a_2, \cdots, \boldsymbol a_m) \left[\begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_{m} \end{array}\right] = \boldsymbol c,  y_i \geq 0(i = 1,2, \cdots, m)</script></li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">情况1</th>
<th style="text-align:center">情况2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://pic.imgdb.cn/item/6282f96d094754312968a721.jpg"></td>
<td style="text-align:center"><img src="https://pic.imgdb.cn/item/6282f9a70947543129696635.jpg"></td>
</tr>
</tbody>
</table>
</div>
<h5 id="二-Farkas引理证明"><a href="#二-Farkas引理证明" class="headerlink" title="(二) Farkas引理证明"></a>(二) Farkas引理证明</h5><ul>
<li><strong>证明方法1：</strong></li>
</ul>
<p><img src="https://pic.imgdb.cn/item/6282fcf9094754312973a43c.jpg" style="zoom:50%"></p>
<ul>
<li><strong>证明方法2：利用线性规划对偶理论</strong></li>
</ul>
<h5 id="三-Farkas引理的推论"><a href="#三-Farkas引理的推论" class="headerlink" title="(三) Farkas引理的推论"></a>(三) Farkas引理的推论</h5><ul>
<li><strong>Gordan定理：</strong>给定矩阵$\boldsymbol A_{m \times n}$，则以下两个问题<strong>有且只有一个有解</strong>：<ul>
<li>问题1——<script type="math/tex">\boldsymbol{Ax < 0}</script></li>
<li>问题2——<script type="math/tex">\boldsymbol{A^Ty = 0}, \boldsymbol{y \geq 0}, \boldsymbol{y \neq 0}</script></li>
</ul>
</li>
<li>给定矩阵$\boldsymbol A_{m \times n}$以及$n$维向量以及$n$维向量$\boldsymbol c$，则以下两个问题<strong>有且只有一个有解</strong>：<ul>
<li>问题1——<script type="math/tex">\boldsymbol{Ax \leq 0}, \boldsymbol{x \geq 0}, \boldsymbol{c^Tx > 0}</script></li>
<li>问题2——<script type="math/tex">\boldsymbol{A^Ty \geq c}, \boldsymbol{y \geq 0}</script></li>
</ul>
</li>
<li>其他还有好几个推论。。。</li>
</ul>
<h3 id="第三讲-凸函数"><a href="#第三讲-凸函数" class="headerlink" title="第三讲 凸函数"></a>第三讲 凸函数</h3><h4 id="3-1-凸函数与凹函数定义"><a href="#3-1-凸函数与凹函数定义" class="headerlink" title="3.1 凸函数与凹函数定义"></a>3.1 凸函数与凹函数定义</h4><h5 id="一-凸函数"><a href="#一-凸函数" class="headerlink" title="(一) 凸函数"></a>(一) 凸函数</h5><p>设$\boldsymbol C$为非空凸集，$f$是定义在$\boldsymbol C$上的函数，如果对任意的$\boldsymbol x, \boldsymbol y \in \boldsymbol C, \alpha \in (0,1)$，均有：</p>
<script type="math/tex; mode=display">
f(\alpha \boldsymbol x + (1-\alpha)\boldsymbol y) \leq \alpha f(\boldsymbol x) + (1-\alpha)f\boldsymbol (y)</script><p>则称$f$为$\boldsymbol C$上的<strong>凸函数</strong>。</p>
<h5 id="二-严格凸函数"><a href="#二-严格凸函数" class="headerlink" title="(二) 严格凸函数"></a>(二) 严格凸函数</h5><script type="math/tex; mode=display">
f(\alpha \boldsymbol x + (1-\alpha)\boldsymbol y) < \alpha f(\boldsymbol x) + (1-\alpha)f\boldsymbol (y)</script><h5 id="三-凹函数"><a href="#三-凹函数" class="headerlink" title="(三) 凹函数"></a>(三) 凹函数</h5><p>设$\boldsymbol C$为非空凸集，$f$是定义在$\boldsymbol C$上的函数，如果对任意的$\boldsymbol x, \boldsymbol y \in \boldsymbol C, \alpha \in (0,1)$，均有：</p>
<script type="math/tex; mode=display">
f(\alpha \boldsymbol x + (1-\alpha)\boldsymbol y) \geq \alpha f(\boldsymbol x) + (1-\alpha)f\boldsymbol (y)</script><p>则称$f$为$\boldsymbol C$上的凹<strong>函数</strong>。</p>
<h5 id="四-严格凹函数"><a href="#四-严格凹函数" class="headerlink" title="(四) 严格凹函数"></a>(四) 严格凹函数</h5><script type="math/tex; mode=display">
f(\alpha \boldsymbol x + (1-\alpha)\boldsymbol y) > \alpha f(\boldsymbol x) + (1-\alpha)f\boldsymbol (y)</script><blockquote>
<p>若$-f$是凸函数，那么$f$为凹函数。</p>
<p>如果原来要求解一个凹函数$f(x)$的最大值，则可以转换为凸函数的最小值：</p>
<script type="math/tex; mode=display">
\max f(x)  \Longleftrightarrow \min -f(x)</script></blockquote>
<h4 id="3-2-常见凸函数"><a href="#3-2-常见凸函数" class="headerlink" title="3.2 常见凸函数"></a>3.2 常见凸函数</h4><h5 id="一-线性函数"><a href="#一-线性函数" class="headerlink" title="(一) 线性函数"></a>(一) 线性函数</h5><script type="math/tex; mode=display">
f(\boldsymbol x) = \boldsymbol a^{\mathrm T} \boldsymbol x + b</script><p>该函数很特殊，它既<strong>唯一一类</strong>是凸函数又是凹函数。</p>
<h5 id="二-二次函数"><a href="#二-二次函数" class="headerlink" title="(二) 二次函数"></a>(二) 二次函数</h5><script type="math/tex; mode=display">
f(\boldsymbol x) =  \boldsymbol x^{\mathrm T} \boldsymbol Q \boldsymbol x + \boldsymbol a^{\mathrm T} \boldsymbol x + b, \boldsymbol Q \in \boldsymbol S_+^n</script><p>其中$\boldsymbol Q \in \boldsymbol S_+^n$是指$\boldsymbol Q$为<strong><font color="blue">半正定</font></strong>矩阵。该式子涉及<strong><font color="red">二次型</font></strong>的知识。</p>
<h5 id="三-最小二乘函数"><a href="#三-最小二乘函数" class="headerlink" title="(三) 最小二乘函数"></a>(三) 最小二乘函数</h5><script type="math/tex; mode=display">
f(\boldsymbol x) = ||\boldsymbol{Ax - b}||_2^2</script><h5 id="四-p-范数"><a href="#四-p-范数" class="headerlink" title="(四) $p$范数"></a>(四) $p$范数</h5><script type="math/tex; mode=display">
f(\boldsymbol x) = \left( \sum_{i = 1}^n |x_i|^p\right)^{\frac{1}{p}}, p \geq 1</script><blockquote>
<p>0范数：$f(\boldsymbol x) = ||\boldsymbol x||_0$表示向量$\boldsymbol x$中<strong>0元素的个数</strong>。</p>
</blockquote>
<h4 id="3-3-凸函数的性质"><a href="#3-3-凸函数的性质" class="headerlink" title="3.3 凸函数的性质"></a>3.3 凸函数的性质</h4><h5 id="一-连续性"><a href="#一-连续性" class="headerlink" title="(一)连续性"></a>(一)连续性</h5><p>一个函数若是凸函数，那么该函数一定是连续函数。</p>
<h5 id="二-性质2"><a href="#二-性质2" class="headerlink" title="(二) 性质2"></a>(二) 性质2</h5><p>$f(\boldsymbol x)$为凸函数，等价于对任意的$\boldsymbol{x,y} \in \mathbb{R}^n$，一元函数：</p>
<script type="math/tex; mode=display">
\varphi(\alpha) = f(\boldsymbol x + \alpha \boldsymbol y)</script><p>为凸函数。相当于把函数竖着切一下得到的切面？？？</p>
<h5 id="三-性质3——判定凸函数的一阶条件"><a href="#三-性质3——判定凸函数的一阶条件" class="headerlink" title="(三) 性质3——判定凸函数的一阶条件"></a>(三) 性质3——判定凸函数的一阶条件</h5><p>$f(\boldsymbol x)$为$\boldsymbol C$上的凸函数的充要条件为：</p>
<script type="math/tex; mode=display">
f(\boldsymbol y) \geq f(\boldsymbol x) + \bigtriangledown f(\boldsymbol x)^{\mathrm T}(\boldsymbol y - \boldsymbol x), \forall \boldsymbol {x, y \in C}</script><p>几何上的意义就是$f(\boldsymbol x)$要在经过某一点的切面的上方，注意到上式中右式为函数在点处的一阶泰勒展开，在$n = 1$情形下的几何意义如下图：</p>
<p><img src="https://pic.imgdb.cn/item/628c479d09475431292ef5f0.jpg" style="zoom:50%"></p>
<p> 对应的切线方程为：$g(y) =  f(x) + \bigtriangledown f(x)^{\mathrm T}(y - x)$</p>
<p>证明：参考网络上的一些博客/b站视频。</p>
<h5 id="四-性质4——判定凸函数的二阶条件"><a href="#四-性质4——判定凸函数的二阶条件" class="headerlink" title="(四) 性质4——判定凸函数的二阶条件"></a>(四) 性质4——判定凸函数的二阶条件</h5><p>一阶条件在工作中很少被使用，我们往往使用的是二阶条件来判定函数的凸性。</p>
<blockquote>
<p>二阶条件涉及到了Hessian matrix，它是这样定义的。 </p>
<p>在数学中，海森矩阵（Hessian matrix 或 Hessian）是一个多变量实值函数的二阶偏导数组成的方块矩阵，假设有一实数函数$f (x_1 , x_2 , . . . , x_n )$ ，如果$f$所有的二阶偏导数都存在，那么$f$的海森矩阵是长下面这样：</p>
<p><img src="https://pic.imgdb.cn/item/628c49750947543129314633.jpg" style="zoom:70%"></p>
</blockquote>
<p>设$\boldsymbol C \in \mathbb{R}^n$是非空开凸集，$f(\boldsymbol x)$在$\boldsymbol C$上二阶连续可微，则$f(\boldsymbol x)$是$\boldsymbol C$上的凸函数，等价于$f(\boldsymbol x)$的二阶Hessian matrix：</p>
<script type="math/tex; mode=display">
\bigtriangledown^2 f(\boldsymbol x) \geq 0, \forall \boldsymbol x \in C</script><p>即函数的二阶Hessian matrix需要是<strong>半正定的</strong>。</p>
<p>对于$\boldsymbol C \in \mathbb{R}$上的函数, 上式退化为$ \dfrac{\mathrm d}{\mathrm d x} \left( \dfrac{\mathrm d f}{\mathrm d x} \right) = f′′(x) ⩾ 0$. 该条件表明函数$f$的导数非减，从几何上解释就是函数$f$在点$x$处具有向上(正)的曲率。</p>
<h4 id="3-4-保持函数凸性的运算"><a href="#3-4-保持函数凸性的运算" class="headerlink" title="3.4 保持函数凸性的运算"></a>3.4 保持函数凸性的运算</h4><h5 id="一-Perspective-function"><a href="#一-Perspective-function" class="headerlink" title="(一) Perspective function"></a>(一) Perspective function</h5><p>若$f(\boldsymbol x)$为凸函数，则：</p>
<script type="math/tex; mode=display">
g(\boldsymbol x, t) = t f(\dfrac{\boldsymbol x}{t}), t > 0</script><p>为凸函数，设$f(x) = x^2$，则$g(x,t)$的可视化如下图所示。</p>
<p><img src="https://pic.imgdb.cn/item/628c4da7094754312936a3c5.jpg" style="zoom:70%"></p>
<h5 id="二-非负组合"><a href="#二-非负组合" class="headerlink" title="(二) 非负组合"></a>(二) 非负组合</h5><p>设有一组凸函数：$f_1(\boldsymbol x), f_2(\boldsymbol x), \cdots, f_m(\boldsymbol x)$，则有：</p>
<script type="math/tex; mode=display">
g(\boldsymbol x) = w_1  f_1(\boldsymbol x) + w_2 f_2(\boldsymbol x) + \cdots + w_m f_m(\boldsymbol x), w_i \geq 0</script><p>为凸函数。</p>
<h5 id="三-凸函数求最大"><a href="#三-凸函数求最大" class="headerlink" title="(三) 凸函数求最大"></a>(三) 凸函数求最大</h5><p>设有一组凸函数：$f_1(\boldsymbol x), f_2(\boldsymbol x), \cdots, f_m(\boldsymbol x)$，则有：</p>
<script type="math/tex; mode=display">
g(\boldsymbol x) = \max \begin{Bmatrix} f_1(\boldsymbol x), f_2(\boldsymbol x), \cdots, f_m(\boldsymbol x) \end{Bmatrix}</script><p>为凸函数。</p>
<h4 id="3-5-凸集与凸函数的关系"><a href="#3-5-凸集与凸函数的关系" class="headerlink" title="3.5 凸集与凸函数的关系"></a>3.5 凸集与凸函数的关系</h4><p>讨论凸集和凸函数的两个工具：<strong>凸集</strong>，<strong>上(镜)图Epigraph</strong></p>
<h5 id="一-水平集"><a href="#一-水平集" class="headerlink" title="(一) 水平集"></a>(一) 水平集</h5><p>任意一个函数(不一定是凸函数)$f(\boldsymbol x)$的<strong>水平集</strong>：</p>
<script type="math/tex; mode=display">
\boldsymbol L_a = \begin{Bmatrix} \boldsymbol x \mid f(\boldsymbol x) \leq a, \boldsymbol x \in C \end{Bmatrix}</script><p>其中$a$为给定的水平值。</p>
<p>$f(\boldsymbol x)​$是凸函数，则其水平集均为凸集。反之，则不成立，即函数不是凸函数时，其水平集也可能是凸集。</p>
<h5 id="二-上-镜-图Epigraph"><a href="#二-上-镜-图Epigraph" class="headerlink" title="(二) 上(镜)图Epigraph"></a>(二) 上(镜)图Epigraph</h5><p>函数$f(\boldsymbol x)$的Epigraph定义为：</p>
<script type="math/tex; mode=display">
\text{epi} (f) = \begin{Bmatrix} (\boldsymbol x, \boldsymbol y)^{\mathrm T} \mid f(\boldsymbol x) \leq \boldsymbol y, \boldsymbol x \in \boldsymbol C \end{Bmatrix}</script><script type="math/tex; mode=display">
f(\boldsymbol x)是凸函数为凸集  \Longleftrightarrow \text{epi} (f)为凸集合</script><p><img src="https://pic.imgdb.cn/item/628c55cb094754312940ee44.jpg" style="zoom:50%"></p>
<p>小问题：给定非空闭凸集$\boldsymbol C$，证明距离函数$f(\boldsymbol y)$是凸函数，其中：</p>
<script type="math/tex; mode=display">
f(\boldsymbol y) = \min \begin{Bmatrix}||\boldsymbol{y-x}||_2 \mid  \boldsymbol x \in \boldsymbol C \end{Bmatrix}</script><h3 id="第四讲-凸优化问题"><a href="#第四讲-凸优化问题" class="headerlink" title="第四讲 凸优化问题"></a>第四讲 凸优化问题</h3><h4 id="4-1-凸优化问题通式"><a href="#4-1-凸优化问题通式" class="headerlink" title="4.1 凸优化问题通式"></a>4.1 凸优化问题通式</h4><p>考虑最优化问题（P）：</p>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & f(\boldsymbol x) \\ 
\text{ s.t. } & g_{i}(\boldsymbol x) \leq 0, \text{  } i=1, \cdots, m \\ 
& h_{i}(\boldsymbol x) = 0, \text{  } i=1, \cdots, l\end{cases}</script><p>记P的可行域为<script type="math/tex">\boldsymbol S = \begin{Bmatrix} \boldsymbol x \in \mathbb{R}^n \mid g_{i}(\boldsymbol x) \leq 0, \text{  } i=1, \cdots, m,  h_{i}(\boldsymbol x) = 0, \text{  } i=1, \cdots, l \end{Bmatrix}</script>，则<font color="blue">当<script type="math/tex">f(\boldsymbol x), g_{i}(\boldsymbol x)</script>是凸函数</font>，<font color="green"><script type="math/tex">h_{i}(\boldsymbol x)</script>是线性函数时</font>，问题（P）称为<strong><font color="red">凸优化问题</font></strong>。</p>
<ul>
<li>为什么特别强调不等式约束条件<script type="math/tex">g_{i}(\boldsymbol x)</script>是凸函数呢？</li>
</ul>
<p>首先，<script type="math/tex">g_{i}(\boldsymbol x) \leq 0</script>刻画的是水平值取0时的一个<strong>水平集</strong>；又知当函数为凸函数时，其水平集均为凸集合。</p>
<ul>
<li>为什么特别强调等式约束条件<script type="math/tex">h_{i}(\boldsymbol x)</script>是线性函数呢？</li>
</ul>
<p>等式约束当为线性函数是，就相当于<script type="math/tex">h_{i}(\boldsymbol x) = \boldsymbol a^{\mathrm T} \boldsymbol x + \boldsymbol b = 0</script>，这种形式所刻画的集合是一个超平面，而超平面也是一个凸集合。</p>
<p>由以上两个小问题分析可知，在上面的条件下，由不等式约束和等式约束所刻画的集合<script type="math/tex">\boldsymbol S</script>也是一个凸集合，这是凸优化问题的一个特点。</p>
<p><img src="https://pic.imgdb.cn/item/628d92730947543129dd4af1.jpg" style="zoom:50%"></p>
<h4 id="4-2-区分凸优化与非凸优化"><a href="#4-2-区分凸优化与非凸优化" class="headerlink" title="4.2 区分凸优化与非凸优化"></a>4.2 区分凸优化与非凸优化</h4><p>区分凸优化与非凸优化是一个非常重要的事情，若是我们能够判断出该问题为凸优化问题，那么就能利用凸优化很多很好的性质进行求解分析问题。下面介绍凸优化问题的两个比较好的性质。</p>
<h5 id="一-局部最优即全局最优"><a href="#一-局部最优即全局最优" class="headerlink" title="(一) 局部最优即全局最优"></a>(一) 局部最优即全局最优</h5><p>对于凸优化问题，其局部最优解就是全局最优解。</p>
<ul>
<li><strong>局部最优解</strong></li>
</ul>
<p>设现在有一个问题可行解为<script type="math/tex">\overline{\boldsymbol x}</script>满足：<script type="math/tex">f(\overline{\boldsymbol x}) \leq f(\boldsymbol x), \forall \boldsymbol x \in \boldsymbol S \cap \boldsymbol N_{\epsilon}(\overline{\boldsymbol x})</script>即在<script type="math/tex">\overline{\boldsymbol x}</script>很小的范围内的函数值都要大于<script type="math/tex">\overline{\boldsymbol x}</script>处的函数值，此时<script type="math/tex">\overline{\boldsymbol x}</script>为局部最优解。</p>
<ul>
<li><strong>全局最优解</strong></li>
</ul>
<p>设现在有一个问题可行解为<script type="math/tex">\overline{\boldsymbol x}^*</script>满足：<script type="math/tex">f(\overline{\boldsymbol x}^*) \leq f(\boldsymbol x), \forall \boldsymbol x \in \boldsymbol S</script>即在整个可行域范围内的函数值都要大于<script type="math/tex">\overline{\boldsymbol x}^*</script>处的函数值，此时<script type="math/tex">\overline{\boldsymbol x}^*</script>为全局最优解。</p>
<p><strong>证明：</strong></p>
<p>设对于凸优化，<script type="math/tex">\overline{\boldsymbol x}</script>是局部最优解，但不是全局最优，即存在<script type="math/tex">{\boldsymbol x}^*</script>使得<script type="math/tex">f({\boldsymbol x}^*) < f(\overline{\boldsymbol x}), \forall \boldsymbol x \in \boldsymbol S</script>，取<script type="math/tex">\lambda \in (0,1)</script>，则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(\overline{\boldsymbol{x}}+\lambda\left(\boldsymbol{x}^{*}-\overline{\boldsymbol{x}}\right)\right) &\left.=f\left(\lambda \boldsymbol{x}^{*}+(1-\lambda) \overline{\boldsymbol{x}}\right)\right) \\
& \leq \lambda f\left(\boldsymbol{x}^{*}\right)+(1-\lambda) f(\overline{\boldsymbol{x}}) \\
&<f(\overline{\boldsymbol{x}})
\end{aligned}</script><p>当$\lambda \to 0$时，$\left(\overline{\boldsymbol{x}}+\lambda\left(\boldsymbol{x}^{*}-\overline{\boldsymbol{x}}\right)\right) $就非常趋近与<script type="math/tex">\overline{\boldsymbol x}</script>，即在<script type="math/tex">\overline{\boldsymbol x}</script>的邻域里面。那么就可以得出结论：凸优化的局部最优一定时全局最优，不然后产生矛盾。</p>
<h5 id="二-凸优化问题最优性条件"><a href="#二-凸优化问题最优性条件" class="headerlink" title="(二) 凸优化问题最优性条件"></a>(二) 凸优化问题最优性条件</h5><p>最优性条件其实就是在讨论优化问题的最优解要满足的条件：充分条件、必要条件、充要条件。</p>
<p>设<script type="math/tex">\overline{\boldsymbol x}^* \in \boldsymbol S</script>为某一最优化问题的最优解，则等价于(<strong>充要条件</strong>)：</p>
<script type="math/tex; mode=display">
\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*) \geq 0, \forall  \boldsymbol x \in \boldsymbol S</script><p><strong>证明——充分性：</strong>已知<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*) \geq 0, \forall  \boldsymbol x \in \boldsymbol S</script>，求证此时<script type="math/tex">\overline{\boldsymbol x}^* \in \boldsymbol S</script>为某一最优化问题的最优解。</p>
<p>首先<script type="math/tex">f(\overline{\boldsymbol x}^*) + \triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*)</script>是一个经过<script type="math/tex">(\overline{\boldsymbol x}^*, f(\overline{\boldsymbol x}^*))</script>点的切平面，因为目标函数<script type="math/tex">f</script>是凸函数，则函数在其切平面的上方，即满足：</p>
<script type="math/tex; mode=display">
f(\boldsymbol x) \geq f(\overline{\boldsymbol x}^*) + \triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*) \geq f(\overline{\boldsymbol x}^*)  , \forall \boldsymbol x \in \boldsymbol S</script><p>即，对于任意<script type="math/tex">\boldsymbol x</script>都有<script type="math/tex">f(\boldsymbol x) \geq  f(\overline{\boldsymbol x}^*)</script>，即<script type="math/tex">\overline{\boldsymbol x}^* \in \boldsymbol S</script>为最优化问题的最优解。</p>
<p><strong>证明——必要性：</strong>已知<script type="math/tex">\overline{\boldsymbol x}^* \in \boldsymbol S</script>为某一最优化问题的最优解，求证<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*) \geq 0, \forall  \boldsymbol x \in \boldsymbol S</script>。</p>
<p>(反证法)：设存在<script type="math/tex">\overline{\boldsymbol x} \in \boldsymbol S</script>使得<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\overline{\boldsymbol x} - \overline{\boldsymbol x}^*) < 0</script>，<script type="math/tex">\lambda \in (0,1)</script></p>
<script type="math/tex; mode=display">
f(\overline{\boldsymbol{x}}^*+\lambda(\overline{\boldsymbol{x}}-\overline{\boldsymbol{x}}^*))\overset{泰勒展开}{=} f(\overline{\boldsymbol{x}}^*) + \lambda \triangledown f (\overline{\boldsymbol{x}}^*)(\boldsymbol x - \overline{\boldsymbol x}^*) + o(\lambda||\boldsymbol x - \overline{\boldsymbol x}^*)||)</script><p>整理可得：</p>
<script type="math/tex; mode=display">
\frac{f(\overline{\boldsymbol{x}}^*+\lambda(\overline{\boldsymbol{x}}-\overline{\boldsymbol{x}}^*))-f(\overline{\boldsymbol{x}}^*)}{\lambda} = \triangledown f (\overline{\boldsymbol{x}}^*)(\boldsymbol x - \overline{\boldsymbol x}^*) + \frac{o(\lambda||\boldsymbol x - \overline{\boldsymbol x}^*)||)}{\lambda}</script><p>当<script type="math/tex">\lambda \to 0</script>时，<script type="math/tex">\dfrac{o(\lambda||\boldsymbol x - \overline{\boldsymbol x}^*)||)}{\lambda} \to 0</script>，<script type="math/tex">\triangledown f (\overline{\boldsymbol{x}}^*)(\boldsymbol x - \overline{\boldsymbol x}^*) < 0</script>，则：</p>
<script type="math/tex; mode=display">
\triangledown f (\overline{\boldsymbol{x}}^*)(\boldsymbol x - \overline{\boldsymbol x}^*) + \dfrac{o(\lambda||\boldsymbol x - \overline{\boldsymbol x}^*)||)}{\lambda} < 0</script><p>此时等式左边<script type="math/tex">\dfrac{f(\overline{\boldsymbol{x}}^*+\lambda(\overline{\boldsymbol{x}}-\overline{\boldsymbol{x}}^*))-f(\overline{\boldsymbol{x}}^*)}{\lambda} < 0</script>，即我们找到比<script type="math/tex">f(\overline{\boldsymbol x}^*)</script>函数值更小的，即与<script type="math/tex">\overline{\boldsymbol x}^*</script>是最优解矛盾。</p>
<p><strong>几何解释：</strong></p>
<script type="math/tex; mode=display">
\overline{\boldsymbol x}^* \in \boldsymbol S是最优解 \\
\Updownarrow \\
\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*) \geq 0, \forall  \boldsymbol x \in \boldsymbol S \\
\Updownarrow \\
-\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}\overline{\boldsymbol x}^* \geq -\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T} \boldsymbol x, \forall  \boldsymbol x \in \boldsymbol S \\
\Updownarrow  \\
令\boldsymbol \alpha = -\triangledown f(\overline{\boldsymbol x}^*) \\
\Updownarrow  \\
\boldsymbol \alpha^{\mathrm T} \overline{\boldsymbol x}^* \geq \boldsymbol \alpha^{\mathrm T} {\boldsymbol x}, \forall  \boldsymbol x \in \boldsymbol S</script><p>由上面的等价表达可知，若<script type="math/tex">\boldsymbol \alpha \neq \boldsymbol 0(\triangledown f(\overline{\boldsymbol x}^*) \neq \boldsymbol 0)</script>，则就找到了一个<strong>支撑超平面</strong>，该支撑超平面经过<script type="math/tex">\overline{\boldsymbol x}^*</script>，在该点处恰好支撑整个凸集合<script type="math/tex">\boldsymbol S</script>。</p>
<p><img src="https://pic.imgdb.cn/item/628f2d8e0947543129edcef9.jpg" style="zoom:35%"></p>
<p><strong>几类特殊凸问题的最优性条件：</strong></p>
<ul>
<li><p>无约束凸优化<script type="math/tex">\min f(\boldsymbol x) \text{ over } \mathbb{R}^n</script>：<script type="math/tex">\overline{\boldsymbol x}^*最优 \Longleftrightarrow  \triangledown f(\overline{\boldsymbol x}^*) = \boldsymbol 0</script></p>
<ul>
<li>根据前面的条件，可知：<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*) \geq 0</script>，由于没有约束条件，即<script type="math/tex">(\boldsymbol x - \overline{\boldsymbol x}^*)</script>可以取遍<script type="math/tex">\mathbb{R}^n</script>中的所有向量，那么此时<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*) = \boldsymbol 0</script>。</li>
</ul>
</li>
<li><p>等式约束凸问题<script type="math/tex">\min \begin{Bmatrix} f(\boldsymbol x) \mid \boldsymbol{Ax = b}\end{Bmatrix}</script>：<script type="math/tex">\overline{\boldsymbol x}^*最优 \Longleftrightarrow  ∃\boldsymbol \mu,使得 \triangledown f(\overline{\boldsymbol x}^*) + \boldsymbol A^{\mathrm T} \boldsymbol \mu= \boldsymbol 0, \boldsymbol A \overline{\boldsymbol x}^* = \boldsymbol b</script></p>
<ul>
<li>根据前面的条件，可知：<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*) \geq 0</script>，满足<script type="math/tex">\boldsymbol{Ax = b}, \boldsymbol{A\overline{\boldsymbol x}^* = b}</script>，令向量<script type="math/tex">\boldsymbol d = \boldsymbol x - \overline{\boldsymbol x}^*</script>，那么将满足等式约束的两个式子相减：<script type="math/tex">\boldsymbol{A(x - \overline{x}^*) = \boldsymbol{Ad}= 0}</script>，即向量<script type="math/tex">\boldsymbol d</script>在<script type="math/tex">\boldsymbol A</script>的<strong>零空间</strong>中。又由于向量<script type="math/tex">\boldsymbol d</script>在<script type="math/tex">\boldsymbol A</script>的零空间中，那么<script type="math/tex">-\boldsymbol d</script>同样在<script type="math/tex">\boldsymbol A</script>的零空间中，因此向量<script type="math/tex">\boldsymbol d</script>和<script type="math/tex">-\boldsymbol d</script>都是<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*) = \triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T} \boldsymbol d\geq 0</script>的解，也就是该不等式添加负号不等号不变方向，也就等价于<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}(\boldsymbol x - \overline{\boldsymbol x}^*) = \triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T} \boldsymbol d = 0</script>。由于<script type="math/tex">\boldsymbol d</script>在<script type="math/tex">\boldsymbol A</script>的<strong>零空间</strong>中，而<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T} \boldsymbol d = 0</script>，所以<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)^{\mathrm T}</script>在<script type="math/tex">\boldsymbol A</script>的正交补空间中，也就是<script type="math/tex">\boldsymbol A</script>的行空间，所以<script type="math/tex">\triangledown f(\overline{\boldsymbol x}^*)</script>可以表示成<script type="math/tex">\boldsymbol A^{\mathrm T}</script>的线性组合：</li>
</ul>
<script type="math/tex; mode=display">
∃\boldsymbol \mu,使得 \triangledown f(\overline{\boldsymbol x}^*) + \boldsymbol A^{\mathrm T} \boldsymbol \mu= \boldsymbol 0, \boldsymbol A \overline{\boldsymbol x}^* = \boldsymbol b</script><ul>
<li>这个就是<strong>KKT条件</strong>在本问题中的具体表现形式。</li>
</ul>
</li>
<li><p>非负约束凸优化<script type="math/tex">\min \begin{Bmatrix} f(\boldsymbol x) \mid \boldsymbol{x \geq 0}\end{Bmatrix}</script>：<script type="math/tex">\overline{\boldsymbol x}^*最优 \Longleftrightarrow  \triangledown f(\overline{\boldsymbol x}^*)_i  \overline{\boldsymbol x}^*_i= \boldsymbol 0, \overline{\boldsymbol x}^*>\boldsymbol 0, \triangledown f(\overline{\boldsymbol x}^*) \geq \boldsymbol 0</script></p>
<ul>
<li>证明暂略，比较复杂，看视频吧</li>
</ul>
</li>
</ul>
<h4 id="4-3-常见凸优化问题分类"><a href="#4-3-常见凸优化问题分类" class="headerlink" title="4.3 常见凸优化问题分类"></a>4.3 常见凸优化问题分类</h4><h5 id="一-线性规划-Linear-Programming-LP"><a href="#一-线性规划-Linear-Programming-LP" class="headerlink" title="(一) 线性规划 (Linear Programming, LP)"></a>(一) 线性规划 (Linear Programming, LP)</h5><p><strong>1 线性规划的标准形式</strong></p>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & \boldsymbol c^{\mathrm T}\boldsymbol x + d \\ 
\text{ s.t.} & \boldsymbol{Gx = b}, \\ 
& \boldsymbol{Hx \leq e}. \end{cases}</script><p>一言以蔽之，目标函数和约束条件都是<strong>仿射函数</strong>。 </p>
<p>注意，凸优化问题中很多形式可以通过转换变为标准形式的！！！如下面几种情况：</p>
<ul>
<li><strong>线性分式规划</strong></li>
</ul>
<p><img src="https://pic.imgdb.cn/item/628f8221094754312973562a.jpg" style="zoom:40%"></p>
<ul>
<li><strong>最小化绝对值函数</strong></li>
</ul>
<script type="math/tex; mode=display">
\min  \begin{Bmatrix}|\boldsymbol a^{\mathrm T} \boldsymbol x + \boldsymbol c| \boldsymbol{AX = b}  \end{Bmatrix}</script><p><img src="https://pic.imgdb.cn/item/628f83b1094754312976493f.jpg" style="zoom:40%"></p>
<ul>
<li><strong>最小化多面体函数</strong></li>
</ul>
<p><img src="https://pic.imgdb.cn/item/628f85720947543129793020.jpg" style="zoom:40%"></p>
<p><strong>2 标准解法——单纯形法</strong></p>
<p>单纯形法的基本思想是——基于线性规划的可行集是一个多面体，目标函数是线性的，绘制其等值面的时候是一组平行的线/面，找到多面体的支撑超平面。所以说，如果线性规划存在最优解，则可在<strong>极点</strong>(多面体的顶点)达到。</p>
<p>若<script type="math/tex">\overline{\boldsymbol x}</script>是一个极点，则需判断<script type="math/tex">\overline{\boldsymbol x}</script>是否是最优解：</p>
<ul>
<li>若是最优解，则求解完成；</li>
<li>若不是最优解，则需从<script type="math/tex">\overline{\boldsymbol x}</script>出发，找一个更优的极点。</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/628f40da09475431290aaad7.jpg" style="zoom:50%"></p>
<blockquote>
<p>详细参见：<a href="https://blog.csdn.net/whn19980801/article/details/115741791?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-115741791-blog-105621647.pc_relevant_default&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4">线性规划问题（LP问题）</a>， <a href="https://www.zhihu.com/question/308189306">解决线性规划问题为什么要加入松弛变量？</a></p>
</blockquote>
<h5 id="二-凸二次规划-Quadratic-Programming-QP"><a href="#二-凸二次规划-Quadratic-Programming-QP" class="headerlink" title="(二) 凸二次规划 (Quadratic Programming, QP)"></a>(二) 凸二次规划 (Quadratic Programming, QP)</h5><p><strong>1 凸二次规划基本形式</strong></p>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & \frac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q \boldsymbol x +  \boldsymbol c^{\mathrm T}  \boldsymbol x \\ 
\text{ s.t.} & \boldsymbol{Ax = b}, \\ 
& \boldsymbol{x \geq 0}. \end{cases}</script><p>其中，<script type="math/tex">\boldsymbol Q \geq \boldsymbol 0</script>。</p>
<p><strong>2 标准解法——有效集法</strong></p>
<blockquote>
<p>后面讲解。</p>
</blockquote>
<p><strong>3 示例</strong></p>
<ul>
<li>均值-方差模型</li>
<li>最小二乘模型</li>
</ul>
<h5 id="三-带凸二次约束的二次规划-Quadraticlly-Constrained-Quadratic-Program-QCQP"><a href="#三-带凸二次约束的二次规划-Quadraticlly-Constrained-Quadratic-Program-QCQP" class="headerlink" title="(三) 带凸二次约束的二次规划(Quadraticlly Constrained Quadratic Program, QCQP)"></a>(三) 带凸二次约束的二次规划(Quadraticlly Constrained Quadratic Program, QCQP)</h5><p><strong>1 基本形式</strong></p>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & \frac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q_0 \boldsymbol x +  \boldsymbol c_0^{\mathrm T}  \boldsymbol x \\ 
\text{ s.t.} & \frac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q_i \boldsymbol x +  \boldsymbol c_i^{\mathrm T}  \boldsymbol x + b_i \leq 0, \text{ } i = 1,2, \cdots, k \\ 
& \boldsymbol{Ax \leq d}. \end{cases}</script><p>其中，<script type="math/tex">\boldsymbol Q_i \geq \boldsymbol 0, i = 0, 1, \cdots, k.</script>。</p>
<h5 id="四-二阶锥规划-Second-Order-Cone-Program-SOCP"><a href="#四-二阶锥规划-Second-Order-Cone-Program-SOCP" class="headerlink" title="(四) 二阶锥规划 (Second-Order Cone Program, SOCP)"></a>(四) 二阶锥规划 (Second-Order Cone Program, SOCP)</h5><p><strong>1 基本形式</strong></p>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & \boldsymbol f^{\mathrm T}\boldsymbol x \\ 
\text{ s.t.} & \|\boldsymbol{A_i x + b_i}  \|_2 \leq \boldsymbol c_i^{\mathrm T}\boldsymbol x + \boldsymbol d_i,  \text{ } i = 1,2, \cdots, k \\ 
& \boldsymbol{Hx \leq e}. \end{cases}</script><p>当$k = 0$时，为LP问题；当所有$\boldsymbol {c_i = 0}$时，为QCQP问题。</p>
<h5 id="五-半定规划-Semidefinite-Program-SDP"><a href="#五-半定规划-Semidefinite-Program-SDP" class="headerlink" title="(五) 半定规划 (Semidefinite Program, SDP)"></a>(五) 半定规划 (Semidefinite Program, SDP)</h5><p><strong>1 半定规划标准形式</strong></p>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & \text{tr}(\boldsymbol C\boldsymbol X) \\ 
\text{ s.t.} & \text{tr}(\boldsymbol Q_i\boldsymbol X) = b_i,  \text{ } i = 1,2, \cdots, m \\ 
& \boldsymbol{x \geq 0}. \end{cases}</script><p>其中，<script type="math/tex">\boldsymbol C, \boldsymbol Q_i</script>均为对称矩阵。</p>
<p><strong>转化为：线性矩阵不等式形式(LMI)：</strong></p>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & \boldsymbol c^{\mathrm T}\boldsymbol x \\ 
\text{ s.t.} & \sum\limits_{i=1}^{n} {\boldsymbol {x_iQ_i}} \leq \boldsymbol Q_0 \\ 
& \boldsymbol{Ax \geq b}. \end{cases}</script><blockquote>
<p>注意：若矩阵<script type="math/tex">\boldsymbol {Q_1 \leq Q_2}</script>，则有<script type="math/tex">\boldsymbol {Q_2 - Q_1}</script>是半正定矩阵。</p>
</blockquote>
<h5 id="六-几何规划-Geometric-Programming-GP"><a href="#六-几何规划-Geometric-Programming-GP" class="headerlink" title="(六) 几何规划  (Geometric Programming, GP)"></a>(六) 几何规划  (Geometric Programming, GP)</h5><p><strong>1 例子：切比雪夫中心(集合中心)</strong></p>
<ul>
<li>给定<strong>有界</strong>的集合<script type="math/tex">\boldsymbol C, \boldsymbol x \in \boldsymbol C</script>的深度定义为：</li>
</ul>
<script type="math/tex; mode=display">
\text{depth}(\boldsymbol x, \boldsymbol C) = \text{dist}(\boldsymbol x, \mathbb{R}^n(去除 \boldsymbol C) )</script><p>可以理解为集合<script type="math/tex">\boldsymbol C</script>中的点到集合<script type="math/tex">\boldsymbol C</script>边界的距离。</p>
<ul>
<li>集合<script type="math/tex">\boldsymbol C</script>的中心(chebyshev center)定义为具有最大深度的点：</li>
</ul>
<script type="math/tex; mode=display">
\boldsymbol x_{\text{center}}(\boldsymbol C) = \arg \max_{\boldsymbol x \in \boldsymbol C} \text{dist}(\boldsymbol x, \mathbb{R}^n(去除 \boldsymbol C) )</script><ul>
<li>集合的中心即包含于该集合的最大球体的球心。</li>
</ul>
<p><strong>2 凸集合的中心问题为凸优化问题</strong></p>
<p>(1) 假设<script type="math/tex">\boldsymbol S = \begin{Bmatrix} \boldsymbol x \mid f_{i}(\boldsymbol x) \leq 0, \text{  } i=1, \cdots, m \end{Bmatrix}</script>，其中<script type="math/tex">f_{i}(\boldsymbol x)</script>是凸函数，寻找包含<script type="math/tex">\boldsymbol S</script>的最大球体。</p>
<p>设该球体球心为<script type="math/tex">\boldsymbol x</script>，半径为<script type="math/tex">r</script>，该球可以表示为：<script type="math/tex">\boldsymbol B =  \begin{Bmatrix} \boldsymbol x+r \boldsymbol u \mid \| \boldsymbol u \|_2 \leq 1 \end{Bmatrix}</script></p>
<script type="math/tex; mode=display">
\begin{cases}\text { max } &\text { } \text { } r \\ 
\text{ s.t.} & \sup\limits_{\| \boldsymbol u \|_2 \leq 1} f_i(\boldsymbol x+r \boldsymbol u) \leq 0 , \text{  } i=1, \cdots, m\\ 
 \end{cases}</script><blockquote>
<p>注意：<a href="https://blog.csdn.net/hyl1181/article/details/111029319">max/sup、min/inf辨析</a>，<a href="https://blog.csdn.net/robert_chen1988/article/details/81233738">sup, inf 与 min, max</a></p>
<p>使用 inf 或 sup 总能保证一个函数的 inf 或 sup 存在，而函数的 min 或 max 有时候不存在。 </p>
</blockquote>
<p>(2) 多面体的中心：假设多面体<script type="math/tex">\boldsymbol P = \begin{Bmatrix} \boldsymbol x \in \R ^n\mid \boldsymbol a_i^{\mathrm T} \boldsymbol x \leq \boldsymbol b_i, \text{  } i=1, \cdots, m \end{Bmatrix}</script>，寻找包含<script type="math/tex">\boldsymbol P</script>的最大球体，其球心即为集合<script type="math/tex">\boldsymbol P</script>的中心。</p>
<p>通过题目，可将优化问题抽象如下：(球体可以表示为：<script type="math/tex">\boldsymbol B =  \begin{Bmatrix} \boldsymbol x+r \boldsymbol u \mid \| \boldsymbol u \|_2 \leq 1 \end{Bmatrix}</script>)</p>
<script type="math/tex; mode=display">
\begin{cases}\text { max } &\text { } \text { } r \\ 
\text{ s.t.} &  \boldsymbol a_i^{\mathrm T} \boldsymbol x + \|\boldsymbol a_i  \|_2 r \leq \boldsymbol b_i, \text{  } i=1, \cdots, m\\ 
 \end{cases}</script><p><img src="https://pic.imgdb.cn/item/62902b2809475431293274b3.jpg" style="zoom:65%"></p>
<p>(3) 椭球交集的中心：假设<script type="math/tex">\boldsymbol S = \begin{Bmatrix} \boldsymbol x \in \R ^n \mid \boldsymbol x^{\mathrm T}\boldsymbol A_i \boldsymbol x + 2\boldsymbol b_i^{\mathrm T}\boldsymbol x + c_i \leq 0, \text{  } i=1, \cdots, m \end{Bmatrix}</script>，其中<script type="math/tex">\boldsymbol A_i \geq \boldsymbol 0</script>，寻找包含<script type="math/tex">\boldsymbol S</script>的最大球体和球心。</p>
<p>通过问题描述，可将该问题建模为：</p>
<script type="math/tex; mode=display">
\begin{cases} \max & r \\

\text { s.t. } & \left(\begin{array}{ccc}
\boldsymbol A_{i}^{-1} & r l & -\left( \boldsymbol x+\boldsymbol A_{i}^{-1} \boldsymbol b_{i} \right) \\
r l & \lambda_{i} l & 0 \\
-\left( \boldsymbol x+\boldsymbol A_{i}^{-1}\boldsymbol  b_{i}\right)^{T} & 0 & -\lambda_{i}-c_{i}+\boldsymbol b_{i}^{T} \boldsymbol A_{i}^{-1}\boldsymbol b_{i}
\end{array}\right) \geq 0,
 \end{cases}</script><p>本问题涉及两个小知识点：</p>
<ul>
<li>S-Procedure：<ul>
<li>$x^{\top} F_{1} x+2 b_{1}^{\top} x+c_{1} \leq 0 \Rightarrow x^{\top} F_{2} x+2 b_{2}^{\top} x+c_{2} \leq 0$，当且仅当$\exist \lambda \geq 0$使得$\left(\begin{array}{ll} F_{1} &amp; b_{1} \\ b_{1}^{T} &amp; c_{1} \end{array}\right)-\left(\begin{array}{ll} F_{2} &amp; b_{2} \\ b_{2}^{T} &amp; c_{2} \end{array}\right) \geq 0$</li>
</ul>
</li>
<li>Schur complement 对称分块矩阵：<ul>
<li>如<script type="math/tex">X=\left(\begin{array}{cc} A & B \\ B^{T} & C \end{array}\right)</script>，若<script type="math/tex">\boldsymbol A \geq \boldsymbol 0</script>，则<script type="math/tex">\boldsymbol X \geq \boldsymbol 0</script>当且仅当<script type="math/tex">C-B^{\top} A^{-1} B \geq0</script></li>
</ul>
</li>
</ul>
<hr>
<p>参考资料：</p>
<p>【1】<a href="https://zhuanlan.zhihu.com/p/32926848">为啥要知道一个对称方阵是否为各种定呢</a></p>
<p>【2】<a href="https://zhuanlan.zhihu.com/p/44860862">浅谈「正定矩阵」和「半正定矩阵」</a></p>
<p>【3】<a href="https://zhuanlan.zhihu.com/p/62589178">正定(positive definite)与半正定(semi-positive definite)</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>数学基础</category>
        <category>最优化方法</category>
      </categories>
      <tags>
        <tag>最优化</tag>
      </tags>
  </entry>
  <entry>
    <title>最优化方法_part2</title>
    <url>/2022/05/27/Optimization-method2/</url>
    <content><![CDATA[<h3 id="第五讲-无约束优化问题（壹）"><a href="#第五讲-无约束优化问题（壹）" class="headerlink" title="第五讲 无约束优化问题（壹）"></a>第五讲 无约束优化问题（壹）</h3><h4 id="5-1-无约束优化问题"><a href="#5-1-无约束优化问题" class="headerlink" title="5.1 无约束优化问题"></a>5.1 无约束优化问题</h4><h5 id="一-无约束优化问题：-min-f-boldsymbol-x"><a href="#一-无约束优化问题：-min-f-boldsymbol-x" class="headerlink" title="(一) 无约束优化问题：$ \min f(\boldsymbol x)$"></a>(一) 无约束优化问题：$ \min f(\boldsymbol x)$</h5><ul>
<li>最小二乘问题：<script type="math/tex">\min \| \boldsymbol{Ax-b} \|</script>；</li>
<li>采用适当的方法可将约束优化问题转换为无约束优化问题；</li>
<li>最优解的定义：<ul>
<li>局部最优解<script type="math/tex">\overline{\boldsymbol x}</script>：<script type="math/tex">\forall \boldsymbol x \in \boldsymbol N_{\epsilon}(\overline{\boldsymbol x}), f({\boldsymbol x}) \geq f(\overline{\boldsymbol x})</script></li>
<li>全局最优解<script type="math/tex">\boldsymbol x^*</script>：<script type="math/tex">\forall \boldsymbol x \in \mathbb{R}^n, f({\boldsymbol x}) \geq f(\boldsymbol x^*)</script></li>
<li>严格局部/全局最优解：即上面式子提到的<script type="math/tex">\geq \to ></script></li>
</ul>
</li>
</ul>
<h5 id="二-无约束优化问题最优性条件"><a href="#二-无约束优化问题最优性条件" class="headerlink" title="(二) 无约束优化问题最优性条件"></a>(二) 无约束优化问题最优性条件</h5><p>考虑无约束优化问题：<script type="math/tex">\min f(\boldsymbol x)</script>：</p>
<ul>
<li>若<script type="math/tex">f(\boldsymbol x)</script>为凸函数，则<script type="math/tex">\boldsymbol x^*</script>是最优解等级于<script type="math/tex">\triangledown f(\boldsymbol x^*) = \boldsymbol 0</script></li>
<li>若<script type="math/tex">f(\boldsymbol x)​</script>为一般函数(即不一定是凸函数)：<ul>
<li>必要条件：若<script type="math/tex">\boldsymbol x^*</script>是最优解，则<script type="math/tex">\triangledown f(\boldsymbol x^*) = \boldsymbol 0</script>；<script type="math/tex">\triangledown^2 f(\boldsymbol x^*) \succeq  \boldsymbol 0, (即黑塞矩阵半正定)</script><ul>
<li>上面两条分别对应<strong>“一阶必要条件”</strong>、<strong>“二阶必要条件”</strong></li>
<li><a href="https://zhuanlan.zhihu.com/p/370959560">泰勒展开形式</a></li>
</ul>
</li>
<li>充分条件：若<script type="math/tex">\triangledown f(\boldsymbol x^*) = \boldsymbol 0</script>且<script type="math/tex">\triangledown^2 f(\boldsymbol x^*) \succ \boldsymbol 0, (即黑塞矩阵半正定)</script>，则<script type="math/tex">\boldsymbol x^*</script>是<strong>严格最优解</strong>(局部？)</li>
</ul>
</li>
</ul>
<blockquote>
<p>修改：判定(半)正定矩阵的特殊大于(等于)简写符号为：<script type="math/tex">\succ, \succeq</script>，并不是<script type="math/tex">>, \geq</script></p>
</blockquote>
<h4 id="5-2-无约束优化算法概要"><a href="#5-2-无约束优化算法概要" class="headerlink" title="5.2 无约束优化算法概要"></a>5.2 无约束优化算法概要</h4><h5 id="一-迭代下降算法"><a href="#一-迭代下降算法" class="headerlink" title="(一) 迭代下降算法"></a>(一) 迭代下降算法</h5><p>给定初始点<script type="math/tex">\boldsymbol x_0</script>，产生点列<script type="math/tex">\begin{Bmatrix} \boldsymbol x_k \end{Bmatrix}_{k=1}^{\infty}</script>，并且<script type="math/tex">f(\boldsymbol x_{k+1}) < f(\boldsymbol x_k)</script>。</p>
<ul>
<li><p>如何从当前点<script type="math/tex">\boldsymbol x_k</script>得到下一个迭代点<script type="math/tex">\boldsymbol x_{k+1}</script>？</p>
<ul>
<li><p><strong>策略1：线搜索方法</strong></p>
<ul>
<li>当前点<script type="math/tex">\boldsymbol x_k</script>，从这个点出发，先找到某一个下降方向<script type="math/tex">\boldsymbol d_k</script>(即沿着该方向函数值减小)，然后需要设置在下降方向走多远(步长)<script type="math/tex">\alpha_k</script>，最终，得到下一个点为：<script type="math/tex">\boldsymbol x_{k+1} = \boldsymbol x_{k} + \alpha_k \boldsymbol d_k</script></li>
</ul>
</li>
<li><p><strong>策略2：信赖域方法</strong></p>
<ul>
<li><p>该方法有点和线搜索步骤反过来的意味，即先确定好需要前进的区域，然后选择前进的方向。</p>
</li>
<li><p>设前进区域设为<script type="math/tex">\boldsymbol p</script>，给定其约束为<script type="math/tex">\Delta</script>，可以表述为</p>
</li>
<li><script type="math/tex; mode=display">
\begin{cases}\text { min } & f(\boldsymbol x_{k} + \boldsymbol p) \\ 
\text{ s.t.} & \| \boldsymbol p \|_2 \leq \Delta \end{cases}</script><blockquote>
<p>注意：在线搜索方法中，选择步长是一个<strong>一元/一维</strong>的问题；但信赖域方法是一个<strong>N维</strong>问题($f$可能是一个比较复杂的函数)，因此信赖域方法直接求解复杂度较大。所以信赖域方法在求解时并不是直接求解上述优化，而是需要<strong><font color="blue">先用一个简单的函数$m_k(\boldsymbol p)$对$f$在$\boldsymbol x_k$点进行近似</font></strong>，比如用泰勒展开等方法。</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & m_k(\boldsymbol p) \\ 
\text{ s.t.} & \| \boldsymbol p \|_2 \leq \Delta \end{cases}</script><blockquote>
<p>信赖域方法有几点需要考虑：</p>
<p>(1) <script type="math/tex">\Delta</script>如何确定；(2) <script type="math/tex">m_k(\boldsymbol p)</script>如何选择，误差较大怎么办。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="二-基于线搜索-下降算法基本思路"><a href="#二-基于线搜索-下降算法基本思路" class="headerlink" title="(二) (基于线搜索)下降算法基本思路"></a>(二) (基于线搜索)下降算法基本思路</h5><ul>
<li>STEP1：给定初始点<script type="math/tex">\boldsymbol x_0, k=0</script></li>
<li>STEP2：判断<script type="math/tex">\boldsymbol x_k</script>是否满足终止条件；是则终止</li>
<li>STEP3：否则寻找<script type="math/tex">\boldsymbol x_k</script>处的下降方向<script type="math/tex">\boldsymbol d_k</script></li>
<li>STEP4：设置步长<script type="math/tex">\alpha_k</script>，使得<script type="math/tex">f(\boldsymbol x_{k} + \alpha_k \boldsymbol d_k) < f(\boldsymbol x_{k+1})</script></li>
<li>STEP5：令<script type="math/tex">\boldsymbol x_{k+1} = \boldsymbol x_k + \alpha_k \boldsymbol d_k</script>；令<script type="math/tex">k=k+1</script>，转STEP1。</li>
</ul>
<p><strong>1 终止条件</strong></p>
<p>常用的终止条件：</p>
<script type="math/tex; mode=display">
\| \triangledown f(\boldsymbol x_k) \|_2 \leq \varepsilon</script><p>其中，<script type="math/tex">\varepsilon</script>理论上取0，但在实际编程中一般取一个量级很小的数。此外要注意对与凸函数，此条件一定是最优解，但是非凸函数该终止条件找到的点就不一定是最优解了。</p>
<p>其他终止条件：</p>
<script type="math/tex; mode=display">
\|\boldsymbol x_k - \boldsymbol x_{k-n} \|_2 \leq \varepsilon</script><script type="math/tex; mode=display">
f(\boldsymbol x_k) - f(\boldsymbol x_{k-n}) \leq \varepsilon</script><p><strong>2 下降方向</strong></p>
<p>常用的下降方向——<strong>负梯度方向</strong></p>
<script type="math/tex; mode=display">
\boldsymbol d_k = -\triangledown f(\boldsymbol x_k)</script><p>该方法又称<strong>最速下降法</strong>（Steepest descent）是梯度<strong>下降法</strong>的一种更具体实现形式。</p>
<p>其他下降方向选择：牛顿方向、共轭梯度方向等。</p>
<p><strong>3 步长</strong></p>
<p>令<script type="math/tex">\phi(\alpha) :=  f(\boldsymbol x_{k} + \alpha \boldsymbol d_k)</script>，为一元函数。则可建模为：</p>
<script type="math/tex; mode=display">
\min \text{ }\phi(\alpha)</script><p>在确定$\alpha_k$的过程称为<strong>一维线搜索</strong>。</p>
<p><strong>4 点列收敛性/收敛速度</strong></p>
<p>我们的算法会产生一个点列<script type="math/tex">\begin{Bmatrix} \boldsymbol x_k \end{Bmatrix}_{k=1}^{\infty}</script>，若点列不收敛，则可认为该算法不好。另外，收敛速度越快，算法越好。</p>
<h5 id="三-求解步长——线搜索方法详解"><a href="#三-求解步长——线搜索方法详解" class="headerlink" title="(三) 求解步长——线搜索方法详解"></a>(三) 求解步长——线搜索方法详解</h5><p>求解一元问题：</p>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & \phi(\alpha) :=  f(\boldsymbol x_{k} + \alpha \boldsymbol d_k) \\ 
\text{ s.t.} & \alpha > 0 \end{cases}</script><p>其解记为<script type="math/tex">\alpha^*</script>。</p>
<p><strong>1 <script type="math/tex">f(\boldsymbol x)</script>为一个简单函数</strong></p>
<p>若<script type="math/tex">f(\boldsymbol x)</script>为一个简单函数，如二次函数：<script type="math/tex">f(\boldsymbol x) = \dfrac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol H \boldsymbol x + \boldsymbol c^{\mathrm T} \boldsymbol x + b, \text{} \boldsymbol H(黑塞矩阵) \succ 0</script>。黑塞矩阵正定则$f$为凸函数，则<script type="math/tex">\phi(\alpha)</script>也是凸函数。代入可得：</p>
<script type="math/tex; mode=display">
\begin{align*}
  \phi(\alpha) & = \dfrac{1}{2} {(\boldsymbol x_{k} + \alpha \boldsymbol d_k)}^{\mathrm T} \boldsymbol H {(\boldsymbol x_{k} + \alpha \boldsymbol d_k)} + \boldsymbol c^{\mathrm T} {(\boldsymbol x_{k} + \alpha \boldsymbol d_k)} + b \\
    &= \dfrac{1}{2} (\boldsymbol d_k^{\mathrm T} \boldsymbol H {\boldsymbol d_k})\alpha^2 +(\boldsymbol d_k^{\mathrm T} \boldsymbol H {\boldsymbol x_k} + \boldsymbol c^{\mathrm T}\boldsymbol d_k) \alpha + b \\
\end{align*}</script><p>对于开口向上的二次函数，求最小，即导数为零即可。</p>
<script type="math/tex; mode=display">
\phi(\alpha)' = 0</script><p><strong>2 <script type="math/tex">f(\boldsymbol x)</script>为一个复杂非线性函数</strong></p>
<p>若<script type="math/tex">f(\boldsymbol x)</script>为一个复杂非线性函数，则<script type="math/tex">\min \phi(\alpha)</script>问题就变得复杂起来了。下面简单介绍几种常见的处理方法：</p>
<ul>
<li><p><strong>基于搜索区间的直接搜索法</strong></p>
<ul>
<li>首先找到一个搜索区间，想办法缩小搜索区间(注意缩小时不用吧$\alpha^*$扔掉)，缩小到一定程度时，则可将区间内的一点作为最优解的近似。</li>
<li>搜索区间<script type="math/tex">[ a_0, b_0]</script>：需要包含<script type="math/tex">\alpha^*</script>；<strong>单谷</strong>(即在搜索区间内，函数只有一个下凸，且要注意以下推导都基于此前提)</li>
<li>方法：选取<script type="math/tex">\lambda, \mu \in (a_0, b_0)</script>且满足<script type="math/tex">\lambda < \mu</script><ul>
<li>若<script type="math/tex">\phi(\lambda) < \phi(\mu)</script>，则得到新的搜索区间为<script type="math/tex">[ a_1, b_1] = [ a_0, \mu]</script></li>
<li>若<script type="math/tex">\phi(\lambda) > \phi(\mu)</script>，则得到新的搜索区间为<script type="math/tex">[ a_1, b_1] = [ \lambda, b_0]</script></li>
</ul>
</li>
<li><strong>常见的直接搜索法：</strong><ul>
<li><strong>均匀搜索法：</strong><ul>
<li>令<script type="math/tex">\delta = (b_0 - a_0)/N, \alpha_i = a_0 + i \delta, \text{ } i=1,2, \cdots, N-1</script></li>
<li>比较相邻3个点对应的函数值，若对于某个<script type="math/tex">i</script>，有<script type="math/tex">\phi(\alpha_{i-1})>\phi(\alpha_i)<\phi(\alpha_{i+1})</script>，则<script type="math/tex">\alpha^* \in [\alpha_{i-1}, \alpha_{i+1}]</script>；</li>
<li>得到新的搜索区间<script type="math/tex">[ a_1, b_1] =  [\alpha_{i-1}, \alpha_{i+1}]</script></li>
</ul>
</li>
<li><strong>黄金区间法(0.618法)：</strong><ul>
<li>令<script type="math/tex">\tau = (\sqrt 5 - 1)/2 =0.618, \text{ }\lambda = a_0 + (1 - \tau)(b_0-a_0), \text{ } \mu = a_0 + \tau (b_0 - a_0)</script></li>
<li>若<script type="math/tex">\phi(\lambda) < \phi(\mu)</script>，则<script type="math/tex">\alpha^* \in [a_0, \mu]</script>，即新的搜索区间；</li>
</ul>
</li>
<li><strong>基于导数信息的二分法</strong><ul>
<li>记区间中点<script type="math/tex">\lambda = \dfrac{b_0+a_0}{2}</script>，计算该点的导数值<script type="math/tex">\phi'(\lambda)</script>；</li>
<li>若<script type="math/tex">\phi'(\lambda) = 0</script>，则<script type="math/tex">\alpha^* = \lambda</script>；</li>
<li>若<script type="math/tex">\phi'(\lambda) < 0</script>，则<script type="math/tex">\alpha^* \in[\lambda, b_0]</script>；</li>
<li>若<script type="math/tex">\phi'(\lambda) > 0</script>，则<script type="math/tex">\alpha^* \in [a_0, \lambda]</script>；</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>非精确线搜索</strong></p>
<blockquote>
<p>实际下降算法中，步长的选取除了直接搜索法以外，还有其他一下方法，我们认为只要<script type="math/tex">\alpha</script>满足一定的准则，此时即使<script type="math/tex">\alpha</script>不是精确解也可以作为最终解的近似解。</p>
</blockquote>
<ul>
<li><strong>Armijo条件：</strong><script type="math/tex">\phi(\alpha) \leq \phi(0) + c_1\alpha \phi'(0), \text{ } 0 \leq c_1 \leq 1</script></li>
</ul>
<p><img src="https://pic.imgdb.cn/item/629197cf0947543129174487.jpg" style="zoom:50%"></p>
<ul>
<li><strong>Goldstein法则：</strong>除Armijo条件外还要去<script type="math/tex">\mu</script>满足：<script type="math/tex">\phi(\alpha) > \phi(0) + c_2\alpha \phi'(0), \text{ } 0 \leq c_1 \leq c_2\leq 1</script></li>
</ul>
</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/629198a00947543129183707.jpg" style="zoom:45%"></p>
<h5 id="四-补充：收敛性"><a href="#四-补充：收敛性" class="headerlink" title="(四) 补充：收敛性"></a>(四) 补充：收敛性</h5><p>基于线搜索的迭代下降算法</p>
<ul>
<li>为保证全局收敛，步长<script type="math/tex">\alpha_k​</script>和下降方向<script type="math/tex">\boldsymbol d_k​</script>的选取均需满足定的条件：<ul>
<li>记当前点<script type="math/tex">\boldsymbol x_k</script>处所选取下降方向<script type="math/tex">\boldsymbol d_k</script>与负梯度方向<script type="math/tex">-\triangledown f(\boldsymbol x_k)</script>的夹角为<script type="math/tex">\theta_k</script>：</li>
<li><script type="math/tex; mode=display">\cos \theta_k = \dfrac{-\triangledown f(\boldsymbol x_k)^{\mathrm T}\boldsymbol d_k}{\|\triangledown f(\boldsymbol x_k) \| \| \boldsymbol d_k\|}</script></li>
<li></li>
</ul>
</li>
</ul>
<p>Zoutendijk定理说明：</p>
<blockquote>
<p>为了保证全局收敛，需要在选取步长和下降方向时候注意满足一定的条件</p>
<p>此外注意区分：全局收敛与收敛到全局最优解的区别</p>
<p>(1) 全局收敛是指在迭代下降算法初始点选取时，任意随机选取初始点都能保证所产生的点列满足收敛性；</p>
<p>(2) 收敛到全局最优解</p>
</blockquote>
<ul>
<li><p>在<strong>适当条件</strong>下，迭代点列<script type="math/tex">\boldsymbol x_k</script>满足：<script type="math/tex">\sum\limits_{k=1}^{\infty} \cos^2 \theta_k \|\triangledown f(\boldsymbol x_k) \|^2 < \infty</script>，点列收敛</p>
</li>
<li><p>当每次选取<script type="math/tex">\boldsymbol d_k</script>时均保证<script type="math/tex">\theta_k</script>满足<script type="math/tex">\cos \theta_k \geq \delta, \delta>0</script>，则：<script type="math/tex">\|\triangledown f(\boldsymbol x_k) \| \to 0</script></p>
<blockquote>
<p>通俗讲就是不要让下降方向<script type="math/tex">\boldsymbol d_k</script>与负梯度方向<script type="math/tex">-\triangledown f(\boldsymbol x_k)</script>的夹角太接近<script type="math/tex">\dfrac{\pi}{2}</script>，夹角尽量小。</p>
</blockquote>
</li>
</ul>
<p>Zoutendijk定理中的适当条件和证明：</p>
<p>假设函数<script type="math/tex">f(\boldsymbol x)</script>有下界(保证有解存在)，且<script type="math/tex">\triangledown f(\boldsymbol x)</script>是Lipschitz连续的，即存在<script type="math/tex">L>0</script>使得：</p>
<script type="math/tex; mode=display">
\|\triangledown f(\boldsymbol x) - \triangledown f(\tilde{\boldsymbol x})  \| \leq L \|\boldsymbol x -  \tilde{\boldsymbol x} \|, \forall \boldsymbol x, \tilde{\boldsymbol x}</script><p>记迭代过程为<script type="math/tex">\boldsymbol x_{k+1} = \boldsymbol x_k + \alpha_k \boldsymbol d_k</script>，则</p>
<script type="math/tex; mode=display">
\sum\limits_{k=1}^{\infty} \cos^2 \theta_k \|\triangledown f(\boldsymbol x_k) \|^2 < \infty</script><p>证明部分没有细看，太复杂了。。。！！！</p>
<blockquote>
<p>此外，这里感觉很难理解，找了一些参考材料，有时间看看吧</p>
<p><a href="https://blog.csdn.net/u010182633/article/details/76554599">漫步最优化二十一——全局收敛</a>、<a href="https://blog.csdn.net/qq_36342854/article/details/103737901">约束优化方法_1_——Zoutendijk可行方向法</a>、<a href="https://zhuanlan.zhihu.com/p/121001066">数值优化|笔记整理（2）——线搜索：步长选取条件的收敛性 - 学弱猹的文章 - 知乎</a></p>
</blockquote>
<h3 id="第六讲-无约束优化问题（贰）"><a href="#第六讲-无约束优化问题（贰）" class="headerlink" title="第六讲 无约束优化问题（贰）"></a>第六讲 无约束优化问题（贰）</h3><p><img src="https://pic.imgdb.cn/item/629311dc09475431290ae7c7.jpg" style="zoom:50%"></p>
<h4 id="6-1-收敛速度"><a href="#6-1-收敛速度" class="headerlink" title="6.1 收敛速度"></a>6.1 收敛速度</h4><p>设序列<script type="math/tex">\begin{Bmatrix} \boldsymbol x_k \end{Bmatrix}_{k=1}^{\infty}</script>收敛到<script type="math/tex">\boldsymbol x^*</script>，若存在极限：</p>
<script type="math/tex; mode=display">
\lim_{k \to \infty} \dfrac{\|\boldsymbol x_{k+1} - \boldsymbol x^* \|}{\|\boldsymbol x_{k} - \boldsymbol x^* \|} = \beta</script><ul>
<li>当<script type="math/tex">0 < \beta < 1</script>时，则称<script type="math/tex">\begin{Bmatrix} \boldsymbol x_k \end{Bmatrix}_{k=1}^{\infty}</script>为线性收敛；</li>
<li>当<script type="math/tex">\beta = 0</script>时，则称<script type="math/tex">\begin{Bmatrix} \boldsymbol x_k \end{Bmatrix}_{k=1}^{\infty}</script>为超线性收敛。</li>
</ul>
<p>若存在某个 <script type="math/tex">p \geq 1</script>，有：</p>
<script type="math/tex; mode=display">
\lim_{k \to \infty} \dfrac{\|\boldsymbol x_{k+1} - \boldsymbol x^* \|}{\|\boldsymbol x_{k} - \boldsymbol x^* \|^p} = \beta < +\infty</script><p>则称<script type="math/tex">\begin{Bmatrix} \boldsymbol x_k \end{Bmatrix}_{k=1}^{\infty}</script>为<script type="math/tex">p</script>阶收敛。当<script type="math/tex">p > 1</script>时，<script type="math/tex">p</script>阶收敛必定为超线性收敛，但反之不一定成立。</p>
<p> 评价一个算法除了收敛速度，还可以考虑把一个算法用于求解一个简单问题，如果该算法在求解简单问题时都会有较大的计算成本，那么说明该算法可能存在问题。一般用<strong>凸二次函数</strong>作为简单问题：</p>
<script type="math/tex; mode=display">
\min \frac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q \boldsymbol x + \boldsymbol c^{\mathrm T} \boldsymbol x,  \boldsymbol Q \succeq 0</script><p>如果算法用于求解该凸二次函数能够在有限步内找到最优解，那么称该算法具有<strong>“二次终止性”</strong>。</p>
<h4 id="6-2-坐标轴交替下降法"><a href="#6-2-坐标轴交替下降法" class="headerlink" title="6.2 坐标轴交替下降法"></a>6.2 坐标轴交替下降法</h4><h5 id="一-基本思想"><a href="#一-基本思想" class="headerlink" title="(一)  基本思想"></a>(一)  基本思想</h5><p>给定初始点<script type="math/tex">\boldsymbol x_0</script>，依次沿着坐标轴<script type="math/tex">\boldsymbol e_1, \boldsymbol e_2, \cdots, \boldsymbol e_n</script>进行搜索。</p>
<h5 id="二-算法框架-流程"><a href="#二-算法框架-流程" class="headerlink" title="(二) 算法框架/流程"></a>(二) 算法框架/流程</h5><ul>
<li><p>STPE 1：给定初始点<script type="math/tex">\boldsymbol x_0, k:=0, \varepsilon>0</script>；</p>
</li>
<li><p>STPE 2：判断是否满足<script type="math/tex">\| \triangledown f(\boldsymbol x_k) \| \leq \varepsilon</script>，是，则终止算法；</p>
</li>
<li><p>STPE 3：记<script type="math/tex">\boldsymbol y_0 = \boldsymbol x_k</script>，令<script type="math/tex">\boldsymbol y_i = \boldsymbol y_{i-1} + \alpha_i \boldsymbol e_i</script></p>
<blockquote>
<p>其中，<script type="math/tex">\alpha_i := \arg \min f(\boldsymbol y_{i-1} + \alpha_i \boldsymbol e_i), i=1,2,\cdots, n</script></p>
<p>arg是argument（自变量、参数）的缩写，那么由此可知：</p>
<p>argmax F(x):使目标函数F(x)能够取到最大值时的变量x的值</p>
<p>argmin F(x):使目标函数F(x)能够取到最小值时的变量x的值</p>
<p>(只是用F(x)举个栗子，实际使用中的函数可能不止x这一个变量，不过意思还是这个意思~)</p>
</blockquote>
</li>
<li><p>STPE4：令<script type="math/tex">\boldsymbol x_{k+1} = \boldsymbol y_n, k:=k+1</script>，转STPE1。</p>
</li>
</ul>
<h5 id="三-优缺点"><a href="#三-优缺点" class="headerlink" title="(三) 优缺点"></a>(三) 优缺点</h5><p>优点：不需要成本即可得到搜索方向。当变量之间的交叉程度较小时非常有效(极端情况——可分离函数)；</p>
<p>缺点：对于一般问题所得到的点列未必收敛。</p>
<h5 id="四-改进方法"><a href="#四-改进方法" class="headerlink" title="(四) 改进方法"></a>(四) 改进方法</h5><p>比如：在交替下降过程中间每一步都加入一个线搜索。</p>
<p><img src="https://pic.imgdb.cn/item/6293758909475431299a817e.jpg" style="zoom:40%"></p>
<h4 id="6-3-梯度下降法-最速下降法"><a href="#6-3-梯度下降法-最速下降法" class="headerlink" title="6.3 梯度下降法(最速下降法)"></a>6.3 梯度下降法(最速下降法)</h4><h5 id="一-基本思想-1"><a href="#一-基本思想-1" class="headerlink" title="(一)  基本思想"></a>(一)  基本思想</h5><p>选择<script type="math/tex">\boldsymbol x_k</script>处负梯度作为搜索方向，即<script type="math/tex">\boldsymbol d_k = - \triangledown f(\boldsymbol x_k)</script>。</p>
<h5 id="二-优缺点"><a href="#二-优缺点" class="headerlink" title="(二) 优缺点"></a>(二) 优缺点</h5><p>优点：简单直观；收敛；搜索方向计算简单(即只需计算梯度)；</p>
<p>缺点：(1) 收敛速度慢(线性收敛)；(2) Zigzag现象(<strong>“之”字形</strong>)；(3) 不具备二次终止性(在有限步内求得凸二次函数的最优解)。</p>
<ul>
<li><p>缺点一：收敛速度慢(线性收敛)</p>
<ul>
<li><p>原因1：只利用了该点处的一阶导数，而没有利用二阶导数信息；</p>
</li>
<li><p>原因2：若迭代中步长<script type="math/tex">\alpha_k</script>是<script type="math/tex">\phi(\alpha):=f(\boldsymbol x_k + \alpha \boldsymbol d_k)</script>的精确最小点，则<script type="math/tex">\phi'(\alpha_k) = 0</script>，即</p>
<script type="math/tex; mode=display">\phi'(\alpha_k) = \triangledown f(\boldsymbol x_k + \alpha_k \boldsymbol d_k)^{\mathrm T}\boldsymbol d_k= -\triangledown f(\boldsymbol x_{k+1}) \triangledown f(\boldsymbol x_k) =  0</script><p>根据上式，可以看到前后两个点之间的<strong>梯度垂直，呈现“之”字形</strong>。示例如下图所示。</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/629379520947543129a01147.jpg" style="zoom:40%"></p>
<h4 id="6-4-牛顿法"><a href="#6-4-牛顿法" class="headerlink" title="6.4 牛顿法"></a>6.4 牛顿法</h4><h5 id="一-基本思想-2"><a href="#一-基本思想-2" class="headerlink" title="(一) 基本思想"></a>(一) 基本思想</h5><p>当前点<script type="math/tex">\boldsymbol x_k</script>处选择<script type="math/tex">\boldsymbol d_k = - [\triangledown^2 f(\boldsymbol x_k)]^{-1} \triangledown f(\boldsymbol x_k)</script>作为下降方向，可以理解为：对<script type="math/tex">\boldsymbol x_k</script>处的二次逼近函数(<strong>泰勒展开式</strong>)进行最小化。</p>
<script type="math/tex; mode=display">
\min f(\boldsymbol x_k) + \triangledown f(\boldsymbol x_k)^{\mathrm T}(\boldsymbol x - \boldsymbol x_k) + \frac{1}{2}(\boldsymbol x - \boldsymbol x_k)^{\mathrm T} \triangledown^2 f(\boldsymbol x_k) (\boldsymbol x - \boldsymbol x_k)</script><p>令上式求导等于零，可以得到：</p>
<script type="math/tex; mode=display">
\triangledown f(\boldsymbol x_k) + \triangledown^2 f(\boldsymbol x_k) (\boldsymbol x - \boldsymbol x_k) = \triangledown f(\boldsymbol x_k) + \triangledown^2 f(\boldsymbol x_k)  \boldsymbol d_k = 0</script><ul>
<li><p><strong>纯牛顿法：</strong>步长<script type="math/tex">\alpha_k = 1</script></p>
<blockquote>
<p>因为<script type="math/tex">\boldsymbol d_k = - [\triangledown^2 f(\boldsymbol x_k)]^{-1} \triangledown f(\boldsymbol x_k)  \Longleftrightarrow \boldsymbol x_{k+1} = \boldsymbol x_k - [\triangledown^2 f(\boldsymbol x_k)]^{-1} \triangledown f(\boldsymbol x_k)</script></p>
</blockquote>
</li>
</ul>
<h5 id="二-算法框架-流程-1"><a href="#二-算法框架-流程-1" class="headerlink" title="(二) 算法框架/流程"></a>(二) 算法框架/流程</h5><ul>
<li>STPE 1：给定初始点<script type="math/tex">\boldsymbol x_0, k:=0, \varepsilon>0</script>；</li>
<li>STPE 2：判断是否满足<script type="math/tex">\| \triangledown f(\boldsymbol x_k) \| \leq \varepsilon</script>，是，则终止算法；</li>
<li>STPE 3：计算<script type="math/tex">\boldsymbol d_k = - [\triangledown^2 f(\boldsymbol x_k)]^{-1} \triangledown f(\boldsymbol x_k)</script></li>
<li>STPE4：令<script type="math/tex">\boldsymbol x_{k+1} = \boldsymbol x_{k} + \boldsymbol d_k, k:=k+1</script>，转STPE1。</li>
</ul>
<h5 id="三-优缺点-1"><a href="#三-优缺点-1" class="headerlink" title="(三) 优缺点"></a>(三) 优缺点</h5><p>优点：牛顿法同时考虑了一阶导数信息和和二阶导数信息(<strong>黑塞矩阵</strong>)；当初始点<script type="math/tex">\boldsymbol x_0</script>取得比较接近于收敛点<script type="math/tex">\boldsymbol x^*</script>，且<script type="math/tex">\triangledown^2 f(\boldsymbol x)</script>满足较好性质时，二阶收敛；二次终止性(一步终止)。</p>
<p>缺点：计算量大(需计算Hesse矩阵)；适用范围较窄。</p>
<p>此外，牛顿法还存在一个问题：二阶导数矩阵不一定是正定的，此时<script type="math/tex">\boldsymbol d_k</script>就不一定再是下降方向了。</p>
<p><img src="https://pic.imgdb.cn/item/62937f780947543129a94517.jpg" style="zoom:40%"></p>
<p><img src="https://pic.imgdb.cn/item/629380240947543129aa7197.jpg" style="zoom:40%"></p>
<h4 id="6-5-修正牛顿法"><a href="#6-5-修正牛顿法" class="headerlink" title="6.5 修正牛顿法"></a>6.5 修正牛顿法</h4><ul>
<li><strong>修改点1：步长</strong><ul>
<li>对于步长<script type="math/tex">\alpha_k​</script>的修正：首先判断<script type="math/tex">\alpha_k = 1​</script>是否让目标函数充分下降；否，则采用线搜索方法重新确定<script type="math/tex">\alpha_k​</script>;</li>
</ul>
</li>
<li><strong>修改点2：方向</strong><ul>
<li>对于方向(Hesse矩阵)的修正：选取<script type="math/tex">\boldsymbol d_k = - \boldsymbol B_k^{-1} \triangledown f(\boldsymbol x_k)</script><ul>
<li>若<script type="math/tex">\triangledown^2 f(\boldsymbol x_k) \succ 0</script>，则选取<script type="math/tex">\boldsymbol B_k = \triangledown^2 f(\boldsymbol x_k)</script></li>
<li>否则，采取修正方法(多种)：<ul>
<li><script type="math/tex">\boldsymbol B_k := \triangledown^2 f(\boldsymbol x_k) + \lambda \boldsymbol I</script>，其中，<script type="math/tex">\lambda</script>为适当正数保证<script type="math/tex">\boldsymbol b_k</script>正定。</li>
<li>考虑特征值分解：<script type="math/tex">\triangledown^2 f(\boldsymbol x_k) = \boldsymbol Q^{\mathrm T} \boldsymbol \Lambda \boldsymbol Q</script>，令<script type="math/tex">\boldsymbol b_k = \boldsymbol Q^{\mathrm T} \text{diag} (\tau_i) \boldsymbol Q</script><ul>
<li><script type="math/tex">\tau_{i}= \begin{cases}\lambda_{i}, & \text { if } \lambda_{i} \geq \delta \\ \delta, & \text { otherwise }\end{cases}</script>，其中<script type="math/tex">\delta</script>为一适当正数。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="6-6-拟牛顿法"><a href="#6-6-拟牛顿法" class="headerlink" title="6.6 拟牛顿法"></a>6.6 拟牛顿法</h4><h5 id="一-基本思想-3"><a href="#一-基本思想-3" class="headerlink" title="(一) 基本思想"></a>(一) 基本思想</h5><p>当前点<script type="math/tex">\boldsymbol x_k</script>处对<script type="math/tex">f(\boldsymbol x_k)</script>使用二次函数进行近似：</p>
<script type="math/tex; mode=display">
m_k(\boldsymbol x) := f(\boldsymbol x_k) + \triangledown f(\boldsymbol x_k)^{\mathrm T}(\boldsymbol x - \boldsymbol x_k) + \frac{1}{2}(\boldsymbol x - \boldsymbol x_k)^{\mathrm T} \boldsymbol B_k (\boldsymbol x - \boldsymbol x_k), \boldsymbol B_k \succ 0</script><p>利用<script type="math/tex">\min m_k(\boldsymbol x)</script>的搜索方向：<script type="math/tex">\boldsymbol d_k = - \boldsymbol B_k^{-1} \triangledown f(\boldsymbol x_k)</script>；我们希望<script type="math/tex">\boldsymbol B_k</script>尽量包含一些二阶信息，然后计算要相对简单。</p>
<p>$\boldsymbol B_k \succ 0$为正定矩阵时，<script type="math/tex">m_k(\boldsymbol x)</script>是一个凸二次函数，求其最小值即可让<script type="math/tex">m_k(\boldsymbol x)</script>的梯度为0即可，求解即可得到：</p>
<script type="math/tex; mode=display">
\boldsymbol x_{k+1} = \boldsymbol x = \boldsymbol x_k - \boldsymbol d_k = \boldsymbol x_k - \boldsymbol B_k^{-1} \triangledown f(\boldsymbol x_k)</script><h5 id="二-算法框架-流程-2"><a href="#二-算法框架-流程-2" class="headerlink" title="(二) 算法框架/流程"></a>(二) 算法框架/流程</h5><ul>
<li>STPE 1：给定初始点<script type="math/tex">\boldsymbol x_0, k:=0, \varepsilon>0, \boldsymbol B_0</script>；</li>
<li>STPE 2：判断是否满足<script type="math/tex">\| \triangledown f(\boldsymbol x_k) \| \leq \varepsilon</script>，是，则终止算法；</li>
<li>STPE 3：计算<script type="math/tex">\boldsymbol d_k = - \boldsymbol B_k^{-1} \triangledown f(\boldsymbol x_k)</script>；</li>
<li>STPE 4：使用线搜索法确定步长<script type="math/tex">\alpha_k</script>；</li>
<li>STPE 5：令<script type="math/tex">\boldsymbol x_{k+1} = \boldsymbol x_{k} + \alpha_k \boldsymbol d_k, k:=k+1</script>，确定<script type="math/tex">\boldsymbol B_{k+1}</script>，转STPE1。</li>
</ul>
<h5 id="三-boldsymbol-B-k-1-矩阵的确定方法"><a href="#三-boldsymbol-B-k-1-矩阵的确定方法" class="headerlink" title="(三) $\boldsymbol B_{k+1}$矩阵的确定方法"></a>(三) $\boldsymbol B_{k+1}$矩阵的确定方法</h5><p><strong>1 拟牛顿方程(基本要求)</strong></p>
<script type="math/tex; mode=display">
\triangledown f(\boldsymbol x_{k+1}) - \triangledown f(\boldsymbol x_k) = \boldsymbol B_{k+1} (\boldsymbol x_{k+1} - \boldsymbol x_{k})</script><blockquote>
<p>对上式的简单理解：</p>
<p>首先由中值定理可得：<script type="math/tex">\triangledown f(\boldsymbol x_{k+1}) - \triangledown f(\boldsymbol x_k) = \triangledown^2 f(\boldsymbol \xi ) (\boldsymbol x_{k+1} - \boldsymbol x_{k}) , \text{ }\xi = \lambda \boldsymbol x_k + (1-\lambda)\boldsymbol x_k, \text{ } \lambda \in (0,1)</script></p>
<p>对比发现：也就是我们希望<script type="math/tex">\boldsymbol B_{k+1}</script>能够体现<script type="math/tex">\triangledown^2 f(\boldsymbol \xi )</script>的作用。</p>
<p>分析易知，<script type="math/tex">\boldsymbol B_{k+1}</script>大小为<script type="math/tex">n \times n</script>，其中变量个数为<script type="math/tex">\dfrac{n(n+1)}{2}</script>(对称阵)，但是等式只要<script type="math/tex">n</script>个方程，因此<script type="math/tex">\boldsymbol B_{k+1}</script>会有<strong>多个</strong>满足条件。</p>
</blockquote>
<p>记<script type="math/tex">\boldsymbol y_k := \triangledown f(\boldsymbol x_{k+1}) - \triangledown f(\boldsymbol x_k)</script>，<script type="math/tex">\boldsymbol s_k := \boldsymbol x_{k+1} - \boldsymbol x_{k}</script>，则拟牛顿方程简记为：</p>
<script type="math/tex; mode=display">
\boldsymbol y_k = \boldsymbol B_{k+1} \boldsymbol s_k</script><p>此外，若记<script type="math/tex">\boldsymbol H_k = \boldsymbol B_k^{-1}</script>，拟牛顿方程也可以表示为：<script type="math/tex">\boldsymbol s_k = \boldsymbol H_{k+1} \boldsymbol y_k</script>。</p>
<p>下面我们将对利用已有信息具体获得<script type="math/tex">\boldsymbol B_{k+1}</script>或者<script type="math/tex">\boldsymbol H_{k+1}</script></p>
<p><strong>2 第一类方法</strong></p>
<p>选择满足拟牛顿方程且与<script type="math/tex">\boldsymbol B_k</script>近似的矩阵。</p>
<script type="math/tex; mode=display">
\begin{cases}\text { min } & \| \boldsymbol B - \boldsymbol B_k \| \\
\text{ s.t. } & \boldsymbol B \boldsymbol s_k = \boldsymbol y_k, \boldsymbol B = \boldsymbol B^T
\end{cases}</script><script type="math/tex; mode=display">
\begin{cases}\text { min } & \| \boldsymbol H - \boldsymbol H_k \| \\
\text{ s.t. } & \boldsymbol H \boldsymbol y_k = \boldsymbol s_k, \boldsymbol H = \boldsymbol H^T
\end{cases}</script><blockquote>
<p>矩阵范数小知识：<a href="https://zhuanlan.zhihu.com/p/35897775">向量范数与矩阵范数</a></p>
</blockquote>
<p><strong>3 第二类方法</strong></p>
<p>对<script type="math/tex">\boldsymbol B_k</script>、<script type="math/tex">\boldsymbol H_k</script>进行校正，如：令<script type="math/tex">\boldsymbol B_{k+1} = \boldsymbol B_k + \Delta \boldsymbol B</script>。</p>
<ul>
<li><p><strong>rank-1</strong>校正：要求<script type="math/tex">\Delta \boldsymbol B</script>的秩为1；</p>
<ul>
<li><p>SR-1方法：可以看作是对<script type="math/tex">\boldsymbol B_k</script>进行rank-1校正</p>
<ul>
<li><script type="math/tex; mode=display">\boldsymbol B_{k+1} = \boldsymbol B_k + \dfrac{(\boldsymbol y_k - \boldsymbol B_k \boldsymbol s_k) (\boldsymbol y_k - \boldsymbol B_k\boldsymbol s_k)^{\mathrm T}}{(\boldsymbol y_k - \boldsymbol B_k \boldsymbol s_k)^{\mathrm T} \boldsymbol s_k}</script></li>
<li><p>如何得到上式：</p>
</li>
<li><p><img src="https://pic.imgdb.cn/item/62941a5d09475431294eec76.jpg" style="zoom:40%"></p>
</li>
<li><p>该方法相对rank-2方法迭代公式更简单，但是不能保证正定性；适当条件下能达到$n$步超线性收敛。</p>
<blockquote>
<p>$n$步超线性收敛：</p>
<script type="math/tex; mode=display">\lim_{k \to \infty} = \dfrac{\| \boldsymbol x_{k+1+n} - \boldsymbol x^* \|}{\boldsymbol x_{k} - \boldsymbol x^*}</script></blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>rank-2</strong>校正：要求<script type="math/tex">\Delta \boldsymbol B</script>的秩为2；</p>
<ul>
<li><strong>DFP方法：</strong>可以看作是对<script type="math/tex">\boldsymbol H_k</script>进行rank-2校正<ul>
<li><script type="math/tex; mode=display">\boldsymbol H_{k+1} = \boldsymbol H_k + \dfrac{\boldsymbol H_k \boldsymbol y_k \boldsymbol y_k^{\mathrm T}\boldsymbol H_k}{\boldsymbol y_k^{\mathrm T}\boldsymbol H_k\boldsymbol y_k} +  \dfrac{\boldsymbol s_k \boldsymbol s_k^{\mathrm T}}{\boldsymbol y_k^{\mathrm T}\boldsymbol s_k}</script></li>
<li>如何得到上式：注意下面图中的证明用到了<strong>对称秩1矩阵<script type="math/tex">\boldsymbol A</script>的性质：</strong><script type="math/tex">\boldsymbol A = \boldsymbol {uv}^T = \boldsymbol {uu}^T</script></li>
<li><img src="https://pic.imgdb.cn/item/629416750947543129494abe.jpg" style="zoom:40%"></li>
</ul>
</li>
<li><strong>BFGS方法：</strong>可以看作是对<script type="math/tex">\boldsymbol B_k</script>进行rank-2校正<ul>
<li><script type="math/tex; mode=display">\boldsymbol B_{k+1} = \boldsymbol B_k + \dfrac{\boldsymbol B_k \boldsymbol s_k \boldsymbol s_k^{\mathrm T}\boldsymbol B_k}{\boldsymbol s_k^{\mathrm T}\boldsymbol B_k\boldsymbol s_k} +  \dfrac{\boldsymbol y_k \boldsymbol y_k^{\mathrm T}}{\boldsymbol y_k^{\mathrm T}\boldsymbol s_k}</script></li>
<li>如何得到上式：原理同上面DFP推导类似。</li>
<li>拟牛顿方向需要计算<script type="math/tex">\boldsymbol B_{k+1}^{-1}</script>，可以利用Sherman-Morrison公式显示写出；</li>
<li>BFGS方法是被认为最有效的拟牛顿法；(适当前提下可证明)超线性收敛。</li>
<li>Broyden族：DFP与BFGS的线性组合！</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="第七讲-无约束优化问题（叁）"><a href="#第七讲-无约束优化问题（叁）" class="headerlink" title="第七讲 无约束优化问题（叁）"></a>第七讲 无约束优化问题（叁）</h3><h4 id="7-1-共轭梯度法背景"><a href="#7-1-共轭梯度法背景" class="headerlink" title="7.1 共轭梯度法背景"></a>7.1 共轭梯度法背景</h4><p>1950年代，求解一类优化问题中：</p>
<script type="math/tex; mode=display">
\min \dfrac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q \boldsymbol x +  \boldsymbol c^{\mathrm T} \boldsymbol x, \boldsymbol Q \succ 0, \boldsymbol c \in \mathbb{R}^n</script><p>该类问题可直接通过求梯度然后令梯度为零即可：</p>
<script type="math/tex; mode=display">
\triangledown f(\boldsymbol x) = \boldsymbol Q \boldsymbol x + \boldsymbol c = \boldsymbol 0</script><p>可知，需要求解线性方程组。当矩阵比较大时候，计算复杂，因此有人提出使用迭代法求解方程组，因此就有了”(线性)共轭梯度法”。</p>
<p>1960年代，该方法推广到求解一般性优化问题，此时该方法也被称为”(非线性)共轭梯度法”。</p>
<h4 id="7-2-线性共轭梯度法"><a href="#7-2-线性共轭梯度法" class="headerlink" title="7.2 线性共轭梯度法"></a>7.2 线性共轭梯度法</h4><p>考虑以下优化问题：</p>
<script type="math/tex; mode=display">
\min \dfrac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q \boldsymbol x +  \boldsymbol c^{\mathrm T} \boldsymbol x, \boldsymbol Q \succ 0, \boldsymbol c \in \mathbb{R}^n</script><ul>
<li>当<script type="math/tex">\boldsymbol Q</script>特殊一点为对角矩阵时：</li>
</ul>
<script type="math/tex; mode=display">
f(\boldsymbol x) = \dfrac{1}{2} \boldsymbol x^{\mathrm T} \left[\begin{array}{ccc}
q_{1} & \cdots & 0 \\
\vdots & \ddots & 0 \\
0 & 0 & q_{n}
\end{array}\right] \boldsymbol x + \boldsymbol c^{\mathrm T}\boldsymbol x</script><p><img src="https://pic.imgdb.cn/item/629471ce0947543129c41093.jpg" style="zoom:50%"></p>
<ul>
<li>当<script type="math/tex">\boldsymbol Q</script>为一般情况(非对角矩阵)时：</li>
</ul>
<p>拿到一个普通二次型，就要想办法将其转换为标准二次型，即就是只有平方项没有交叉项。常用的方法有线性替换，最常见的线性替换又是：对<script type="math/tex">\boldsymbol Q</script>矩阵进行特征值和特征向量分解：</p>
<script type="math/tex; mode=display">
\boldsymbol Q = \boldsymbol S^{\mathrm T} \boldsymbol \Lambda \boldsymbol S</script><p>其中，<script type="math/tex">\boldsymbol S</script>是由标准正交化之后的特征向量组成。</p>
<p>将<script type="math/tex">\dfrac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q \boldsymbol x +  \boldsymbol c^{\mathrm T} \boldsymbol x</script>中<script type="math/tex">\boldsymbol x</script>用<script type="math/tex">\hat{\boldsymbol x} = \boldsymbol{Sx}</script>进行替换：</p>
<script type="math/tex; mode=display">
f(\hat{\boldsymbol x}) = \dfrac{1}{2} \hat{\boldsymbol x}^{\mathrm T} \boldsymbol \Lambda \hat{\boldsymbol x} +  \boldsymbol{(Sc)}^{\mathrm T} \hat{\boldsymbol x}</script><p>此时就可以像对角矩阵情况类似，求解变得简单。求解得到<script type="math/tex">\hat{\boldsymbol x}^*</script>后即可得到最终解：<script type="math/tex">\boldsymbol x^* = \boldsymbol P^{-1} \hat{\boldsymbol x}^* = \boldsymbol P^{\mathrm T} \hat{\boldsymbol x}^*</script></p>
<p>一定要记住，线性共轭梯度法被设计出来就是用来求解线性方程组。</p>
<h5 id="一-共轭方向"><a href="#一-共轭方向" class="headerlink" title="(一) 共轭方向"></a>(一) 共轭方向</h5><p>考虑正定矩阵<script type="math/tex">\boldsymbol Q</script>和非零向量<script type="math/tex">\boldsymbol d_i, \boldsymbol d_j</script>，若：</p>
<script type="math/tex; mode=display">
\boldsymbol d_i^{\mathrm T} \boldsymbol Q \boldsymbol d_j = \boldsymbol 0</script><p>则称<script type="math/tex">\boldsymbol d_i, \boldsymbol d_j</script>关于矩阵<script type="math/tex">\boldsymbol Q</script>共轭。</p>
<p>向量组<script type="math/tex">\boldsymbol d_0, \boldsymbol d_1, \cdots,  \boldsymbol d_k</script>关于矩阵<script type="math/tex">\boldsymbol Q</script>共轭，两两共轭。</p>
<h5 id="二-共轭-VS-正交"><a href="#二-共轭-VS-正交" class="headerlink" title="(二) 共轭 VS 正交"></a>(二) 共轭 VS 正交</h5><ul>
<li><p>若向量组<script type="math/tex">\boldsymbol d_0, \boldsymbol d_1, \cdots,  \boldsymbol d_k</script>关于<script type="math/tex">\boldsymbol I</script>共轭，则向量组<script type="math/tex">\boldsymbol d_0, \boldsymbol d_1, \cdots,  \boldsymbol d_k</script>是正交向量组；</p>
</li>
<li><p>若向量组<script type="math/tex">\boldsymbol d_0, \boldsymbol d_1, \cdots,  \boldsymbol d_k</script>关于正定矩阵<script type="math/tex">\boldsymbol Q</script>共轭，令<script type="math/tex">\boldsymbol{Q = P}^2</script>，则有：</p>
<ul>
<li><script type="math/tex">\boldsymbol d_i^{\mathrm T} \boldsymbol Q \boldsymbol d_j  = \boldsymbol d_i^{\mathrm T} \boldsymbol P^{\mathrm T} \boldsymbol P \boldsymbol d_j = (\boldsymbol P \boldsymbol d_i)^{\mathrm T}  (\boldsymbol P \boldsymbol d_j) = \boldsymbol 0</script>正交</li>
</ul>
<blockquote>
<p>涉及小知识点：</p>
<p>一个正定矩阵一定能分解为另一个正定矩阵的平方：<script type="math/tex">\boldsymbol{Q = P}^2</script>；</p>
<p>共轭向量组线性无关。</p>
</blockquote>
</li>
</ul>
<h5 id="三-共轭方向法"><a href="#三-共轭方向法" class="headerlink" title="(三) 共轭方向法"></a>(三) 共轭方向法</h5><script type="math/tex; mode=display">
\min \text{ } \dfrac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q \boldsymbol x +  \boldsymbol c^{\mathrm T} \boldsymbol x, \boldsymbol Q \succ 0, \boldsymbol c \in \mathbb{R}^n</script><p>给定初始点<script type="math/tex">\boldsymbol x_0</script>以及一组关于<script type="math/tex">\boldsymbol Q</script>共轭方向<script type="math/tex">\boldsymbol d_0, \boldsymbol d_1, \cdots,  \boldsymbol d_{n-1}</script>，令：</p>
<script type="math/tex; mode=display">
\boldsymbol x_{k+1} := \boldsymbol x_k + \alpha_k \boldsymbol d_k, \text{ } k = 0, 1, \cdots, n-1</script><p>其中，<script type="math/tex">\alpha_k = \arg \min \phi(\alpha) := f(\boldsymbol x_k + \alpha \boldsymbol d_{k})</script>。计算可得<strong>步长</strong>为：</p>
<script type="math/tex; mode=display">
\phi'(\alpha_k) = 0 \Longrightarrow \alpha_k = -\dfrac{(\boldsymbol Q \boldsymbol x_k + \boldsymbol c)^{\mathrm T}\boldsymbol d_k}{\boldsymbol d_k^{\mathrm T}\boldsymbol Q \boldsymbol d_k} = -\dfrac{\triangledown f(\boldsymbol x)^{\mathrm T} \boldsymbol d_k}{\boldsymbol d_k^{\mathrm T}\boldsymbol Q \boldsymbol d_k}</script><p><img src="https://pic.imgdb.cn/item/6294a4bf0947543129090093.jpg" style="zoom:50%"></p>
<p><strong>共轭方向法为一类方法，不同的选取共轭方向的方式就对应不同的共轭方向法，<font color="red">共轭梯度法</font>是其中一种</strong>。</p>
<p><img src="https://pic.imgdb.cn/item/6294a9d909475431290f59c5.jpg" style="zoom:50%"></p>
<h5 id="四-共轭方向法具备的特征"><a href="#四-共轭方向法具备的特征" class="headerlink" title="(四) 共轭方向法具备的特征"></a>(四) 共轭方向法具备的特征</h5><p>以下特征均是基于问题：<script type="math/tex">\min \dfrac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q \boldsymbol x +  \boldsymbol c^{\mathrm T} \boldsymbol x, \boldsymbol Q \succ 0, \boldsymbol c \in \mathbb{R}^n</script></p>
<p>点列<script type="math/tex">\begin{Bmatrix} \boldsymbol x_k \end{Bmatrix}_{k=1}^{\infty}</script>具有如下特征：</p>
<ul>
<li>特征1：<script type="math/tex">\triangledown f(\boldsymbol x_k )^{\mathrm T} \boldsymbol d_i = 0, i = 0, 1, \cdots, k-1</script></li>
<li>特征2：<script type="math/tex">\boldsymbol x_k := \arg \min \begin{Bmatrix} \dfrac{1}{2} \boldsymbol x^{\mathrm T} \boldsymbol Q \boldsymbol x +  \boldsymbol c^{\mathrm T} \boldsymbol x \mid \boldsymbol x \in \boldsymbol X_k \end{Bmatrix}</script><ul>
<li>其中，<script type="math/tex">\boldsymbol X_k =  \begin{Bmatrix} \boldsymbol x_ 0 + \sum\limits_{i=0}^{k-1} a_i \boldsymbol d_i \mid a_i \in \mathbb{R}, i = 1, 2, \cdots, k-1 \end{Bmatrix}</script></li>
</ul>
</li>
</ul>
<p>证明：参考视频。</p>
<h4 id="7-3-共轭梯度法具体"><a href="#7-3-共轭梯度法具体" class="headerlink" title="7.3 共轭梯度法具体"></a>7.3 共轭梯度法具体</h4><h5 id="一-基本思想-4"><a href="#一-基本思想-4" class="headerlink" title="(一)  基本思想"></a>(一)  基本思想</h5><p>在迭代下降过程中，借助当前点<script type="math/tex">\boldsymbol x_k</script>处的梯度信息构造共轭方向。</p>
<h5 id="二-算法框架-流程-3"><a href="#二-算法框架-流程-3" class="headerlink" title="(二) 算法框架/流程"></a>(二) 算法框架/流程</h5><ul>
<li>STPE 1：给定初始点<script type="math/tex">\boldsymbol x_0, k:=0, \varepsilon>0</script>，记<script type="math/tex">\boldsymbol d_0 := -\triangledown f(\boldsymbol x_0)</script></li>
<li>STPE 2：判断是否满足<script type="math/tex">\| \triangledown f(\boldsymbol x_k) \| \leq \varepsilon</script>，是，则终止算法；</li>
<li>STPE 3：计算<script type="math/tex">\alpha_k = -\dfrac{\triangledown f(\boldsymbol x)^{\mathrm T} \boldsymbol d_k}{\boldsymbol d_k^{\mathrm T}\boldsymbol Q \boldsymbol d_k}</script>；</li>
<li>STPE4：令<script type="math/tex">\boldsymbol x_{k+1} = \boldsymbol x_{k} + \alpha_k \boldsymbol d_k</script>，计算方向<script type="math/tex">\boldsymbol d_{k+1} = -\triangledown f(\boldsymbol x_{k+1}) + \text{some-term}</script>，<script type="math/tex">k:=k+1</script>转STPE1。<ul>
<li>其中，<script type="math/tex">\boldsymbol d_{k+1} = -\triangledown f(\boldsymbol x_{k+1}) + \beta_k \boldsymbol d_k</script>，<script type="math/tex">\beta_k  = \dfrac{\triangledown f(\boldsymbol x_{k+1}) \boldsymbol Q \boldsymbol d_k}{\boldsymbol d_k^{\mathrm T}\boldsymbol Q \boldsymbol d_k}</script></li>
</ul>
</li>
</ul>
<h5 id="三-公式简化"><a href="#三-公式简化" class="headerlink" title="(三) 公式简化"></a>(三) 公式简化</h5><p>简化的目的：将这些表达式与目标函数的梯度比较直观地表现出来。</p>
<p>共轭梯度法的步长公式：</p>
<script type="math/tex; mode=display">
\alpha_k = -\dfrac{\triangledown f(\boldsymbol x)^{\mathrm T} \boldsymbol d_k}{\boldsymbol d_k^{\mathrm T}\boldsymbol Q \boldsymbol d_k}</script><p>可简化为：</p>
<script type="math/tex; mode=display">
\alpha_k = \dfrac{\triangledown f(\boldsymbol x)^{\mathrm T}\triangledown f(\boldsymbol x)}{\boldsymbol d_k^{\mathrm T}\boldsymbol Q \boldsymbol d_k}</script><p>共轭梯度法的步长公式中的系数：</p>
<script type="math/tex; mode=display">
\beta_k  = \dfrac{\triangledown f(\boldsymbol x_{k+1}) \boldsymbol Q \boldsymbol d_k}{\boldsymbol d_k^{\mathrm T}\boldsymbol Q \boldsymbol d_k}</script><p>可简化为：</p>
<script type="math/tex; mode=display">
\beta_k  = \dfrac

{\triangledown f(\boldsymbol x_{k+1})^{\mathrm T}[\triangledown f(\boldsymbol x_{k+1}) -  \triangledown f(\boldsymbol x_{k+1})]}

{\triangledown f(\boldsymbol x_k)^{\mathrm T}\triangledown f(\boldsymbol x_k)}</script><p>进一步简化为：</p>
<script type="math/tex; mode=display">
\beta_k  = \dfrac

{\triangledown f(\boldsymbol x_{k+1})^{\mathrm T} \triangledown f(\boldsymbol x_{k+1}) }

{\triangledown f(\boldsymbol x_k)^{\mathrm T}\triangledown f(\boldsymbol x_k)}</script><h4 id="7-4-非线性共轭梯度法"><a href="#7-4-非线性共轭梯度法" class="headerlink" title="7.4 非线性共轭梯度法"></a>7.4 非线性共轭梯度法</h4><p>注意注意注意：上述推导全部是建立在线性共轭梯度法，基本函数是二次函数。</p>
<ul>
<li>STPE 1：给定初始点<script type="math/tex">\boldsymbol x_0, k:=0, \varepsilon>0</script>，记<script type="math/tex">\boldsymbol d_0 := -\triangledown f(\boldsymbol x_0)</script></li>
<li>STPE 2：判断是否满足<script type="math/tex">\| \triangledown f(\boldsymbol x_k) \| \leq \varepsilon</script>，是，则终止算法；</li>
<li>STPE 3：利用线性搜索计算<script type="math/tex">\alpha_k</script>；</li>
<li>STPE4：令<script type="math/tex">\boldsymbol x_{k+1} = \boldsymbol x_{k} + \alpha_k \boldsymbol d_k</script>，计算方向<script type="math/tex">\boldsymbol d_{k+1} = -\triangledown f(\boldsymbol x_{k+1}) + \beta_k \boldsymbol d_k</script>，<script type="math/tex">k:=k+1</script>转STPE1。<ul>
<li>PRP方法：<script type="math/tex">\beta_k  = \dfrac{\triangledown f(\boldsymbol x_{k+1})^{\mathrm T}[\triangledown f(\boldsymbol x_{k+1}) -  \triangledown f(\boldsymbol x_{k+1})]} {\triangledown f(\boldsymbol x_k)^{\mathrm T}\triangledown f(\boldsymbol x_k)}</script></li>
<li>FR方法：<script type="math/tex">\beta_k  = \dfrac{\triangledown f(\boldsymbol x_{k+1})^{\mathrm T} \triangledown f(\boldsymbol x_{k+1}) }{\triangledown f(\boldsymbol x_k)^{\mathrm T}\triangledown f(\boldsymbol x_k)}</script></li>
</ul>
</li>
</ul>
<h4 id="7-5-一些说明"><a href="#7-5-一些说明" class="headerlink" title="7.5 一些说明"></a>7.5 一些说明</h4><ul>
<li><p>在实践中，为保证每次产生的方向为下降方向，可能会对<script type="math/tex">\beta_k</script>进行调整；</p>
</li>
<li><p>具有二次终止性；</p>
</li>
<li><p>实现过程中常采用$n$步重启策略，可达到$n$步二阶收敛；</p>
<blockquote>
<p>n步二阶收敛：<script type="math/tex">\lim\limits_{k \to \infty}\dfrac{\| \boldsymbol x_{k+n} - \boldsymbol x^*\|}{\| \boldsymbol x_{k+n} - \boldsymbol x^*\|^2} = 某一有限值 \gamma</script></p>
</blockquote>
<ul>
<li>原因1：较远的点对当前点贡献很小，可以忽略，因此重启；</li>
<li>原因2：可能会将非线性共轭转变为线性共轭梯度法；</li>
</ul>
</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/6294c6a309475431293567f4.jpg" style="zoom:60%"></p>
<h3 id="第八讲-约束优化理论-壹-—最优性条件"><a href="#第八讲-约束优化理论-壹-—最优性条件" class="headerlink" title="第八讲 约束优化理论(壹)—最优性条件"></a>第八讲 约束优化理论(壹)—最优性条件</h3><p>本讲要讨论的约束优化问题为：</p>
<script type="math/tex; mode=display">
(P) \text{ }\begin{cases}\text { min } & f(\boldsymbol x) \\ 
\text{ s.t.} & g_{i}(\boldsymbol x) \leq 0, \text{  } i=1, \cdots, m \\ 
& h_{i}(\boldsymbol x) = 0, \text{  } i=1, \cdots, l\end{cases}</script><p>记问题(P)的可行集为集合<script type="math/tex">\boldsymbol S = \begin{Bmatrix} \boldsymbol x \mid g_{i}(\boldsymbol x) \leq 0, \text{  } i=1, \cdots, m,  h_{i}(\boldsymbol x) = 0, \text{  } i=1, \cdots, l \end{Bmatrix}</script>。</p>
<p>本讲讨论问题的基本假设/前提：</p>
<ul>
<li><p>问题(P)中函数<script type="math/tex">f(\boldsymbol x), g_i(\boldsymbol x), h_i(\boldsymbol x)</script>均为连续可微函数；</p>
<blockquote>
<p>注意，有几类特殊的非光滑函数存在不可微点，但是可以将其转化，例如：</p>
<script type="math/tex; mode=display">f(x) = \max  \begin{Bmatrix} x, x^2 \end{Bmatrix} $$，绘图易知该函数存在两个不可微的点，那么优化问题：</script><p>\min f( x)</p>
<script type="math/tex; mode=display">
可以转化为：令$$t = f(\boldsymbol x) $$：</script><p>\min f( x) \\<br>\Downarrow \\<br>\begin{cases}\text { min } &amp;t \\<br>\text{ s.t.} &amp; f( x) \leq t\end{cases} \\<br>\Downarrow \\<br>\begin{cases}\text { min } &amp;t \\<br>\text{ s.t.} &amp;  x \leq t\\<br>&amp; x^2\leq t\end{cases}<br><script type="math/tex">推广来看：</script>f(x) = \max \begin{Bmatrix} a_i x + b_i \end{Bmatrix}$$这一类函数可以进行转换。</p>
</blockquote>
<p>&gt;</p>
</li>
</ul>
<p><img src="https://pic.imgdb.cn/item/629560260947543129c8f1f8.jpg" style="zoom:40%"></p>
<h4 id="8-1-最优解的一阶必要条件-KKT条件"><a href="#8-1-最优解的一阶必要条件-KKT条件" class="headerlink" title="8.1 最优解的一阶必要条件(KKT条件)"></a>8.1 最优解的一阶必要条件(KKT条件)</h4><h5 id="一-KKT条件的内容"><a href="#一-KKT条件的内容" class="headerlink" title="(一) KKT条件的内容"></a>(一) KKT条件的内容</h5><p>假设<script type="math/tex">\boldsymbol x^*</script>是问题(P)的局部最优解，且<script type="math/tex">\boldsymbol x^*</script>处某个“适当条件”成立，则存在<script type="math/tex">\boldsymbol \lambda \in \mathbb{R}^m, \boldsymbol \mu \in \mathbb{R}^l</script>(<script type="math/tex">m</script>是不等式约束个数，<script type="math/tex">l</script>是等式约束个数)使得：</p>
<script type="math/tex; mode=display">
\triangledown f(\boldsymbol x^*) + \sum_{i=1}^{m}\lambda_i \triangledown g_i(\boldsymbol x^*) + \sum_{i=1}^{l} \mu_i \triangledown h_i(\boldsymbol x^*) = 0 \\
\lambda_i > 0, i = 1,2, \cdots, m \\
g_i(\boldsymbol x^*) \leq 0,  i = 1,2, \cdots, m \\
h_i(\boldsymbol x^*) = 0,  i = 1,2, \cdots, l \\
\lambda_i g_i(\boldsymbol x^*) = 0,  i = 1,2, \cdots, m</script><p>其中，以上5个条件就是著名的<strong><font color="red">KKT条件</font></strong>。<script type="math/tex">\lambda_i, \mu_i</script>是两组系数，又称为<font color="blue">“拉格朗日乘子”</font>。</p>
<p>针对非凸问题设计优化算法时，如果能证明算法收敛到一个KKT点，那么就说明算法达到了一个基本要求，<em><u>KKT点相当于无约束优化问题中的梯度为零的作用</u></em>。</p>
<h5 id="二-证明KKT必要条件"><a href="#二-证明KKT必要条件" class="headerlink" title="(二) 证明KKT必要条件"></a>(二) 证明KKT必要条件</h5><p><strong>1 几个概念</strong></p>
<ul>
<li><p>对于<script type="math/tex">\boldsymbol x^* \in \boldsymbol S</script>，若点列<script type="math/tex">\begin{Bmatrix} \boldsymbol x_k \end{Bmatrix} ⊂ \boldsymbol S</script>满足所有<script type="math/tex">\boldsymbol x_k \neq \boldsymbol x^*, \lim\limits_{k \to \infty} \boldsymbol x_k  = \boldsymbol x^*</script>，则称为<strong>可行点列</strong>。</p>
</li>
<li><p>基本思路：若<script type="math/tex">\boldsymbol x^* \in \boldsymbol S</script>是局部最优解，则沿着任意可行点列目标函数不会下降(即当<script type="math/tex">k</script>充分大时，有<script type="math/tex">f(\boldsymbol x_k) > f(\boldsymbol x^*)</script>)。</p>
</li>
<li><p>考虑<script type="math/tex">\boldsymbol x^*</script>处集合<script type="math/tex">\boldsymbol D(\boldsymbol x^*) = \begin{Bmatrix} \boldsymbol d \mid \triangledown f(\boldsymbol x^*)^{\mathrm T} \boldsymbol d < 0 \end{Bmatrix} : \forall \boldsymbol d \in \boldsymbol D</script>均为<script type="math/tex">f(\boldsymbol x_k)</script>是<script type="math/tex">\boldsymbol x^*</script>处的下降方向。</p>
</li>
<li><p>考虑<script type="math/tex">\boldsymbol x^*</script>处集合<script type="math/tex">\boldsymbol T(\boldsymbol x^*) = \begin{Bmatrix} \alpha \boldsymbol d \mid \alpha > 0, \boldsymbol d = \lim\limits_{k \to \infty}\dfrac{\boldsymbol x_k- \boldsymbol x^*}{\| \boldsymbol x_k- \boldsymbol x^* \|} , \boldsymbol x_k \to \boldsymbol x^*, \boldsymbol x_k \neq \boldsymbol x^*, \boldsymbol x_k \in \boldsymbol S \end{Bmatrix}</script>，该集合称为<script type="math/tex">\boldsymbol x^*</script>处的<strong>切锥</strong>。切锥里面包含的方向可以告诉我们：从<script type="math/tex">\boldsymbol x^*</script>点可以验证哪一些方向/曲线可以移动，示意图如下： </p>
<blockquote>
<p><img src="https://pic.imgdb.cn/item/62957f4e0947543129e80269.jpg" style="zoom:40%"></p>
<p>上图中，红色箭头夹住部分为切锥包含的可行方向。</p>
</blockquote>
</li>
</ul>
<p><strong>2 最优解的必要条件</strong></p>
<p>若<script type="math/tex">\boldsymbol x^*</script>是问题(P)的局部最优解，则有：</p>
<script type="math/tex; mode=display">
\boldsymbol D(\boldsymbol x^*) ∩ \boldsymbol T(\boldsymbol x^*) = \boldsymbol \emptyset</script><p>只要证明上述表达式中描述的两个集合交集为空，等价于：任取<script type="math/tex">\alpha \boldsymbol d \in \boldsymbol T(\boldsymbol x^*)</script>，都有<script type="math/tex">\triangledown f(\boldsymbol x^*)^{\mathrm T} \boldsymbol d \geq 0</script></p>
<p><img src="https://pic.imgdb.cn/item/629586160947543129efd126.jpg" style="zoom:50%"></p>
<h5 id="三-与切锥关系紧密的两个集合"><a href="#三-与切锥关系紧密的两个集合" class="headerlink" title="(三) 与切锥关系紧密的两个集合"></a>(三) 与切锥关系紧密的两个集合</h5><ul>
<li><strong>可行方向集合</strong>：<ul>
<li><script type="math/tex; mode=display">\boldsymbol F(\boldsymbol x^*) = \begin{Bmatrix} \boldsymbol d \mid \boldsymbol x^* + \lambda \boldsymbol d \in \boldsymbol S, \forall \lambda \in (0, \delta), \text{ some } \delta > 0 \end{Bmatrix}</script></li>
<li>易知：<script type="math/tex">\boldsymbol F(\boldsymbol x^*) ⊆ \boldsymbol T(\boldsymbol x^*)</script>，即可行方向一定在<strong>切锥</strong>中。</li>
</ul>
</li>
<li><p>集合2：记<script type="math/tex">\boldsymbol x^*</script>处的<strong>有效指标集</strong><script type="math/tex">\boldsymbol I =  \begin{Bmatrix} i \mid g_i(\boldsymbol x^*)=0 \end{Bmatrix}</script>，定义下面集合</p>
<ul>
<li><script type="math/tex; mode=display">\boldsymbol F_1(\boldsymbol x^*) = \begin{Bmatrix} \boldsymbol d \mid \triangledown g_i(\boldsymbol x^*)^{\mathrm T} \boldsymbol d \leq 0, i \in \boldsymbol I, \triangledown h_j(\boldsymbol x^*)^{\mathrm T} \boldsymbol d = 0, j = 1,2,\cdots, l \end{Bmatrix}</script></li>
<li>易知：<script type="math/tex">\boldsymbol T(\boldsymbol x^*) ⊆ \boldsymbol F_1(\boldsymbol x^*)</script></li>
<li>证明：<ul>
<li><img src="https://pic.imgdb.cn/item/629586dc0947543129f0b2bc.jpg" style="zoom:40%"></li>
</ul>
</li>
</ul>
</li>
<li><p>综上：<script type="math/tex">\boldsymbol F(\boldsymbol x^*) ⊆ \boldsymbol T(\boldsymbol x^*) ⊆ \boldsymbol F_1(\boldsymbol x^*)</script></p>
</li>
</ul>
<h5 id="二-适当条件：约束规范-constrain-qualification"><a href="#二-适当条件：约束规范-constrain-qualification" class="headerlink" title="(二) 适当条件：约束规范(constrain qualification)"></a>(二) 适当条件：约束规范(constrain qualification)</h5><p>上文KKT条件描述中提到的<strong>适当条件</strong>就是此处要讨论的<strong>约束规范</strong>。在适当条件(约束规范下)：<script type="math/tex">\boldsymbol T(\boldsymbol x^*) = \boldsymbol F_1(\boldsymbol x^*)</script></p>
<ul>
<li>常用的约束规范有：<ul>
<li>函数——<script type="math/tex">g_i(\boldsymbol x^*), i \in \boldsymbol I, h_j(\boldsymbol x^*), j = 1,2,\cdots, l</script>均为线性函数；</li>
<li>向量组——<script type="math/tex">\triangledown g_i(\boldsymbol x^*), i \in \boldsymbol I, \triangledown h_j(\boldsymbol x^*), j = 1,2,\cdots, l</script>线性无关(LICQ)；</li>
<li>Slater Condition</li>
<li>……</li>
</ul>
</li>
</ul>
<p>若<script type="math/tex">\boldsymbol x^* \in \boldsymbol S</script>是问题(P)的局部最优解，在满足上述某一种约束条件下，有：</p>
<script type="math/tex; mode=display">
\boldsymbol D(\boldsymbol x^*) ∩ \boldsymbol T(\boldsymbol x^*) = \boldsymbol D(\boldsymbol x^*) ∩ \boldsymbol F_1(\boldsymbol x^*) = \boldsymbol \emptyset</script><p>即KKT条件的一种形式。</p>
<p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@</p>
<p>若<script type="math/tex">\boldsymbol x^* \in \boldsymbol S</script>是问题(P)的局部最优解，在满足上述某一种约束条件下，则KKT条件成立。</p>
<p>因为：根据Farkas引理，<script type="math/tex">\boldsymbol D(\boldsymbol x^*) ∩ \boldsymbol F_1(\boldsymbol x^*) = \boldsymbol \emptyset</script>当且仅当KKT条件成立。</p>
<p>证明：观看视频，太难了。。。</p>
<p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@</p>
<h4 id="8-2-KKT条件的解释说明"><a href="#8-2-KKT条件的解释说明" class="headerlink" title="8.2 KKT条件的解释说明"></a>8.2 KKT条件的解释说明</h4><h5 id="一-KKT条件中各部分的名字"><a href="#一-KKT条件中各部分的名字" class="headerlink" title="(一) KKT条件中各部分的名字"></a>(一) KKT条件中各部分的名字</h5><script type="math/tex; mode=display">
\triangledown f(\boldsymbol x^*) + \sum_{i=1}^{m}\lambda_i \triangledown g_i(\boldsymbol x^*) + \sum_{i=1}^{l} \mu_i \triangledown h_i(\boldsymbol x^*) = 0 \\
\lambda_i > 0, i = 1,2, \cdots, m \\
g_i(\boldsymbol x^*) \leq 0,  i = 1,2, \cdots, m \\
h_i(\boldsymbol x^*) = 0,  i = 1,2, \cdots, l \\
\lambda_i g_i(\boldsymbol x^*) = 0,  i = 1,2, \cdots, m</script><p>各部分组合的名字如下：</p>
<script type="math/tex; mode=display">
(\text{对偶可行(Dual Feasible, DF)}) \text{ }\begin{cases} & \triangledown f(\boldsymbol x^*) + \sum_{i=1}^{m}\lambda_i \triangledown g_i(\boldsymbol x^*) + \sum_{i=1}^{l} \mu_i \triangledown h_i(\boldsymbol x^*) = 0 \\ 
& \lambda_i > 0, i = 1,2, \cdots, m \end{cases}</script><script type="math/tex; mode=display">
(\text{原问题可行(Primal Feasible, PF)}) \text{ }\begin{cases} & g_i(\boldsymbol x^*) \leq 0,  i = 1,2, \cdots, m \\
& h_i(\boldsymbol x^*) = 0,  i = 1,2, \cdots, l \end{cases}</script><script type="math/tex; mode=display">
(\text{互补松弛条件(Complementary Slack, CS)})  ) \text{ }\begin{cases} & \lambda_i g_i(\boldsymbol x^*) = 0,  i = 1,2, \cdots, m \end{cases}</script><blockquote>
<p>不知道前面有没有说互补松弛的作用：保证那些不起作用的不等式约束在第一行中对应的乘子为零；</p>
<p>传统的互补问题就是：两个向量都是非负的，求内积等于0，其要求为对应分量相乘为0。</p>
</blockquote>
<h5 id="二-KKT条件中拉格朗日乘子意义"><a href="#二-KKT条件中拉格朗日乘子意义" class="headerlink" title="(二) KKT条件中拉格朗日乘子意义"></a>(二) KKT条件中拉格朗日乘子意义</h5><p>KKT条件中拉格朗日乘子可反映约束条件右端项发生扰动时最优目标函数值的变化情况。</p>
<p>设<script type="math/tex">\boldsymbol x^*</script>是问题(P)的局部最优解，且设<script type="math/tex">\boldsymbol x^*</script>满足KKT条件：</p>
<script type="math/tex; mode=display">
(P) \text{ }\begin{cases}\text { min } & f(\boldsymbol x) \\ 
\text{ s.t.} & g_{i}(\boldsymbol x) \leq 0, \text{  } i=1, \cdots, m \\ 
& h_{i}(\boldsymbol x) = 0, \text{  } i=1, \cdots, l\end{cases}</script><h5 id="三-KKT条件充分性"><a href="#三-KKT条件充分性" class="headerlink" title="(三) KKT条件充分性"></a>(三) KKT条件充分性</h5><p>以上好像都是在讨论KKT条件的必要性，即已经知道<script type="math/tex">\boldsymbol x^*</script>是局部最优解，那么在满足某一种约束条件下，KKT条件成立。</p>
<p>但我们也想知道什么时候会变成充分条件，即一个点满足KKT条件，则该点是问题(P)的最优解。</p>
<ul>
<li><p>当问题(P)中满足：</p>
<ul>
<li>(1) <script type="math/tex">f(\boldsymbol x), g_i(\boldsymbol x), i =1,2,\cdots, m</script>均为凸函数；</li>
<li>(2) <script type="math/tex">h_i(\boldsymbol x), i =1,2,\cdots, l</script>为线性函数；</li>
</ul>
</li>
<li><p>则满足条件的KKT点也是问题(P)(此两条条件限制下，问题P就是<strong>凸问题</strong>)的局部(全局)最优解。</p>
<blockquote>
<p>注意2点：</p>
<p>一、凸问题局部最优解即为全局最优解；</p>
<p>二、可能在一些其他问题，不满足上述两条条件(即问题P非凸)，KKT点也有可能是局部最优解，但判断方法需要借助<strong>黑塞矩阵</strong>，下面会讲到。</p>
</blockquote>
</li>
</ul>
<h3 id="第九讲-约束优化理论-贰-—最优性条件—二阶充分条件"><a href="#第九讲-约束优化理论-贰-—最优性条件—二阶充分条件" class="headerlink" title="第九讲 约束优化理论(贰)—最优性条件—二阶充分条件"></a>第九讲 约束优化理论(贰)—最优性条件—二阶充分条件</h3><h4 id="9-1-本讲基础"><a href="#9-1-本讲基础" class="headerlink" title="9.1  本讲基础"></a>9.1  本讲基础</h4><p>由<strong>第八讲(三)KKT条件充分性</strong>部分内容可知：</p>
<ul>
<li>当问题(P)中满足：<ul>
<li>(1) <script type="math/tex">f(\boldsymbol x), g_i(\boldsymbol x), i =1,2,\cdots, m</script>均为凸函数；</li>
<li>(2) <script type="math/tex">h_i(\boldsymbol x), i =1,2,\cdots, l</script>为线性函数；</li>
</ul>
</li>
<li>则满足条件的KKT点也是问题(P)(此两条条件限制下，问题P就是<strong>凸问题</strong>)的局部(全局)最优解。</li>
</ul>
<p>本将要解决的问题是，若通过验证，发现上面的条件(1)或者(2)不满足，那么还有没有什么其他条件说明当<script type="math/tex">\boldsymbol x^*</script>满足KKT条件时，KKT点<script type="math/tex">\boldsymbol x^*</script>是问题(P)的最优解。</p>
<p>假设<script type="math/tex">\boldsymbol x^*</script>满足KKT条件：</p>
<script type="math/tex; mode=display">
\triangledown f(\boldsymbol x^*) + \sum_{i=1}^{m}\lambda_i \triangledown g_i(\boldsymbol x^*) + \sum_{i=1}^{l} \mu_i \triangledown h_i(\boldsymbol x^*) = 0 \\
\lambda_i > 0, i = 1,2, \cdots, m \\
g_i(\boldsymbol x^*) \leq 0,  i = 1,2, \cdots, m \\
h_i(\boldsymbol x^*) = 0,  i = 1,2, \cdots, l \\
\lambda_i g_i(\boldsymbol x^*) = 0,  i = 1,2, \cdots, m</script><p>令：</p>
<script type="math/tex; mode=display">
L(\boldsymbol x) = f(\boldsymbol x) + \sum_{i=1}^{m}\lambda_i g_i(\boldsymbol x) + \sum_{i=1}^{l} \mu_i  h_i(\boldsymbol x)</script><script type="math/tex; mode=display">L(\boldsymbol x)$$称为**<font color=red>拉格朗日函数</font>**。

简单分析$$L(\boldsymbol x)$$可知其具备几个特征：

- **(1) 梯度为0**：$$\triangledown L(\boldsymbol x^*)  = \triangledown f(\boldsymbol x^*) + \sum_{i=1}^{m}\lambda_i \triangledown g_i(\boldsymbol x^*) + \sum_{i=1}^{l} \mu_i \triangledown h_i(\boldsymbol x^*) = 0</script><ul>
<li><strong>(2)</strong>：<script type="math/tex">L(\boldsymbol x^*)  = f(\boldsymbol x^*) + \sum_{i=1}^{m}\lambda_i g_i(\boldsymbol x^*) + \sum_{i=1}^{l} \mu_i h_i(\boldsymbol x^*) = f(\boldsymbol x^*)</script></li>
<li>(3)：<script type="math/tex">L(\boldsymbol x)  = f(\boldsymbol x) + \sum_{i=1}^{m}\lambda_i g_i(\boldsymbol x) + \sum_{i=1}^{l} \mu_i h_i(\boldsymbol x) \leq f(\boldsymbol x), \forall \boldsymbol x \in  \boldsymbol S</script></li>
</ul>
<p>由上面(2)、(3)可知：若<script type="math/tex">\boldsymbol x^*</script>是<script type="math/tex">L(\boldsymbol x)</script>的最优解(局部或者全局最优解都可以)，则<script type="math/tex">\boldsymbol x^*</script>是问题(P)的最优解。</p>
<blockquote>
<p>若<script type="math/tex">\boldsymbol x^*</script>是<script type="math/tex">L(\boldsymbol x)</script>的局部最优解，则<script type="math/tex">\boldsymbol x^*</script>是问题(P)的局部最优解；</p>
<p>若<script type="math/tex">\boldsymbol x^*</script>是<script type="math/tex">L(\boldsymbol x)</script>的全局最优解，则<script type="math/tex">\boldsymbol x^*</script>是问题(P)的全局最优解。</p>
<p>小证明：</p>
<p>局部最优解情况：<script type="math/tex">f(\boldsymbol x^*)  = L(\boldsymbol x^*) = L(\boldsymbol x)  = f(\boldsymbol x), \forall \boldsymbol x \in \boldsymbol S ∩ \boldsymbol N_\delta(\boldsymbol x^*)</script></p>
<p>全局最优解情况：<script type="math/tex">f(\boldsymbol x^*)  = L(\boldsymbol x^*) = L(\boldsymbol x)  = f(\boldsymbol x), \forall \boldsymbol x \in \boldsymbol S</script></p>
</blockquote>
<p>假设<script type="math/tex">\boldsymbol x^*</script>满足KKT条件，则有：</p>
<ul>
<li>(1)：若<script type="math/tex">\triangledown^2 L(\boldsymbol x)  \succeq 0, \forall \boldsymbol x \in \boldsymbol S</script>，则<script type="math/tex">L(\boldsymbol x)</script>在集合<script type="math/tex">\boldsymbol S</script>上是凸函数，则<script type="math/tex">\boldsymbol x^*</script>是问题(P)的全局最优解；</li>
<li>(2)：若<script type="math/tex">\triangledown^2 L(\boldsymbol x)  \succeq 0, \forall \boldsymbol x \in \boldsymbol S ∩ \boldsymbol N_\delta(\boldsymbol x^*)</script>，则<script type="math/tex">L(\boldsymbol x)</script>在上<script type="math/tex">\boldsymbol x^*</script>某一邻域是凸函数，则<script type="math/tex">\boldsymbol x^*</script>是问题(P)的局部最优解；</li>
<li>(3)：若<script type="math/tex">\triangledown^2 L(\boldsymbol x^*)  \succ 0</script>，则<script type="math/tex">\boldsymbol x^*</script>是问题(P)的严格局部最优解；</li>
</ul>
<blockquote>
<p>视频课程中这里有一部分分析推导，有时间记录</p>
</blockquote>
<h4 id="9-2-二阶充分条件"><a href="#9-2-二阶充分条件" class="headerlink" title="9.2 二阶充分条件"></a>9.2 二阶充分条件</h4><p>下面是本讲内容的核心：</p>
<h5 id="一-二阶充分条件定义"><a href="#一-二阶充分条件定义" class="headerlink" title="(一) 二阶充分条件定义"></a>(一) 二阶充分条件定义</h5><p>假设<script type="math/tex">\boldsymbol x^*</script>是KKT点，易知：</p>
<ul>
<li><script type="math/tex; mode=display">\triangledown L(\boldsymbol x^*)  = 0</script></li>
<li><script type="math/tex; mode=display">\boldsymbol d^T \triangledown^2 L(\boldsymbol x^*) \boldsymbol d > 0, \forall \boldsymbol d \in F_2(\boldsymbol x^*)</script></li>
</ul>
<p>则<script type="math/tex">\boldsymbol x^*</script>是问题(P)的严格局部最优解。</p>
<h5 id="二-证明"><a href="#二-证明" class="headerlink" title="(二) 证明"></a>(二) 证明</h5><p>反证：假设<script type="math/tex">\boldsymbol x^*​</script>是KKT点，但<script type="math/tex">\boldsymbol x^*​</script>不是问题(P)的严格局部最优解。则能找到一个点<script type="math/tex">\boldsymbol x_k \to \boldsymbol x^* ​</script>，使得：</p>
<script type="math/tex; mode=display">
f(\boldsymbol x_k) \leq f(\boldsymbol x^*), \forall \boldsymbol x_k \in \boldsymbol S</script><p>记<script type="math/tex">\boldsymbol d_k = \dfrac{\boldsymbol x_k- \boldsymbol x^*}{\| \boldsymbol x_k- \boldsymbol x^* \|}</script>，<script type="math/tex">\alpha_k = {\| \boldsymbol x_k- \boldsymbol x^* \|}</script>，则会得到两个点列：<script type="math/tex">\begin{Bmatrix} \boldsymbol d_k \end{Bmatrix}</script>，<script type="math/tex">\begin{Bmatrix} \alpha_k \end{Bmatrix}</script>，易得：</p>
<ul>
<li>因为<script type="math/tex">\boldsymbol x_k \to \boldsymbol x^*</script>，因此<script type="math/tex">\begin{Bmatrix} \alpha_k \end{Bmatrix} \to 0</script></li>
<li><script type="math/tex; mode=display">\begin{Bmatrix} \boldsymbol d_k \end{Bmatrix}$$是有界的，因此必然存在收敛子列，设最终$$\begin{Bmatrix} \boldsymbol d_k \end{Bmatrix} \to \boldsymbol d</script></li>
</ul>
<p>则有：<script type="math/tex">\boldsymbol x_k = \boldsymbol x^* + \alpha_k \boldsymbol d_k</script>，</p>
<script type="math/tex; mode=display">
f(\boldsymbol x_k) - f(\boldsymbol x^*) \xlongequal{泰勒展开} \triangledown f(\boldsymbol x^*)^{\mathrm T}(\boldsymbol x_k - \boldsymbol x^*) +\dfrac{\alpha_k^2}{2} \boldsymbol d_k^{\mathrm T}\triangledown f(\boldsymbol x^*)\boldsymbol d_k+\text{o}(\alpha_k^2) \\= \alpha_k \triangledown f(\boldsymbol x^*)^{\mathrm T} \boldsymbol d_k +\dfrac{\alpha_k^2}{2} \boldsymbol d_k^{\mathrm T}\triangledown f(\boldsymbol x^*)\boldsymbol d_k+\text{o}(\alpha_k^2) \leq 0</script><script type="math/tex; mode=display">
g_i(\boldsymbol x_k) - g_i(\boldsymbol x^*) \xlongequal{泰勒展开} \triangledown g_i(\boldsymbol x^*)^{\mathrm T}(\boldsymbol x_k - \boldsymbol x^*) +\dfrac{\alpha_k^2}{2} \boldsymbol d_k^{\mathrm T}\triangledown g_i(\boldsymbol x^*)\boldsymbol d_k+\text{o}(\alpha_k^2) \\= \alpha_k \triangledown g_i(\boldsymbol x^*)^{\mathrm T} \boldsymbol d_k +\dfrac{\alpha_k^2}{2} \boldsymbol d_k^{\mathrm T}\triangledown g_i(\boldsymbol x^*)\boldsymbol d_k+\text{o}(\alpha_k^2) \leq 0</script><script type="math/tex; mode=display">
h_i(\boldsymbol x_k) - h_i(\boldsymbol x^*) \xlongequal{泰勒展开} \triangledown h_i(\boldsymbol x^*)^{\mathrm T}(\boldsymbol x_k - \boldsymbol x^*) +\dfrac{\alpha_k^2}{2} \boldsymbol d_k^{\mathrm T}\triangledown h_i(\boldsymbol x^*)\boldsymbol d_k+\text{o}(\alpha_k^2) \\= \alpha_k \triangledown h_i(\boldsymbol x^*)^{\mathrm T} \boldsymbol d_k +\dfrac{\alpha_k^2}{2} \boldsymbol d_k^{\mathrm T}\triangledown h_i(\boldsymbol x^*)\boldsymbol d_k+\text{o}(\alpha_k^2) = 0</script><link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>数学基础</category>
        <category>最优化方法</category>
      </categories>
      <tags>
        <tag>最优化</tag>
      </tags>
  </entry>
  <entry>
    <title>先验概率与后验概率</title>
    <url>/2022/01/22/Prior-and-Posterior-Probabilities/</url>
    <content><![CDATA[<h4 id="1-先验概率"><a href="#1-先验概率" class="headerlink" title="1. 先验概率"></a>1. 先验概率</h4><p>事情还没有发生，根据以往的经验来判断事情发生的概率，是“由因求果”。(根据统计和规律得出得概率 )</p>
<blockquote>
<p>扔一个硬币，在扔之前就知道正面向上的概率为0.5。这是根据我们之前的经验得到的。这个0.5就是先验概率。</p>
</blockquote>
<h4 id="2-后验概率"><a href="#2-后验概率" class="headerlink" title="2. 后验概率"></a>2. 后验概率</h4><p>事情已发生，有多种原因，判断事情的发生是由哪一种原因引起的，是“由果求因”。(根据贝叶斯公式可以得到) </p>
<blockquote>
<p>今天上学迟到了，有两个原因，一个是自行车坏了，一个是生病了。后验概率就是根据结果（迟到）来计算原因（生病/自行车坏了）的概率。</p>
</blockquote>
<p><strong><font color="red"> 数学表达上，后验概率和条件概率有相同的形式</font></strong></p>
<p><strong><font color="red">贝叶斯公式就是由先验概率求后验概率的公式</font></strong></p>
<h4 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h4><p>先验概率是对某一件事情发生可能性的预先客观评估，而后验概率是对事情发生是由某一个原因导致的概率。</p>
<p>先验概率是以全事件为背景下A事件发生的概率$P(A|\Omega)$，后验概率是以新事件B为背景下A事件发生的概率$P(A|B)$全事件一般是统计获得的，所以称为先验概率，没有实验前的概率。新事件一般是实验，如试验B，此时的事件背景从全事件变成了B，该事件B可能对A的概率有影响，那么需要对A现在的概率进行一个修正，从$P(A|Ω)$变成$P(A|B)$，所以称 $P(A|B)$为后验概率，也就是试验(事件B发生)后的概率。</p>
<p>后验概率分布是条件概率的一个例子。 后验概率告诉我们，在我们观察到一组特定数据的情况下，参数值的可能性有多大。 当然，条件概率不需要描述参数和数据，所以后验概率是条件概率的一个特例。</p>
<h4 id="4-补充-条件概率与后验概率"><a href="#4-补充-条件概率与后验概率" class="headerlink" title="4. 补充-条件概率与后验概率"></a>4. 补充-条件概率与后验概率</h4><p>后验概率就是一种条件概率，但是与其它条件概率的不同之处在于，它限定了目标事件为隐变量取值，而其中的条件为观测结果。 一般的条件概率，条件和事件都可以是任意的。</p>
<p><strong>举例区分普通条件概率与后验概率的区别</strong>：</p>
<blockquote>
<p>1）那么如果我们出门之前我们听到新闻说今天路上出了个交通事故，那么我们想算一下堵车的概率，这个就叫做条件概率 。也就是P(堵车|交通事故)。这是有因求果。</p>
<p>2）如果我们已经出了门，然后遇到了堵车，那么我们想算一下堵车时由交通事故引起的概率有多大，那这个就叫做后验概率 （其实也是条件概率，但是通常习惯这么说） 。也就是P(交通事故|堵车)。这是有果求因。</p>
</blockquote>
<p> 从上述例子可知道，后验概率就是在已知某B事件发生的情况下，求解其中A事件发生的概率是多少，而A事件正是B事件发生的一个隐状态事件，所以A与B是有前后关联的。在利用贝叶斯进行文本分类的时候也是这个意思：</p>
<script type="math/tex; mode=display">
P(\frac{c}{d})=\frac{p(\frac{d}{c})p(c)}{p(d)}</script><p>d文档分为c类的概率，$p(c)$就是先验概率，$p(c/d)$就是后验概率，所以贝叶斯就是用先验概率估计后验概率。</p>
<p>而一般的条件概率，目标事件A和条件事件B，是可以没有任何关系的。</p>
<hr>
<p>概率论老师说：</p>
<p>A的概率——$P(A)$</p>
<p>A在条件C下的概率——$P(A|C)$</p>
<p>统计学老师说：</p>
<p>A事件发生的先验概率——$P(A)$</p>
<p>知道C事件发生后，A事件发生的后验概率——$P(A|C)$</p>
<p><strong>小结</strong></p>
<p>条件概率和后验概率理论上是没区别，唯一的区别在于后验概率来自后验概率分布上。后验概率按理应该是在贝叶斯推论框架下，在这个框架下，必须将后验概率分布和后验概率结合来看。</p>
<p>综上，后验概率是贝叶斯推论中的概念，而条件概率在频率学派和贝叶斯学派中都有使用，注意这两个学派的区别后，应该能能理解这两者的区别。</p>
<h4 id="相关链接："><a href="#相关链接：" class="headerlink" title="相关链接："></a>相关链接：</h4><p>参考链接1：<a href="https://www.zhihu.com/question/20587681">贝叶斯学派与频率学派有何不同？ - 知乎</a></p>
<p>参考链接2：<a href="https://zhuanlan.zhihu.com/p/27006631">用贝叶斯 计算后验概率(一) - 住奥马哈的张校长的文章 - 知乎</a></p>
<p>参考链接3：<a href="https://www.zhihu.com/question/27398304">贝叶斯推断如何更新后验概率？ - 知乎</a></p>
<p>参考链接4：<a href="https://blog.csdn.net/FWing/article/details/4850068?utm_source=app&amp;app_version=5.0.0">概率与似然 - CSDN</a> </p>
<p>参考链接5：<a href="https://blog.csdn.net/songyu0120/article/details/85059149?utm_source=app&amp;app_version=5.0.0">似然(likelihood)和概率(probability)的区别与联系 - CSDN</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>数学基础</category>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>概率</tag>
      </tags>
  </entry>
  <entry>
    <title>概率基础系统学习</title>
    <url>/2022/02/12/Probabilistic-Basic-Systems-Learning/</url>
    <content><![CDATA[<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1ot411y7mU" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i1.hdslb.com/bfs/archive/0ccbc2307d31f9fe6d0877ac025a729333823ed8.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">65:24:09</span>
            </div>
            <div class="bvideo-info">
                <p class="title">《概率论与数理统计》教学视频全集（宋浩）</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>3531.1万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>88.9万</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">宋浩老师官方</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<h2 id="第一章-随机事件及其概率"><a href="#第一章-随机事件及其概率" class="headerlink" title="第一章 随机事件及其概率"></a>第一章 随机事件及其概率</h2><h3 id="1-1-随机事件"><a href="#1-1-随机事件" class="headerlink" title="1.1 随机事件"></a>1.1 随机事件</h3><h4 id="1-1-1-随机试验与随机事件"><a href="#1-1-1-随机试验与随机事件" class="headerlink" title="1.1.1 随机试验与随机事件"></a>1.1.1 随机试验与随机事件</h4><ul>
<li><h5 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h5></li>
</ul>
<p><strong>随机实验：</strong>① 在相同条件下可重复；② 实验结果不只有一个；③ 无法预测。</p>
<p><strong>事件：</strong>随机实验的每一种结果就称为一个事件。</p>
<p><strong>基本事件：</strong>相对于实验目的不可以再分的事件称为基本事件。</p>
<p><strong>复合事件：</strong>几个基本事件的复合。</p>
<p><strong>必然事件：</strong>一定会发生的事件，使用符号$\Omega$表示(全集)。</p>
<p><strong>不可能事件：</strong>一定不会发生的事件，使用符号$\phi$表示(空集)。</p>
<h4 id="1-1-2-样本空间与事件的集合表示"><a href="#1-1-2-样本空间与事件的集合表示" class="headerlink" title="1.1.2 样本空间与事件的集合表示"></a>1.1.2 样本空间与事件的集合表示</h4><ul>
<li><h5 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h5></li>
</ul>
<p><strong>样本空间：</strong>所有基本事件的集合，用$\Omega$表示(把样本点是看作一个整体，就可以说样本空间在每次试验中都出现了，因而<strong>样本空间是随机试验的必然事件</strong> )</p>
<p><strong>样本点：</strong>样本空间的元素，使用$\omega$表示。</p>
<h4 id="1-1-3-事件间的关系"><a href="#1-1-3-事件间的关系" class="headerlink" title="1.1.3 事件间的关系"></a>1.1.3 事件间的关系</h4><h5 id="一、事件之间的关系"><a href="#一、事件之间的关系" class="headerlink" title="一、事件之间的关系"></a>一、事件之间的关系</h5><p>1、包含——$\boldsymbol {\subseteq}$</p>
<p>$A \subseteq B$：A发生必然导致B发生。</p>
<p>一定存在的包含关系：$\phi \subseteq A \subseteq \Omega$</p>
<p>2、相等——$\boldsymbol  =$</p>
<p>$A = B$：A事件和B事件相等，即$A \subseteq  B$ AND $B \subseteq A$。</p>
<p>3、并/和——$\boldsymbol  \cup $</p>
<p>$A \cup B = A+B$：A、B中至少有一个发生。</p>
<p>4、交/积——$\boldsymbol \cap$</p>
<p>$A \cap B = AB$：A、B同时发生。</p>
<p>5、差——$\boldsymbol  -$</p>
<p>$A-B$：A发生而B不发生。</p>
<p>$A - B = A - AB = A \bar B$</p>
<p>6、互不相容事件</p>
<p>$AB = \phi$：A、B不会同时发生。</p>
<p>7、对立事件</p>
<p>$A = \bar B$：A、B互不相容$AB = \phi$，且$A \cup B = \Omega$。</p>
<p>8、完备事件组</p>
<p>$A_1,A_2,…,A_n$两两互不相容，且$\bigcup\limits_{i=1}^n = \Omega$</p>
<h5 id="二、集合的运算关系"><a href="#二、集合的运算关系" class="headerlink" title="二、集合的运算关系"></a>二、集合的运算关系</h5><p>1、交换律：$A \cup B = B \cup A$，$A \cap B = B \cap A$</p>
<p>2、结合律：$(A \cup B) \cup C = A \cup (B \cup C)$，$(A \cap B) \cap C = A \cap (B \cap C)$</p>
<p>3、分配率：$(A\cup B) \cap C = (A \cap C) \cup (B \cap C)$，$(A \cap B) \cup C = (A \cup C) \cap (B \cup C)$</p>
<p>4、对偶率：$\overline{A \cup B} = \bar A \cap \bar B$，$\overline{A \cap B} = \bar A \cup \bar B$</p>
<p>$\overline{A_1 \cup A_2 \cup \cdots \cup A_n} = \bar A_1 \cap \bar A_2 \cap \cdots \cap A_n$，$\overline{A_1 \cap A_2 \cap \cdots \cap A_n} = \bar A_1 \cup \bar A_2 \cup \cdots \cup A_n$</p>
<h3 id="1-2-事件的概率"><a href="#1-2-事件的概率" class="headerlink" title="1.2 事件的概率"></a>1.2 事件的概率</h3><h4 id="1-2-1-概率的初等描述"><a href="#1-2-1-概率的初等描述" class="headerlink" title="1.2.1 概率的初等描述"></a>1.2.1 概率的初等描述</h4><p><strong>概率：</strong>随机事件出现的可能性大小。 常用$P(A)$ 表示A事件出现的概率。</p>
<h4 id="1-2-2-古典概率模型"><a href="#1-2-2-古典概率模型" class="headerlink" title="1.2.2 古典概率模型"></a>1.2.2 古典概率模型</h4><h5 id="一、定义"><a href="#一、定义" class="headerlink" title="一、定义"></a>一、定义</h5><p>古典概率模型要求：有限个样本点(e.g. 掷一枚骰子只有有六种可能)且所有样本点出现的可能性是<strong>相等的</strong>，则古典概率中事件A的概率为：</p>
<script type="math/tex; mode=display">
P(A) = \frac{A中包含的基本事件个数}{\Omega中包含的基本事件个数}</script><h5 id="二、排列组合的基础知识"><a href="#二、排列组合的基础知识" class="headerlink" title="二、排列组合的基础知识"></a>二、排列组合的基础知识</h5><p>……</p>
<h5 id="三、古典概率模型的性质"><a href="#三、古典概率模型的性质" class="headerlink" title="三、古典概率模型的性质"></a>三、古典概率模型的性质</h5><p>1、非负性：$0 \leq P(A) \leq 1$；</p>
<p>2、规范性：$P(\Omega) = 1，P(\phi) = 0$；</p>
<p>3、有限可加：$A_1, A_2, \cdots ,A_n $互不相容，则有$P(A_1+A_2+ \cdots +A_n) = P(A_1)+P(A_2)+ \cdots +P(A_n) $；</p>
<h4 id="1-2-3-几何概率模型"><a href="#1-2-3-几何概率模型" class="headerlink" title="1.2.3 几何概率模型"></a>1.2.3 几何概率模型</h4><h5 id="一、定义-1"><a href="#一、定义-1" class="headerlink" title="一、定义"></a>一、定义</h5><p>几何概型是一种概率模型。在这个模型下，随机实验所有可能的结果是无限的，并且每个基本结果发生的概率是相同的。例如一个人到单位的时间可能是8:00~9:00之间的任意一个时刻；往一个方格中投一个石子，石子落在方格中任何一点上……这些试验出现的结果都是无限多个，属于几何概型。一个试验是否为几何概型在于这个试验是否具有几何概型的两个特征——无限性和等可能性，只有同时具备这两个特点的概型才是几何概型。 </p>
<h5 id="二、性质"><a href="#二、性质" class="headerlink" title="二、性质"></a>二、性质</h5><p>1、完全可加性：<script type="math/tex">A_1, A_2, \cdots ,A_n</script>互不相容，则有<script type="math/tex">P(\bigcup\limits_{i=1}^{\infty}A_i) = \sum\limits_{i=1}^{\infty}A_i</script>；</p>
<h5 id="三、古典概率模型与几何概率模型的区别"><a href="#三、古典概率模型与几何概率模型的区别" class="headerlink" title="三、古典概率模型与几何概率模型的区别"></a>三、古典概率模型与几何概率模型的区别</h5><p>古典概型的基本事件都是有限的，概率为事件所包含的基本事件除以总基本事件个数。 几何概型的基本事件通常不可计数，只能通过一定的测度，像长度，面积，体积的的比值来表示。 </p>
<h5 id="四、随机模拟-蒙特卡洛法："><a href="#四、随机模拟-蒙特卡洛法：" class="headerlink" title="四、随机模拟/蒙特卡洛法："></a>四、随机模拟/蒙特卡洛法：</h5><p>1、基本思想</p>
<p>蒙特卡罗方法是一种计算方法。原理是通过大量随机样本，去了解一个系统，进而得到所要计算的值。 </p>
<p>蒙特卡洛法的基本思想是：为了求解问题，首先建立一个概率模型或随机过程，使它的参数或数字特征等于问题的解；然后通过对模型或过程的观察或抽样试验来计算这些参数或数字特征，最后给出所求解的近似值。解的精确度用估计值的标准误差来表示。蒙特卡洛法的主要理论基础是概率统计理论，主要手段是随机抽样、统计试验。 </p>
<p>2、求解问题的基本步骤 </p>
<ul>
<li>根据实际问题的特点，构造简单而又便于实现的概率统计模型，使所求的解恰好是所求问题的概率分布或数学期望；</li>
<li>给出模型中各种不同分布随机变量的抽样方法；</li>
<li>统计处理模拟结果，给出问题解的统计估计值和精度估计值。</li>
</ul>
<p>3、例子：$\pi$的计算</p>
<p>正方形内部有一个相切的圆，它们的面积之比是$\dfrac{\pi}{4}$。 </p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL3d3dy5ydWFueWlmZW5nLmNvbS9ibG9naW1nL2Fzc2V0LzIwMTUvYmcyMDE1MDcyNjExLmpwZw?x-oss-process=image/format,png" style="zoom:70%"></p>
<p>现在，在这个正方形内部，随机产生10000个点（即10000个坐标对 (x, y)），计算它们与中心点的距离，从而判断是否落在圆的内部。 </p>
<p><img src="https://pic.imgdb.cn/item/620cb0fe2ab3f51d910c4f91.jpg" style="zoom:40%"></p>
<p>如果这些点均匀分布，那么圆内的点应该占到所有点的$\dfrac{\pi}{4}$，因此将这个比值乘以4，就是π的值。通过R语言脚本随机模拟30000个点，π的估算值与真实值相差0.07%。 </p>
<h4 id="1-2-4-频率与概率"><a href="#1-2-4-频率与概率" class="headerlink" title="1.2.4 频率与概率"></a>1.2.4 频率与概率</h4><h5 id="一、频率"><a href="#一、频率" class="headerlink" title="一、频率"></a>一、频率</h5><p>1、定义：做了n次实验，事件A发生了m次，则定义频率为$\omega_n(A) = \dfrac{m}{n}$。</p>
<p>2、性质：</p>
<ul>
<li>非负性——$0 \leq \omega_n(A) \leq 1$；</li>
<li>规范性——$\omega_n(必然事件) = \omega_n(\Omega) = 1$，$\omega_n(不可能事件) = \omega_n(\phi) = 0$；</li>
<li>可加性——设事件$A_1, \cdots, A_k$两两互不相容，则有$\omega_n(A_1 + \cdots +A_k) = \omega_n(A_1)+\cdots + \omega_n(A_k)$；</li>
</ul>
<h5 id="二、概率"><a href="#二、概率" class="headerlink" title="二、概率"></a>二、概率</h5><p>(PS：这里主要是讲频率与概率的关系)</p>
<p>概率可以视为事件的一个属性，先于实验而存在，不随实验的变换而变化，随着实验次数的增加，频率逐渐接近概率。</p>
<h4 id="1-2-5-概率的公理化"><a href="#1-2-5-概率的公理化" class="headerlink" title="1.2.5 概率的公理化"></a>1.2.5 概率的公理化</h4><h5 id="一、概率的定义方法"><a href="#一、概率的定义方法" class="headerlink" title="一、概率的定义方法"></a>一、概率的定义方法</h5><p>描述概率定义、古典概率定义、几何概率定义、统计概率定义</p>
<p>以上这四种定义的概率都有公共的3种性质：① 非负性；② 规范性；③ 可加性；</p>
<h5 id="二、概率的公理化定义"><a href="#二、概率的公理化定义" class="headerlink" title="二、概率的公理化定义"></a>二、概率的公理化定义</h5><p>1、首先提炼出3条公理</p>
<p><strong>公理1(非负性)：</strong>$0 \leq P(A) \leq 1$</p>
<p><strong>公理2(规范性)：</strong>$P(\Omega) = 1$，注意——公理2中没有定义$P(\phi) = 0$</p>
<p><strong>公理3(完全可加性)：</strong>若事件$A_1, A_2, \cdots $不相容，则$P(A_1+A_2+ \cdots) = P(A_1)+P(A_2)+ \cdots$</p>
<blockquote>
<p><strong>可加性性质：有限可加性，可列可加性，完全可加性</strong></p>
<p>其中：可列可加性=完全可加性</p>
<p>可列可加性的理解可能要深入<strong>测度论</strong>的学习</p>
</blockquote>
<p>2、从3条公理推导其他性质</p>
<p><strong>性质1：</strong>$P(\phi) = 0$</p>
<blockquote>
<p>证明：将$\Omega$写为$\Omega = \Omega + \phi + \phi + \phi + \cdots$，易知它们之间互不相容，则有<br>$P(\Omega) = P(\Omega + \phi + \phi + \phi + \cdots) = P(\Omega) + P(\phi) + P(\phi) + \cdots$<br>两边同时减去$P(\Omega)$，则有$0 = P(\phi) + P(\phi) + \cdots$<br>又知$0 \leq P(A) \leq 1$，故$P(\phi) = 0$，得证！</p>
</blockquote>
<p><strong>性质2：</strong>有限可加性——本性质说明完全可加性可以推出有限可加性</p>
<p>若事件$A_1, A_2, \cdots ,A_n$不相容，则$P(A_1+A_2+ \cdots +A_n) = P(A_1)+P(A_2)+ \cdots + P(A_n)$</p>
<blockquote>
<p>证明：$A_1, A_2, \cdots, A_n, \phi + \phi + \phi + \cdots$，易知它们之间互不相容，则有<br>$P(A_1 + A_2 + \cdots + A_n) = P(A_1 + A_2 + \cdots + A_n + \phi + \phi + \phi + \cdots) = P(A_1) + P(A_2) + \cdots + P(A_n)+P(\phi) + P(\phi) + \cdots$<br>又由性质1可知$P(\phi) = 0$，则有$P(A_1 + A_2 + \cdots + A_n) =P(A_1) + P(A_2) + \cdots + P(A_n)$</p>
</blockquote>
<p><strong>性质3：</strong>$P(\bar A) = 1 - P(A)$</p>
<p><strong>性质4：</strong></p>
<p>—— ① $P(A-B) = P(A) - P(AB)$</p>
<p>—— ② $B \subset A, P(A-B) = P(A) - P(B) ,且P(A) \geq P(B)$</p>
<blockquote>
<p>证明①：易知$A = (A-B) \cup AB$，$A-B$与$AB$是互不相容的，则有<br>$P(A) = P(A-B) + P(AB)$，移项得证。</p>
<p>证明②：略</p>
</blockquote>
<p><strong>性质5(加法性质)：</strong>$P(A + B) = P(A) + P(B) - P(AB)$</p>
<blockquote>
<p>证明：略</p>
</blockquote>
<h3 id="1-3-条件概率与乘法公式"><a href="#1-3-条件概率与乘法公式" class="headerlink" title="1.3 条件概率与乘法公式"></a>1.3 条件概率与乘法公式</h3><h4 id="1-3-1-条件概率"><a href="#1-3-1-条件概率" class="headerlink" title="1.3.1 条件概率"></a>1.3.1 条件概率</h4><h5 id="一、定义-2"><a href="#一、定义-2" class="headerlink" title="一、定义"></a>一、定义</h5><p>样本空间为$\Omega$，A、B两个事件，$P(B) &gt; 0$，在B已经发生的条件下A发生的概率叫做A对B的条件概率，记为：$P(A|B)$</p>
<h5 id="二、计算"><a href="#二、计算" class="headerlink" title="二、计算"></a>二、计算</h5><p>$P(A|B) = \dfrac{P(AB)}{P(B)}$</p>
<h4 id="1-3-2-乘法公式"><a href="#1-3-2-乘法公式" class="headerlink" title="1.3.2 乘法公式"></a>1.3.2 乘法公式</h4><p>由条件概率可以推导得：</p>
<p>$P(AB) = P(B)P(A|B) = P(A)P(B|A)$</p>
<p><strong>推广：</strong><script type="math/tex">P(A_1 A_2 \cdots A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1 A_2) \cdots P(A_n | A_1 A_2 \cdots A_{n-1})</script></p>
<h3 id="1-4-全概率公式与贝叶斯公式"><a href="#1-4-全概率公式与贝叶斯公式" class="headerlink" title="1.4 全概率公式与贝叶斯公式"></a>1.4 全概率公式与贝叶斯公式</h3><h4 id="1-4-1-全概率公式"><a href="#1-4-1-全概率公式" class="headerlink" title="1.4.1 全概率公式"></a>1.4.1 全概率公式</h4><h5 id="一、定义-3"><a href="#一、定义-3" class="headerlink" title="一、定义"></a>一、定义</h5><p>假设$A_1, A_2, \cdots, A_n$是实验E的完备事件组(实际中不一定是完备的，只要能把事件B全部包括进去就行)，则对于某一事件B有：</p>
<script type="math/tex; mode=display">
P(B) = \sum_{i=1}^{n}P(A_i)P(B|A_i)</script><p><img src="https://pic.imgdb.cn/item/620f368d2ab3f51d919636fb.jpg" style="zoom:40%"></p>
<h4 id="1-4-2-贝叶斯公式"><a href="#1-4-2-贝叶斯公式" class="headerlink" title="1.4.2 贝叶斯公式"></a>1.4.2 贝叶斯公式</h4><h5 id="一、定义-4"><a href="#一、定义-4" class="headerlink" title="一、定义"></a>一、定义</h5><p>假设$A_1, A_2, \cdots, A_n$是实验E的完备事件组，则对于某一事件B有：</p>
<script type="math/tex; mode=display">
P(A_k | B) = \frac{P(A_k B)}{P(B)} = \frac{P(A_k B)的乘法公式}{P(B)的全概率公式} = \frac{P(A_k)P(B|A_k)}{\sum\limits_{i = 1}^{n}P(A_i)P(B|A_i)}</script><p>其中，$P(A_i)$称为<font color="red"><strong>先验概率</strong></font>，$P(A_i | B)$称为<font color="red"><strong>后验概率</strong></font> </p>
<h5 id="二、与全概率公式的辨析"><a href="#二、与全概率公式的辨析" class="headerlink" title="二、与全概率公式的辨析"></a>二、与全概率公式的辨析</h5><p>全概率公式是<font color="red"><strong>由因求果</strong></font></p>
<p>贝叶斯公式是<font color="red"><strong>由果溯因</strong></font></p>
<h3 id="1-5-事件的独立性与伯努利概型"><a href="#1-5-事件的独立性与伯努利概型" class="headerlink" title="1.5 事件的独立性与伯努利概型"></a>1.5 事件的独立性与伯努利概型</h3><h4 id="1-5-1-事件的独立性"><a href="#1-5-1-事件的独立性" class="headerlink" title="1.5.1 事件的独立性"></a>1.5.1 事件的独立性</h4><h5 id="一、定义-5"><a href="#一、定义-5" class="headerlink" title="一、定义"></a>一、定义</h5><p>A事件发生的概率不受B事件发生与否的影响，用公式可以表述为：$P(A|B) = P(A)$，或者说：</p>
<script type="math/tex; mode=display">
P(AB) = P(A)P(B)</script><p><strong>推论：</strong></p>
<p>① $\phi,\Omega$与任意事件均独立；</p>
<p>② 若A与B相互独立，则$\bar A 与 B$、$A 与 \bar B$、$\bar A 与 \bar B$也相互独立；</p>
<p>③ 若事件A满足$P(A) = 0 或 P(A) = 1$，则事件A与任意事件都独立；(注意：事件的概率等于0或者1不一定就是空集$\phi$或者全集$\Omega$) </p>
<p>证明略。。。</p>
<h5 id="二、独立与互不相容-互斥-辨析"><a href="#二、独立与互不相容-互斥-辨析" class="headerlink" title="二、独立与互不相容(互斥)辨析"></a>二、独立与互不相容(互斥)辨析</h5><p>1、从定义来讲：</p>
<p>A与B不相容，是$P(AB) = \phi$</p>
<p>A与B独立，是$P(AB) = P(A)P(B)$</p>
<p>2、通俗来讲：</p>
<p>A、B不相容是A发生了，B就一定不能发生，反之亦然；</p>
<p>A、B相互独立是A是否发生并不影响事件A发生的概率，所以A发生了B也可能会发生。 </p>
<p>3、推论：</p>
<p>当$P(A) &gt; 0$、$P(B) &gt; 0$时，互不相容与独立不会同时发生。</p>
<h5 id="三、补充知识——对立"><a href="#三、补充知识——对立" class="headerlink" title="三、补充知识——对立"></a>三、补充知识——对立</h5><p>最后再说一下对立事件，对立是指A、B必然发生一个且仅有一个发生，比如今天下雨或者不下雨，<strong>对立一定互不相容(互斥)，但是互不相容不一定对立</strong></p>
<h4 id="1-5-2-伯努利模型"><a href="#1-5-2-伯努利模型" class="headerlink" title="1.5.2 伯努利模型"></a>1.5.2 伯努利模型</h4><h5 id="一、几个相关概念"><a href="#一、几个相关概念" class="headerlink" title="一、几个相关概念"></a>一、几个相关概念</h5><p>1、独立实验序列：实验$E_1,E_2, \cdots, E_n$相互独立；</p>
<p>2、N重独立实验：把一个实验做N次，每次都是相互独立的；</p>
<p>3、伯努利实验：实验结果只有两种的实验(e.g. 硬币的正反)</p>
<p>4、N重伯努利实验：重复做N次实验，每次实验都相互独立，且实验的结果只有两种的实验。</p>
<h5 id="二、定义"><a href="#二、定义" class="headerlink" title="二、定义"></a>二、定义</h5><p>在伯努利实验模型中，事件A发生的概率是$P$，则$\bar A$的概率为$1-P$，那么在N重伯努利实验中，事件A发生k次的概率为：</p>
<script type="math/tex; mode=display">
P_N(k) = C_N^k P^k (1-P)^{N-k}</script><p>上面的公式又称为二项概率公式。</p>
<h2 id="第二章-随机变量及其分布"><a href="#第二章-随机变量及其分布" class="headerlink" title="第二章 随机变量及其分布"></a>第二章 随机变量及其分布</h2><h3 id="2-1-随机变量的概念"><a href="#2-1-随机变量的概念" class="headerlink" title="2.1 随机变量的概念"></a>2.1 随机变量的概念</h3><h5 id="一、定义-6"><a href="#一、定义-6" class="headerlink" title="一、定义"></a>一、定义</h5><p><strong><font color="red">重点：随机变量的本质是函数——样本点的函数</font></strong></p>
<p>设$X(w)$是定义在概率空间$(\Omega, \mathscr{F}, P)$上的单值实函数(也就是说它的自变量就是一个个的样本点，因变量是实函数)，如果对直线上任意一博雷尔点集$B$，有$\begin{Bmatrix} \omega, X(\omega) \in B \end{Bmatrix} \subset \mathscr{F}$则称$X(\omega)$为随机变量(r.v.)。</p>
<h5 id="二、表示"><a href="#二、表示" class="headerlink" title="二、表示"></a>二、表示</h5><p>有了随机变量的概念，我们就可以将一个事件表示为：$\begin{Bmatrix} \omega | X(\omega) = a \end{Bmatrix}$，简记为$\begin{Bmatrix} X = a \end{Bmatrix}$，此时事件的概率可表示为：</p>
<script type="math/tex; mode=display">
P(X=a)</script><p>为了理解其本质，我将上面的简写形式的完整形式写在下面：</p>
<script type="math/tex; mode=display">
P(X=a) = P(\begin{Bmatrix} \omega | X(\omega) = a \end{Bmatrix})</script><h5 id="三、分类"><a href="#三、分类" class="headerlink" title="三、分类"></a>三、分类</h5><p>1、离散型随机变量：变量是离散的，且满足<strong>个数有限</strong>或者<strong>无限但可列</strong>；</p>
<p>2、连续性随机变量：变量是连续的。</p>
<h3 id="2-2-随机变量的分布"><a href="#2-2-随机变量的分布" class="headerlink" title="2.2 随机变量的分布"></a>2.2 随机变量的分布</h3><h4 id="2-2-1-离散型随机变量及其概率分布"><a href="#2-2-1-离散型随机变量及其概率分布" class="headerlink" title="2.2.1 离散型随机变量及其概率分布"></a>2.2.1 离散型随机变量及其概率分布</h4><h5 id="一、定义-7"><a href="#一、定义-7" class="headerlink" title="一、定义"></a>一、定义</h5><p>离散随机变量X的取值为$x_k(k = 1,2, \cdots)$，其概率为$P(X = x_k) = p_k$，则其称为随机变量的概率分布。</p>
<h4 id="2-2-2-连续型随机变量及其概率密度函数"><a href="#2-2-2-连续型随机变量及其概率密度函数" class="headerlink" title="2.2.2 连续型随机变量及其概率密度函数"></a>2.2.2 连续型随机变量及其概率密度函数</h4><p>关于概率分布与概率密度的具体细节，参见自己的博客文章<strong>“概率基础概念本质的辨析理解_三、概率函数P(x)、概率分布函数F(x)、概率密度函数f(x)”部分</strong></p>
<h4 id="2-2-3-随机变量的分布函数"><a href="#2-2-3-随机变量的分布函数" class="headerlink" title="2.2.3 随机变量的分布函数"></a>2.2.3 随机变量的分布函数</h4><p>关于概率分布与概率密度的具体细节，参见自己的博客文章<strong>“概率基础概念本质的辨析理解_三、概率函数P(x)、概率分布函数F(x)、概率密度函数f(x)”部分</strong></p>
<h3 id="2-3-常见随机变量的分布"><a href="#2-3-常见随机变量的分布" class="headerlink" title="2.3 常见随机变量的分布"></a>2.3 常见随机变量的分布</h3><h4 id="2-3-1-常见离散型随机变量的分布"><a href="#2-3-1-常见离散型随机变量的分布" class="headerlink" title="2.3.1 常见离散型随机变量的分布"></a>2.3.1 常见离散型随机变量的分布</h4><h5 id="一、0-1分布"><a href="#一、0-1分布" class="headerlink" title="一、0-1分布"></a>一、0-1分布</h5><p>0-1分布是指随机变量X的取值只有0或1，其用公式表示为：</p>
<script type="math/tex; mode=display">
P(X = k) = p^k (1-p)^{1-k},(k = 0, 1)</script><h5 id="二、几何分布"><a href="#二、几何分布" class="headerlink" title="二、几何分布"></a>二、几何分布</h5><p>设事件A的发生概率是$P(A) = p$，则重复做实验，在第k次的时候<strong>A首次发生</strong>的概率即为几何分布，记为$X \sim G(p)$</p>
<script type="math/tex; mode=display">
P(X=k) = (1-p)^{k-1}p</script><h5 id="三、二项分布"><a href="#三、二项分布" class="headerlink" title="三、二项分布"></a>三、二项分布</h5><p>设事件A的发生概率是$P(A) = p$，则重复做n次实验事件A发生k次的概率即为二项分布，记为$X \sim B(n, p)$</p>
<script type="math/tex; mode=display">
P(X = k) = C_n^k p^k (1-p)^{n-k}</script><h5 id="四、泊松分布"><a href="#四、泊松分布" class="headerlink" title="四、泊松分布"></a>四、泊松分布</h5><p>假设在一定时间间隔 (interval)中一个事件可能会发生0,1,2,…次，在一个间隔中平均发生事件的次数由$λ$决定，  $\lambda$ 是事件发生比率 (event rate)。在一定时间间隔中发生k次事件的概率如下，记为$X \sim P(\lambda)$</p>
<script type="math/tex; mode=display">
P(X=k) = \frac{\lambda^k}{k!}e^{-\lambda},(\lambda > 0, k = 0,1,2,\cdots)</script><p><strong>泊松定理：</strong>n重伯努利实验中，如果$n→\infty，np→\lambda$。</p>
<ul>
<li><strong>二项分布可以用泊松分布来近似</strong><ul>
<li>条件：当二项分布中n比较大，p比较小，np的值适中，则可以用泊松分布来近似计算二项分布；</li>
<li>具体：$n \geq 100, np \leq 10$，此时泊松分布中$\lambda = np$。</li>
</ul>
</li>
</ul>
<h5 id="五、超几何分布"><a href="#五、超几何分布" class="headerlink" title="五、超几何分布"></a>五、超几何分布</h5><p>假设N个元素，其中$N_1$个属于第一类，$N_2$个属于第二类，那么从中任意取n个则其中属于第一类的个数为k的概率为超几何分布，</p>
<script type="math/tex; mode=display">
P(X = k) = \frac{C_{N_1}^k C_{N_2}^{n-k}}{C_N^n},(k = 0,1,2, \cdots, \min \begin{Bmatrix} n, N_1 \end{Bmatrix})</script><blockquote>
<p>① 超几何分布有一个比较明显的特征：<strong>实验只有两种情况/两类</strong>；</p>
<p>② 超几何分布可以用来描述不放回抽样实验；</p>
<p>③ 当不放回抽样中总体数量巨大，抽出的数量相对于总体数量远远小于，则超几何分布可以使用二项分布来近似</p>
</blockquote>
<h5 id="六、均匀分布"><a href="#六、均匀分布" class="headerlink" title="六、均匀分布"></a>六、均匀分布</h5><p> 定义式为：</p>
<script type="math/tex; mode=display">
f(x)=\left\{
\begin{aligned}
\frac{1}{b-a}, &a \leq x \leq b \\
0, &x<a,x>b
\end{aligned}
\right.</script><p>记为$X \sim U[a, b]$，其分布函数为：</p>
<script type="math/tex; mode=display">
F(x) = \left\{
\begin{aligned}
0, &x<a \\
\frac{x-a}{b-a}, &a \leq x \leq b \\
1, &x>b
\end{aligned}
\right.</script><h5 id="七、指数分布"><a href="#七、指数分布" class="headerlink" title="七、指数分布"></a>七、指数分布</h5><p>定义式为：</p>
<script type="math/tex; mode=display">
f(x)=\left\{
\begin{aligned}
\lambda e^{-\lambda x}, &x > 0 \\
0, &x \leq 0
\end{aligned}
\right.</script><p>其中$\lambda &gt; 0$，记为$X \sim \text{Exp}(\lambda)$，其分布函数为：</p>
<script type="math/tex; mode=display">
F(x) = \left\{
\begin{aligned}
1-e^{-\lambda x}, &x>0 \\
0, &x \leq 0
\end{aligned}
\right.</script><p>指数分布有一个特性：<strong>无记忆性</strong></p>
<script type="math/tex; mode=display">
P(X>s+t|X>s) = P(X>t),s>0,t>0</script><h5 id="八、正态分布"><a href="#八、正态分布" class="headerlink" title="八、正态分布"></a>八、正态分布</h5><p>定义式为：</p>
<script type="math/tex; mode=display">
\phi(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x- \mu)^2}{2 \sigma^2}}, -\infty < x < \infty</script><p>记为$X \sim N(\mu, \sigma^2)$，其分布函数为：</p>
<script type="math/tex; mode=display">
\Phi(x) = \frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^x e^{-\frac{(t- \mu)^2}{2 \sigma^2}}dt</script><ul>
<li><strong>正态分布的性质：</strong><ul>
<li>① 其密度函数$\phi(x)$是以$x = \mu$为对称轴，且在$x = \mu$处取得最大值；</li>
<li>② 其密度函数$\phi(x)$是以x轴为渐近线；</li>
<li>③ $\sigma$固定$\mu$变化，则$\phi(x)$左右移动，$\mu$固定$\sigma$变化，则$\phi(x)$窄宽变化；</li>
</ul>
</li>
</ul>
<p><strong>标准正太分布</strong>：$\mu = 0, \sigma = 1$，其密度函数$\phi_0(x)$为：</p>
<script type="math/tex; mode=display">
\phi_0(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}, -\infty < x < \infty</script><p>记为$X \sim N(0,1)$，其分布函数$\Phi_0(x)$为：</p>
<script type="math/tex; mode=display">
\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-\frac{t^2}{2}}dt</script><p>一个一般的正态分布转换为标准正态分布：$x \to \dfrac{x-\mu}{\sigma}$。</p>
<h4 id="2-3-2-随机变量函数的分布"><a href="#2-3-2-随机变量函数的分布" class="headerlink" title="2.3.2 随机变量函数的分布"></a>2.3.2 随机变量<font color="red">函数</font>的分布</h4><p>本节要研究的内容可以简单概况为——已知X是某分布，求Y = aX+b的分布。</p>
<h5 id="一、离散型"><a href="#一、离散型" class="headerlink" title="一、离散型"></a>一、离散型</h5><h5 id="二、连续型"><a href="#二、连续型" class="headerlink" title="二、连续型"></a>二、连续型</h5><p>设X的概率密度为$f_X(x)​$，令$Y = g(X)​$，求Y的密度函数$f_Y(x)​$。求解方法步骤：</p>
<ul>
<li>① $F_Y(x) \to F_X(x)$</li>
<li>② $f_Y(x) \leftarrow f_X(x)$</li>
</ul>
<h2 id="第三章-多维随机变量及其分布"><a href="#第三章-多维随机变量及其分布" class="headerlink" title="第三章 多维随机变量及其分布"></a>第三章 多维随机变量及其分布</h2><h3 id="3-1-二维随机变量"><a href="#3-1-二维随机变量" class="headerlink" title="3.1 二维随机变量"></a>3.1 二维随机变量</h3><h4 id="3-1-1-二维随机变量及其分布函数"><a href="#3-1-1-二维随机变量及其分布函数" class="headerlink" title="3.1.1 二维随机变量及其分布函数"></a>3.1.1 二维随机变量及其分布函数</h4><h5 id="一、二维随机变量定义"><a href="#一、二维随机变量定义" class="headerlink" title="一、二维随机变量定义"></a>一、二维随机变量定义</h5><p>实验E的样本空间为$\Omega$，X、Y是$\Omega$内的两个变量，则称$(X,Y)$为二维随机变量。</p>
<h5 id="二、二维随机变量的分布函数"><a href="#二、二维随机变量的分布函数" class="headerlink" title="二、二维随机变量的分布函数"></a>二、二维随机变量的分布函数</h5><p>1、定义</p>
<p>二维随机变量$(X,Y)$的联合分布函数为：</p>
<script type="math/tex; mode=display">
F(X,Y) = P(X \leq x , Y \leq y)</script><p>2、性质</p>
<h5 id="三、边缘分布"><a href="#三、边缘分布" class="headerlink" title="三、边缘分布"></a>三、边缘分布</h5><p>1、定义</p>
<p>X的边缘分布为：</p>
<script type="math/tex; mode=display">
F_X(x) = P(X \leq x) = F(x, +\infty) = P(X \leq x, Y<+\infty)</script><p>Y的边缘分布为：</p>
<script type="math/tex; mode=display">
F_Y(y) = P(Y \leq y) = F(+\infty, y) = P(X \leq +\infty, Y \leq y)</script><h4 id="3-1-2-二维离散型随机变量的联合概率分布及其边缘概率分布"><a href="#3-1-2-二维离散型随机变量的联合概率分布及其边缘概率分布" class="headerlink" title="3.1.2 二维离散型随机变量的联合概率分布及其边缘概率分布"></a>3.1.2 二维离散型随机变量的联合概率分布及其边缘概率分布</h4><h5 id="一、定义-8"><a href="#一、定义-8" class="headerlink" title="一、定义"></a>一、定义</h5><p>设$(X,Y)$的所有可能的取值为$(x_i, y_j)$，则称</p>
<script type="math/tex; mode=display">
P(X = x_i, Y = y_j) = p_{ij}</script><p>为二维离散型随机变量的联合概率分布。</p>
<h5 id="二、联合概率分布函数"><a href="#二、联合概率分布函数" class="headerlink" title="二、联合概率分布函数"></a>二、联合概率分布函数</h5><p>由定义可知：</p>
<script type="math/tex; mode=display">
F(x,y) = P(X \leq x, Y \leq y) = \sum\limits_{x_i \leq x}\sum\limits_{y_j \leq y}p_{ij}</script><h5 id="三、边缘分布-1"><a href="#三、边缘分布-1" class="headerlink" title="三、边缘分布"></a>三、边缘分布</h5><p>离散型随机变量X ,Y的概率分布,称为随机向量$(X, Y)$的边缘概率分布。</p>
<p>X和Y的边缘分布，就是一维随机变量X和Y的分布，之所以称其为边缘分布是相对于$(X,Y)$的联合分布而言的。</p>
<script type="math/tex; mode=display">
P(X = x_i) = P(X = x_i, -\infty \leq Y \leq \infty) = \sum\limits_j p_{ij}</script><h4 id="3-1-3-二维连续型的联合概率密度函数及边缘概率密度函数"><a href="#3-1-3-二维连续型的联合概率密度函数及边缘概率密度函数" class="headerlink" title="3.1.3 二维连续型的联合概率密度函数及边缘概率密度函数"></a>3.1.3 二维连续型的联合概率密度函数及边缘概率密度函数</h4><h5 id="一、联合概率分布"><a href="#一、联合概率分布" class="headerlink" title="一、联合概率分布"></a>一、联合概率分布</h5><p>由概率分布的定义易知：</p>
<script type="math/tex; mode=display">
F(x, y) = P(X \leq x, Y \leq y) = \int_{-\infty}^x \int_{-\infty}^y f(s , t)dsdt</script><p>称$F(x,y)$为二维连续型随机的联合概率分布，式中，我们定义其联合概率密度函数为$f(x,y)$，即$F(x, y)$与$f(x,y)$的关系为：</p>
<script type="math/tex; mode=display">
f(x, y) = \frac{\partial^2 F(x,y)}{\partial x \partial y}</script><h5 id="二、边缘分布"><a href="#二、边缘分布" class="headerlink" title="二、边缘分布"></a>二、边缘分布</h5><p>由定义易知：</p>
<script type="math/tex; mode=display">
F_X(x) = \lim_{y \to \infty} F(x, y)</script><script type="math/tex; mode=display">
F_Y(y) = \lim_{x \to \infty} F(x, y)</script><h5 id="三、边缘密度函数"><a href="#三、边缘密度函数" class="headerlink" title="三、边缘密度函数"></a>三、边缘密度函数</h5><p>由上一节可知，二维连续型随机变量的边缘分布：</p>
<script type="math/tex; mode=display">
F_X(x) = F(x, \infty) = \int_{-\infty}^x \int_{-\infty}^{\infty} f(s, t)ds dt</script><p>则边缘密度函数就是边缘分布的导数：</p>
<script type="math/tex; mode=display">
f_X(x) = \int_{-\infty}^{\infty} f(x ,t) dt = \int_{-\infty}^{\infty} f(x ,y) dy</script><script type="math/tex; mode=display">
f_Y(y) = \int_{-\infty}^{\infty} f(s ,y) dy = \int_{-\infty}^{\infty} f(x ,y) dx</script><blockquote>
<p><strong>补充：变上限积分求导</strong></p>
<p>$\int_a^{f(x)} g(t)dt = g(f(x)) ×\dfrac{d}{dx}f(x)$</p>
</blockquote>
<p>二维正态分布的边缘分布也是正态分布；两个边缘分布是正太的，其二维随机变量并不一定是正态分布。</p>
<h3 id="3-2-条件分布与随机变量的独立性"><a href="#3-2-条件分布与随机变量的独立性" class="headerlink" title="3.2 条件分布与随机变量的独立性"></a>3.2 条件分布与随机变量的独立性</h3><h4 id="3-2-1-条件分布的概念"><a href="#3-2-1-条件分布的概念" class="headerlink" title="3.2.1 条件分布的概念"></a>3.2.1 条件分布的概念</h4><h5 id="一、定义-9"><a href="#一、定义-9" class="headerlink" title="一、定义"></a>一、定义</h5><p>条件分布是指在事件A发生的条件下随机变量的分布：</p>
<script type="math/tex; mode=display">
F(x | A) = P(X \leq x | A)</script><h4 id="3-2-2-离散型随机变量条件分布"><a href="#3-2-2-离散型随机变量条件分布" class="headerlink" title="3.2.2 离散型随机变量条件分布"></a>3.2.2 离散型随机变量条件分布</h4><h5 id="一、定义式"><a href="#一、定义式" class="headerlink" title="一、定义式"></a>一、定义式</h5><script type="math/tex; mode=display">
P(X = x_i | Y = y_i) = \frac{p_{ij}}{p_j^{(2)}}</script><h4 id="3-2-3-连续型随机变量条件分布"><a href="#3-2-3-连续型随机变量条件分布" class="headerlink" title="3.2.3 连续型随机变量条件分布"></a>3.2.3 连续型随机变量条件分布</h4><h5 id="一、定义-10"><a href="#一、定义-10" class="headerlink" title="一、定义"></a>一、定义</h5><p>二维随机变量$(X, Y)$，其联合概率密度函数为$f(x, y)$，边缘分布为$f_X(x),f_Y(y)$，在$Y = y$的条件下，其条件分布：</p>
<script type="math/tex; mode=display">
F(x | y) = \int_{-\infty}^x \frac{f(u, y)}{f_Y(y)} du</script><script type="math/tex; mode=display">
F(y | x) = \int_{-\infty}^y \frac{f(x, v)}{f_X(x)} dv</script><h5 id="二、条件密度函数"><a href="#二、条件密度函数" class="headerlink" title="二、条件密度函数"></a>二、条件密度函数</h5><script type="math/tex; mode=display">
f(x | y) = \frac{f(x,y)}{f_Y(y)}</script><script type="math/tex; mode=display">
f(y | x) = \frac{f(x,y)}{f_X(x)}</script><script type="math/tex; mode=display">
P(X \leq x | Y = y) = \dfrac{P(X \leq x, Y = y)}{P(Y = y)}</script><p>由于对于连续型随机变量而言，其在一点$Y = y$的概率为零，所以此处应该将分母修改为：</p>
<script type="math/tex; mode=display">
P(X \leq x | Y = y) = \lim\limits_{\epsilon \to 0}\dfrac{P(X \leq x, y \leq Y \leq y+\epsilon )}{P(y \leq Y \leq y+\epsilon )} = \lim\limits_{\epsilon \to 0} \dfrac{\int_{-\infty}^{x} \frac{1}{\epsilon} \int_y^{y+\epsilon}f(u,v)dvdu}{\frac{1}{\epsilon}\int_y^{y+\epsilon}f_Y(v)dv}</script><p>由积分中值定理可知，上式等于<script type="math/tex">=\int_{-\infty}^{x} \dfrac{f(u, y)}{f_Y(y)} \mathrm du</script>。</p>
<h4 id="3-2-4-随机变量的独立性"><a href="#3-2-4-随机变量的独立性" class="headerlink" title="3.2.4 随机变量的独立性"></a>3.2.4 随机变量的独立性</h4><h5 id="一、定义-11"><a href="#一、定义-11" class="headerlink" title="一、定义"></a>一、定义</h5><p>结合上一节的条件分布的概念，可以定义两个随机变量独立满足：</p>
<script type="math/tex; mode=display">
f(x | y) = f_X(x)</script><p>带入条件分布的公式，整理可得：</p>
<script type="math/tex; mode=display">
f(x, y) = f_X(x)f_Y(y)</script><p>同理，也可以使用分布来定义：</p>
<script type="math/tex; mode=display">
F(x, y) = F_X(x)F_Y(y)</script><p>同时，还可以使用概率来判断：</p>
<script type="math/tex; mode=display">
P(x \in S_x, y \in S_y) = P(x \in S_x)P(y \in S_y)</script><h5 id="二、二维离散型随机变量的独立性"><a href="#二、二维离散型随机变量的独立性" class="headerlink" title="二、二维离散型随机变量的独立性"></a>二、二维离散型随机变量的独立性</h5><p>根据定义易知：</p>
<script type="math/tex; mode=display">
P(X = x_i, Y = y_j) = P(X= x_i)P(Y = y_j)</script><h5 id="三、二维连续型随机变量的独立性"><a href="#三、二维连续型随机变量的独立性" class="headerlink" title="三、二维连续型随机变量的独立性"></a>三、二维连续型随机变量的独立性</h5><p>根据定义，判断两个二维连续型随机变量的独立性通过：</p>
<script type="math/tex; mode=display">
f(x, y) = f_X(x)f_Y(y)</script><h5 id="四、变量函数独立"><a href="#四、变量函数独立" class="headerlink" title="四、变量函数独立"></a>四、变量函数独立</h5><p>若X、Y变量是相互独立的，那么由变量构造的函数$g_1(X), g_2(Y)$也是相互独立的。</p>
<h3 id="3-3-二维随机变量函数的分布"><a href="#3-3-二维随机变量函数的分布" class="headerlink" title="3.3 二维随机变量函数的分布"></a>3.3 二维随机变量函数的分布</h3><h4 id="3-3-1-二维离散型随机变量函数的分布"><a href="#3-3-1-二维离散型随机变量函数的分布" class="headerlink" title="3.3.1 二维离散型随机变量函数的分布"></a>3.3.1 二维离散型随机变量函数的分布</h4><p>略。。。</p>
<p>泊松分布具有可加性！</p>
<h4 id="3-3-2-二维连续型随机变量函数的分布"><a href="#3-3-2-二维连续型随机变量函数的分布" class="headerlink" title="3.3.2 二维连续型随机变量函数的分布"></a>3.3.2 二维连续型随机变量函数的分布</h4><p>已知二维连续型随机变量$(X, Y)$，我们设由其构造一新函数$Z = g(X, Y)$，求Z的分布。</p>
<p>求解步骤：</p>
<ul>
<li>① 求<script type="math/tex">F_Z(z) = P(Z \leq z) = P(g(X, Y) \leq z) = \int\int\limits_{Dz} f(x, y)dxdy</script>，其中<script type="math/tex">D_z = \begin{Bmatrix} (x, y) | g(x, y) \leq z \end{Bmatrix}</script>；</li>
<li>② 对$F_Z(z)$求导，得到$f_Z(z)$。</li>
</ul>
<h2 id="第四章-随机变量的数字特征"><a href="#第四章-随机变量的数字特征" class="headerlink" title="第四章 随机变量的数字特征"></a>第四章 随机变量的数字特征</h2><h3 id="4-1-数学期望"><a href="#4-1-数学期望" class="headerlink" title="4.1 数学期望"></a>4.1 数学期望</h3><h4 id="4-1-1-离散型随机变量的数学期望"><a href="#4-1-1-离散型随机变量的数学期望" class="headerlink" title="4.1.1 离散型随机变量的数学期望"></a>4.1.1 离散型随机变量的数学期望</h4><h5 id="一、定义-12"><a href="#一、定义-12" class="headerlink" title="一、定义"></a>一、定义</h5><p>若<script type="math/tex">P(X = x_k) = p_k</script>，当<script type="math/tex">\sum\limits_{k = 1}^{\infty} x_k p_k</script>绝对收敛时，离散型随机变量的数学期望为$E(X)$：</p>
<script type="math/tex; mode=display">
E(X) = \sum\limits_{k = 1}^{\infty} x_k p_k</script><h4 id="4-1-2-连续型随机变量的数学期望"><a href="#4-1-2-连续型随机变量的数学期望" class="headerlink" title="4.1.2 连续型随机变量的数学期望"></a>4.1.2 连续型随机变量的数学期望</h4><h5 id="一、定义-13"><a href="#一、定义-13" class="headerlink" title="一、定义"></a>一、定义</h5><p>随机变量X的密度函数为$f(x)$，当$\int_{-\infty}^{\infty} xf(x)dx$绝对收敛时，连续型随机变量的数学期望为$E(X)$：</p>
<script type="math/tex; mode=display">
E(X) = \int_{-\infty}^{\infty} xf(x)dx</script><h4 id="4-1-3-随机变量函数的数学期望"><a href="#4-1-3-随机变量函数的数学期望" class="headerlink" title="4.1.3 随机变量函数的数学期望"></a>4.1.3 随机变量函数的数学期望</h4><h5 id="一、一维情况"><a href="#一、一维情况" class="headerlink" title="一、一维情况"></a>一、一维情况</h5><p>由随机变量构造的新函数$Y = g(X)$，求Y的数学期望$E(Y)$。根据定义易得：</p>
<ul>
<li>离散型</li>
</ul>
<script type="math/tex; mode=display">
E(Y) = \sum_{i = 1}^{\infty} g(x_i)p_i</script><ul>
<li>连续型</li>
</ul>
<script type="math/tex; mode=display">
E(Y) = \int_{-\infty}^{\infty} g(x)f(x)dx</script><h5 id="二、二维情况"><a href="#二、二维情况" class="headerlink" title="二、二维情况"></a>二、二维情况</h5><p>由二维随机变量$(X, Y)$构成的函数$Z = g(X, Y)$，求Z的数学期望$E(Z)$。根据定义易得：</p>
<ul>
<li>离散型</li>
</ul>
<script type="math/tex; mode=display">
E(Z) = \sum_i \sum_j g(x_i, y_j) p_{ij}</script><ul>
<li>连续型</li>
</ul>
<script type="math/tex; mode=display">
E(Z) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y)f(x, y) dxdy</script><h4 id="4-1-4-数学期望的性质"><a href="#4-1-4-数学期望的性质" class="headerlink" title="4.1.4 数学期望的性质"></a>4.1.4 数学期望的性质</h4><p><strong>性质1：常数的数学期望等于其本身</strong></p>
<script type="math/tex; mode=display">
E(C) = C</script><p><strong>性质2：线性关系</strong></p>
<script type="math/tex; mode=display">
E(kX+b) = kE(X) + b</script><p><strong>性质3：</strong></p>
<script type="math/tex; mode=display">
E(X \pm Y) = E(X) \pm E(Y)</script><p><strong>性质4：</strong></p>
<script type="math/tex; mode=display">
E(\sum_i C_i X_i) =  \sum_i C_i E(X_i)</script><p><strong>性质5：</strong>当X、Y相互独立时</p>
<script type="math/tex; mode=display">
E(XY) = E(X)E(Y)</script><h4 id="4-1-5-条件期望"><a href="#4-1-5-条件期望" class="headerlink" title="4.1.5 条件期望"></a>4.1.5 条件期望</h4><h5 id="一、定义-14"><a href="#一、定义-14" class="headerlink" title="一、定义"></a>一、定义</h5><p>一个变量取定值的条件下，另一个变量的数学期望。</p>
<h5 id="二、离散型"><a href="#二、离散型" class="headerlink" title="二、离散型"></a>二、离散型</h5><p>直接由定义可得：</p>
<script type="math/tex; mode=display">
E(X | Y = y_j) = \sum x_i p(X = x_i | Y = y_j)</script><h5 id="三、连续型"><a href="#三、连续型" class="headerlink" title="三、连续型"></a>三、连续型</h5><p>直接由定义可得：</p>
<script type="math/tex; mode=display">
E(X | Y = y) = \int_{-\infty}^{\infty} x f(x | y) dx</script><h3 id="4-2-方差"><a href="#4-2-方差" class="headerlink" title="4.2 方差"></a>4.2 方差</h3><h4 id="4-2-1-方差的概念"><a href="#4-2-1-方差的概念" class="headerlink" title="4.2.1 方差的概念"></a>4.2.1 方差的概念</h4><h5 id="一、方差的基本概念"><a href="#一、方差的基本概念" class="headerlink" title="一、方差的基本概念"></a>一、方差的基本概念</h5><p>方差是在概率论和统计方差衡量随机变量或一组数据时离散程度的度量。概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。统计中的方差（样本方差）是每个样本值与全体样本值的平均数之差的平方值的平均数。</p>
<p>方差在统计描述和概率分布中各有不同的定义，并有不同的公式。 </p>
<h5 id="二、概率分布中的定义"><a href="#二、概率分布中的定义" class="headerlink" title="二、概率分布中的定义"></a>二、概率分布中的定义</h5><p>在概率分布中，设X是一个离散型随机变量，若$E((X-E(X))^2)$存在，则称$E((X-E(X))^2)$为X的方差，记为$D(X)$。将其开根号后即为标准差$\sigma = \sqrt{D(X)}$，可以保证量纲不变。</p>
<p>离散型随机变量方差计算公式：</p>
<script type="math/tex; mode=display">
D(X) = E((X - E(X))^2) = E(X^2) - E(X)^2</script><p>对于连续型随机变量X，若其定义域为$(a, b)$，概率密度函数为$f(x)$，则连续型随机变量方差为：</p>
<script type="math/tex; mode=display">
D(X) = \int_{-\infty}^{\infty}(x - E(X))^2 f(x) dx</script><h5 id="三、统计中的定义"><a href="#三、统计中的定义" class="headerlink" title="三、统计中的定义"></a>三、统计中的定义</h5><p>在统计描述中，方差用来计算每一个变量（观察值）与总体均数之间的差异。为避免出现离均差总和为零，离均差平方和受样本含量的影响，统计学采用平均离均差平方和来描述变量的变异程度。总体方差计算公式： </p>
<script type="math/tex; mode=display">
\sigma^2 = \frac{\sum(X - \mu)^2}{N}</script><p>$\sigma^2$为总体方差，$\mu$为总体均值，$N$为总体例数。</p>
<p>实际工作中，总体均数难以得到时，应用样本统计量代替总体参数，经校正后，样本方差计算公式： </p>
<script type="math/tex; mode=display">
S^2 = \frac{\sum(X-\bar X)^2}{n-1}</script><p>$S^2$为总体方差，$\bar X$为样本均值，$n$为样本例数。</p>
<h4 id="4-2-2-方差的性质"><a href="#4-2-2-方差的性质" class="headerlink" title="4.2.2 方差的性质"></a>4.2.2 方差的性质</h4><p><strong>性质1：常数的方差为零</strong></p>
<script type="math/tex; mode=display">
D(C) = 0</script><p><strong>性质2：设X是随机变量，是C常数，则有</strong></p>
<script type="math/tex; mode=display">
D(CX) = C^2 D(X), D(X+C) = D(X)</script><p><strong>性质3：设X与Y是两个随机变量，则</strong></p>
<script type="math/tex; mode=display">
D(X \pm Y) = D(X) + D(Y) \pm 2Cov(X, Y)</script><p>其中协方差$Cov(X, Y) = E((X - E(X))(Y - E(Y)))$</p>
<p>特别的，当X，Y是两个不相关<strong>(相互独立)</strong>的随机变量则 </p>
<script type="math/tex; mode=display">
D(X \pm Y) = D(X) + D(Y)</script><p><strong><font color="red">标准化</font></strong></p>
<p>构造一个新的随机变量：</p>
<script type="math/tex; mode=display">
X^* = \frac{X - E(X)}{\sqrt{D(X)}}</script><p>则<script type="math/tex">X^*</script>的数学期望<script type="math/tex">E(X^*) = 0</script>，方差为<script type="math/tex">D(X^*) = 1</script>。</p>
<h3 id="4-3-常见分布的数学期望与方差"><a href="#4-3-常见分布的数学期望与方差" class="headerlink" title="4.3 常见分布的数学期望与方差"></a>4.3 常见分布的数学期望与方差</h3><h4 id="4-3-1-常见离散型分布的数学期望和方差"><a href="#4-3-1-常见离散型分布的数学期望和方差" class="headerlink" title="4.3.1 常见离散型分布的数学期望和方差"></a>4.3.1 常见离散型分布的数学期望和方差</h4><p>略过了。。。</p>
<h4 id="4-3-2-常见连续型分布的数学期望和方差"><a href="#4-3-2-常见连续型分布的数学期望和方差" class="headerlink" title="4.3.2 常见连续型分布的数学期望和方差"></a>4.3.2 常见连续型分布的数学期望和方差</h4><p>略过了。。。</p>
<h3 id="4-4-协方差与相关系数"><a href="#4-4-协方差与相关系数" class="headerlink" title="4.4 协方差与相关系数"></a>4.4 协方差与相关系数</h3><h4 id="4-4-1-协方差"><a href="#4-4-1-协方差" class="headerlink" title="4.4.1 协方差"></a>4.4.1 协方差</h4><h5 id="一、定义-15"><a href="#一、定义-15" class="headerlink" title="一、定义"></a>一、定义</h5><p><strong>协方差</strong>（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。</p>
<p>协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。</p>
<h5 id="二、计算-1"><a href="#二、计算-1" class="headerlink" title="二、计算"></a>二、计算</h5><p>期望值分别为$E[X]$与$E[Y]$的两个实随机变量$X$与$Y$之间的<strong>协方差</strong>Cov(X,Y)定义为： </p>
<script type="math/tex; mode=display">
Cov(X, Y) = E((X-E(X))(Y-E(Y))) = E(XY) - E(X)E(Y)</script><p>独立能推出协方差等于0，但是协方差等于0不能推出独立。</p>
<h5 id="三、协方差的性质"><a href="#三、协方差的性质" class="headerlink" title="三、协方差的性质"></a>三、协方差的性质</h5><p><strong>性质1：</strong></p>
<script type="math/tex; mode=display">
Cov(X, Y) = Cov(Y, X)</script><p><strong>性质2：</strong></p>
<script type="math/tex; mode=display">
Cov(aX, bY) = ab \times Cov(X,Y)</script><p><strong>性质3：</strong></p>
<script type="math/tex; mode=display">
Cov(X_1 + X_2, Y) = Cov(X_1, Y)+Cov(X_2, Y)</script><p><strong>性质4：</strong></p>
<script type="math/tex; mode=display">
Cov(C, X) = 0</script><p><strong>性质5：若X、Y相互独立则</strong></p>
<script type="math/tex; mode=display">
Cov(X, Y) = 0</script><h5 id="四、协方差矩阵"><a href="#四、协方差矩阵" class="headerlink" title="四、协方差矩阵"></a>四、协方差矩阵</h5><p>协方差主要反应的是两个随机变量的线性相关性，也就是他们是线性的还是非线性的</p>
<ul>
<li>针对一维样本集合时，求出的协方差其实就是方差，即方差是协方差的一种特殊情况，意义和方差一样，都是反应集合中各元素离散度的</li>
<li>针对二维样本集合时，求出的协方差反映的就是两维度之间的相关性，正相关性或负相关性，或无关</li>
<li>针对三维样本集合时，求出的是各个维度总体的相关性，针对各维度之间的关系，所以二维以上计算协方差，用的就是<font color="red"><strong>协方差矩阵</strong></font>。</li>
</ul>
<p>当出现多维数据时，若要对多维数据的相关性进行分析，那么就要用到协方差矩阵。其形式如下：</p>
<script type="math/tex; mode=display">
V=\left[\begin{array}{cccc}
\operatorname{Cov}\left(X_{1}, X_{1}\right) & \operatorname{Cov}\left(X_{1}, X_{2}\right) & \cdots & \operatorname{Cov}\left(X_{1}, X_{n}\right) \\
\operatorname{Cov}\left(X_{2}, X_{1}\right) & \operatorname{Cov}\left(X_{2}, X_{2}\right) & \cdots & \operatorname{Cov}\left(X_{2}, X_{n}\right) \\
\vdots & \vdots & \ddots & \vdots \\
\operatorname{Cov}\left(X_{n}, X_{1}\right) & \operatorname{Cov}\left(X_{n}, X_{2}\right) & \cdots & \operatorname{Cov}\left(X_{n}, X_{d}\right)
\end{array}\right]</script><h5 id="五、相关系数"><a href="#五、相关系数" class="headerlink" title="五、相关系数"></a>五、相关系数</h5><p><strong>协方差会受单位/量纲的影响</strong>，我们可以使用<font color="red"><strong>标准化</strong></font>方法去除单位的影响：</p>
<script type="math/tex; mode=display">
X^* = \frac{X - E(X)}{\sqrt{D(X)}},Y^* = \frac{Y - E(Y)}{\sqrt{D(Y)}}</script><p>此时$X^<em>,Y^</em>$的协方差为：</p>
<script type="math/tex; mode=display">
Cov(X^*, Y^*) = E(X^* Y^*) - E(X^*)E(Y^*)</script><p>带入化简得：</p>
<script type="math/tex; mode=display">
\rho = \frac{E((X-E(X))(Y-E(Y)))}{\sqrt{D(X)}\sqrt{D(Y)}} = \frac{Cov(X, Y)}{\sqrt{D(X)}\sqrt{D(Y)}}</script><p>定义$\rho$为相关系数，它不受单位的影响。</p>
<h4 id="4-4-2-相关系数"><a href="#4-4-2-相关系数" class="headerlink" title="4.4.2 相关系数"></a>4.4.2 相关系数</h4><h5 id="一、定义-16"><a href="#一、定义-16" class="headerlink" title="一、定义"></a>一、定义</h5><p>由上一节最后一部分我们知道，相关系数$\rho$的定义式为：</p>
<script type="math/tex; mode=display">
\rho = \frac{Cov(X, Y)}{\sqrt{D(X) D(Y)}}</script><h5 id="二、性质-1"><a href="#二、性质-1" class="headerlink" title="二、性质"></a>二、性质</h5><p>1、$|\rho| \leq 1$</p>
<p>2、$|\rho| = 1 \Longleftrightarrow X与Y以概率P=1呈线性关系$，即$P(Y = aX+b) = 1$</p>
<blockquote>
<p>由性质2，我们可以知道其实$\rho$衡量的是两个随机变量之间的<font color="red"><strong>线性相关关系</strong></font></p>
</blockquote>
<h5 id="三、-rho-的取值"><a href="#三、-rho-的取值" class="headerlink" title="三、$\rho$的取值"></a>三、$\rho$的取值</h5><ul>
<li>$\rho = 1$，则X、Y完全正相关；</li>
<li>$\rho = -1$，则X、Y完全负相关；</li>
<li>$|\rho| \to 0$，则X、Y线性关系很弱；</li>
<li>$|\rho| = 0$，则X、Y不存在线性关系。</li>
</ul>
<h5 id="四、相关与独立"><a href="#四、相关与独立" class="headerlink" title="四、相关与独立"></a>四、相关与独立</h5><p>注意本节所说的相关均是指<strong>线性相关</strong>，因此有以下关系：</p>
<ul>
<li>X、Y相互独立，则X、Y不相关；</li>
<li>X、Y不相关，X、Y不一定独立；</li>
</ul>
<p><strong>但是，有一个特例：二维正态分布$(X, Y)$，独立与不相关是等价的！！！</strong></p>
<h3 id="4-5-随机变量的矩——原点矩与中心矩"><a href="#4-5-随机变量的矩——原点矩与中心矩" class="headerlink" title="4.5 随机变量的矩——原点矩与中心矩"></a>4.5 随机变量的矩——原点矩与中心矩</h3><h5 id="一、原点矩定义"><a href="#一、原点矩定义" class="headerlink" title="一、原点矩定义"></a>一、原点矩定义</h5><p>1、在数理统计学中有一类数字特征称为矩。</p>
<p>原点矩：令k为正整数（或为0），a为任何实数，X为随机变量，则期望值$E((X - a)^k)$叫做随机变量X对a的k阶矩，或叫动差。如果$a=0$，则有$E(X^k)$，叫做<strong>k阶原点矩</strong>，记作$v_k(X)$，也叫k阶矩。</p>
<script type="math/tex; mode=display">
v_k(X) = E((X - a)^k), k = 1,2,\cdots</script><p>显然，<strong>一阶原点矩就是数学期望</strong>，即</p>
<script type="math/tex; mode=display">
v_1(X) = E(X)</script><p>2、原点矩计算</p>
<p>(1) 离散</p>
<script type="math/tex; mode=display">
v_k(X) = \sum x_i^kp_i</script><p>(2) 连续</p>
<script type="math/tex; mode=display">
v_k(X) = \int_{-\infty}^{\infty}x^k f(x) dx</script><h5 id="二、中心矩定义"><a href="#二、中心矩定义" class="headerlink" title="二、中心矩定义"></a>二、中心矩定义</h5><p>设随机变量X的函数$(X-E(X))^k,(k = 1, 2, \cdots)$的数学期望存在，则称$E((X - E(X))^k)$为X的k阶中心矩，记作$u_k(X)$：</p>
<script type="math/tex; mode=display">
u_k(X) = E((X - E(X))^k), k = 1, 2, \cdots</script><p>易知，<strong>一阶中心矩恒等于零</strong>，即$u_1(X) = 0$；二阶中心矩就是方差，即$u_2(X) = D(X)$。</p>
<p>2、中心矩的计算</p>
<p>(1) 离散</p>
<script type="math/tex; mode=display">
u_k(X) = \sum (x_i - E(X))^k p_i</script><p>(2) 连续</p>
<script type="math/tex; mode=display">
u_k(X) = \int_{-\infty}^{\infty}(x - E(X))^k f(x)dx</script><h2 id="第五章-大数定律与中心极限定理"><a href="#第五章-大数定律与中心极限定理" class="headerlink" title="第五章 大数定律与中心极限定理"></a>第五章 大数定律与中心极限定理</h2><h3 id="5-1-大数定律"><a href="#5-1-大数定律" class="headerlink" title="5.1 大数定律"></a>5.1 大数定律</h3><p><strong><font color="red">大数定律想揭示这样一个定律：大量重复实验的平均结果的稳定性</font></strong></p>
<h4 id="5-1-1-切比雪夫不等式"><a href="#5-1-1-切比雪夫不等式" class="headerlink" title="5.1.1 切比雪夫不等式"></a>5.1.1 切比雪夫不等式</h4><h5 id="一、定理"><a href="#一、定理" class="headerlink" title="一、定理"></a>一、定理</h5><p>假设X为一随机变量，X的数学期望$E(X)$与方差$D(X)$都存在，则对于$\forall \varepsilon &gt;0$，都有</p>
<script type="math/tex; mode=display">
P(|X - E(X)| \geq \varepsilon) \leq \frac{D(X)}{\varepsilon^2}</script><script type="math/tex; mode=display">
P(|X - E(X)| < \varepsilon) \geq 1-\frac{D(X)}{\varepsilon^2}</script><h4 id="5-1-2-切比雪夫大数定律"><a href="#5-1-2-切比雪夫大数定律" class="headerlink" title="5.1.2 切比雪夫大数定律"></a>5.1.2 切比雪夫大数定律</h4><h5 id="一、普通收敛的定义"><a href="#一、普通收敛的定义" class="headerlink" title="一、普通收敛的定义"></a>一、普通收敛的定义</h5><p>对于$\forall \varepsilon &gt;0, ∃ N &gt;0$，使得当$n &gt; N$时，有$|a_n - a| &lt; \varepsilon$。</p>
<h5 id="二、依概率收敛定义"><a href="#二、依概率收敛定义" class="headerlink" title="二、依概率收敛定义"></a>二、依概率收敛定义</h5><p>对于$\forall \varepsilon &gt;0, ∃  N &gt;0$，使得当$n &gt; N$时，有$\lim\limits_{n \to \infty}P(|x_n - a| &lt; \varepsilon) = 1$。</p>
<h5 id="三、伯努利大数定律"><a href="#三、伯努利大数定律" class="headerlink" title="三、伯努利大数定律"></a>三、伯努利大数定律</h5><p>假设做了n重伯努利实验，事件A发生了$m_n$次，p是事件A的概率，$\dfrac{m_n}{n}$是事件A的频率，则有</p>
<script type="math/tex; mode=display">
\lim\limits_{n \to \infty} P(|\frac{m_n}{n}-p| < \varepsilon) = 1</script><p>另一种表示：</p>
<script type="math/tex; mode=display">
\lim\limits_{n \to \infty} P(|\frac{m_n}{n}-p| \geq \varepsilon) = 0</script><p><strong>通俗的说，伯努利大数定律就是想表达：当实验次数足够多的时候，就能用频率近似代替概率 ，能用样本均值近似代替总体均值。</strong> </p>
<h5 id="四、辛钦大数定理"><a href="#四、辛钦大数定理" class="headerlink" title="四、辛钦大数定理"></a>四、辛钦大数定理</h5><p>设$X_1, X_2, \cdots$是<strong><font color="red">独立同分布(iid)</font></strong>的随机变量序列，且它们的期望值存在，记为$E(X_i)=μ$(同分布隐含条件即为期望相同)，则对于$\forall \varepsilon &gt;0$，有 </p>
<script type="math/tex; mode=display">
\lim _{n \rightarrow \infty} P\left\{\left|\frac{1}{n} \sum_{i=1}^{n} X_{i}- E\left(X_{i}\right)\right|<\varepsilon\right\}=1</script><p><strong>通俗的说，辛钦大数定律从理论上指出：用算术平均值来近似实际真值是合理的。</strong></p>
<p>当Xi为服从0-1分布的随机变量时，辛钦大数定律就是伯努利大数定律，故<strong>伯努利大数定律是辛钦伯努利大数定律的一个特例。</strong></p>
<h5 id="五、切比雪夫大数定律"><a href="#五、切比雪夫大数定律" class="headerlink" title="五、切比雪夫大数定律"></a>五、切比雪夫大数定律</h5><p>设$X_1, X_2, \cdots, X_n, \cdots$是(线性)不相关的随机变量序列，$E(X_i),D(X_i)$均存在，并且方差都是有界的$D(X_i) \leq M$，则有$\forall \varepsilon &gt;0$</p>
<script type="math/tex; mode=display">
\lim _{n \rightarrow+\infty} P\left\{\left|\frac{1}{n} \sum_{i=1}^{n} X_{i}-\frac{1}{n} \sum_{i=1}^{n} E(X_i)\right|<\varepsilon\right\}=1</script><p><strong>通俗的说，切比雪夫大数定律就是想表达：随机变量的均值依概率收敛于期望的均值。</strong> </p>
<h5 id="六、辛钦与切比雪夫大数定律的区别"><a href="#六、辛钦与切比雪夫大数定律的区别" class="headerlink" title="六、辛钦与切比雪夫大数定律的区别"></a>六、辛钦与切比雪夫大数定律的区别</h5><p>辛钦大数定理规定必须是独立同分布的</p>
<p>切比雪夫大数定律没规定同分布，但规定了方差存在。 </p>
<p>辛钦定律：序列${X_i}$的期望存在,则{Xi}服从大数定律</p>
<p>切比雪夫：序列{Xi}的方差存在,则${X_i}$服从大数定律</p>
<h5 id="七、总结——大数定律有这么几个版本"><a href="#七、总结——大数定律有这么几个版本" class="headerlink" title="七、总结——大数定律有这么几个版本"></a>七、总结——大数定律有这么几个版本</h5><p>1、<strong>伯努利大数定理：</strong>从定义概率的角度，<strong>揭示了概率与频率的关系</strong>，当N很大的时候，事件A发生的概率等于A发生的频率。</p>
<p>2、<strong>辛钦大数定理：揭示了算术平均值和数学期望的关系</strong> </p>
<p>3、<strong>切比雪夫大数定律：揭示了样本均值和真实期望的关系</strong> </p>
<p>伯努利大数定律是300年前瑞士数学家伯努利潜心研究20年证明出来的，是人类历史上第一个严格证明的大数定律。它是辛钦大数定律的特殊情况，不过由于它有一定的历史意义并且二项分布的大数定律在日常生活中最为常见，所以编教材的人喜欢把这个大数定律单独列出来。</p>
<p>切比雪夫大数定律和辛钦大数定律针对的是两种不同的情况，谁也不是谁的特例。切比雪夫大数定律说的是一列独立变量（可以不同分布）的均值收敛到一个常数，但前提是每个变量的期望和方差均存在且有限，并且满足方差的平均值是样本数n的高阶无穷小这一额外条件。辛钦大数定律是说一列独立同分布的随机变量的均值收敛到一个常数，条件是分布的绝对期望存在且有限就够了。</p>
<p>对两个大数定律做一总结，就是切比雪夫大数定律不要求随机变量有相同分布但是成立的条件更加严格，辛钦大数定律要求同分布不过是在比较弱的条件下就成立。</p>
<h3 id="5-2-中心极限定律"><a href="#5-2-中心极限定律" class="headerlink" title="5.2 中心极限定律"></a>5.2 中心极限定律</h3><p>一个随机现象是由大量相互独立的因素影响的，中心极限定理描述的是这样一个事情：</p>
<center>**<font color="blue">大量独立同分布(iid)的随机变量和的极限分布是正态分布</font>**</center> 

<p>设$X_1, X_2, \cdots$是<strong><font color="red">独立同分布(iid)</font></strong>的随机变量序列，$E(X_i) = \mu$ ，$D(X_i) = \sigma^2, 0&lt;\sigma^2&lt;\infty$，那么：</p>
<script type="math/tex; mode=display">
\lim_{n \to \infty}P \left(\frac{\sum\limits_{i =1}^{n} X_i - n\mu}{\sqrt n \sigma} \leq x \right) = \Phi_0(x)</script><p>上式也就是说，令$Y = \dfrac{\sum\limits_{i =1}^{n} X_i - n\mu}{\sqrt n \sigma} $，则Y可以近似为标准正态分布：$Y \sim N(0, 1)$。</p>
<h2 id="第六章-数理统计的基本概念"><a href="#第六章-数理统计的基本概念" class="headerlink" title="第六章 数理统计的基本概念"></a>第六章 数理统计的基本概念</h2><h3 id="6-1-总体与样本"><a href="#6-1-总体与样本" class="headerlink" title="6.1 总体与样本"></a>6.1 总体与样本</h3><h5 id="一、总体-population-和样本-sample"><a href="#一、总体-population-和样本-sample" class="headerlink" title="一、总体(population)和样本(sample)"></a>一、总体(population)和样本(sample)</h5><p>总体：研究对象的整个群体，就是一个<strong>概率分布</strong>。 </p>
<p>样本：从总体中选取的一部分。其中，<strong>样本具有和总体相同的分布，样本之间两两独立。</strong> </p>
<p>样本数量：有多少个样本。</p>
<p>样本大小（样本容量）：每个样本里包含多少个数据。</p>
<p>抽样分布：将样本平均值的分布可视化。</p>
<blockquote>
<p>1、总体是一个概率分布或服从这个概率分布的随机变量</p>
<p>2、如何理解一个总体就是一个具有确定概率分布的随机变量?</p>
<p>3、我感到奇怪的是，样本是从总体体里抽出来的，为什么样本和总体服从同样的分布？或者说，总体是随机变量，为什么从整体里抽出来的还叫随机变量。 </p>
</blockquote>
<h5 id="二、中心极限定理-central-limit-theorem"><a href="#二、中心极限定理-central-limit-theorem" class="headerlink" title="二、中心极限定理(central limit theorem)"></a>二、中心极限定理(central limit theorem)</h5><p>中心极限定理：样本平均值约等于总体平均值，且不管总体是什么分布，任意一个总体的样本平均值都会围绕在总体的平均值周围，并且呈正态分布。如下：</p>
<p><img src="https://pic.imgdb.cn/item/622579695baa1a80abcff1d7.jpg" style="zoom:80%"></p>
<p>中心极限定理的作用：</p>
<p>1）用样本来估计总体（民意调查）</p>
<p>2）根据总体信息，判断某个样本是否属于总体（3个标准差，概率97%）</p>
<h3 id="6-2-统计量"><a href="#6-2-统计量" class="headerlink" title="6.2 统计量"></a>6.2 统计量</h3><h5 id="一、统计量的定义"><a href="#一、统计量的定义" class="headerlink" title="一、统计量的定义"></a>一、统计量的定义</h5><p>不含任何未知参数的样本构造的函数。</p>
<p>e.g. 直接将样本相加$X_1+X_2+ \cdots + X_n$就是一个统计量。</p>
<h5 id="二、常见统计量"><a href="#二、常见统计量" class="headerlink" title="二、常见统计量"></a>二、常见统计量</h5><p><strong>1、样本均值</strong></p>
<script type="math/tex; mode=display">
\overline X = \frac{1}{n}\sum_{i =1}^{n} X_i</script><p><strong>2、未修正的样本方差/总体方差</strong></p>
<script type="math/tex; mode=display">
S_0^2 = \frac{1}{n}\sum_{i =1}^{n} (X_i - \overline X)</script><p><strong>3、(已修正的)样本方差</strong></p>
<script type="math/tex; mode=display">
S^2 = \frac{1}{n-1}\sum_{i =1}^{n} (X_i - \overline X)</script><p>为什么样本方差自由度(分母)为n-1？</p>
<p><a href="https://www.cnblogs.com/ohshit/p/5590629.html">https://www.cnblogs.com/ohshit/p/5590629.html</a></p>
<p><a href="https://blog.csdn.net/qq_39521554/article/details/79633207">https://blog.csdn.net/qq_39521554/article/details/79633207</a></p>
<p><a href="http://www.noobyard.com/article/p-emeczand-nh.html">http://www.noobyard.com/article/p-emeczand-nh.html</a></p>
<p><a href="https://benpaodewoniu.github.io/2018/06/15/math3/">https://benpaodewoniu.github.io/2018/06/15/math3/</a></p>
<p><a href="https://www.zhihu.com/question/30440035">https://www.zhihu.com/question/30440035</a></p>
<p><a href="https://www.zhihu.com/question/20099757">https://www.zhihu.com/question/20099757</a></p>
<p><a href="https://www.bilibili.com/video/BV183411r7tZ/?spm_id_from=333.788.recommend_more_video.4">https://www.bilibili.com/video/BV183411r7tZ/?spm_id_from=333.788.recommend_more_video.4</a></p>
<p><strong>4、样本的k阶原点矩</strong></p>
<script type="math/tex; mode=display">
A_k = \frac{1}{n}\sum_{i = 1}^{n} X_i^k</script><p><strong>5、样本的k阶中心矩</strong></p>
<script type="math/tex; mode=display">
B_k = \frac{1}{n}\sum_{i = 1}^{n} (X_i - \overline X)^k</script><p><strong>6、样本的协方差</strong></p>
<script type="math/tex; mode=display">
S_{12} = \frac{1}{n} \sum(X_i - \overline X)(Y_i - \overline Y)</script><p><strong>7、样本的相关系数</strong></p>
<script type="math/tex; mode=display">
R = \frac{S_{12}}{S_1 S_2}</script><p>式中$S_1、S_2$是样本的标准差。</p>
<h5 id="三、样本均值与样本方差的性质"><a href="#三、样本均值与样本方差的性质" class="headerlink" title="三、样本均值与样本方差的性质"></a>三、样本均值与样本方差的性质</h5><p>设总体X的均值为$E(X) = \mu$，方差为$D(X)  = \sigma^2$，样本$(X_1,X_2, \cdots, X_n)$来自总体X，则有</p>
<script type="math/tex; mode=display">
E(\overline X) = \mu</script><script type="math/tex; mode=display">
D(\overline X) = \frac{1}{n}\sigma^2</script><script type="math/tex; mode=display">
E(S^2) = \sigma^2</script><p>注意：这里样本均值还是一个随机变量。</p>
<h3 id="6-3-抽样分布"><a href="#6-3-抽样分布" class="headerlink" title="6.3 抽样分布"></a>6.3 抽样分布</h3><p>构造的统计量的分布就是抽样分布</p>
<h4 id="6-3-1-数理统计中的重要分布"><a href="#6-3-1-数理统计中的重要分布" class="headerlink" title="6.3.1 数理统计中的重要分布"></a>6.3.1 数理统计中的重要分布</h4><h5 id="一、正态分布"><a href="#一、正态分布" class="headerlink" title="一、正态分布"></a>一、正态分布</h5><p>略。。。</p>
<h5 id="二、卡方分布-chi-2-分布"><a href="#二、卡方分布-chi-2-分布" class="headerlink" title="二、卡方分布/$\chi ^2$分布"></a>二、卡方分布/$\chi ^2$分布</h5><p><strong>1、卡方分布的一些性质</strong></p>
<p>(1) $\chi^2(n)$：中n为自由度；</p>
<p>(2) $\chi^2(2)$：是$\lambda = \frac{1}{2}$的指数分布；</p>
<p>(3) $\chi ^2$分布是单峰曲线；，且在$n-2$处取得最大值；</p>
<p>(4) $\chi ^2$分布中当$n \to \infty$时，可以用正态分布近似。</p>
<p><strong>2、$\chi ^2$分布的定义</strong></p>
<p>若$X_1,X_2, \cdots, X_n$是相互独立的，并且都服从标准正态分布，那么有：</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{n}X_i^2 \sim \chi^2(n)</script><p><strong>3、$\chi ^2$分布的期望与方差</strong></p>
<script type="math/tex; mode=display">
E(X) = n;D(X) = 2n</script><p>由中心极限定理，$X \sim \chi^2(n)$，当n充分大的时候有：$\dfrac{X-n}{\sqrt{2n}} \sim N(0,1)$</p>
<p><strong>4、$\chi ^2$分布的和性质</strong></p>
<p>若$X \sim \chi^2(n), Y \sim \chi^2(m)$，且X、Y相互独立，则有$X+Y \sim \chi^2(n+m)$。</p>
<p><strong>5、$\chi ^2$分布的上$\alpha$分位数</strong></p>
<script type="math/tex; mode=display">
P(\chi^2 > \chi^2_\alpha(n)) = \alpha</script><h5 id="三、t分布"><a href="#三、t分布" class="headerlink" title="三、t分布"></a>三、t分布</h5><p> 如果$X \sim N(0, 1)；Y \sim \chi^2(n)$ ，且X、Y相互独立，那么：</p>
<script type="math/tex; mode=display">
\frac{X}{\sqrt{Y/n}} \sim t(n)</script><h5 id="四、F分布"><a href="#四、F分布" class="headerlink" title="四、F分布"></a>四、F分布</h5><p> 如果$X \sim \chi^2(n_1)；Y \sim \chi^2(n_2)$ ，且X、Y相互独立，那么：</p>
<script type="math/tex; mode=display">
\frac{X/n_1}{Y/n_2} \sim F(n_1, n_2)</script><p>我们由定义易得：若$F \sim F(n_1, n_2)$则$\dfrac{1}{F} \sim F(n_2, n_1)$。</p>
<h4 id="6-3-2-正态总体下的抽样分布"><a href="#6-3-2-正态总体下的抽样分布" class="headerlink" title="6.3.2 正态总体下的抽样分布"></a>6.3.2 正态总体下的抽样分布</h4><p>总体是正态分布，我们从里面抽取样本，由这些样本构造的统计量的分布</p>
<h5 id="一、一个正态总体"><a href="#一、一个正态总体" class="headerlink" title="一、一个正态总体"></a>一、一个正态总体</h5><p>$X \sim N(\mu, \sigma^2)$，$X_1, X_2, \cdots, X_n$是样本，则有以下结论：</p>
<p>(1) $\overline X \sim N(\mu, \dfrac{\sigma^2}{n})$</p>
<blockquote>
<p>证明：</p>
<p>$E(\overline X) = E(\dfrac{1}{n} \sum X_i) = \dfrac{1}{n} \sum E(X_i) = \dfrac{1}{n} n \times \mu = \mu$；</p>
<p>$D(\overline X) = D(\dfrac{1}{n} \sum X_i) = \dfrac{1}{n^2} \sum D(X_i) = \dfrac{1}{n^2} n\times \sigma^2 = \dfrac{\sigma^2}{n}$</p>
<p>证毕</p>
</blockquote>
<p>(2) $\dfrac{(n-1)S^2}{\sigma^2} = \dfrac{1}{\sigma^2} \sum\limits_{i = 1}^{n}(X_i-\overline X) \sim \chi^2(n-1)$</p>
<p>(3) $\overline X$与$S^2$相互独立</p>
<hr>
<p>$X \sim N(\mu, \sigma^2)$，$X_1, X_2, \cdots, X_n$是样本，则有以下结论：</p>
<p>(1) $\dfrac{1}{\sigma^2} \sum\limits_{i = 1}^{n}(X_i- \mu) \sim \chi^2(n)$</p>
<blockquote>
<p>此处与分割线上面的(2)式不同之处就是上面用的是$[\overline X——样本均值]$，而该处是$[\mu——总体期望]$。</p>
</blockquote>
<p>(2) $\dfrac{\overline X - \mu}{S}\sqrt n \sim t(n-1)$</p>
<h5 id="二、两个正态总体"><a href="#二、两个正态总体" class="headerlink" title="二、两个正态总体"></a>二、两个正态总体</h5><p>$X \sim N(\mu_1, \sigma_1^2), Y \sim N(\mu_2, \sigma_2^2)$，$X_1, X_2, \cdots, X_{n1}$、$Y_1, Y_2, \cdots, Y_{n2}$是样本，则有以下结论：</p>
<p>(1) $\dfrac{(\overline X - \overline Y) - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}} \sim N(0, 1)$</p>
<p>(2) $\dfrac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2} \sim F(n_1, n_2)$</p>
<p>(3) 当$\sigma_1^2 = \sigma_2^2 = \sigma$时，有$T = \dfrac{(\overline X - \overline Y) - (\mu_1-\mu_2)}{\sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}} \sim t(n_1, n_2)$</p>
<h2 id="第七章-参数估计"><a href="#第七章-参数估计" class="headerlink" title="第七章 参数估计"></a>第七章 参数估计</h2><h3 id="7-1-参数的点估计"><a href="#7-1-参数的点估计" class="headerlink" title="7.1 参数的点估计"></a>7.1 参数的点估计</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">总体分布</th>
<th style="text-align:center">参数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">正态分布$X \sim N(\mu, \sigma^2)$</td>
<td style="text-align:center">$\mu, \sigma^2$</td>
</tr>
<tr>
<td style="text-align:center">泊松分布$X \sim P(\lambda)$</td>
<td style="text-align:center">$\lambda$</td>
</tr>
<tr>
<td style="text-align:center">均匀分布$X \sim U(a, b)$</td>
<td style="text-align:center">$a, b$</td>
</tr>
<tr>
<td style="text-align:center">$\cdots$</td>
<td style="text-align:center">$\cdots$</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>什么是参数估计？</strong></li>
</ul>
<p><strong>总体分布已知，但是分布中的参数不知道，所以需要通过取样本，然后根据样本构造一些函数/统计量来估计这些参数</strong>。</p>
<p>参数空间：参数的取值范围。</p>
<p>函数/统计量：$\hat\theta = \hat\theta(X_1, X_2, \cdots, X_n)$。</p>
<ul>
<li><strong>什么是点估计、区间估计</strong></li>
</ul>
<p>点估计就是将参数估计为一个值，而区间估计是将参数估计为一个区间。</p>
<h4 id="7-1-1-矩估计法"><a href="#7-1-1-矩估计法" class="headerlink" title="7.1.1 矩估计法"></a>7.1.1 矩估计法</h4><h5 id="一、矩估计的思想"><a href="#一、矩估计的思想" class="headerlink" title="一、矩估计的思想"></a>一、矩估计的思想</h5><p>用样本的矩来等效代替总体的矩。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">样本的矩</th>
<th style="text-align:center">等效于</th>
<th style="text-align:center">总体的矩</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">样本一阶原点矩$\overline X = \dfrac{1}{n}\sum X_i$</td>
<td style="text-align:center"></td>
<td style="text-align:center">样本一阶原点矩$E(X)$</td>
</tr>
<tr>
<td style="text-align:center">样本二阶原点矩$A_2 = \dfrac{1}{n}\sum X_i$</td>
<td style="text-align:center"></td>
<td style="text-align:center">样本二阶原点矩$E(X^2)$</td>
</tr>
<tr>
<td style="text-align:center">$\cdots$</td>
<td style="text-align:center"></td>
<td style="text-align:center">$\cdots$</td>
</tr>
</tbody>
</table>
</div>
<p>例题：</p>
<p><img src="https://pic.imgdb.cn/item/6226a2465baa1a80abe09222.jpg" style="zoom:70%"></p>
<h4 id="7-1-2-极大似然估计法（Maximum-Likelihood-Estimate，MLE）"><a href="#7-1-2-极大似然估计法（Maximum-Likelihood-Estimate，MLE）" class="headerlink" title="7.1.2 极大似然估计法（Maximum Likelihood Estimate，MLE）"></a>7.1.2 极大似然估计法（Maximum Likelihood Estimate，MLE）</h4><p>概率大的事件比概率小的事件更容易发生；</p>
<p>将使事件A发生的概率最大的参数值作为估计值（事件A是指样本中发生的事件）。</p>
<h5 id="一、-似然函数"><a href="#一、-似然函数" class="headerlink" title="一、 似然函数"></a>一、 似然函数</h5><p>我们来考察下面这个函数：</p>
<script type="math/tex; mode=display">
p(x|\theta)</script><p>输入有两个：$x$表示某一个具体的数据；$\theta$表示模型的参数。如果是$\theta$已知确定的，$x$是变量，这个函数叫做<strong>概率函数</strong>(probability function)，它描述对于不同的样本点$x$，其出现概率是多少。如果$x$是已知确定的，$\theta$是变量，这个函数叫做<strong>似然函数</strong>(likelihood function), 它描述对于不同的模型参数，出现$x$这个样本点的概率是多少。 </p>
<h5 id="二、-极大似然函数估计值一般步骤"><a href="#二、-极大似然函数估计值一般步骤" class="headerlink" title="二、 极大似然函数估计值一般步骤"></a>二、 极大似然函数估计值一般步骤</h5><ul>
<li><p>写出总体的概率(密度)函数<strong>（极大似然估计的前提一定是要假设数据总体的分布，如果不知道数据分布，是无法使用极大似然估计的）</strong>；</p>
</li>
<li><p>写出似然函数$L(参数)$；</p>
</li>
<li>对似然函数取对数，并整理；</li>
<li>求偏导令偏导为 0，得到似然方程；(注意，此处一个特例均匀分布，用求导的方法不行，需要通过思考分析)</li>
<li>解似然方程，得到的参数。 </li>
</ul>
<h5 id="三、极大似然估计总结"><a href="#三、极大似然估计总结" class="headerlink" title="三、极大似然估计总结"></a>三、极大似然估计总结</h5><p>① 多数情况下我们是根据已知条件来推算结果，而极大似然估计是已经知道了结果，然后寻求使该结果出现的可能性极大的条件，以此作为估计值。</p>
<p>② 极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。</p>
<p>③ 已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，通过若干次试验，观察其结果，利用结果推出参数的大概值。</p>
<p>④ 极大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率极大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p>
<p>⑤ 极大似然估计可以看作是一个<font color="red"><strong>反推思想</strong></font>。 </p>
<h3 id="7-2-点估计的优良性准则"><a href="#7-2-点估计的优良性准则" class="headerlink" title="7.2 点估计的优良性准则"></a>7.2 点估计的优良性准则</h3><h5 id="一、无偏性"><a href="#一、无偏性" class="headerlink" title="一、无偏性"></a>一、无偏性</h5><p>估计的参数的值的期望是参数的真实值：</p>
<script type="math/tex; mode=display">
E(\hat \theta) = \theta</script><p><strong>无偏性保证没有系统误差，通俗说就是努力的大方向没错。</strong></p>
<p>一般将系统误差定义为：</p>
<script type="math/tex; mode=display">
E(\hat \theta) - \theta</script><p>1、总体X(<strong>任何分布均成立</strong>)，$E(X) = \mu,D(X) = \sigma^2$，样本为$X_1, X_2, \cdots, X_n$</p>
<ul>
<li>$\overline X$是$\mu$的无偏估计，因为$E(\overline X) = \mu$</li>
</ul>
<script type="math/tex; mode=display">
e_n = \dfrac{(x1+x2+...+xn)}{n}</script><script type="math/tex; mode=display">
E[e_n]=E\left[\frac{(x1+x2+...+xn)}{n}\right] =\frac{E(X1)+E(X2)+...+E(Xn)}{n} = E(X)</script><ul>
<li>(已修正)样本方差$S^2$是$\sigma^2$的无偏估计，这里可以大概解释为什么分母是$n-1$</li>
<li>(未修正)样本方差$S_0^2$是$\sigma^2$的有偏估计</li>
</ul>
<p>注意：$\hat \theta$是$\theta$的无偏估计，$g(\hat \theta)$不一定是$g(\theta)$的无偏估计</p>
<h5 id="二、有效性"><a href="#二、有效性" class="headerlink" title="二、有效性"></a>二、有效性</h5><p>估计的参数的值的方差越小就越有效</p>
<h5 id="三、相合性-一致性"><a href="#三、相合性-一致性" class="headerlink" title="三、相合性/一致性"></a>三、相合性/一致性</h5><p>也就是说样本取的越多，估计的参数的值与准确值也越近：</p>
<script type="math/tex; mode=display">
\lim_{n \to\infty} P(|\hat \theta - \theta| < \varepsilon) = 1, \forall \varepsilon > 0</script><h3 id="7-3-参数的区间估计"><a href="#7-3-参数的区间估计" class="headerlink" title="7.3 参数的区间估计"></a>7.3 参数的区间估计</h3><h4 id="7-3-1-置信区间、枢轴变量、区间估计的基本概念"><a href="#7-3-1-置信区间、枢轴变量、区间估计的基本概念" class="headerlink" title="7.3.1 置信区间、枢轴变量、区间估计的基本概念"></a>7.3.1 置信区间、枢轴变量、区间估计的基本概念</h4><h5 id="一、置信区间"><a href="#一、置信区间" class="headerlink" title="一、置信区间"></a>一、置信区间</h5><p>区间估计有两个重点——区间长度、以多大的概率落在该区间；这两个概念是相互矛盾的，区间长度越大，概率就越大，区间长度越小，概率就越小。</p>
<script type="math/tex; mode=display">
P(\hat \theta_1 \leq \theta \leq \hat \theta_2) = 1 - \alpha</script><p>上式表示，估计的区间为：$[\hat \theta_1 , \hat \theta_2]$，置信度为$1-\alpha$(置信度也就是通俗说的以多大的概率落在该区间)。</p>
<h5 id="二、枢轴变量"><a href="#二、枢轴变量" class="headerlink" title="二、枢轴变量"></a>二、枢轴变量</h5><p>1) 取$I = I(T, \theta)$，其中$\theta$是要估计的参数，$T$是已知的，$I$的分布F也要是已知的且与$\theta$无关；</p>
<p>2) 给定$1 - \alpha$，确定F的上$\dfrac{\alpha}{2}$分位数为$v_{\frac{\alpha}{2}}$，上$(1-\dfrac{\alpha}{2})$分位数为$v_{1-\frac{\alpha}{2}}$，则</p>
<script type="math/tex; mode=display">
P(v_{1-\frac{\alpha}{2}} \leq I(T, \theta) \leq v_{\frac{\alpha}{2}}) = 1-\alpha</script><h5 id="三、区间估计"><a href="#三、区间估计" class="headerlink" title="三、区间估计"></a>三、区间估计</h5><p>区间估计基本都是通过枢轴变量的方法来计算的。</p>
<h4 id="7-3-2-一个正态总体均值和方差的区间估计"><a href="#7-3-2-一个正态总体均值和方差的区间估计" class="headerlink" title="7.3.2 一个正态总体均值和方差的区间估计"></a>7.3.2 一个正态总体均值和方差的区间估计</h4><p>略。。。</p>
<h2 id="第八章-假设检验"><a href="#第八章-假设检验" class="headerlink" title="第八章 假设检验"></a>第八章 假设检验</h2><h3 id="8-1-假设检验的基本概念"><a href="#8-1-假设检验的基本概念" class="headerlink" title="8.1 假设检验的基本概念"></a>8.1 假设检验的基本概念</h3><h5 id="一、假设检验问题"><a href="#一、假设检验问题" class="headerlink" title="一、假设检验问题"></a>一、假设检验问题</h5><p>假设检验是<font color="red"><strong>统计推断</strong></font>的主要内容，统计推断有两大内容：</p>
<ul>
<li><strong>参数估计</strong>——点估计、区间估计</li>
<li><strong>假设检验</strong></li>
</ul>
<p>假设检验是解决这样一个问题：在数理统计中总体的分布通常是未知的，而这个未知包含两种情况【一是总体分布的类型未知】【二是总体分布已知但是参数未知】，然后我们对总体分布提出一种推断，就叫“假设”，根据上面的两种情况“假设”可分为【非参数假设】【参数假设】。假设完了之后我们要利用样本的信息检验假设是否正确，所以检验也分为两种【非参数假设检验】【参数假设检验】。总上就是“假设检验”。</p>
<p>提出假设：</p>
<ul>
<li>原/零假设$H_0$：不发生改变</li>
<li>备择假设$H_1$：发生改变的假设</li>
</ul>
<h5 id="二、假设检验的基本概念"><a href="#二、假设检验的基本概念" class="headerlink" title="二、假设检验的基本概念"></a>二、假设检验的基本概念</h5><p>1、我们对总体分布提出一种推断，就叫“假设”；</p>
<p>2、假设完了之后我们要利用样本的信息检验假设是否正确，就叫“假设检验”。</p>
<p>3、假设检验问题分为两种情况：</p>
<ul>
<li>显著性假设检验问题——只提出一个假设$H_0$，没有$H_1$；</li>
<li>$H_0$对$H_1$的假设问题。</li>
</ul>
<h5 id="三、假设检验的基本思想与步骤"><a href="#三、假设检验的基本思想与步骤" class="headerlink" title="三、假设检验的基本思想与步骤"></a>三、假设检验的基本思想与步骤</h5><p><strong>1、基本思想</strong></p>
<p>从样本出发，构造一个检验统计量T，用来检验$H_0$成立与否。假定$H_0$成立，那么T的(近似)分布就已知了，然后根据<font color="red"><strong>检验法则</strong></font>（小概率事件在一次抽样中不可能发生），可以表达为：</p>
<script type="math/tex; mode=display">
P\begin{Bmatrix}(X_1, X_2, \cdots, X_n) \sub W \end{Bmatrix} = \alpha</script><script type="math/tex; mode=display">
P\begin{Bmatrix}(X_1, X_2, \cdots, X_n) \sub \overline W \end{Bmatrix} =1- \alpha</script><p>若第一个式子成立，就说明小概率事件在一次抽样中发生了，说明我们要拒绝假设$H_0$，此时$W$就叫做$H_0$的拒绝域；</p>
<p>若第二个式子成立，就说明小概率事件在一次抽样中没有发生了，说明我们要接收假设$H_0$，此时$\overline W$就叫做$H_0$的接受域；</p>
<p><strong>2、步骤</strong></p>
<ul>
<li>提出假设$H_0、H_1$；</li>
<li>假定$H_0$成立，选取统计量T(T的分布已知)；</li>
<li>对于给定的$\alpha$，我们关注小概率事件；</li>
<li>由样本值求出统计量T的值，看是否发生小概率事件。</li>
</ul>
<blockquote>
<p>注意双侧检验与单侧检验</p>
</blockquote>
<h5 id="四、假设检验存在的两类错误"><a href="#四、假设检验存在的两类错误" class="headerlink" title="四、假设检验存在的两类错误"></a>四、假设检验存在的两类错误</h5><p>第一类错误：弃真——即实际上$H_0$为真，但却拒绝掉了；</p>
<p>犯第一类错误的概率：$P(拒绝H_0|H_0为真) = \alpha$ </p>
<p>第二类错误：取伪——即实际上$H_0$为假，但接受了；</p>
<p>犯第二类错误的概率：$P(接收H_0|H_0为假) = \beta$ </p>
<p><img src="https://pic.imgdb.cn/item/622718cf5baa1a80ab41bd2b.jpg"></p>
<h3 id="8-2-一个正态总体的参数假设检验"><a href="#8-2-一个正态总体的参数假设检验" class="headerlink" title="8.2 一个正态总体的参数假设检验"></a>8.2 一个正态总体的参数假设检验</h3><h2 id="第九章-我的问题"><a href="#第九章-我的问题" class="headerlink" title="第九章 我的问题"></a>第九章 我的问题</h2><h3 id="A-随机变化的量与随机变量"><a href="#A-随机变化的量与随机变量" class="headerlink" title="A. 随机变化的量与随机变量"></a>A. 随机变化的量与随机变量</h3><p>参考链接：<a href="https://blog.sciencenet.cn/blog-3418723-1317469.html">https://blog.sciencenet.cn/blog-3418723-1317469.html</a></p>
<h3 id="B-为什么总体和样本都是随机变量"><a href="#B-为什么总体和样本都是随机变量" class="headerlink" title="B. 为什么总体和样本都是随机变量"></a>B. 为什么总体和样本都是随机变量</h3><p>参考链接：<a href="https://www.zhihu.com/question/301826927">https://www.zhihu.com/question/301826927</a></p>
<h3 id="C-为什么样本可以看成是同分布的随机变量"><a href="#C-为什么样本可以看成是同分布的随机变量" class="headerlink" title="C. 为什么样本可以看成是同分布的随机变量"></a>C. 为什么样本可以看成是同分布的随机变量</h3><p>参考链接：<a href="https://wap.sciencenet.cn/blog-405700-573544.html?mobile=1">https://wap.sciencenet.cn/blog-405700-573544.html?mobile=1</a></p>
<h3 id="D-如何理解一个总体就是一个具有确定概率分布的随机变量"><a href="#D-如何理解一个总体就是一个具有确定概率分布的随机变量" class="headerlink" title="D. 如何理解一个总体就是一个具有确定概率分布的随机变量"></a>D. 如何理解一个总体就是一个具有确定概率分布的随机变量</h3><p>实际问题中的总体,都对应着具体的指标，而这一指标取值或取某一范围内的值的比例(或可能性)是客观存在且确定的，因此一个总体可以看成一个具有确定概率分布的随机变量。</p>
<h3 id="E-为什么样本方差自由度-分母-为-n-1"><a href="#E-为什么样本方差自由度-分母-为-n-1" class="headerlink" title="E. 为什么样本方差自由度(分母)为$n-1$"></a>E. 为什么样本方差自由度(分母)为$n-1$</h3><h5 id="1、什么是自由度"><a href="#1、什么是自由度" class="headerlink" title="1、什么是自由度"></a>1、什么是自由度</h5><p>要理解样本方差的自由度为什么是$n-1$，得先理解自由度的概念：</p>
<p>自由度，是指附加给独立的观测值的约束或限制的个数，即一组数据中可以自由取值的个数。所谓自由取值，是指抽样时选取样本，也就是说：只有当以样本的统计量来估计总体的参数时才有自由度的概念，直接统计总体参数时是没有自由度概念的。 </p>
<p>自由度概念，是为了在通过样本进行参数估计时，剔除系统误差，实现无偏估计。</p>
<p>设$\hat A = g(X_1, X_2, …, X_n)$是未知参数$A$的一个点估计量，若$\hat A$满足$E(\hat A）= A$ ，则称$E(\hat A)$是$A$的无偏估计量，否则为有偏估计量。所以，<strong>无偏估计就是系统误差为零的估计</strong>。</p>
<h5 id="2、详解自由度"><a href="#2、详解自由度" class="headerlink" title="2、详解自由度"></a>2、详解自由度</h5><p>当样本数据的个数为$n$时，若样本平均数$\overline X$ 确定后，则附加给$n$个观测值的约束个数就是1个，一次只有$n-1$个数据可以自由取值，其中必有一个数据不能自由取值。按照这一逻辑，如果对$n$个观测值附加的约束个数为$k$个，自由度则为$n-k$。</p>
<p>例如假设样本有3个值，即$x_1=2, x_2=4, x_3=9$，则当$\overline X =5$确定后，$x_1、x_2、x_3$只有两个数据可以自由取值，另一个则不能自由取值，比如$x_1=6, x_2=7$,那么$x_3$必然取2，而不能取其他值。</p>
<p>样本方差自由度为什么为$n-1$呢，因为在计算离差平方和 $∑(X_i -\overline X)^2$ 时，必须先求出样本平均数$\overline X$，而$\overline X$ 则是附加给 $∑(X_i -\overline X)^2$的一个约束，因此，计算离差平方和时只有$n-1$个独立的观测值，而不是$n$个。</p>
<p>参考链接：<a href="https://www.cnblogs.com/ohshit/p/5590629.html">https://www.cnblogs.com/ohshit/p/5590629.html</a></p>
<p>参考链接：<a href="https://www.zhihu.com/question/22983179/answer/404391738">https://www.zhihu.com/question/22983179/answer/404391738</a> </p>
<p>参考链接：<a href="https://www.matongxue.com/madocs/607/">https://www.matongxue.com/madocs/607/</a></p>
<p>参考链接：<a href="https://www.zhihu.com/question/30440035">https://www.zhihu.com/question/30440035</a></p>
<p>参考链接：<a href="https://www.zhihu.com/question/20099757">为什么样本方差（sample variance）的分母是 n-1？ - 知乎 (zhihu.com)</a> </p>
<p>参考链接：<a href="http://www.noobyard.com/article/p-emeczand-nh.html">http://www.noobyard.com/article/p-emeczand-nh.html</a></p>
<p>参考链接：<a href="https://www.hanspub.org/journal/PaperInformation.aspx?paperID=41815">https://www.hanspub.org/journal/PaperInformation.aspx?paperID=41815</a></p>
<p>参考链接：<a href="https://zhuanlan.zhihu.com/p/138136495">样本方差分母为n-1的看法</a>【重要】</p>
<h3 id="F-如何理解统计学中「自由度」这个概念？"><a href="#F-如何理解统计学中「自由度」这个概念？" class="headerlink" title="F. 如何理解统计学中「自由度」这个概念？"></a>F. 如何理解统计学中「自由度」这个概念？</h3><p>参考链接：<a href="https://www.zhihu.com/question/20983193">https://www.zhihu.com/question/20983193</a></p>
<p>参考链接：<a href="https://www.ltncg.com/info/%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%94%B1%E5%BA%A6%2C%E6%80%8E%E4%B9%88%E6%95%B0%E8%87%AA%E7%94%B1%E5%BA%A6.html">https://www.ltncg.com/info/%E4%BB%80%E4%B9%88%E6%98%AF%E8%87%AA%E7%94%B1%E5%BA%A6%2C%E6%80%8E%E4%B9%88%E6%95%B0%E8%87%AA%E7%94%B1%E5%BA%A6.html</a></p>
<h3 id="G-为什么总是把样本方差计算公式中的分母n-1跟自由度扯上关系？"><a href="#G-为什么总是把样本方差计算公式中的分母n-1跟自由度扯上关系？" class="headerlink" title="G. 为什么总是把样本方差计算公式中的分母n-1跟自由度扯上关系？"></a>G. 为什么总是把样本方差计算公式中的分母n-1跟自由度扯上关系？</h3><p>样本方差计算公式中的分母n-1跟自由度可能是刚好相同，结果“自由度”就被用去解释n-1了。现在很多人能够说清楚分母用n是不对的，但是却说不清为什么用n-1，所以就往自由度上扯。</p>
<p>这里除以n-1大概率因为是因为这个能做出个无偏估计</p>
<p>但是这个n-1和那个关于变量的能自由变化的自由度n-1是否有关系?</p>
<p>感觉是有的，因为很多更复杂的统计量的无偏估计和自由度居然也有这么和谐的等同关系</p>
<p>集合测试：$\Omega = \begin{Bmatrix} 1, 2, 3 \end{Bmatrix}​$、$\begin{Bmatrix} 1, 2, 3 \end{Bmatrix}​$</p>
<h2 id="参考内容"><a href="#参考内容" class="headerlink" title="参考内容"></a>参考内容</h2><p>[1]B站宋浩视频 <a href="https://www.bilibili.com/video/BV1ot411y7mU">https://www.bilibili.com/video/BV1ot411y7mU</a></p>
<p>[2]书籍目录 <a href="https://www.taobao.com/list/item/610183739684.htm">https://www.taobao.com/list/item/610183739684.htm</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>数学基础</category>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>概率基础</tag>
        <tag>概率论学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Python其他小知识</title>
    <url>/2022/04/25/Python-code-tips/</url>
    <content><![CDATA[<h2 id="一、编码格式规范"><a href="#一、编码格式规范" class="headerlink" title="一、编码格式规范"></a>一、编码格式规范</h2><h3 id="1-编码"><a href="#1-编码" class="headerlink" title="1 编码"></a>1 编码</h3><p>无特殊情况，建议Python脚本程序一律使用 UTF-8 编码，并且在文件头部必须加入<code>#-*-coding:utf-8-*-</code>标识，声明文件编码方式，程序文件编码要和声明编码保持一致。 </p>
<h3 id="2-代码格式"><a href="#2-代码格式" class="headerlink" title="2 代码格式"></a>2 代码格式</h3><ul>
<li>缩进： 统一使用 4 个空格进行缩进。（默认一个tab键就是4个空格，有时程序格式问题可能出现在这里，tab键不一定就是4个空格）</li>
<li>行宽： 每行代码尽量不超过 80 个字符(在特殊情况下可以略微超过 80 ，但最长不得超过 120)。如果代码过长，说明代码设计方面可能不合理。除此之外也方便在控制台查看代码(linux)以及通过对side-by-side的diff时有帮助。</li>
<li>引号：自然语言，也就是说一般使用双引号；机器标示使用单引号，例如字典中的key，正则表达式使用原生的双引号 r”…”；文档字符串 (docstring) 使用三个双引号 “”” ……”””。当然实际中需要灵活运用，毕竟三种引号使用是有区别的。</li>
<li><strong>空行：</strong><ul>
<li>模块级函数和类定义之间空两行；</li>
<li>类成员函数之间空一行；</li>
</ul>
</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line"><span class="comment"># one line</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># one line</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hello</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="comment"># one line</span></span><br><span class="line"><span class="comment"># one line</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">pass</span>   </span><br></pre></td></tr></table></figure>
<h3 id="3-import语句"><a href="#3-import语句" class="headerlink" title="3 import语句"></a>3 import语句</h3><ul>
<li>import 语句应该分行书写；</li>
<li>import语句应该放在文件头部，置于模块说明及docstring之后，于全局变量之前；</li>
<li>import语句应该按照顺序排列，每组之间用一个空行分隔(也就是说同一类型的在一块)；</li>
<li>导入其他模块的类定义时，可以使用相对导入；</li>
</ul>
<h3 id="4-空格使用"><a href="#4-空格使用" class="headerlink" title="4 空格使用"></a>4 空格使用</h3><h5 id="1-在二元运算符两边各空一格-gt-in-is-not-and"><a href="#1-在二元运算符两边各空一格-gt-in-is-not-and" class="headerlink" title="(1) 在二元运算符两边各空一格[=,-,+=,==,&gt;,in,is not, and]"></a>(1) 在二元运算符两边各空一格[=,-,+=,==,&gt;,in,is not, and]</h5><ul>
<li>函数的参数列表中，<code>,</code>之后要有空格</li>
<li>函数的参数列表中，默认值等号两边不要添加空格</li>
<li>左括号之后，右括号之前不要加多余的空格</li>
<li>字典对象的左括号之前不要多余的空格</li>
<li>不要为对齐赋值语句而使用的额外空格</li>
</ul>
<h3 id="5-换行"><a href="#5-换行" class="headerlink" title="5 换行"></a>5 换行</h3><p><strong>Python 支持括号内的换行</strong>。这时有两种情况，如下：</p>
<p><strong>(1) 第二行缩进到括号的起始处</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">foo = long_function_name(var_one, var_two,</span><br><span class="line">                         var_three, var_four)</span><br></pre></td></tr></table></figure>
<p><strong>(2) 第二行缩进 4 个空格，适用于起始括号就换行的情</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_function_name</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        var_one, var_two, var_three,</span></span></span><br><span class="line"><span class="params"><span class="function">        var_four</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(var_one)</span><br></pre></td></tr></table></figure>
<p><strong>(3) 使用反斜杠\换行</strong></p>
<p>二元运算符<code>+</code> 、<code>.</code>等应出现在行末；长字符串也可以用此法换行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">session.query(MyTable).\</span><br><span class="line">        filter_by(<span class="built_in">id</span>=<span class="number">1</span>).\</span><br><span class="line">        one()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;Hello, &#x27;</span>\</span><br><span class="line">      <span class="string">&#x27;%s %s!&#x27;</span> %\</span><br><span class="line">      (<span class="string">&#x27;Harry&#x27;</span>, <span class="string">&#x27;Potter&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>(4) 禁止复合语句</strong></p>
<p> 即一行中包含多个语句： </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正确的写法</span></span><br><span class="line">do_first()</span><br><span class="line">do_second()</span><br><span class="line">do_third()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不推荐的写法</span></span><br><span class="line">do_first();do_second();do_third();</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="6-注释"><a href="#6-注释" class="headerlink" title="6 注释"></a>6 注释</h3><p><strong>块注释</strong> ：“#”号后空一格，段落件用空行分开（同样需要“#”号）</p>
<p><strong>行注释：</strong> 至少使用两个空格和语句分开，不要使用无意义的注释 </p>
<p><strong>补充：</strong> 在代码的关键部分(或比较复杂的地方), 能写注释的要尽量写注释；比较重要的注释段, 使用多个等号隔开, 可以更加醒目, 突出重要性。 </p>
<h3 id="7-命名规范"><a href="#7-命名规范" class="headerlink" title="7 命名规范"></a>7 命名规范</h3><p><strong>模块</strong>： 模块尽量使用小写命名，首字母保持小写，尽量不要用下划线(除非多个单词，且数量不多的情况)</p>
<p><strong>类名</strong>： 类名使用驼峰(CamelCase)命名风格，首字母大写，私有类可用一个下划线开头；将相关的类和顶级函数放在同一个模块里. 不像Java, 没必要限制一个类一个模块。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Farm</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnimalFarm</span>(<span class="params">Farm</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_PrivateFarm</span>(<span class="params">Farm</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>函数名：</strong></p>
<p>函数名一律小写，如有多个单词，用<strong>下划线</strong>隔开；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_with_env</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p><strong>变量名</strong></p>
<ul>
<li>变量名尽量小写, 如有多个单词，用下划线隔开</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    school_name = <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>常量采用全大写，如有多个单词，使用下划线隔开</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_CLIENT = <span class="number">100</span></span><br><span class="line">MAX_CONNECTION = <span class="number">1000</span></span><br><span class="line">CONNECTION_TIMEOUT = <span class="number">600</span></span><br></pre></td></tr></table></figure>
<p>【参考资料1】：<a href="https://blog.csdn.net/meiqi0538/article/details/104993760">【Python】编程代码书写规范！ -  科皮子菊 - CSDN</a></p>
<p>【参考资料2】：<a href="https://python.freelycode.com/contribution/detail/47">PEP8中文版 — Python编码风格指南</a></p>
<h2 id="二、Numpy学习"><a href="#二、Numpy学习" class="headerlink" title="二、Numpy学习"></a>二、Numpy学习</h2><div class="bvideo">
    <a href="//www.bilibili.com/video/BV1Wy4y1h7ii" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i0.hdslb.com/bfs/archive/ab719949488afaf673a171c1b2ae61f525450b12.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:08:44</span>
            </div>
            <div class="bvideo-info">
                <p class="title">一个10分钟的numpy入门教程</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>11.8万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>271</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">奇乐编程学院</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div>
<div class="bvideo">
    <a href="//www.bilibili.com/video/BV1Ym4y1U7at" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i2.hdslb.com/bfs/archive/6394a64929382e1451f20ed843a1091df68d121e.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">01:17:48</span>
            </div>
            <div class="bvideo-info">
                <p class="title">巨硬的Numpy：从小白到入门</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>3.1万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>8</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">二次元的Datawhale</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div> 
<h2 id="三、Tensor学习"><a href="#三、Tensor学习" class="headerlink" title="三、Tensor学习"></a>三、Tensor学习</h2><div class="bvideo">
    <a href="//www.bilibili.com/video/BV1dp4y1U7mD" target="_blank">
        <div class="bvideo-box">
            <div class="bvideo-cover">
                <div class="cover-default"></div>
                <div class="bvideo-cover-layer" style="background-image:url(https://images.weserv.nl/?url=http://i0.hdslb.com/bfs/archive/46a4f02702b678e9abf1cf2b4c7bae525222f1c4.jpg)">
                    <i class="icon-video"></i>
                </div>
                <span class="duration">00:31:03</span>
            </div>
            <div class="bvideo-info">
                <p class="title">02 轻松学 PyTorch 初识Tensor</p>
                <p class="card-status">
                    <span class="play-num">
                        <i class="fa fa-youtube-play"></i>
                        <span>1.1万</span></span>
                    <span>
                        <i class="fa fa-list-alt"></i>
                        <span>58</span></span></p>
                <div class="partition">
                    <label class="card-label">视频</label>
                    <label class="up-label"></label>
                    <label class="up-name">唐国梁Tommy</label>
                </div>
                <div class="actions hide"></div>
            </div>
        </div>
    </a>
</div> 
<h2 id="四、matplotlib学习"><a href="#四、matplotlib学习" class="headerlink" title="四、matplotlib学习"></a>四、matplotlib学习</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># python中画图所用到的第三方库</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1956-1920年对应的GDP值，单位亿元</span></span><br><span class="line">y = [<span class="number">1470.1</span>, <span class="number">1447.5</span>, <span class="number">1312.3</span>, <span class="number">1071.4</span>, <span class="number">1030.7</span>, <span class="number">911.6</span>, <span class="number">859.8</span>, <span class="number">824.4</span>, <span class="number">679.1</span>]  </span><br><span class="line"><span class="comment"># 1952-1960年</span></span><br><span class="line">x = [<span class="number">1960</span>, <span class="number">1959</span>, <span class="number">1958</span>, <span class="number">1957</span>, <span class="number">1956</span>, <span class="number">1955</span>, <span class="number">1954</span>, <span class="number">1953</span>, <span class="number">1952</span>]  </span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)  <span class="comment"># 画点，将列表中的值分别画在plot中</span></span><br><span class="line">plt.plot(x, y, color=<span class="string">&#x27;r&#x27;</span>)  <span class="comment"># 连线，按顺序将</span></span><br><span class="line">plt.xlabel(<span class="string">&quot;year&quot;</span>)  <span class="comment"># 横轴的标签</span></span><br><span class="line">plt.ylabel(<span class="string">&quot;GDP/Billion&quot;</span>)  <span class="comment"># 纵轴的标签</span></span><br><span class="line">plt.title(<span class="string">&quot;Changes in China&#x27;s GDP since the founding of the PRC&quot;</span>)  <span class="comment"># 图像标头</span></span><br><span class="line">plt.show()  <span class="comment"># 相当于将图像显示出，必须要加上才能显示出图像</span></span><br></pre></td></tr></table></figure>
<p><img src="https://pic.imgdb.cn/item/6268bae1239250f7c5112e80.jpg" alt="显示结果" style="zoom:80%"></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>编程与深度学习</category>
        <category>编程语言学习</category>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python基础</tag>
        <tag>编写规范</tag>
      </tags>
  </entry>
  <entry>
    <title>科研日记</title>
    <url>/2023/07/16/Research-Diary/</url>
    <content><![CDATA[<h4 id="2023-07-14"><a href="#2023-07-14" class="headerlink" title="2023-07-14"></a>2023-07-14</h4><p>今日主要完成工作：泛读并简要整理如下5篇文献。</p>
<ol>
<li>第1篇是2022年西安电子科技大学的任启宏的硕士论文——《复杂杂波背景下的雷达目标检测方法研究》 </li>
<li>第2篇是2017年海军航空工程学院的陈小龙发表的一篇小论文——《高分辨稀疏表示及其在雷达动目标检测中的应用》 </li>
<li>第3篇是2020年江苏自动化研究所的潘冬华发表的一篇小论文——《海空小目标雷达检测算法研究》 </li>
<li>第4篇是2021年西安电子科技大学的许述文发表的一篇小论文——《海杂波背景下雷达目标贝叶斯检测算法》 </li>
<li>第5篇是2013年中国民航大学的吴仁彪发表的一篇小论文——《机载雷达高速空中机动目标检测新方法》  </li>
</ol>
<h4 id="2023-07-15"><a href="#2023-07-15" class="headerlink" title="2023-07-15"></a>2023-07-15</h4><p>今日主要完成工作：泛读并简要整理如下6篇文献。</p>
<ol>
<li>第1篇是2022年电子科技大学的吴佳奇的硕士论文——《机载雷达高速目标积累检测算法研究》 </li>
<li>第2篇是2013年中国民航大学的贾琼琼发表的一篇小论文——《基于修正STAP的高速空中机动目标检测方法》 </li>
<li>第3篇是2019年国防科技大学的回丙伟发表的一篇小论文——《空中弱小目标检测跟踪测试基准》 </li>
<li>第4篇是2015年西安电子科技大学的饶烜的博士论文——《空中弱小目标长时间相参积累检测技术研究》 </li>
<li>第5篇是2022年西安电子科技大学的董佳鑫的硕士论文——《雷达低慢小目标检测算法研究》  </li>
<li>第6篇是2023年烟台大学的秦天慈发表的一篇综述文献——《雷达恒虚警目标检测处理技术综述》  </li>
</ol>
<h4 id="2023-07-16"><a href="#2023-07-16" class="headerlink" title="2023-07-16"></a>2023-07-16</h4><p>今日主要完成工作：泛读并简要整理如下4篇文献。</p>
<ol>
<li>第1篇是2007年深圳大学的郭尊华发表的一篇综述文献——《雷达目标联合跟踪与识别技术研究》 </li>
<li>第2篇是2017年兰州市27支局的胡志宏发表的一篇小文章——《浅谈基于粒了滤波的微弱雷达目标检测方法》 </li>
<li>第3篇是2012年南京航空航天大学的张弓发表的一篇综述文献——《压缩感知在雷达目标探测与识别中的研究进展》</li>
<li>第4篇是2023年电子科技大学的林兰瑾的博士论文——《运动平台雷达目标检测中的相参积累方法研究》 </li>
</ol>
<h4 id="2023-07-17"><a href="#2023-07-17" class="headerlink" title="2023-07-17"></a>2023-07-17</h4><link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>学术科研</category>
      </categories>
      <tags>
        <tag>科研</tag>
      </tags>
  </entry>
  <entry>
    <title>空时自适应处理STAP算法梳理</title>
    <url>/2023/08/08/STAP-Algorithm-Foundation/</url>
    <content><![CDATA[<h4 id="1-STAP算法简介"><a href="#1-STAP算法简介" class="headerlink" title="1. STAP算法简介"></a>1. STAP算法简介</h4><p>空时自适应处理算法(STAP)最早是由L.E. Brennan等人<sup><a href="#[1]">[1]</a></sup>于1973年提出，利用机载雷达杂波信号存在空时耦合特性。</p>
<p>STAP技术充分利用多通道雷达提供的多个空域通道信息和相干脉冲串提供的时域信息，通过空域和时域二维自适应滤波的方式，实现杂波的有效抑制。STAP 技术的自适应体现在对外部杂波环境的准确感知及应对，其依赖于待检测距离单元( Rangecell Under Test, RUT) 杂波协方差矩阵(Clutter Covariance Matrix, CCM) 的实时获取，而CCM 在实际应用中通常是未知的，需要通过一定数量的独立同分布( Independent Identically Distributed, IID)训练样本<strong>最大似然估计</strong>得到。根据Reed-Mallett-Brennan准则<sup><a href="#[2]">[2]</a></sup>，确保STAP输出信杂噪比损失小于$3\text{ dB}$以内所需<strong>IID样本</strong>数应至少为<strong>2倍系统自由度</strong>。但机载雷达通常工作在非均匀杂波环境中，难以获得足够的IID训练样本<sup><a href="#[3]">[3]</a></sup>。</p>
<p>目前，国内外解决上述问题主要两类技术： 一类是将含奇异值样本剔除，使得样本均匀化；另一类是降低均匀样本需求，设计小样本条件下次最优STAP处理器。具体方法划分如下。</p>
<p><img src="https://pic.imgdb.cn/item/64ddcb58661c6c8e543b0a68.jpg" alt="blob"></p>
<ul>
<li>降维STAP将样本需求由全局系统自由度降至局域系统自由度范畴。</li>
<li>降秩STAP基于子空间处理，摒除了由于噪声发散引起的自适应方向图畸变问题，将样本需求降至杂波秩量级，但性能严重依赖于杂波秩估计准确性，且运算量巨大。</li>
<li>平滑STAP可利用有限样本的空时平滑获取更多样本，但样本间的强相关性及其固有孔径损失使得该类算法性能不甚理想。</li>
<li>直接数据域STAP仅利用RUT数据，消除了非均匀杂波影响，但易受噪声影响且存在空时孔径损失导致性能无法达到次最优，同时空域平滑处理也决定了其仅适用于均匀线/面阵机载雷达。</li>
<li>参数化STAP本质为空-时最小二乘有限冲激响应滤波器，在理想条件下可显著降低样本需求，但在实际应用中性能受其模型准确性影响较大。</li>
<li>知识辅助STAP利用先验信息所估CCM对杂波进行预白化，以降低后续STAP 处理负担，但如何准确获取及有效利用先验信息仍是当前待解决难题。</li>
</ul>
<p><style type="text/css"><br>.tg  {border-collapse:collapse;border-spacing:0;}<br>.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;<br>  overflow:hidden;padding:10px 5px;word-break:normal;}<br>.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;<br>  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}<br>.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}<br>.tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}<br>.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}<br>.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}<br>.tg .tg-0lax{text-align:left;vertical-align:top}
</style></p>
<table class="tg" style="undefined;table-layout: fixed; width: 574px">
<colgroup>
<col style="width: 50.0px">
<col style="width: 150.0px">
<col style="width: 210.0px">
<col style="width: 280.0px">
</colgroup>
<thead>
  <tr>
    <td class="tg-7btt" rowspan="20"><br><br><br><br>S<br>T<br>A<br>P<br>算<br>法<br>概<br>述</td>
    <td class="tg-fymr">关键问题</td>
    <td class="tg-fymr">涉及的相关方法</td>
    <td class="tg-1wig">注释</td>
  </tr>
  <tr>
    <td class="tg-fymr" rowspan="2">运算量和误差<br></td>
    <td class="tg-0pky">降维STAP</td>
    <td class="tg-0lax" rowspan="2"></td>
  </tr>
  <tr>
    <td class="tg-0pky">降秩STAP</td>
  </tr>
  <tr>
    <td class="tg-fymr" rowspan="7"><br><br><br><br>非均匀杂波</td>
    <td class="tg-0pky">功率非均匀抑制法</td>
    <td class="tg-0lax" rowspan="7">机载雷达所照射的范围非常广，能达到几百公里，在这个范围内地面/地形的情况不可能是一成不变的(如陆海交界)。</td>
  </tr>
  <tr>
    <td class="tg-0pky">非均匀检测器</td>
  </tr>
  <tr>
    <td class="tg-0pky">直接数据域法</td>
  </tr>
  <tr>
    <td class="tg-0pky">模型参数化STAP法</td>
  </tr>
  <tr>
    <td class="tg-0pky">知识辅助的STAP法</td>
  </tr>
  <tr>
    <td class="tg-0pky">稀疏恢复STAP法</td>
  </tr>
  <tr>
    <td class="tg-0pky">混合STAP法</td>
  </tr>
  <tr>
    <td class="tg-fymr" rowspan="7"><br><br><br><br>非平稳杂波问题</td>
    <td class="tg-0pky">一维补偿类法</td>
    <td class="tg-0lax" rowspan="7">主要是由雷达天线放置的形式和载机飞行方向之间的几何关系，例如若机载相控阵阵面于飞行方向不平行(存在夹角)，此时杂波回波谱会随着距离变化。</td>
  </tr>
  <tr>
    <td class="tg-0lax">二维补偿类法</td>
  </tr>
  <tr>
    <td class="tg-0lax">空时内插类法</td>
  </tr>
  <tr>
    <td class="tg-0lax">权值调整类法</td>
  </tr>
  <tr>
    <td class="tg-0lax">逆协方差矩阵预测类法</td>
  </tr>
  <tr>
    <td class="tg-0lax">基于俯仰维预滤波法</td>
  </tr>
  <tr>
    <td class="tg-0pky">3D-STAP法</td>
  </tr>
  <tr>
    <td class="tg-fymr" rowspan="3"><br>空时自适应检测<br></td>
    <td class="tg-0pky">基于GLRT准则的STAD</td>
    <td class="tg-0lax" rowspan="3">传统STAP主要用于杂波抑制，STAD则是将杂波抑制与目标检测结合到一起考虑。</td>
  </tr>
  <tr>
    <td class="tg-0pky">基于Rao准则的STAD</td>
  </tr>
  <tr>
    <td class="tg-0pky">基于Wald准则的STAD</td>
  </tr>
</thead>
</table>





<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献 "></a>参考文献 <font face="times new roman"></font></h4><p><a id="[1]"></a> [1] Brennan L E, Reed L S. Theory of adaptive radar[J]. IEEE transactions on Aerospace and Electronic Systems, 1973 (2): 237-252. </p>
<p><a id="[2]"></a> [2] Reed I S, Mallett J D, Brennan L E. Rapid convergence rate in adaptive arrays[J]. IEEE Transactions on Aerospace and Electronic Systems, 1974 (6): 853-863. </p>
<p><a id="[3]"></a> [3] Melvin W L, Showman G A. An approach to knowledge-aided covariance estimation[J]. IEEE Transactions on Aerospace and Electronic Systems, 2006, 42(3): 1021-1042. </p>
<p><a id="[4]"></a> [4] Melvin W L, Guerci J R. Adaptive detection in dense target environments[C]//Proceedings of the 2001 IEEE Radar Conference (Cat. No. 01CH37200). IEEE, 2001: 187-192. </p>
<p><a id="[5]"></a> [5] Guerci J R, Baranoski E J. Knowledge-aided adaptive radar at DARPA: An overview[J]. IEEE Signal Processing Magazine, 2006, 23(1): 41-50. </p>
<p><a id="[6]"></a> [6] Brown R D, Wicks M C, Zhang Y, et al. A space-time adaptive processing approach for improved performance and affordability[C]//Proceedings of the 1996 IEEE National Radar Conference. IEEE, 1996: 321-326. </p>
<p><a id="[7]"></a> [7] Goldstein J S, Reed I S. Reduced-rank adaptive filtering[J]. IEEE Transactions on Signal Processing, 1997, 45(2): 492-496. </p>
<p><a id="[8]"></a> [8] Pillai S U, Lim Y L, Guerci J R. Generalized forward/backward subaperture smoothing techniques for sample starved STAP[J]. IEEE Transactions on Signal Processing, 2000, 48(12): 3569-3574. </p>
<p><a id="[9]"></a> [9] Sarkar T K, Wang H, Park S, et al. A deterministic least-squares approach to space-time adaptive processing (STAP)[J]. IEEE Transactions on Antennas and Propagation, 2001, 49(1): 91-103. </p>
<p><a id="[10]"></a> [10] Parker P, Swindlehurst A. Space-time autoregressive filtering for matched subspace STAP[J]. IEEE Transactions on Aerospace and Electronic Systems, 2003, 39(2): 510-520. </p>
<p><a id="[11]"></a> [11] Melvin W L, Showman G A. An approach to knowledge-aided covariance estimation[J]. IEEE Transactions on Aerospace and Electronic Systems, 2006, 42(3): 1021-1042. </p>
<p>&lt;/font&gt;</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>信号处理与雷达</category>
        <category>信号与系统</category>
      </categories>
      <tags>
        <tag>雷达信号处理</tag>
        <tag>STAP</tag>
      </tags>
  </entry>
  <entry>
    <title>如何作科研</title>
    <url>/2022/03/18/Scientific-Research/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="11080732f47c4591b8313eacf32f092250948f430bacd1e4db31477afda41044">270cb3bbed971930ec4ebcba9f8cc138f5d4800f50eb8f4502a546dcfd41431b30998da6854b2e15eb22a7e101761fe2b3ed00e98f1f91479488e71aa95499c395f2da24a1a990b53a4de9ac0615424691ef8c4296fdd3e7f3665920e08d9281e077e7b0f6f6451bd4d7fa5c6baebccfe6a9542cb5bc7623d935af3d8a9479367991556a60945b8fd167599426660b6fecac4510bb724cdb230e0e65b8abff829a823fb3cf6fb8553aad575a0c56f140e2445340d04b8a7d23fa8babc90c6204cf338db488b04532dbdc3f4b5c2b55c8b4a1ec45df45227b2cbfafaa97c19893dfe050b432cd5db51fbae44f2dd2112ea755ad3b6b7dfe894540a8ee94238d0a4dc448577dac9254628aea179a9bacbe31a2256493b2d7ec6972fb9bad69b1848730ee7cb6ac99ff0ab741ddc8146e505a5ddc236e5ca7eb1677182cd27e50890f930b5c6e7f91acad7439f8f59623354bf4360d7aafb47cbf5f1ef121bd66e16e20cc1f2d5eba7936dbc1fd443524367ddf6605ec7dc032492515812ae9e6d31c45213445561ea148b3dedbb0d9870d1bf25ff1d9c6d7b38d0285f2b8fdbd9bbcecf7afd8a4ae96b4b40d3c521cec24acd3b8a93fce71dcccc2dd3b51fcfdf1979652f7591a6313ce581effa6d731cfcbfed0ef4d6079c66c4e83221e0aa6ed653c1e39729bbc4c620d3b26beb551b6685c3d8de8e28c368fe6249ca3be78eccbbba97ccccbf44cd90820054419ef2f06c6e268326f0a09b26bcea8581a727b1cce1b6a3aedf0f82df7d83b69773e10cf89988098bbfc37a062927a6243a8f8da231681648f0bc220b0f15a41e337720ab40e3e5123852c2a8ef32c8cfb5599fba20d084f689f350787b683fb12c3529c4e97bc6f3cdae109842884295e3c0f4bf2c5bfcbd2782d1b8c890169d57b9548131b62ae8c9b78148a3cd533661793bc33b64e358577e69f63b8a2b4bafd766eee9678a09df069c92d758f24a82aeba0d6a34d2c39a781ad6b5991d5ccfdeb5b5cd730adbe2216e3b0450f0685cd08b1c1404aa8930540c6118e53daf768c2e1d9e636f6168538d75e5f8b98281fe84bc5c78740a93216ba4fa2ceb91ce8bc3ddacf90cc56c47ddcb11e0bca9d13b315fedc1acda808d646f36d32a4730f18b2ee57cbdf02c80c227e5cfbfb75d63b0d5fd09abeb269d4eb4b769c857909279eff838f3ae047ecb9263d96ea2cbc8c1c6c6c1a3cab57f019a4ceab9b59c4e37b7a2e7128829165f0ed2706b52916fc274558be4c5611e075be1b0c0b3eb8cdfc979319ec31d0c424731ebebc0e8a8c73313ed6ee9462e8cc60992cd0ffdd8f0b3dce8adde71abaf5572a6a7879755fc824baadaf0acf5605348428182ed0113845adfc0279d14e5254cfec372ae4dc1ce9045fcc1ab716c246670ce56bf3fde3f7840b11c49a322320a4ad0824952111ef752c76ab2a52238cfff19005d74b3101227879f64f68b634416edcee782c68c1a2eae9e9c40b4faef56262d630ac3efb4b1cd51e06ffa06c64abc2c2980117da1adaa19ddbf1a71ca8fa9c389559beb05efc6741d0c31b0008d64c5c63414d0afe7a7f3fe62f30a22cd833948f88d9c1e0b4116c6c66a6d5ca30701b017a9c23d7ad76453b84c6ebf35d18e5cd6bd9e06224ac32153b0e325142d3bcb4f9844d64d27498a7d1048d89d4b8de3b72034994ca27f5ea1816ad2b838d898cd57bc90137d6cd9bf6aab78d0839927ec7396cc7a121c640253d9c2efbc1901da0878ec28ded888c48b06da8a576b7255fe35b1e15d589e8d1e08043230c7b480d3f26ad477ca6902a1e81c8dff6c0c0bfbcc7395fa1a4465ba5069e62d0f9bbd0058ebc6f8af5d3fa63b5374363d47b0c6b719b324773c2c18ae0fe37670648d3aa129c89c48559973a616f52509d2a71cde838893fa505f2d3102623ee81e5ae49f1cc7b718101fffe4fa82e0ff8f20abaa963f4e33ecbf0263a8237d3f9ff794e9dad7522ce7546667d4443a7f1c61ae03bdc79b33254ff7b9cd0e23fe3deee23a8169eaec0ea50c52044ff12f909d8344557fbd3eb9b89b290ecede6dc45bb03492f05e91e9d962862b2f0d59326d38214de0a0780d1c848e9acb08b33a81dad11bb128984879a7264fdbb4e637a2e11092d1460045d827c714b3f39f37f00c7e31c0f69a63705316040644f3ad7bf6c221d030f5d0bb23a09b03d1bec1abf8be8b6bbd5801c82caed5251829726d9c687068b457c0c31f19654bfd10975508e61e693008c80e3ce2ccab29b12b0a3fe5151f4365cf4a94cb4577a1751f78b8b908105a8c2681e3442c96fb20dddb58bea26c4f429d5b45478275a4722273493470770bfec61b8ec982f7e146dcf242067cb67fe9e49903e1190e36753a052261f6d42425d8bed38a4486bab22115aff071f91409e188634b348a5f05053ed731182f1d67c40d97f655f5c36690b9cf5fa2e779e4e3ebebccab43c67fb0b834a80daf571775cce4688e31d1c3ca6e612a55d1415c8e20fe0f260dfd0bf052224dcf23f344d2e8e490f7ea6ed32cf13c9c297c67c31550aab27cc8c52c706ad736912ec8c02fa7b641c4de7970d09d183ad3721d1d8f9963d8fb77d42d7a71116486e99c0fb6d74b7556639013a15eb690f7716e28abbdd84758cad10d1cb3a4df9e803b0dd79dcd0158c8fd5ba5fd9e6b97a29ba79a51fa6fe961734cb2f2838244301efc56f92d29473db79ee0bea29eb7b416eabc50dd0c504aa6955a2fae235bf36deca38f978e8f385987bf5267e427903d47870cf3f4efeb84ed4b91f018d974b30ff972ba91f19886c3fb4ce4a09d73a1531dfa47d6173a6a06e10fe0859042f7c069a1a7d3064b27e5bb1dcb9b4f3dacf7631158e2d0c92f019e8db76618ef27b7f0a9964097a074a34f7253d0d82e2635f353641cbe78cd30faf20d8fda9404a1d57ef6ae573fedd11043d1eec40e331dc482f9c7e3fa2645f829eb518e9bd7145a307b06bd44850bf4f7a4c859a16447819e56b7c8e98e697578da42004007da368163e15343865b922215759d4522a27c5ab5517d33fca024d539931d0394cdef5bd8aa3c28b861376f87b8df2c576599e33b5cbc973ffb7a0303364b2064fa0b5b7713cb311b8af773e137340ac1b82e8205b9f450e964226ab4dab20fe6d88c9336211cbbffea6563cc3d2f3b04344f7c12040b621e6ca6aafafb58888d6e72873470f8f702668d89163707256ce85bee1a599f83bbdfd42b50b20aaacb53725e84c0523d3a657fa018c3140c46b08ffcd6abc0efa1ebb029acfc2d57b4213325d94da792ae82ed6f0043cd74f3f8bfbfec7ea330a9e34bc90213f66963e958d3e1cd5a788d8c720f18d503e9096bc2a4840b0bb0131dae19656928a8d4c443bd7f2b13b321f5d113fd19087673cf658f667d5b9f11a813b000b17b86176bf303782e5f2c78871b07b3e45c4d253aa223c4fac7d5069ea760347e5650ebede243bfd6e248827f6de6ca0ac98bf1ead4d8f776f429be37dd97e50cf655dc83ccacaa6ec83e1112cec73ef9652c5ec48021d7b38c9e4f47e4eaf0661ef51839292c62b23b4dd7e1a6ff6ddad991ca7b35d9b22408dca168dc1111e3e3b5e3a1219812ea5c39d5759943b24aa37af743f7aea87c999b19785f654c5f862ba6a9533d7539270ea9e1bb60e329ac02765324c7abed8683449b04d235de4ac3c65cac172eb62ceb84185da5ef46274784f2d89465d3108a7b91c3d647f034dd33a6b40535eded7bfdec991d2be149baf1ffc0a19a50f9176ca75067c6bd35f2fc1de673f82f4a6e8ee1a3356f0ed4aaf1a0cc5400ef0621b735616e939dc6d2f696c0023865639385d1c38ac34820d62885cf9c05610392bc058ac0bf7bc8802e0d31b5ee1d448b1b5db6f67d15bbc076f5e150f89f05c8bec03a616937bb99bb7c7524f9e9b5d9a770ebe0eb21f4596dd6c9181e7b36e9e602e3c7c3604cc2512eb444a5b38f4666b09e20a8961bc1dd6082dd64afef6d2df37200dbf9e1b68423defbccd33ce6b2de2bf5993abb798c5e595e80145fbbced1994be6b5065460647cc6a8ae97a74186ec896b89233439c6f4409ff7b1ced148da42dede3dcf8dc41d37ec9a1fe7d5ce31effac3274887ab290d8d8b185629efb3bd9297d64996beb77501e4e4725fb7af5be79ab87a7d77ec86ea4bac14338498c0e65664c99870914cb20e4a25dbeddfac0484b917ecfb9fc2917f051cb98bea85e9b54705a79d12d5800e5ad9298b51e396a91d6467a7305a3dfc8dcd31479d9d55c5d3b9a621666e19b6331064b46da374986449238e4b8e64b5ee0f37ce5718a818253d69e415463e6f0e98a53dcbb8d54b3344b1955d127bbd6119004568a2aefadc69d5343184d6b5451ec63f71f441bcf6445166acd07463cd986d731e86c63aab691b0f31ba8aa8b8ba488682f0eacaf1bc116589d18d2d3fc8dd70fc32167aae6810eeea698cde7771c45269e1867168c4c45524146fe23dfe3adfcff42655326aaa7483c5da862d5f3e0a0fba7ec344a0eebb4e1ddcc6beebdb2753337c8ab53c526e9b327fb03003089df2fb56a48f1fc90286381a88d642285c89d01568d9cb0229ad460ad80c9013e91e403c53e6198b8ffbdbd8705b49ff7b4cd48a15df3bad1728f9e0ad503120ff0fee21aa6e6519e99ce216232bc9dd74e352f653c03a145319d7263f09f1b88dfb800dec2576b03a0eb3b131c9233c7697ef9e291c8cfee9d9fac9a5ad1ac350fb74db20a4ee66241a842f5eac7e6a1adcbdcd94e10bab0ce93adcff221f9445ae938ebaa535eeca83e4536399ec7657a776c26740072d32af91da53f30ba9e584a79fa5d7f5a5b9dc0009d9d5a583cd427e9ccb81d81007bfba75e421e27d26738e1bae52ddf2694e964f0150444b1bf561b29d09cb801911e5bcc1a37927cdd7c87cc6de5a9d4fa6af30ba0ace8ef34eb4e5b0457b3dd18c53a4627530e697225b53fb2ef2e2accbe7e19a8d40a70663eb266fc803badd3876deef1694fb03fb0db1eb2ed2d519f787d7bdd93fe16c34eb44cf1b7c687ffbda6aea57e0c89216f427e854d9c63ede97d79cd6ebf1abf321c190dbe28326c40bf05c757ffd4df309ed9113f32da9421fb010ed879efead37ff510e9dccec6de2541d56a8eb798410850b70d92c436ec36095b1c6cf730005b654b0184edeeccaa3191293e45bb63c831b354e467c87aa99eabcd468b3a0e0f11fc99f4d2c12175b98a540d084c9fac263ca19f5c371c4e91255a66089d6e735b5ddd30965be6956f25a41cffd1bb7569eafcc1b5bcd6225d1ba9439244b55d2abe44d2f43bb3c95f6894e5150a6958d7e2f9b63d43bf86f94ef9bf9250f84124e4d27b017f209a913179613008c069a5ceccd3f885275fb8ec1b1788d0c06906772a214ed81bdcd05631d5fcf376e759285c7c00eb2370c4c05e253f0fc54ac9d70b9fa22d55f177e64b9191cca2f6044d838117043bf193a176261b458d7e0095102a83db39b9c575d10c8b6b8ebf1c341a1fc4bf39e7d96310de3af71cbe9bf59ff09757069092f06f96d17f85bfe4f5d35b1d98803c70ca5d62d435a68cb6b4a32c02c1ded28ee0030f75f709033598fe05e1bd61ec985312034a745d8ce1af4690f6021a093bfa70fae2c1f9d88b0fcd1f395ee92bf6ccea1d39be4a9b21a109bbb7b2c8a427046a98fba7e654bd3971b4ba040ebc65c4af729f7ad79b151df3bda60ac01f20f1042a7a3d4d9534a5836085d09a29d803f127208abaac2d6a9e39cb4e732ba8026d53627c94558c59ed0797c631d2a0507dd8045319e98b2d6dfc0c9af014cc223dde74446eb45dbb15cb6d25f6dc4b8735b8031b44a5620000e0677a576067a3bdeaf001d34800d48c3c72864bcafb14cacdacbeeb101a31786404c32d9fe3bd71bcdfc29aebd11f403a6238dff94c05bf676e9c57f994c23ac797cd2d0220a01478f780dec1a686b750a49ba3a0435ef646389368003dd6688a1999bf9c3af4eeb7ab5f28f3cfb91940f536b31885db3d7ef71bf67cf4fdc87b89dc8f4632b502f27a29385f5aa2c69643a2b64d8b1401cf8625dc425d913aaa9e7e5a53703b19c4f65c201985d257286f882577e2abb74716bfc9bef1ba173eb3c93fa4a03875d3bae3b4448ac266a236d2ce812263585942f944dc2d5e8a02f54d70dbc2ef8cc175f69905c9289fd4c9bf2d87ec6dc581d8c7eb6e4b6927f983a729f14578e630a017584648cb50c0f9a866271fcaf87c8f30a984e0fbd70b60637fd522ec2e48096bc30664c5d827e5e5b70d21a24c1445eac69874b47a0453e3466c092b677aaf0c7e123c310e4ddc4e607caed385016c155d3978edb7508ce30c3f8a681deb31456a5df36d859dac964ecf6cac1fbc7e233f990659984039762efb500197979789eb85f861b8901c9e8d70bd451e59294c9c075a81c43f99f6c246eca7da828b10e616d8c00b31ed07639b641ae9e5e3a1be2ae922961b96f2c94cc9d1524ab5faa37f875f2c969f114d1c4721d79add661fbd733a3c2d46de66b560ff5eb786de5a226c92fc8f119b5d58c6d825a07bfe3714b92babbfca0677a4d031fb8723a1c1163c83236b35e5b3257d2ffcd73dd9124af5be75da8563902a27718e338a7f84603733a9c7401f5b50894503191e69c1f7b1ec6b38c7c07eef4f13882c996c2f532aeab534da5f7eb846277c2e5077b859c81cb57edf5a6f46f30ba226880a45721e9bdb47a2ec7f7db4ab8b550b87b45371b11612e1667b9ba78bbf5b8ba9caf64a1ed461f19728351c4136ceb3f2cf314253fc50d8d0492378ea2683834f4ac2b74ab677ce068648fe79a49f409ff90b5793597785b9ad8eccaabd2997dcc8ecc4d7cca616fd47ff3f0cfc416dd13dec69cc4862781260e08b146f0e220a12f100a741301f5f5ad0c724374d0e916c938e5a2c3966292e20c4862cbadfe3a564dd974251e0c628417a293c82f466b4a2683db190fbe306dbea131ae0fab54769be6a84d05f87a00ba6d41f9ca5ec9645112570e90cf85266c1317331e661d7dcd5b4a86563ea8eeb82bcc3dab971b2c59b3ee50841871624a8628e8dd5c2429fbe9c8b19db6cfe581a2f2b5f3fd698bdc04ddc86cfe8bec70b138cac591994e6a58f4af9568468fa6908c3af74180044838f52abdd695f34ee6301ad661befc725cf03268ac2df538f9f6fd6375408f4488ca7082711089cb2f8bb8960c04894df51cc40e64e4b2911efdc17a312540b14fd7cb8bf5ad24aaf87428478400d07b6e8e0c79196a338d74acb5efc7d4dc045bf7f6e10808f8c65f05313f7aec0709180c45b5da0f728d7827f82ccf0a04c6c1d7002e898b36d9bbf7287ef2059308add3a85651e597c942ec496b0b38ae16b62fae3c066367227afdbb2eb508dd37ecfc4a69ce71adec3854c70fb5f1d7fa04db192af166fe2040c5d055f446ab7d3c7ff38bcebf5f5c9489574461ab74c6a7297896a9b7e868426c0d64220fcf7207d54b751fdfea750393a40fb778bcd0d3902616c471b807f3e94a273543710adf27aec6970e0a987624092260e42efbd721d5923a7fc431cb60859ea86e28476fefb4ff9c7a8e801e4d9c0ab99dd6656609bd6fd773e4a8bc0eae95634a88a3e1fa1c388fb2e684b442c85c8625d8b0ade7d8f3e56bf3ab203ea0890f20e58a0949ef33cfa1588252a50fa58c394f28150b8d1ea9c3e4a0bb06b08c01995dbc8980d59fc4a34aaeb46f46df8e3ac8c559932cf0b467b6fa4cbc5ef09b663ffb995029aaae2fcd8b237cfa8e708c07e7b3bcb3a5c05490e133b3525b660adc9ad1d5ac233bb50f403eceffd742629991a21baa34a91188ed095c24a28324a4b62efa977fe788be917533cc89f66fd918c59f463e0d073794936cb6b32dca6f9883574badc327b05ce8dc75630e404f732b8e2ce9e11c324ec2813f069fdb60b31bd0ba3cd74d4fdc12598a364f1e6e7a3c7574eeacc428bbcb1773b1d7097e06b0809d50dffe4347585e8f48baa4d13da0def795fb22b56ba27564947b8b8c27222f6494e4fb28835690fcf4d6477b3f6d2b8be7ff7375f07dc9d9353479db7ba7242cc514bd8a6335c835630bb5cd5b7f863d208923df8de6d3a1ab6b5d75181a473c2a548396153056710c60309d64b3284d983f37cbd08b131010a41b1baab969f7759e8c0f07de92fe9d5df4cb5c8f769f8f67cad153197f07146670ac8bcfe17b33df1cc7eb649e99cda3ee600d48f758b2bed2980e150e1ace5423b33e17c9ab849519fbf0abb7626880392d10438a362c61d5a9c226624f447af16bb7d7d8916b0876ac984ecc064d80c7d26e3e6e5fe75989a8a889a8e9b125eddc0481844e753ee5a727e392ec279eeeb3c589b19ece08fbf74357b557acbeda7d38aeec0bd335fed31ec502e467aee668b3acc8a92784186c53bcb5a924d45af813ed6f3e66e26e657e0eadcb60444ac0d97da9ff3141f9f95d66bc87f772ee6fc6b44d3ba8367429a037ed08eeee7a5a0e34088ce648affe7c03b6665eddc61dfacc3816e85980286a8ced27482a7297c50c513728326433054dc9a2a202b1cf8c5a8d6b3a6f670ffa052b93674529d47e5081b36789084157c8134ecc7cc9b27b59f61db9fdbdc794653289ea0cbb8c6cfcf1ef20326f1eaa1b6e52ed676e5daeff235344ed9f15735f8538f2a77e683caf3b1d795561d7533f4a74dfead682523863fc977ce2c00e080033423814ccec600ac3ec5c559ece019f6aa834a3ba59159354bc9821a9ac087b28b9de04cc17923842a1f7293da85ffd7f219f065016d29f83454841ca9d2bd2f04eaf29783d9de99e2d2bf31185958cc7b2ce59ba72cd2edf60709d2570affefd8a7e269ae8bf11299a8b0f39b4d3e3c422384601d8d9ac9d921cd9d67cbb5c3559ab0821ceb5799765dd1d1f449234819ac557ea77c3389888c1e207cb6b2e32bb52c610a44ce49508c3592150a6b139b5d31bd4338d47254b9a22fcf7d6e8904127493cd617297cad7b4a6e39c796be23869e073e64fb727379e7b4e34c951306117d66e346aabca3995a8432dade2bf9bbfae0d480e88462db6ff220639a373d949349fc6b6b45025f6afad39b58eeeb89c09782e8da06b93ca3b4b2308f652441596e40abc1a92b08ce012f26f55fd25c8fe052a6718c20697d8fb744a45ce782c3804039df40012e797831ae80f8803db045d2360c2745f11e8ab77fe598080347df577e55c3fce5b7d52918b0dd3392a8fd6d854a5e6dae0fcc399660ef37fb9a58b7150831f06c0413335ff58295ed970ee7d6c42b08e46dcd4082f24ef7859a77deec9568e5d6c919cd4a2d670a9736c1d2fb9741933fce43b6e1ee7cea0387a3a4c228c964f9cf02c5975246ddaedf99edf1bea086c2623f78366ea27e1a3b07997396e077edf1aa94c608560d8fa3277443da115c64027631fd58d050130621fcedb47e39e93bd47ec4f5c9503ad324b402d91ee86258b8937dcb6bce91f290fdb05bc2c2303519251c06d48edb77bb8b1ef35f14416aed25f2a5f505d5ff34dc6256de2631072a819833bf73f2fab0b1b955e5d734c97893d172612bf878e90ff2f5647d6b118d21362c0435d71c7f5ff21c1e11dec6ecdd9f33aa9f32ca1cbb7ea5fbc51ccc2ba5d189b46756346a9d818c87f224586f515694e2eb62f42825c727d544f8ac3f75bcdab88d941916adbc826207a2d7101ccd551df462e274942f07652fccddd754d861273b71ed68448198eb63d61ceb78b6da22f3dce70f778318f1a68110fbcbb99d865341849d465c7f408ed7247a2d24278e03ac8de7a714b17c9acbf63448724110058b0423ea71a2542b87224d9159d98587510bb4adf0969438153a8d8bab946feeabc7a26baffa3b5086cb9b195e256517a9e4885266be771e0887b24955d4cda82f55bd11d4d86595920631d4825c2ba89c59a19f5e6fdc0ec0b412aeba47b93d7608babc51a9306439e76c90e5873c940692b36284bd26642921b22cedecefd873fbbe5b805f3aa814a52bfc6a1b86b6e456a2016ba58a98fb98fa26613ea73bf86f602d0ba3641d4a9a77b803e1ac77c9f24ac6eee386e3394728800b495b063e8478247ef23bd5546c80acef91da4a1edbc0526007b12212c8838851e05ec9560c895a5b70e9c7240b9b4c00924c7186f8000927389e948d05bbecc096ae577bcb498c6f1acdc1344bef81036b8eeb16884a08ee193e5d443ecbebb1f6072d8c0137b56256b3c4fd5d77422f1cceb6427ee7b10881eca48dfcc4f41a8b2024d08cbab70b1c5e20a331f467d7418b99d763fd9532683424496dede7d4c955894076d35702e1901b343419ec27adbdcc30f3bdf16fa0f56372e016311ae8f60f571aaab2966ac5ef03929df28f1d5eff5981f7e62a98a9e8ebad7d19aa7d9d764b3816974a8d4213ee242bda1ff3ce2e3bc8f062c9180d050d9dede7549e55e29590cebe9b6c7792ea517baf54655a9254188faed45cd0f06862afb788a785bac9d77b524ff8c6a1fd42d0ab95a0224b1fa189527365f478029ba9b513442646022e454ccc823c8d28c9cc1f1b1c527a7613da78e475f8081b187920944c81e9064bccef699b235e895d1c33f94de8c79280ab0a97178b537fc243a1b8e7fa7ac83ce45edb3df20bd2c2935508c806b58a7d3d9283f1ebfb7e65c4cf54056b869c01a0dfa6e6f81a01292c60d516b33b7afaee1be049cb7390b01985b7919a09ef832f7e60f7c4e95d4ff351bf94e02d7c6deac8a930dc9ad34227711083b3f8f97303a4e065987b15ff23c75d1afb8f7dc82cf2252523e2dc6a8767b84630f022b7a591c200a1950911608eea6125fc3dc176b916ddbbb839906bef52199a9e50f48d2b6a0ddede86301215f9b01040bace31a00b33f4122f62120af90eb2b0e418e851f472a2852b60ec73d378c028f37169775ad2b8105adcdd0f7da01414f8169637df8ca93d2151a4f7c92a5f32a4516fac291f8fafa360b7a4fb2d19eb55cefd3a5b3dbabe05727d379ebc68a790fdc94f8e83acb4db9f091c3efa52f5d9ab45a1512ab188712c459f081347d2291abe8009e79f51666957345cb8a070861227fa6572ce78c8c175bc3534a9c6611e0d02927af1b76822c3be37387deb1f9e53cee0373a089b7b5383c6337da148d3c51919bb15a56a9e74d5a799053536dee3f91e95fc879594c52b6e947883974e5ed8b68c43bbb77f368c49516f81321ca330593152f9ce73cac8f457a3476b5b003ce5f4a152dcfd7755d5f01785e19373b2bbf691bfaa8a1d5da65d7b4a039b27af6ddd92e0624c935fa6bcc5dc37439d1c8021a5c1f6e0443cb1787383176b5690b1437adeada3c5cd22200c326cd4a974defe487b34c5d7d6ff2c1f598352c044cd2d0d1a32af87361aa723f04b60f8f71ec9f8face6457c012779cb36a636bb5ecdf6d3c348c3e84d2a4f37208713e9d9598888b7fa4b50da58818266e0e018a33b039a50a7f787a536e96493955e2f4ffdc730efe483d0dc5853f54f940ab912908f2f66fa1f607a1f664a0b32f61223cf7763b5a542e7fba980714cdfc64a7fd4fb333f097533a51e2140c161dab4409f0094b418c5e933230e0bd86dd4819ccf95de1696b70a9868c20771bbd868b4932773cbb062510d6099b97156c283739186759ac1a0bf1c88b2137c9cf5a26e5ca6c6cd54c7adb9d9f583f6ee5c5d3affc2be3d798e47d835b70ae98bc2b9258249d3ce9928f354d9a9148eb6cbc6f57d16d71fc548a7a337a0eb352bfe42b2cd1523c3fbffa9a69c306937dd1996131dcbbebb8cf40f2600e780217415ce0dc18d24176b4997c3d60f930454aa50a0ab8b2681ef7e9f204ced9590ae559809416bf4072d1678d8c8a4f4b8aa87878f48676f119e037e8b98d6a8c9f2e6f814349b4465d95f96ec6bebbe6d9a5095143f29830da5d41750bf4e0f2ad1eb2ac3ba3a9ddeed4c26531806535f3344d3119c44afb7faf46f727d54a94f57e2adeee2a6c0fda343635c3901ef0720062464e5954df382ade4a4d103a3c022daddfacbd67bc8806fddc2d0dbd6990a898d0dc1f00ffa46e4dde4108dd5d8d006a925e3bfed62e21bbcb0bac84126b3eb1290889d3281e68faa3d73b788a4f0808e8dd8efb33170ee55a2bd6f8c14201e64db875a2e5963cf1256cebd4eb87cf2b0de944cb7b25cf1b4cef49c4ee5555d789fb9948021516b057168571ca69228e59a1394a1df5f4b78d0d86293fc0ee61265d41d87c07c2f4bfb716d1a87f414d66775efa8149bcf839d585709f9f9b7caf24ae6ad4307178707300ca35d52ac10a7fa0fe6ed8d655de254ba8c9a9a48c9c26429bdd9b2349aee7be55149ec1150a829d874b9e4c0d96f9e5ae1a26eebee067d7b73bcadd85ff47afb60647c56cc93876a82518f6e7a09c94558430ce46b00510191fae629e9f03ca7be981615245d7939ad9250e2f9dddcd3f9147280b61c1d1bbf75a250bc0b2ea9c5bf82db99a81a8c00fc4d05d1cf3758d35bc6c3ce49a887e1a237093dfc284ef10d5b265e7a7427a2a9127949e713ed2a2a7329099537e22bc983b310a6a881b0beb46fa9e27dd4864fa8e76cd9275684a60f5038e8e9934ef734630ae7bb3536da2f350d3e626fe49aff77cbf44b3343152c3bc114c9866f4e67283bbb509816738444df6c80840ae2e6c0b4c703cc7076abdadffed463f161c1fc751e3760c9895122180248a37d417834ab6a870894956c584e5ee9ba66074a54ecbf6a2430b4aa9a989f1d9407400c475b4d2c02993de5432c1c500e419f8e70893c9b66a7ebc609efe52bd15224234cdae3246e43d3ce7f885b869c17115361ff1b7a4b9685c381356a745ae33f3a0b86d50587d762865c8c5da7739263e1ad97d9e7af71f23df9e209d8ac545817bf5ef40dd22d915faa1aaaa98aa6e51d417dc81f9bdf711bfd4d63c904503ef12babc8502ebf60df5b83a86a3ae904e06b73656832894e2c373d766ba7b0fbf82f981abf35c345be081fceb57d2ee8afd9addc962fb123188d876c79196b8e92fe6c8b62e3b33b6b36381cb706ec968e8556dd0134aa1f456b23986153771781435d010b56a390ad900aaf7e8dca5dbbaea4b631b850831b01ba5a4ebb530ca53ef29d2cff9217284bb9486d2ec8b6d435e10d8a466e4d50f3475c0b57782b0f023421c75150a3b4ba879fa82ce0948ee1130b9517869b27a97d220160e10330628e593fd9f8d52f05ef273562817acff81ae29204b862cda87f04ad55c8c54e50172b8a334a8bb90d565c81afd75bb55225aafab351be1c33d40f0f8ca162ad252014863fb50fcf5b973d6206b3ce1f15e364657bea522929b8d47ac45ab2d438c27c97859e64f3832f592d9fa5993add01e159f3f70412966b7fd55bb84ef31eab1aa29064203170093246c12f02174674e163717b23eb1dec63b453547e4350a97cab4a5bfa90ca369b836e60414c5a51c4f957b3001f38430b07f85e8ac0c34bf4a954318c8c47e30845c4d0d784df53adfd5a182c70354d9127a6f1d3e98244d3dac6a6478647b609fdd04a42270a981f348e10dc6912e7b78e65747043368265e98b0580e2fdccd1b2338a6427935b7f3f3a0690d0ed347484d887b7853d254aac86515fc9490bb18c428ce1fed55feadba1b5276d84246a9b60de2508af3302a4097d38834126444b026616f940d94871de3bf3c3af6531c3b1094ffc6ac1121a800536bcaf4edd2c7441332de711ba0bdbeacebf6c5356199fdd36654579dd39adcf19fe0193e8a6b15da29e716d5f793438c38d527afbd39c1917710dd032adc6f8983153f3a60648d83a4c26e6ca74c3581beb743b2645090dfcc25f7b6107e198db6b4701b944361cf18e708f052e02cc418b0aeb067c370b5fe0cc708b1bf54909eb7da48e12533941127c18f8bfacd27dd7f9b2fa876499b2286f37d4cf436e3ee9c4e55287fae3375961a0034105d439d2209964266af823873dfcc859604201bed4c5b822f536e46099a8637b00cdae98024fff76d4b5047802db3d28a7d800a54da8b7031962fc225f8fc84634429fdfb729d657828fbf71aa54ea2dc3e1bdafd013f662e8ff322861c622a935bf5728bd6390c1a95ef8ad9a74612a0bf34ef56fe81b516beb5ab0dac40e83d2833903bce0bda8c1dbee89066cd442a05aa9bdef1cbf8ac43eadd70874987c9c29c0bcb1c88ed066d89346dbe362eb280bbde2a72e774ea2758c9fc942e2376d790149a0876cf98c79260ad20aad8c9d715cc0e265a80857a3fd3236e56728bb6c414fb1aa762ff8882cdb2feba3b102bfd7b2f54e205bfbbe4da54cfaebdfbf710d4fbc86eb25595f2423aaca53fe671447d67c69dce000694c0699a142ab37df0619af280649df7bafab15870f410c17b8a45456c8df8fca252f8dc7551a637dabd235f1e3c3877e87550b340fa25aee01ef79fb402631d0dbe5532d0036bbb3ab2c267c0f8ea10c7844bbc95387b03ce36e774b1e526bd0238188d7e32d754d14086b387b4ddac389efb7027fe3a5b581ba8ea4c6ebed6ae4173a546279ad07f4b65253fd3379c87fe5611979073478b8afd543e84c9241824045a6cd2430c8d74820dce23ec5dc872cf245ccdaf6fddfeaef7754132b32d7730e831bccb0f26d7e5d7730d6b3b6f7f84937d2fc62b33b31263adc94e68d2baa99d47557dace7b16f9f38e3452e3641bf93216a3eac6e9db30c263e3096ef8d956a7f1dda845e747c4fa9fb760dfec58d83737038b79552d2867a676b8873b9f7103253757d720f66a3bbb37e2fe7c0278789cf987bb8c321814081533bdf6e09b2ad2a2f46e9e9443ce6adf3349546026d158ed9f516aa6669e8ff762c45a7e56c64c9b843318ad9775d08ec2b85ececf8b11343e18d3e8eff9e630c647bfda6c99690a7fbc7ffa56164e7f998ba7656fd190ef8d13e910eff56eaaf8f67834deeb960bf3d6178829c4b451e7c90b0d109409c76fa733f0a2f0daefcc3616788b8fb3395f65adc6fe6e7856e408f6c31d9836252c7f54092fc6941c289775ddb3575c6687f9e82f135465c05b81257ed65a5db9e1c602dc6ff7fab32c2c997d0bbd82dbac3f3a45ab1632eeaeb9f9498a3765e28a4831dbcabd95b3fd615890ca48609e90e48650cadfb2102d87e57cbd6d32d6e557bbb0b59bfc943d5ef9b02cce6851dfc27168305d925efcc70e071e4edd1c67f698444cfb589d5bec0d9e591db5e4bf43a634108fc6ee0234f71fa51b20e5277d27f6513416c1c796661423ae7359c7aba0fd545f56d8aa5998b1a59ea15862fa1c4881a71be744c45e8eb19b02bde7636fb3ec63ee0c952db70371f270adccb18d904028d9012b0e68ef87d8d7c86d884f29ad31e2e40ed7d2430dfbb328cf99bd9afbe7940567470cbdcba2d9855a34f0b2e9fd05dae7c93cc40c20fd2825f40872743d2535fb1da7bb1b306086607fa768d98364b146c124f9020aed8f998bd2da23d2f45e81d06606f34c774aa92d0abb5e1bc28e269ba0df8887e3f33634e546cc01e5094e4f88e740abaade989bfbb1cc8751455f75a97f5aa2e5437593d2d169217aae93fff375bc77d7cff440d3e69d5f270478dd2fcfb4f509be4a355507dc243754964b672ac4079b52f5aff9dab526691fcb0cadab93d6cf782369644aa99bc001d083598f845fa17577db3154dfc640e7321ee0d6b9db4b5797ca84d8facf6c4a3414aed6861fc5aaca9e48753b5104252bbf68512c0f9ab8309176ad69db3e52d72aa41b9812010d967de3843fe056bc622fc91d12fa88f0a11aa5acd757597e3e23d850a7b1e64217b1be6ef08848654daa976c4454b2c38af2aaec153804e62217d67b0f80dd3119f81a5a7e87eb243f98446dd6a0e10a09cd0f2f8ccdd36c8203c6324f2751a569199a81d120b610c1c0141aad29e5dfed36e0875e48dc793bccd2ab744c594d62b4ab6f6919142c26dd41540ec30146c3e85862ce6e4e4d7a5ad5e75fad7513ebf3f591b7046f2e42117101a9ff92092c9628d158091d3cf6d293d6cf922c1c28cc68187163dcbabde66544b28cea6f76722ee8d5b6edec899529e22dbdf494e44a1eb62a9975a0036c57bf92173e795504333d372ddac4ea2c5a7110dcc536c545168eb74ea7ffad60cc82f2143b401e728def3589c9513d0884e5db326d7081509ca7f6e9c7be7fe97686e9e04a4b8ff48ab5255b396bf01b604c5331899f199ee71a05e99ea31460485e4239b41e9632b3c2adb1039cd4f4bf8dec2447b29117f2a18c3074f2af5c15b8e8b3eb8bf161729c6648da2c40c755cccc422699da51be8c8fc166f3b3a852277fabe7d4553ad6af1e7b8b69314c3b74d3ace12fc4c48dffde3a4c87ec286f6f74bdd832f4596873367e69e0df35cdb6ced4acab65b475c5f8c89fad5c58a62f49df598a4290fae787010e0bfc10d68685b4cf54b352e8e83d44847fca7b48a615b61118f96280733a2d74884d4ada9f612bdfe91cd6ef4e3e9e8ede1eee52d851a895e927b8b072b45095128a5638f48befb0fa594a64cd894690bbde3380a558c30063d16b91fdd81b3f854798ad4d2a1b3b59cab35b1052e683f195b3ca8f4c12a7c90f0f82a754ade51a8dfa9f2af75aad4415eedb717f5eb5d071752f1148ead5a36cafa6dca8ecbcc053e0e706e753b9c9846f8a223f4e5fc5076ba1eec5596ce25e4995c392d085dae976da0affe9ef66cbb2b1eacce7d06188c29b4ab311df3d73e2af3576aea737ef7d781dfab5bb8ca4f95f3af344b3f3349ee6fbf1d24f1941e784cf065affc47af24c222e5d012afb52395d39d560e0038807de3ee7aadebbd5f944c41b6c7a41192902f0121c2d50a7d73c659b4c3dbdd6f30c25ffd593fc78c940210b446aeea83743ba14ce6e2d4352032f14e9213f890c4117fb3003d93cbdfd6e87457bf41b79d37e9029536ba377fa1c88b754475ce703983da4edb15a4e6cf4b1e50bc6ac0ed36a45e480af4041c3ed49d1ea8555a82b864bf50c17a3759c7f4242a031735a8896ff805b32e35055e13fb23991a4871b06e0ac89c8191018d477cf94250de026e0e11dca6db84cf93bf2c104c29fa41c47abeb908f93e35855992eee810814a50b263d260670a1dd0596ea5a22877871f68d086ff5608830bf4000687dee3cbcc039d37db04f5ef47556b9547757cc7fde839d63d1a6cdad7852c27f406318eae5505967173d7871740a1d0aa1124e9e145574e20810898efad909f2f6c015726562169f0175bcbbc94d6a12b2db79655b3c032f969ac21361393026f3da0eddb26dfd016cfdd8f3a7e41fd4166eb50baa113994a555f8737cd763a85bf075792c68153d05816831e47ff54eb1451d21a78a8147c124dd88a29da13223f00909b35f6903f4193b486855c378e3de9303b2a001bfe59a658e7db6954fa5ece47e6e8f89229b6ba0625bfd916c7b7c83506d88eb0a2c1da4531803e85d3a489c2c03db47dd0f3dd989ff2708c9771cb7c1e63517afbc559ce7210d0d4d7bf15db543f5ccc0c1e1f4e3e5f3d7eeaa1f92d703b4536aa4a63120855230535b41f862e522a46438a71bc967cd1ecf27d3604e1e1224e4e63248341400369807244be856afa1e361b2c7898b07a7b8ac05d3a20cc7dfbb735865ca3ef43f8fe97dcb596f98b75c35ce38be742c798e01b4698fb4c9993b206a321775f90ab3741fd586cf950745c562050142c2c60986cda6f56d22afe5634d40aad4da66efc0b47bc6a6fa47ddcd9562a4d214e7e97f02db53b62ca07812879f73a870e71629e93d4f47a18d193b8ed1a70e9215ffdd0e6e65e615bcf9c43d7e3099210899e3ba9acfb39027adc3d24adc0b73cb57431b31f55a1d29dda08a6786b0913a2436e08101d4c11f73fab76e973ac69ccf270d27a303968d893b61f15314bf42ee9d1f8ac7ac3b3467770cd266c0965de2ff72ff1d53d2134d72ba03ec670053aed36f90151cd3b9f31a986d4c02a02042f3c7232911bd6650801fc7b5a239e7bf41949d60f6e5912a334c7049844b2b24c8e5caa130ab3186e6eba8581d2a695197d5e61f4f55c50856dff4a64a12c88033817aad003cb161cc46a38b18e7f17846937de0c1192f28707b87331bd6afc4564dd575fe3445cedfe995d3ea84fe1641f54b2793798a4272913aca4678</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>学术科研</category>
      </categories>
      <tags>
        <tag>科研技能</tag>
      </tags>
  </entry>
  <entry>
    <title>王老师科研资料</title>
    <url>/2022/05/06/Teacher-Wang-s-scientific-research-materials/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="6aa9e4ac22c1eaae9b7388833a580db2d2bb67c18e67204068e5cf4f3656d7d9">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>学术科研</category>
      </categories>
      <tags>
        <tag>科研技能</tag>
      </tags>
  </entry>
  <entry>
    <title>编码规范</title>
    <url>/2022/12/13/coding-standard/</url>
    <content><![CDATA[<h3 id="1-Matlab编码规范"><a href="#1-Matlab编码规范" class="headerlink" title="1 Matlab编码规范"></a>1 Matlab编码规范</h3><h4 id="1-1-命名规则"><a href="#1-1-命名规则" class="headerlink" title="1.1 命名规则"></a>1.1 命名规则</h4><h5 id="1-1-1-变量"><a href="#1-1-1-变量" class="headerlink" title="1.1.1 变量"></a>1.1.1 变量</h5><ul>
<li><p><strong>小驼峰命名法</strong></p>
<ul>
<li>第一个单词以小写字母开始，后面单词的首字母大写。例如：<code>firstName</code>、<code>lastName</code>。</li>
</ul>
</li>
<li><p><strong>使用英文</strong></p>
<ul>
<li>变量名最重要的原则就是，一看就知道这个变量是什么意思。 </li>
</ul>
</li>
<li><strong>大范围的变量应带有意义的名称，小范围变量可用短变量名(无意义)</strong> <ul>
<li>例如：<code>stepSize</code>：大范围意义、<code>dt</code>(δt)：小范围。</li>
</ul>
</li>
<li><strong>循环变量</strong><ul>
<li>应该以<code>i</code>、<code>j</code>、<code>k</code>为前缀。例如：<code>iFiles</code>、<code>jPositions</code>。不使用<code>i</code>、<code>j</code>的原因是：这两个在matlab中是虚数。 </li>
</ul>
</li>
<li><strong>代表单个实体数据的变量可以加以后缀No</strong><ul>
<li>例如：<code>tableNo</code>、<code>employeeNo</code>。</li>
</ul>
</li>
<li><strong>避免否定式的布尔变量命名</strong> <ul>
<li>例如：若命名<code>isNotFound</code>，在使用判断的时候，<code>~isNotFound</code>，搞半天才知道啥意思。所以不适用否定式布尔变量命名。</li>
</ul>
</li>
</ul>
<h5 id="1-1-2-常数"><a href="#1-1-2-常数" class="headerlink" title="1.1.2 常数"></a>1.1.2 常数</h5><ul>
<li><strong>全大写字母+下划线</strong> <ul>
<li>例如：<code>MAX_ITERATIONS</code>、<code>COLOR_RED</code>。</li>
</ul>
</li>
<li><strong>参数可以以某些通用类型名作为前缀</strong><ul>
<li>例如：<code>COLOR_RED</code>，<code>COLOR_GREEN</code>，<code>COLOR_BLUE</code>。</li>
</ul>
</li>
</ul>
<h5 id="1-1-3-结构体"><a href="#1-1-3-结构体" class="headerlink" title="1.1.3 结构体"></a>1.1.3 结构体</h5><ul>
<li><strong>大驼峰命名法</strong><ul>
<li>这是区别一般变量，例如：<code>ParameterSet</code>。</li>
</ul>
</li>
<li><strong>结构体的命名应该是隐性的，并且不需要包括字段名</strong><ul>
<li>例如：用<code>Segment.length</code>，避免<code>Segment.segmentLength</code>。</li>
</ul>
</li>
</ul>
<h5 id="1-1-4-函数"><a href="#1-1-4-函数" class="headerlink" title="1.1.4 函数"></a>1.1.4 函数</h5><ul>
<li><strong>函数名应该采用全小写字母+下划线</strong></li>
<li><strong>函数名与它的.m文件名必须相同</strong></li>
<li><strong>函数名应该有具体的意义</strong><ul>
<li>避免短的函数名/缩写，这经常使得其名字含糊不清。</li>
<li>例如：采用：<code>compute_total_width()</code>，避免：<code>compwid()</code></li>
</ul>
</li>
<li><strong>避免无意识地覆盖</strong><ul>
<li>有时候我们取的名字，可能在MATLAB中含有这个函数名了，可以用<code>exist</code>检查是否含有了。</li>
</ul>
</li>
</ul>
<h4 id="1-2-文件与程序结构"><a href="#1-2-文件与程序结构" class="headerlink" title="1.2 文件与程序结构"></a>1.2 文件与程序结构</h4><h5 id="1-2-1-M文件"><a href="#1-2-1-M文件" class="headerlink" title="1.2.1  .M文件"></a>1.2.1  .M文件</h5><ul>
<li><p><strong>模块化</strong></p>
<ul>
<li>编写一个大程序的最好的方法是将它以好的设计分化为小块（通常采用函数的方式）。</li>
<li>这种方式通过减少为了理解代码的作用而必须阅读的代码数量使得程序的可读性、易于理解性和可测试性得到了增强。超过编辑器两屏幕的代码都应该考虑进行分割。并且设计规划很好的函数也使得它在其他的应用中可用性增强了。</li>
</ul>
</li>
<li><p><strong>确保交互过程清晰</strong></p>
<ul>
<li>函数通过输入输出参数以及全局变量与其他代码交互通信。使用参数几乎总是比使用全局变量清楚明了。采用<font color="blue">结构体</font>可以避免那种一长串儿的输入输出参数的形式。</li>
</ul>
</li>
<li><p><strong>分割与封装</strong></p>
<ul>
<li>所有的子函数和所有的函数都应只把一件事情做好。每个函数应该隐藏(hide)一些东西。</li>
<li>任何在多个.m文件中出现的代码块都应该考虑用函数的形式封装起来。</li>
</ul>
</li>
<li><p><strong>利用现有的函数</strong></p>
<ul>
<li>开发一个有正确功能的、可读的、合理灵活性的函数是一项有重大意义的任务。或许寻找一个现成的提供了要求的部分、甚至全部功能的函数应该更快也更具有正确性。</li>
</ul>
</li>
<li><p><strong>子函数</strong></p>
<ul>
<li>只被另外一个函数调用的函数应该作为一个子函数写在同一个文件中。这使得代码更加利于理解与维护。</li>
</ul>
</li>
<li><p><strong>测试脚本</strong></p>
<ul>
<li>为每一个函数写一个测试脚本。这样可以提高初期版本的质量和改进版本的可靠性。</li>
</ul>
</li>
</ul>
<h4 id="1-3-注释、文档与排版"><a href="#1-3-注释、文档与排版" class="headerlink" title="1.3 注释、文档与排版"></a>1.3 注释、文档与排版</h4><h5 id="1-3-1-注释"><a href="#1-3-1-注释" class="headerlink" title="1.3.1 注释"></a>1.3.1 注释</h5><p>注释的目的是为代码增加信息。注释的典型应用是解释用法、提供参考信息、证明结果、阐述需要的改进等。经验表明，在写代码的同时就加上注释比后来再补充注释要好。 </p>
<ul>
<li><strong>注释文字应该简洁易读</strong><ul>
<li>一个糟糕的或者是无用的注释反而会影响读者的正常理解。N.Schryer提到：“如果代码与注释不一致，那么或许两者都是错误的。”</li>
<li>一个通常更重要的是注释应该讲的是“为什么(Why)”和“怎么做(how)“，而不是“是什么(what)”。</li>
</ul>
</li>
<li><strong>函数的注释写法（英文好的话尽量使用英文）</strong> </li>
</ul>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%================================================================</span></span><br><span class="line"><span class="comment">% 功能：       传统LMS滤波算法</span></span><br><span class="line"><span class="comment">% 输入参数：   </span></span><br><span class="line"><span class="comment">%              xn           输入信号</span></span><br><span class="line"><span class="comment">%              mu           步长因子</span></span><br><span class="line"><span class="comment">%              dn           参考信号</span></span><br><span class="line"><span class="comment">%              numIte       迭代次数</span></span><br><span class="line"><span class="comment">%              M            滤波器阶数</span></span><br><span class="line"><span class="comment">% 输出参数：</span></span><br><span class="line"><span class="comment">%              theta        滤波器的系数矩阵</span></span><br><span class="line"><span class="comment">%              en           误差信号</span></span><br><span class="line"><span class="comment">%              yn           滤波器的输出信号</span></span><br><span class="line"><span class="comment">% 备注：		入射角只考虑一个维度的</span></span><br><span class="line"><span class="comment">% 调用方法：	   见XXX文件</span></span><br><span class="line"><span class="comment">% 日期：   	2011/7/12 20:37</span></span><br><span class="line"><span class="comment">%================================================================</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta,en,yn]</span> = <span class="title">my_lms</span><span class="params">(un, dn, mu, Num_iteration, M)</span></span></span><br><span class="line">。。。（具体编程开始）</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%================================================================</span></span><br><span class="line"><span class="comment">% 功能：	求圆孔的夫琅禾费衍射光强分布</span></span><br><span class="line"><span class="comment">% 参数：	CircleHoleFD为圆孔结构体，包含圆孔衍射相关信息；</span></span><br><span class="line"><span class="comment">% 		   theta为衍射场的次波方向，可以为向量，求取各方向的光强</span></span><br><span class="line"><span class="comment">% 返回值：	I为衍射光强分布</span></span><br><span class="line"><span class="comment">% 主要思路：使用夫琅禾费单缝衍射公式计算</span></span><br><span class="line"><span class="comment">% 备注：	入射角只考虑一个维度的</span></span><br><span class="line"><span class="comment">% 调用方法：见CalcCircleHoleFD_Test文件</span></span><br><span class="line"><span class="comment">% 日期：   2011/7/12 20:37</span></span><br><span class="line"><span class="comment">%================================================================</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">I</span> = <span class="title">calcCircleholefd</span><span class="params">(circleHoleFD, theta)</span></span></span><br><span class="line">。。。（具体编程开始）</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>一段代码注释写法</strong><ul>
<li>用于理解一小段代码含义的注释，统一写在代码上方，如：</li>
</ul>
</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%（1）初始化 </span></span><br><span class="line">Number = <span class="built_in">zeros</span>(<span class="number">1</span>,PNumber);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:PNumber </span><br><span class="line">    Number(<span class="built_in">i</span>) = MNumber;</span><br><span class="line"><span class="keyword">end</span>  </span><br></pre></td></tr></table></figure>
<ul>
<li><strong>一句代码注释写法</strong><ul>
<li>用于理解一句代码的意思可以写在代码后方，但是注意不能超过180字符。太长可以考虑写在代码上方，如：</li>
</ul>
</li>
</ul>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">gen = <span class="number">0</span>;  <span class="comment">%迭代计数器 </span></span><br><span class="line">JmNumber = Max_Cell(Jm);               <span class="comment">% 调用Max_Cell子函数求机器的数量 </span></span><br><span class="line">[PNumber,MNumber] = <span class="built_in">size</span>(Jm);          <span class="comment">% PNumber为工件个数，MNumber为工序个数 </span></span><br><span class="line">trace = <span class="built_in">zeros</span>(<span class="number">2</span>, MAXGEN);               <span class="comment">% 寻优结果的初始值，一行存放各代的最优解，一行存放各代解的均值 </span></span><br><span class="line">TotalOP_Number = PNumber*MNumber;      <span class="comment">% 工序总个数 </span></span><br></pre></td></tr></table></figure>
<h5 id="1-3-2-文档"><a href="#1-3-2-文档" class="headerlink" title="1.3.2 文档"></a>1.3.2 文档</h5><ul>
<li><strong>文档规范化</strong><ul>
<li>作为有用的文档应该包含一个对如下内容的可读性的描述：代码打算干什么（要求），它是如何工作的（设计），它依赖于什其他什么函数以及怎么被其他代码调用（接口），以及它是如何测试的等。对于额外的考虑，文档可以包含解决方案的选择性的讨论以及扩展与维护的建议。</li>
</ul>
</li>
<li><p><strong>首先考虑书写文档</strong></p>
<ul>
<li>一些程序员相信的方法是：“代码第一，回答问题是以后的事情。”而通过经验，我们绝大多数人知道先开发设计然后再实现可以导致更加满意的结果。如果将测试与文档留在最后，那么开发项目几乎不能够按期完成的。<font color="red">首先书写文档</font>可以确保其按时完成甚至可能减少开发时间。</li>
</ul>
</li>
<li><p><strong>修改</strong></p>
<ul>
<li>一个专业的对代码修改进行管理和写文档的方法是采用源程序控制工具。对于很简单的工程，在函数文件的<font color="red">注释中加入修改历史</font>比什么都不做要好。</li>
</ul>
</li>
</ul>
<h5 id="1-3-3-排版"><a href="#1-3-3-排版" class="headerlink" title="1.3.3 排版"></a>1.3.3 排版</h5><ul>
<li><strong>空格的使用</strong><ul>
<li>在二元运算符两边各空一格[=, -, +=, ==, &gt;, in, is not, and]；</li>
<li>函数的参数列表中，<code>,</code>之后要有空格；</li>
<li>函数的参数列表中，默认值等号两边不要添加空格；</li>
<li>左括号之后，右括号之前不要加多余的空格；</li>
<li>不要为对齐赋值语句而使用的额外空格</li>
</ul>
</li>
</ul>
<blockquote>
<p>本文转自：何亮科学网博客。 </p>
<p>链接地址：<a href="https://blog.sciencenet.cn/blog-437026-779476.html">一些MATLAB的编程规范总结1.0版</a></p>
<p>其他参考链接：</p>
<p><a href="https://zhuanlan.zhihu.com/p/150926389">科研小技巧——MATLAB的编码规范 - 西涯先生的文章 - 知乎</a></p>
<p><a href="https://blog.csdn.net/wuwuku123/article/details/106666382">matlab 编程之代码规范 -枯荣有常的文章 - CSDN</a></p>
<p><a href="https://blog.csdn.net/magic_yu42/article/details/111467556">MATLAB编程规范 - magic_yu42的文章 - CSDN</a></p>
</blockquote>
<h5 id="1-3-4-绘图、字体设置"><a href="#1-3-4-绘图、字体设置" class="headerlink" title="1.3.4 绘图、字体设置"></a>1.3.4 绘图、字体设置</h5><p>王老师关于绘图、字体设置的一些简单设置：<code>PlotCurvesFontSizeFontName.m</code></p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% MATLAB曲线图示例设置字号字体</span></span><br><span class="line"></span><br><span class="line">clear all; clc; close all;</span><br><span class="line"><span class="comment">%% Lena512.bmp图像</span></span><br><span class="line"><span class="comment">%% X Y分别为编码比特率和峰值信噪比</span></span><br><span class="line">X1=[<span class="number">0.15</span> <span class="number">0.20</span> <span class="number">0.25</span> <span class="number">0.30</span> <span class="number">0.40</span> <span class="number">0.50</span> <span class="number">0.60</span> <span class="number">0.75</span> <span class="number">1.00</span> <span class="number">1.25</span>];</span><br><span class="line">X2=X1; X3=X1; X4=X1;</span><br><span class="line">Y1=[<span class="number">22.0370</span> <span class="number">23.4718</span> <span class="number">24.3595</span> <span class="number">25.1125</span> <span class="number">26.5516</span> <span class="number">27.8992</span> <span class="number">29.1981</span> <span class="number">30.8826</span> <span class="number">33.2239</span> <span class="number">35.0960</span>];</span><br><span class="line">Y2=[<span class="number">22.5249</span> <span class="number">23.5379</span> <span class="number">24.2606</span> <span class="number">24.9596</span> <span class="number">26.3507</span> <span class="number">27.5956</span> <span class="number">28.7035</span> <span class="number">30.0974</span> <span class="number">32.1077</span> <span class="number">33.7383</span>];</span><br><span class="line">Y3=[<span class="number">22.5594</span> <span class="number">23.6605</span> <span class="number">24.3931</span> <span class="number">25.1403</span> <span class="number">26.5922</span> <span class="number">27.8500</span> <span class="number">28.9936</span> <span class="number">30.4864</span> <span class="number">32.5300</span> <span class="number">34.1851</span>];</span><br><span class="line">Y4=[<span class="number">22.6049</span> <span class="number">23.7881</span> <span class="number">24.6194</span> <span class="number">25.4626</span> <span class="number">27.0361</span> <span class="number">28.3933</span> <span class="number">29.6686</span> <span class="number">31.3865</span> <span class="number">33.5905</span> <span class="number">35.3823</span>];</span><br><span class="line"><span class="built_in">plot</span>(X1, Y1, <span class="string">&#x27;- r .&#x27;</span>, X2, Y2, <span class="string">&#x27;-. b o&#x27;</span>, X3, Y3, <span class="string">&#x27;: m *&#x27;</span>, X4, Y4, <span class="string">&#x27;-- k p&#x27;</span>, <span class="string">&#x27;LineWidth&#x27;</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">legend</span>(<span class="string">&#x27;DCT-JPEG&#x27;</span>, <span class="string">&#x27;APWBT-JPEG&#x27;</span>, <span class="string">&#x27;APDCBT-JPEG&#x27;</span>, <span class="string">&#x27;APIDCBT-JPEG&#x27;</span>)</span><br><span class="line">axis([<span class="number">0</span>, <span class="number">1.4</span>, <span class="number">22</span>, <span class="number">36</span>]);</span><br><span class="line"><span class="comment">%% MATLAB的默认字体是Helvetica，fontname的设置，中文字体：宋体、楷体、仿宋、隶书、微软雅黑、幼圆等</span></span><br><span class="line"><span class="comment">%% 英文字体: Times New Roman, Arial, Bodoni, Calibri, Courier New, Frutiger, Futura, Garamond, Geogia, Platino Linotype, Verdana等</span></span><br><span class="line">xlabel(<span class="string">&#x27;\fontsize&#123;12&#125;\fontname&#123;宋体&#125;码率\fontname&#123;Times New Roman&#125;(bpp)&#x27;</span>)</span><br><span class="line">ylabel(<span class="string">&#x27;\fontsize&#123;12&#125;\fontname&#123;Times New Roman&#125;PSNR (dB)&#x27;</span>)</span><br><span class="line">set(gca, <span class="string">&#x27;xtick&#x27;</span>, [<span class="number">0</span>:<span class="number">0.2</span>:<span class="number">1.4</span>], <span class="string">&#x27;ytick&#x27;</span>, [<span class="number">22</span>:<span class="number">2</span>:<span class="number">36</span>], <span class="string">&#x27;FontSize&#x27;</span>, <span class="number">12</span>, <span class="string">&#x27;Fontname&#x27;</span>, <span class="string">&#x27;Times New Roman&#x27;</span>);</span><br></pre></td></tr></table></figure>
<h3 id="2-Python编码规范"><a href="#2-Python编码规范" class="headerlink" title="2 Python编码规范"></a>2 Python编码规范</h3><h4 id="2-1-编码"><a href="#2-1-编码" class="headerlink" title="2.1 编码"></a>2.1 编码</h4><p>无特殊情况，建议Python脚本程序一律使用 UTF-8 编码，并且在文件头部必须加入<code>#-*-coding:utf-8-*-</code>标识，声明文件编码方式，程序文件编码要和声明编码保持一致。 </p>
<h4 id="2-2-命名规范"><a href="#2-2-命名规范" class="headerlink" title="2.2 命名规范"></a>2.2 命名规范</h4><p><strong>模块</strong>： 模块尽量使用小写命名，首字母保持小写，尽量不要用下划线(除非多个单词，且数量不多的情况)</p>
<p><strong>类名</strong>： 类名使用驼峰(CamelCase)命名风格，首字母大写，私有类可用一个下划线开头；将相关的类和顶级函数放在同一个模块里。不像Java，没必要限制一个类一个模块。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Farm</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AnimalFarm</span>(<span class="params">Farm</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_PrivateFarm</span>(<span class="params">Farm</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>函数名：</strong></p>
<p>函数名一律小写，如有多个单词，用<strong>下划线</strong>隔开；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_with_env</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p><strong>变量名</strong></p>
<ul>
<li>变量名尽量小写，如有多个单词，用下划线隔开；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    school_name = <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>常量采用全大写，如有多个单词，使用下划线隔开；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MAX_CLIENT = <span class="number">100</span></span><br><span class="line">MAX_CONNECTION = <span class="number">1000</span></span><br><span class="line">CONNECTION_TIMEOUT = <span class="number">600</span></span><br></pre></td></tr></table></figure>
<p>【参考资料1】：<a href="https://blog.csdn.net/meiqi0538/article/details/104993760">【Python】编程代码书写规范！- 科皮子菊的文章  - CSDN</a></p>
<p>【参考资料2】：<a href="https://python.freelycode.com/contribution/detail/47">PEP8中文版 — Python编码风格指南</a></p>
<h4 id="2-3-注释"><a href="#2-3-注释" class="headerlink" title="2.3 注释"></a>2.3 注释</h4><ul>
<li><strong>行注释：</strong> 以 <code>#</code> 开头，<code>#</code> 右边的所有内容都被当做说明文字，只起到辅助说明作用。 </li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello python&quot;</span>)  <span class="comment"># 输出 `hello python`</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：为了保证代码的可读性，<code>#</code> 后面建议先添加一个空格，然后再编写相应的说明文字。</p>
</blockquote>
<ul>
<li><strong>多行注释(块注释)</strong> ：<code>#</code>号后空一格，段落件用空行分开（同样需要“#”号）。<ul>
<li>如果希望编写的 <strong>注释信息很多，一行无法显示</strong>，就可以使用多行注释。</li>
<li>要在 Python 程序中使用多行注释，可以用 <strong>一对连续的 三个引号</strong>(单引号和双引号都可以)。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">这是一个多行注释；</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="string">在多行注释之间，可以写很多很多的内容</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello python&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>补充：</strong> 什么时候需要使用注释？</p>
<p>1、 <strong>注释不是越多越好</strong>，对于一目了然的代码，不需要添加注释；<br>2、 对于<strong>复杂的操作</strong>，应该在操作开始前写上若干行注释；<br>3、 对于<strong>不是一目了然的代码</strong>，应在其行尾添加注释（为了提高可读性，注释应该至少离开代码 2 个空格）。</p>
<h4 id="2-4-空格使用"><a href="#2-4-空格使用" class="headerlink" title="2.4 空格使用"></a>2.4 空格使用</h4><ul>
<li><p>在二元运算符两边各空一格[=, -, +=, ==, &gt;, in, is not, and]</p>
</li>
<li><p>函数的参数列表中，<code>,</code>之后要有空格</p>
</li>
<li>函数的参数列表中，默认值等号两边不要添加空格</li>
<li>左括号之后，右括号之前不要加多余的空格</li>
<li>字典对象的左括号之前不要多余的空格</li>
<li>不要为对齐赋值语句而使用的额外空格</li>
</ul>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>编程与深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Pytorch基础</tag>
      </tags>
  </entry>
  <entry>
    <title>Learn Git</title>
    <url>/2022/01/18/learnGit/</url>
    <content><![CDATA[<h3 id="一、Git安装"><a href="#一、Git安装" class="headerlink" title="一、Git安装"></a>一、Git安装</h3><h4 id="1-在Windows上安装Git"><a href="#1-在Windows上安装Git" class="headerlink" title="1. 在Windows上安装Git"></a>1. 在Windows上安装Git</h4><p>在Windows上使用Git，可以从Git官网直接<a href="https://git-scm.com/downloads">下载安装程序</a>，然后按默认选项安装即可。</p>
<p>安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，蹦出一个类似命令行窗口的东西，就说明Git安装成功！</p>
<h4 id="2-设置名字-邮箱"><a href="#2-设置名字-邮箱" class="headerlink" title="2. 设置名字+邮箱"></a>2. 设置名字+邮箱</h4><p>安装完成后，还需要最后一步设置，在命令行输入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git config --global user.name &quot;Your Name&quot;</span><br><span class="line">$ git config --global user.email &quot;email@example.com&quot;</span><br></pre></td></tr></table></figure>
<p>因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。你也许会担心，如果有人故意冒充别人怎么办？这个不必担心，首先我们相信大家都是善良无知的群众，其次，真的有冒充的也是有办法可查的。</p>
<p>注意<code>git config</code>命令的<code>--global</code>参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。</p>
<h3 id="二、创建版本库"><a href="#二、创建版本库" class="headerlink" title="二、创建版本库"></a>二、创建版本库</h3><h4 id="1-定义与创建"><a href="#1-定义与创建" class="headerlink" title="1. 定义与创建"></a>1. 定义与创建</h4><p>什么是版本库呢？版本库又名仓库，英文名<strong>repository</strong>，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。</p>
<p>所以，创建一个版本库非常简单，首先，选择一个合适的地方，创建一个空目录：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ mkdir learngit</span><br><span class="line">$ cd learngit</span><br><span class="line">$ pwd</span><br><span class="line">/Users/michael/learngit</span><br></pre></td></tr></table></figure>
<p><code>pwd</code>命令用于显示当前目录。在我的Mac上，这个仓库位于<code>/Users/michael/learngit</code>。</p>
<p>使用Windows系统，为了避免遇到各种莫名其妙的问题，请确保目录名(包括父目录)不包含中文。 </p>
<p>第二步，通过<code>git init</code>命令把这个目录变成Git可以管理的仓库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git init</span><br><span class="line">Initialized empty Git repository in /Users/michael/learngit/.git/</span><br></pre></td></tr></table></figure>
<p>瞬间Git就把仓库建好了，而且告诉你是一个空的仓库(empty Git repository)，细心的读者可以发现当前目录下多了一个<code>.git</code>的目录，这个目录是Git来跟踪管理版本库的，没事千万不要手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。</p>
<p>如果你没有看到<code>.git</code>目录，那是因为这个目录默认是隐藏的，用<code>ls -ah</code>命令就可以看见。</p>
<h4 id="2-把文件添加到版本库"><a href="#2-把文件添加到版本库" class="headerlink" title="2. 把文件添加到版本库"></a>2. 把文件添加到版本库</h4><p>首先这里再明确一下，所有的版本控制系统，其实只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等，Git也不例外。版本控制系统可以告诉你每次的改动，比如在第5行加了一个单词“Linux”，在第8行删了一个单词“Windows”。而图片、视频这些二进制文件，虽然也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道，也没法知道。</p>
<p>不幸的是，Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的，前面我们举的例子只是为了演示，如果要真正使用版本控制系统，就要以纯文本方式编写文件。</p>
<p>因为文本是有编码的，比如中文有常用的GBK编码，日文有Shift_JIS编码，如果没有历史遗留问题，强烈建议使用标准的UTF-8编码，所有语言使用同一种编码，既没有冲突，又被所有平台所支持。</p>
<font color="red">使用Windows的童鞋要特别注意：</font>

<p>千万不要使用Windows自带的<strong>记事本</strong>编辑任何文本文件。原因是Microsoft开发记事本的团队使用了一个非常弱智的行为来保存UTF-8编码的文件，他们自作聪明地在每个文件开头添加了0xefbbbf（十六进制）的字符，你会遇到很多不可思议的问题，比如，网页第一行可能会显示一个“?”，明明正确的程序一编译就报语法错误，等等，都是由记事本的弱智行为带来的。建议你下载<a href="https://code.visualstudio.com/">Visual Studio Code</a>代替记事本，不但功能强大，而且免费！</p>
<p>言归正传，现在我们编写一个<code>readme.txt</code>文件，内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Git is a version control system.</span><br><span class="line">Git is free software.</span><br></pre></td></tr></table></figure>
<p>一定要放到<code>learngit</code>目录下（子目录也行），因为这是一个Git仓库，放到其他地方Git再厉害也找不到这个文件。</p>
<p>和把大象放到冰箱需要3步相比，把一个文件放到Git仓库只需要两步。</p>
<p>第一步，用命令<code>git add</code>告诉Git，把文件添加到仓库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt</span><br></pre></td></tr></table></figure>
<p>执行上面的命令，没有任何显示，这就对了，Unix的哲学是“没有消息就是好消息”，说明添加成功。</p>
<p>第二步，用命令<code>git commit</code>告诉Git，把文件提交到仓库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git commit -m &quot;wrote a readme file&quot;</span><br><span class="line">[master (root-commit) eaadf4e] wrote a readme file</span><br><span class="line"> 1 file changed, 2 insertions(+)</span><br><span class="line"> create mode 100644 readme.txt</span><br></pre></td></tr></table></figure>
<p>简单解释一下<code>git commit</code>命令，<code>-m</code>后面输入的是本次提交的说明，可以输入任意内容，当然最好是有意义的，这样你就能从历史记录里方便地找到改动记录。</p>
<p>嫌麻烦不想输入<code>-m &quot;xxx&quot;</code>行不行？确实有办法可以这么干，但是强烈不建议你这么干，因为输入说明对自己对别人阅读都很重要。实在不想输入说明的童鞋请自行Google，我不告诉你这个参数。</p>
<p><code>git commit</code>命令执行成功后会告诉你，<code>1 file changed</code>：1个文件被改动（我们新添加的readme.txt文件）；<code>2 insertions</code>：插入了两行内容（readme.txt有两行内容）。</p>
<p>为什么Git添加文件需要<code>add</code>，<code>commit</code>一共两步呢？因为<code>commit</code>可以一次提交很多文件，所以你可以多次<code>add</code>不同的文件，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add file1.txt</span><br><span class="line">$ git add file2.txt file3.txt</span><br><span class="line">$ git commit -m &quot;add 3 files.&quot;</span><br></pre></td></tr></table></figure>
<h4 id="3-疑难解答"><a href="#3-疑难解答" class="headerlink" title="3. 疑难解答"></a>3. 疑难解答</h4><p>Q：输入<code>git add readme.txt</code>，得到错误：<code>fatal: not a git repository (or any of the parent directories)</code>。</p>
<p>A：Git命令必须在Git仓库目录内执行（<code>git init</code>除外），在仓库目录外执行是没有意义的。</p>
<p>Q：输入<code>git add readme.txt</code>，得到错误<code>fatal: pathspec &#39;readme.txt&#39; did not match any files</code>。</p>
<p>A：添加某个文件时，该文件必须在当前目录下存在，用<code>ls</code>或者<code>dir</code>命令查看当前目录的文件，看看文件是否存在，或者是否写错了文件名。</p>
<h4 id="4-小结"><a href="#4-小结" class="headerlink" title="4. 小结"></a>4. 小结</h4><p>现在总结一下今天学的两点内容：</p>
<p>初始化一个Git仓库，使用<code>git init</code>命令。</p>
<p>添加文件到Git仓库，分两步：</p>
<ol>
<li>使用命令<code>git add &lt;file&gt;</code>，注意，可反复多次使用，添加多个文件；</li>
<li>使用命令<code>git commit -m &lt;message&gt;</code>，完成。</li>
</ol>
<h3 id="三、版本控制"><a href="#三、版本控制" class="headerlink" title="三、版本控制"></a>三、版本控制</h3><p>我们已经成功地添加并提交了一个readme.txt文件，现在，是时候继续工作了，于是，我们继续修改readme.txt文件，改成如下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software.</span><br></pre></td></tr></table></figure>
<p>现在，运行<code>git status</code>命令看看结果：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	modified:   readme.txt</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br></pre></td></tr></table></figure>
<p><code>git status</code>命令可以让我们时刻掌握仓库当前的状态，上面的命令输出告诉我们，<code>readme.txt</code>被修改过了，但还没有准备提交的修改。</p>
<p>虽然Git告诉我们<code>readme.txt</code>被修改了，但如果能看看具体修改了什么内容，自然是很好的。比如你休假两周从国外回来，第一天上班时，已经记不清上次怎么修改的<code>readme.txt</code>，所以，需要用<code>git diff</code>这个命令看看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git diff readme.txt </span><br><span class="line">diff --git a/readme.txt b/readme.txt</span><br><span class="line">index 46d49bf..9247db6 100644</span><br><span class="line">--- a/readme.txt</span><br><span class="line">+++ b/readme.txt</span><br><span class="line">@@ -1,2 +1,2 @@</span><br><span class="line">-Git is a version control system.</span><br><span class="line">+Git is a distributed version control system.</span><br><span class="line"> Git is free software.</span><br></pre></td></tr></table></figure>
<p><code>git diff</code>顾名思义就是查看difference，显示的格式正是Unix通用的diff格式，可以从上面的命令输出看到，我们在第一行添加了一个<code>distributed</code>单词。</p>
<p>知道了对<code>readme.txt</code>作了什么修改后，再把它提交到仓库就放心多了，提交修改和提交新文件是一样的两步，第一步是<code>git add</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt</span><br></pre></td></tr></table></figure>
<p>同样没有任何输出。在执行第二步<code>git commit</code>之前，我们再运行<code>git status</code>看看当前仓库的状态：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">	modified:   readme.txt</span><br></pre></td></tr></table></figure>
<p><code>git status</code>告诉我们，将要被提交的修改包括<code>readme.txt</code>，下一步，就可以放心地提交了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git commit -m &quot;add distributed&quot;</span><br><span class="line">[master e475afc] add distributed</span><br><span class="line"> 1 file changed, 1 insertion(+), 1 deletion(-)</span><br></pre></td></tr></table></figure>
<p>提交后，我们再用<code>git status</code>命令看看仓库的当前状态：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">nothing to commit, working tree clean</span><br></pre></td></tr></table></figure>
<p>Git告诉我们当前没有需要提交的修改，而且，工作目录是干净（working tree clean）的。</p>
<p><strong>小结</strong></p>
<ul>
<li>要随时掌握工作区的状态，使用<code>git status</code>命令。</li>
<li>如果<code>git status</code>告诉你有文件被修改过，用<code>git diff</code>可以查看修改内容。</li>
</ul>
<h4 id="1-版本回退"><a href="#1-版本回退" class="headerlink" title="1. 版本回退"></a>1. 版本回退</h4><p>现在，你已经学会了修改文件，然后把修改提交到Git版本库，现在，再练习一次，修改readme.txt文件如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br></pre></td></tr></table></figure>
<p>然后尝试提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt</span><br><span class="line">$ git commit -m &quot;append GPL&quot;</span><br><span class="line">[master 1094adb] append GPL</span><br><span class="line"> 1 file changed, 1 insertion(+), 1 deletion(-)</span><br></pre></td></tr></table></figure>
<p>像这样，你不断对文件进行修改，然后不断提交修改到版本库里，就好比玩RPG游戏时，每通过一关就会自动把游戏状态存盘，如果某一关没过去，你还可以选择读取前一关的状态。有些时候，在打Boss之前，你会手动存盘，以便万一打Boss失败了，可以从最近的地方重新开始。Git也是一样，每当你觉得文件修改到一定程度的时候，就可以“保存一个快照”，这个快照在Git中被称为<code>commit</code>。一旦你把文件改乱了，或者误删了文件，还可以从最近的一个<code>commit</code>恢复，然后继续工作，而不是把几个月的工作成果全部丢失。</p>
<p>现在，我们回顾一下<code>readme.txt</code>文件一共有几个版本被提交到Git仓库里了：</p>
<p>版本1：wrote a readme file</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Git is a version control system.</span><br><span class="line">Git is free software.</span><br></pre></td></tr></table></figure>
<p>版本2：add distributed</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software.</span><br></pre></td></tr></table></figure>
<p>版本3：append GPL</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br></pre></td></tr></table></figure>
<p>当然了，在实际工作中，我们脑子里怎么可能记得一个几千行的文件每次都改了什么内容，不然要版本控制系统干什么。版本控制系统肯定有某个命令可以告诉我们历史记录，在Git中，我们用<code>git log</code>命令查看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log</span><br><span class="line">commit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -&gt; master)</span><br><span class="line">Author: Michael Liao &lt;askxuefeng@gmail.com&gt;</span><br><span class="line">Date:   Fri May 18 21:06:15 2018 +0800</span><br><span class="line"></span><br><span class="line">    append GPL</span><br><span class="line"></span><br><span class="line">commit e475afc93c209a690c39c13a46716e8fa000c366</span><br><span class="line">Author: Michael Liao &lt;askxuefeng@gmail.com&gt;</span><br><span class="line">Date:   Fri May 18 21:03:36 2018 +0800</span><br><span class="line"></span><br><span class="line">    add distributed</span><br><span class="line"></span><br><span class="line">commit eaadf4e385e865d25c48e7ca9c8395c3f7dfaef0</span><br><span class="line">Author: Michael Liao &lt;askxuefeng@gmail.com&gt;</span><br><span class="line">Date:   Fri May 18 20:59:18 2018 +0800</span><br><span class="line"></span><br><span class="line">    wrote a readme file</span><br></pre></td></tr></table></figure>
<p><code>git log</code>命令显示从最近到最远的提交日志，我们可以看到3次提交，最近的一次是<code>append GPL</code>，上一次是<code>add distributed</code>，最早的一次是<code>wrote a readme file</code>。</p>
<p>如果嫌输出信息太多，看得眼花缭乱的，可以试试加上<code>--pretty=oneline</code>参数：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --pretty=oneline</span><br><span class="line">1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -&gt; master) append GPL</span><br><span class="line">e475afc93c209a690c39c13a46716e8fa000c366 add distributed</span><br><span class="line">eaadf4e385e865d25c48e7ca9c8395c3f7dfaef0 wrote a readme file</span><br></pre></td></tr></table></figure>
<p>需要友情提示的是，你看到的一大串类似<code>1094adb...</code>的是<code>commit id</code>（版本号），和SVN不一样，Git的<code>commit id</code>不是1，2，3……递增的数字，而是一个SHA1计算出来的一个非常大的数字，用十六进制表示，而且你看到的<code>commit id</code>和我的肯定不一样，以你自己的为准。为什么<code>commit id</code>需要用这么一大串数字表示呢？因为Git是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。</p>
<p>每提交一个新版本，实际上Git就会把它们自动串成一条时间线。 </p>
<p>好了，现在我们启动时光穿梭机，准备把<code>readme.txt</code>回退到上一个版本，也就是<code>add distributed</code>的那个版本，怎么做呢？</p>
<p>首先，Git必须知道当前版本是哪个版本，在Git中，用<code>HEAD</code>表示当前版本，也就是最新的提交<code>1094adb...</code>（注意我的提交ID和你的肯定不一样），上一个版本就是<code>HEAD^</code>，上上一个版本就是<code>HEAD^^</code>，当然往上100个版本写100个<code>^</code>比较容易数不过来，所以写成<code>HEAD~100</code>。</p>
<p>现在，我们要把当前版本<code>append GPL</code>回退到上一个版本<code>add distributed</code>，就可以使用<code>git reset</code>命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git reset --hard HEAD^</span><br><span class="line">HEAD is now at e475afc add distributed</span><br></pre></td></tr></table></figure>
<p><code>--hard</code>参数有啥意义？这个后面再讲，现在你先放心使用。</p>
<p>看看<code>readme.txt</code>的内容是不是版本<code>add distributed</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat readme.txt</span><br><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software.</span><br></pre></td></tr></table></figure>
<p>果然被还原了。</p>
<p>还可以继续回退到上一个版本<code>wrote a readme file</code>，不过且慢，让我们用<code>git log</code>再看看现在版本库的状态：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log</span><br><span class="line">commit e475afc93c209a690c39c13a46716e8fa000c366 (HEAD -&gt; master)</span><br><span class="line">Author: Michael Liao &lt;askxuefeng@gmail.com&gt;</span><br><span class="line">Date:   Fri May 18 21:03:36 2018 +0800</span><br><span class="line"></span><br><span class="line">    add distributed</span><br><span class="line"></span><br><span class="line">commit eaadf4e385e865d25c48e7ca9c8395c3f7dfaef0</span><br><span class="line">Author: Michael Liao &lt;askxuefeng@gmail.com&gt;</span><br><span class="line">Date:   Fri May 18 20:59:18 2018 +0800</span><br><span class="line"></span><br><span class="line">    wrote a readme file</span><br></pre></td></tr></table></figure>
<p>最新的那个版本<code>append GPL</code>已经看不到了！好比你从21世纪坐时光穿梭机来到了19世纪，想再回去已经回不去了，肿么办？</p>
<p>办法其实还是有的，只要上面的命令行窗口还没有被关掉，你就可以顺着往上找啊找啊，找到那个<code>append GPL</code>的<code>commit id</code>是<code>1094adb...</code>，于是就可以指定回到未来的某个版本：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git reset --hard 1094a</span><br><span class="line">HEAD is now at 83b0afe append GPL</span><br></pre></td></tr></table></figure>
<p>版本号没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了。</p>
<p>再小心翼翼地看看<code>readme.txt</code>的内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat readme.txt</span><br><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br></pre></td></tr></table></figure>
<p>果然，我胡汉三又回来了。</p>
<p>Git的版本回退速度非常快，因为Git在内部有个指向当前版本的<code>HEAD</code>指针，当你回退版本的时候，Git仅仅是把HEAD从指向<code>append GPL</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">┌────┐</span><br><span class="line">│HEAD│</span><br><span class="line">└────┘</span><br><span class="line">   │</span><br><span class="line">   └──&gt; ○ append GPL</span><br><span class="line">        │</span><br><span class="line">        ○ add distributed</span><br><span class="line">        │</span><br><span class="line">        ○ wrote a readme file</span><br></pre></td></tr></table></figure>
<p>改为指向<code>add distributed</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">┌────┐</span><br><span class="line">│HEAD│</span><br><span class="line">└────┘</span><br><span class="line">   │</span><br><span class="line">   │    ○ append GPL</span><br><span class="line">   │    │</span><br><span class="line">   └──&gt; ○ add distributed</span><br><span class="line">        │</span><br><span class="line">        ○ wrote a readme file</span><br></pre></td></tr></table></figure>
<p>然后顺便把工作区的文件更新了。所以你让<code>HEAD</code>指向哪个版本号，你就把当前版本定位在哪。</p>
<p>现在，你回退到了某个版本，关掉了电脑，第二天早上就后悔了，想恢复到新版本怎么办？找不到新版本的<code>commit id</code>怎么办？</p>
<p>在Git中，总是有后悔药可以吃的。当你用<code>$ git reset --hard HEAD^</code>回退到<code>add distributed</code>版本时，再想恢复到<code>append GPL</code>，就必须找到<code>append GPL</code>的commit id。Git提供了一个命令<code>git reflog</code>用来记录你的每一次命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git reflog</span><br><span class="line">e475afc HEAD@&#123;1&#125;: reset: moving to HEAD^</span><br><span class="line">1094adb (HEAD -&gt; master) HEAD@&#123;2&#125;: commit: append GPL</span><br><span class="line">e475afc HEAD@&#123;3&#125;: commit: add distributed</span><br><span class="line">eaadf4e HEAD@&#123;4&#125;: commit (initial): wrote a readme file</span><br></pre></td></tr></table></figure>
<p>终于舒了口气，从输出可知，<code>append GPL</code>的commit id是<code>1094adb</code>，现在，你又可以乘坐时光机回到未来了。</p>
<p><strong>小结</strong></p>
<p>现在总结一下：</p>
<ul>
<li><code>HEAD</code>指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令<code>git reset --hard commit_id</code>。</li>
<li>穿梭前，用<code>git log</code>可以查看提交历史，以便确定要回退到哪个版本。</li>
<li>要重返未来，用<code>git reflog</code>查看命令历史，以便确定要回到未来的哪个版本。</li>
</ul>
<h4 id="2-工作区和暂存区"><a href="#2-工作区和暂存区" class="headerlink" title="2. 工作区和暂存区"></a>2. 工作区和暂存区</h4><h5 id="2-1-工作区"><a href="#2-1-工作区" class="headerlink" title="2.1 工作区"></a>2.1 工作区</h5><p>就是你在电脑里能看到的目录，比如我的<code>learngit</code>文件夹就是一个工作区。</p>
<h5 id="2-2-版本库（Repository）"><a href="#2-2-版本库（Repository）" class="headerlink" title="2.2 版本库（Repository）"></a>2.2 版本库（Repository）</h5><p>工作区有一个隐藏目录<code>.git</code>，这个不算工作区，而是Git的版本库。</p>
<p>Git的版本库里存了很多东西，其中最重要的就是称为<strong>stage(或者叫index)的暂存区</strong>，还有Git为我们自动创建的第一个分支<code>master</code>，以及指向<code>master</code>的一个指针叫<code>HEAD</code>。</p>
<p><img src="https://pic.imgdb.cn/item/61e67be02ab3f51d91087ff2.jpg" alt="Git指针关系"></p>
<p>分支和<code>HEAD</code>的概念我们以后再讲。</p>
<p>前面讲了我们把文件往Git版本库里添加的时候，是分两步执行的：</p>
<p>第一步是用<code>git add</code>把文件添加进去，实际上就是把文件修改添加到暂存区；</p>
<p>第二步是用<code>git commit</code>提交更改，实际上就是把暂存区的所有内容提交到当前分支。</p>
<p>因为我们创建Git版本库时，Git自动为我们创建了唯一一个<code>master</code>分支，所以，现在，<code>git commit</code>就是往<code>master</code>分支上提交更改。</p>
<p>你可以简单理解为，需要提交的文件修改通通放到暂存区，然后，一次性提交暂存区的所有修改。</p>
<p>俗话说，实践出真知。现在，我们再练习一遍，先对<code>readme.txt</code>做个修改，比如加上一行内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br><span class="line">Git has a mutable index called stage.</span><br></pre></td></tr></table></figure>
<p>然后，在工作区新增一个<code>LICENSE</code>文本文件（内容随便写）。</p>
<p>先用<code>git status</code>查看一下状态：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	modified:   readme.txt</span><br><span class="line"></span><br><span class="line">Untracked files:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</span><br><span class="line"></span><br><span class="line">	LICENSE</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br></pre></td></tr></table></figure>
<p>Git非常清楚地告诉我们，<code>readme.txt</code>被修改了，而<code>LICENSE</code>还从来没有被添加过，所以它的状态是<code>Untracked</code>。</p>
<p>现在，使用两次命令<code>git add</code>，把<code>readme.txt</code>和<code>LICENSE</code>都添加后，用<code>git status</code>再查看一下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">	new file:   LICENSE</span><br><span class="line">	modified:   readme.txt</span><br></pre></td></tr></table></figure>
<p>现在，暂存区的状态就变成这样了：</p>
<p><img src="https://pic.imgdb.cn/item/61e67c9b2ab3f51d91095d39.jpg" alt="当前暂存区"></p>
<p>所以，<code>git add</code>命令实际上就是把要提交的所有修改放到暂存区（Stage），然后，执行<code>git commit</code>就可以一次性把暂存区的所有修改提交到分支。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git commit -m &quot;understand how stage works&quot;</span><br><span class="line">[master e43a48b] understand how stage works</span><br><span class="line"> 2 files changed, 2 insertions(+)</span><br><span class="line"> create mode 100644 LICENSE</span><br></pre></td></tr></table></figure>
<p>一旦提交后，如果你又没有对工作区做任何修改，那么工作区就是“干净”的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">nothing to commit, working tree clean</span><br></pre></td></tr></table></figure>
<p>现在版本库变成了这样，暂存区就没有任何内容了：</p>
<p><img src="https://pic.imgdb.cn/item/61e67d0c2ab3f51d9109f1d7.jpg" alt="当前库内情况"></p>
<p><strong>小结</strong></p>
<p>暂存区是Git非常重要的概念，弄明白了暂存区，就弄明白了Git的很多操作到底干了什么。</p>
<p>没弄明白暂存区是怎么回事的童鞋，请向上滚动页面，再看一次。</p>
<h4 id="3-管理修改"><a href="#3-管理修改" class="headerlink" title="3. 管理修改"></a>3. 管理修改</h4><p>现在，假定你已经完全掌握了暂存区的概念。下面，我们要讨论的就是，为什么Git比其他版本控制系统设计得优秀，因为Git跟踪并管理的是修改，而非文件。</p>
<p>你会问，什么是修改？比如你新增了一行，这就是一个修改，删除了一行，也是一个修改，更改了某些字符，也是一个修改，删了一些又加了一些，也是一个修改，甚至创建一个新文件，也算一个修改。</p>
<p>为什么说Git管理的是修改，而不是文件呢？我们还是做实验。第一步，对readme.txt做一个修改，比如加一行内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat readme.txt</span><br><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br><span class="line">Git has a mutable index called stage.</span><br><span class="line">Git tracks changes.</span><br></pre></td></tr></table></figure>
<p>然后，添加：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt</span><br><span class="line">$ git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#   (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line">#</span><br><span class="line">#       modified:   readme.txt</span><br><span class="line">#</span><br></pre></td></tr></table></figure>
<p>然后，再修改readme.txt：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat readme.txt </span><br><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br><span class="line">Git has a mutable index called stage.</span><br><span class="line">Git tracks changes of files.</span><br></pre></td></tr></table></figure>
<p>提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git commit -m &quot;git tracks changes&quot;</span><br><span class="line">[master 519219b] git tracks changes</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>
<p>提交后，再看看状态：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	modified:   readme.txt</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br></pre></td></tr></table></figure>
<p>咦，怎么第二次的修改没有被提交？</p>
<p>别激动，我们回顾一下操作过程：</p>
<p>第一次修改 -&gt; <code>git add</code> -&gt; 第二次修改 -&gt; <code>git commit</code></p>
<p>你看，我们前面讲了，Git管理的是修改，当你用<code>git add</code>命令后，在工作区的第一次修改被放入暂存区，准备提交，但是，在工作区的第二次修改并没有放入暂存区，所以，<code>git commit</code>只负责把暂存区的修改提交了，也就是第一次的修改被提交了，第二次的修改不会被提交。</p>
<p>提交后，用<code>git diff HEAD -- readme.txt</code>命令可以查看工作区和版本库里面最新版本的区别：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git diff HEAD -- readme.txt </span><br><span class="line">diff --git a/readme.txt b/readme.txt</span><br><span class="line">index 76d770f..a9c5755 100644</span><br><span class="line">--- a/readme.txt</span><br><span class="line">+++ b/readme.txt</span><br><span class="line">@@ -1,4 +1,4 @@</span><br><span class="line"> Git is a distributed version control system.</span><br><span class="line"> Git is free software distributed under the GPL.</span><br><span class="line"> Git has a mutable index called stage.</span><br><span class="line">-Git tracks changes.</span><br><span class="line">+Git tracks changes of files.</span><br></pre></td></tr></table></figure>
<p>可见，第二次修改确实没有被提交。</p>
<p>那怎么提交第二次修改呢？你可以继续<code>git add</code>再<code>git commit</code>，也可以别着急提交第一次修改，先<code>git add</code>第二次修改，再<code>git commit</code>，就相当于把两次修改合并后一块提交了：</p>
<p>第一次修改 -&gt; <code>git add</code> -&gt; 第二次修改 -&gt; <code>git add</code> -&gt; <code>git commit</code></p>
<p>好，现在，把第二次修改提交了，然后开始小结。</p>
<p><strong>小结</strong></p>
<p>现在，你又理解了Git是如何跟踪修改的，每次修改，如果不用<code>git add</code>到暂存区，那就不会加入到<code>commit</code>中。</p>
<h4 id="4-撤销修改"><a href="#4-撤销修改" class="headerlink" title="4. 撤销修改"></a>4. 撤销修改</h4><p>自然，你是不会犯错的。不过现在是凌晨两点，你正在赶一份工作报告，你在<code>readme.txt</code>中添加了一行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat readme.txt</span><br><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br><span class="line">Git has a mutable index called stage.</span><br><span class="line">Git tracks changes of files.</span><br><span class="line">My stupid boss still prefers SVN.</span><br></pre></td></tr></table></figure>
<p>在你准备提交前，一杯咖啡起了作用，你猛然发现了<code>stupid boss</code>可能会让你丢掉这个月的奖金！</p>
<p>既然错误发现得很及时，就可以很容易地纠正它。你可以删掉最后一行，手动把文件恢复到上一个版本的状态。如果用<code>git status</code>查看一下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	modified:   readme.txt</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br></pre></td></tr></table></figure>
<p>你可以发现，Git会告诉你，<code>git checkout -- file</code>可以丢弃工作区的修改：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout -- readme.txt</span><br></pre></td></tr></table></figure>
<p>命令<code>git checkout -- readme.txt</code>意思就是，把<code>readme.txt</code>文件在工作区的修改全部撤销，这里有两种情况：</p>
<p>一种是<code>readme.txt</code>自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；</p>
<p>一种是<code>readme.txt</code>已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。</p>
<p>总之，就是让这个文件回到最近一次<code>git commit</code>或<code>git add</code>时的状态。</p>
<p>现在，看看<code>readme.txt</code>的文件内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat readme.txt</span><br><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br><span class="line">Git has a mutable index called stage.</span><br><span class="line">Git tracks changes of files.</span><br></pre></td></tr></table></figure>
<p>文件内容果然复原了。</p>
<p><code>git checkout -- file</code>命令中的<code>--</code>很重要，没有<code>--</code>，就变成了“切换到另一个分支”的命令，我们在后面的分支管理中会再次遇到<code>git checkout</code>命令。</p>
<p>现在假定是凌晨3点，你不但写了一些胡话，还<code>git add</code>到暂存区了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat readme.txt</span><br><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br><span class="line">Git has a mutable index called stage.</span><br><span class="line">Git tracks changes of files.</span><br><span class="line">My stupid boss still prefers SVN.</span><br><span class="line"></span><br><span class="line">$ git add readme.txt</span><br></pre></td></tr></table></figure>
<p>庆幸的是，在<code>commit</code>之前，你发现了这个问题。用<code>git status</code>查看一下，修改只是添加到了暂存区，还没有提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">	modified:   readme.txt</span><br></pre></td></tr></table></figure>
<p>Git同样告诉我们，用命令<code>git reset HEAD &lt;file&gt;</code>可以把暂存区的修改撤销掉（unstage），重新放回工作区：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git reset HEAD readme.txt</span><br><span class="line">Unstaged changes after reset:</span><br><span class="line">M	readme.txt</span><br></pre></td></tr></table></figure>
<p><code>git reset</code>命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用<code>HEAD</code>时，表示最新的版本。</p>
<p>再用<code>git status</code>查看一下，现在暂存区是干净的，工作区有修改：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	modified:   readme.txt</span><br></pre></td></tr></table></figure>
<p>还记得如何丢弃工作区的修改吗？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout -- readme.txt</span><br><span class="line"></span><br><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">nothing to commit, working tree clean</span><br></pre></td></tr></table></figure>
<p>整个世界终于清静了！</p>
<p>现在，假设你不但改错了东西，还从暂存区提交到了版本库，怎么办呢？还记得<a href="https://www.liaoxuefeng.com/wiki/896043488029600/897013573512192">版本回退</a>一节吗？可以回退到上一个版本。不过，这是有条件的，就是你还没有把自己的本地版本库推送到远程。还记得Git是分布式版本控制系统吗？我们后面会讲到远程版本库，一旦你把<code>stupid boss</code>提交推送到远程版本库，你就真的惨了……</p>
<p><strong>小结</strong></p>
<p>又到了小结时间。</p>
<p>场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令<code>git checkout -- file</code>。</p>
<p>场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令<code>git reset HEAD &lt;file&gt;</code>，就回到了场景1，第二步按场景1操作。</p>
<p>场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考<a href="https://www.liaoxuefeng.com/wiki/896043488029600/897013573512192">版本回退</a>一节，不过前提是没有推送到远程库。</p>
<h4 id="5-删除文件"><a href="#5-删除文件" class="headerlink" title="5. 删除文件"></a>5. 删除文件</h4><p>在Git中，删除也是一个修改操作，我们实战一下，先添加一个新文件<code>test.txt</code>到Git并且提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add test.txt</span><br><span class="line"></span><br><span class="line">$ git commit -m &quot;add test.txt&quot;</span><br><span class="line">[master b84166e] add test.txt</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line"> create mode 100644 test.txt</span><br></pre></td></tr></table></figure>
<p>一般情况下，你通常直接在文件管理器中把没用的文件删了，或者用<code>rm</code>命令删了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ rm test.txt</span><br></pre></td></tr></table></figure>
<p>这个时候，Git知道你删除了文件，因此，工作区和版本库就不一致了，<code>git status</code>命令会立刻告诉你哪些文件被删除了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	deleted:    test.txt</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br></pre></td></tr></table></figure>
<p>现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令<code>git rm</code>删掉，并且<code>git commit</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git rm test.txt</span><br><span class="line">rm &#x27;test.txt&#x27;</span><br><span class="line"></span><br><span class="line">$ git commit -m &quot;remove test.txt&quot;</span><br><span class="line">[master d46f35e] remove test.txt</span><br><span class="line"> 1 file changed, 1 deletion(-)</span><br><span class="line"> delete mode 100644 test.txt</span><br></pre></td></tr></table></figure>
<p>现在，文件就从版本库中被删除了。</p>
<p>小提示：先手动删除文件，然后使用git rm <file>和git add<file>效果是一样的。 </file></file></p>
<p>另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout -- test.txt</span><br></pre></td></tr></table></figure>
<p><code>git checkout</code>其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。</p>
<p> 注意：从来没有被添加到版本库就被删除的文件，是无法恢复的！ </p>
<p><strong>小结</strong></p>
<p>命令<code>git rm</code>用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失<strong>最近一次提交后你修改的内容</strong>。</p>
<h3 id="四、远程仓库"><a href="#四、远程仓库" class="headerlink" title="四、远程仓库"></a>四、远程仓库</h3><h4 id="1-什么是远程库"><a href="#1-什么是远程库" class="headerlink" title="1. 什么是远程库"></a>1. 什么是远程库</h4><p>到目前为止，我们已经掌握了如何在Git仓库里对一个文件进行时光穿梭，你再也不用担心文件备份或者丢失的问题了。</p>
<p>可是有用过集中式版本控制系统SVN的童鞋会站出来说，这些功能在SVN里早就有了，没看出Git有什么特别的地方。</p>
<p>没错，如果只是在一个仓库里管理文件历史，Git和SVN真没啥区别。为了保证你现在所学的Git物超所值，将来绝对不会后悔，同时为了打击已经不幸学了SVN的童鞋，本章开始介绍Git的杀手级功能之一（注意是之一，也就是后面还有之二，之三……）：<font color="red"><strong>远程仓库</strong></font>。</p>
<p>Git是分布式版本控制系统，同一个Git仓库，可以分布到不同的机器上。怎么分布呢？最早，肯定只有一台机器有一个原始版本库，此后，别的机器可以“克隆”这个原始版本库，而且每台机器的版本库其实都是一样的，并没有主次之分。</p>
<p>你肯定会想，至少需要两台机器才能玩远程库不是？但是我只有一台电脑，怎么玩？</p>
<p>其实一台电脑上也是可以克隆多个版本库的，只要不在同一个目录下。不过，现实生活中是不会有人这么傻的在一台电脑上搞几个远程库玩，因为一台电脑上搞几个远程库完全没有意义，而且硬盘挂了会导致所有库都挂掉，所以我也不告诉你在一台电脑上怎么克隆多个仓库。</p>
<p>实际情况往往是这样，找一台电脑充当服务器的角色，每天24小时开机，其他每个人都从这个“服务器”仓库克隆一份到自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交。</p>
<p>完全可以自己搭建一台运行Git的服务器，不过现阶段，为了学Git先搭个服务器绝对是小题大作。好在这个世界上有个叫<a href="https://github.com/">GitHub</a>的神奇的网站，从名字就可以看出，这个网站就是提供Git仓库托管服务的，所以，只要注册一个GitHub账号，就可以免费获得Git远程仓库。</p>
<p>在继续阅读后续内容前，请自行注册GitHub账号。由于你的本地Git仓库和GitHub仓库之间的传输是通过<font color="red"><strong>SSH加密的</strong></font>，所以需要一点设置：</p>
<p>第1步：创建SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有<code>id_rsa</code>和<code>id_rsa.pub</code>这两个文件，如果已经有了，可直接跳到下一步。如果没有，打开Shell（Windows下打开Git Bash），创建SSH Key：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</span><br></pre></td></tr></table></figure>
<p>你需要把邮件地址换成你自己的邮件地址，然后一路回车，使用默认值即可，由于这个Key也不是用于军事目的，所以也无需设置密码。</p>
<p>如果一切顺利的话，可以在用户主目录里找到<code>.ssh</code>目录，里面有<code>id_rsa</code>和<code>id_rsa.pub</code>两个文件，这两个就是SSH Key的秘钥对，<code>id_rsa</code>是私钥，不能泄露出去，<code>id_rsa.pub</code>是公钥，可以放心地告诉任何人。</p>
<p>第2步：登陆GitHub，打开“Account settings”，“SSH Keys”页面。</p>
<p>然后，点“Add SSH Key”，填上任意Title，在Key文本框里粘贴<code>id_rsa.pub</code>文件的内容。</p>
<p>点“Add Key”，你就应该看到已经添加的Key。</p>
<p>为什么GitHub需要SSH Key呢？因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以，GitHub只要知道了你的公钥，就可以确认只有你自己才能推送。</p>
<p>当然，GitHub允许你添加多个Key。假定你有若干电脑，你一会儿在公司提交，一会儿在家里提交，只要把每台电脑的Key都添加到GitHub，就可以在每台电脑上往GitHub推送了。</p>
<p>最后友情提示，在GitHub上免费托管的Git仓库，任何人都可以看到喔（但只有你自己才能改）。所以，不要把敏感信息放进去。</p>
<p>如果你不想让别人看到Git库，有两个办法，一个是交点保护费，让GitHub把公开的仓库变成私有的，这样别人就看不见了（不可读更不可写）。另一个办法是自己动手，搭一个Git服务器，因为是你自己的Git服务器，所以别人也是看不见的。这个方法我们后面会讲到的，相当简单，公司内部开发必备。</p>
<p>确保你拥有一个GitHub账号后，我们就即将开始远程仓库的学习。</p>
<p><strong>小结</strong></p>
<p>“有了远程仓库，妈妈再也不用担心我的硬盘了。”——Git点读机</p>
<h4 id="2-添加远程库"><a href="#2-添加远程库" class="headerlink" title="2. 添加远程库"></a>2. 添加远程库</h4><h5 id="2-1创建并关联远程库"><a href="#2-1创建并关联远程库" class="headerlink" title="2.1创建并关联远程库"></a>2.1创建并关联远程库</h5><p>现在的情景是，你已经在本地创建了一个Git仓库后，又想在GitHub创建一个Git仓库，并且让这两个仓库进行远程同步，这样，GitHub上的仓库既可以作为备份，又可以让其他人通过该仓库来协作，真是一举多得。</p>
<p>首先，登陆GitHub，然后，在右上角找到“Create a new repo”按钮，创建一个新的仓库。</p>
<p>在Repository name填入<code>learngit</code>，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库。</p>
<p>目前，在GitHub上的这个<code>learngit</code>仓库还是空的，GitHub告诉我们，可以从这个仓库克隆出新的仓库，也可以把一个已有的本地仓库与之关联，然后，把本地仓库的内容推送到GitHub仓库。</p>
<p>现在，我们根据GitHub的提示，在本地的<code>learngit</code>仓库下运行命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git remote add origin git@github.com:michaelliao/learngit.git</span><br></pre></td></tr></table></figure>
<p>请千万注意，把上面的<code>michaelliao</code>替换成你自己的GitHub账户名，否则，你在本地关联的就是我的远程库，关联没有问题，但是你以后推送是推不上去的，因为你的SSH Key公钥不在我的账户列表中。</p>
<p>添加后，远程库的名字就是<code>origin</code>，这是Git默认的叫法，也可以改成别的，但是<code>origin</code>这个名字一看就知道是远程库。</p>
<p>下一步，就可以把本地库的所有内容推送到远程库上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push -u origin master</span><br><span class="line">Counting objects: 20, done.</span><br><span class="line">Delta compression using up to 4 threads.</span><br><span class="line">Compressing objects: 100% (15/15), done.</span><br><span class="line">Writing objects: 100% (20/20), 1.64 KiB | 560.00 KiB/s, done.</span><br><span class="line">Total 20 (delta 5), reused 0 (delta 0)</span><br><span class="line">remote: Resolving deltas: 100% (5/5), done.</span><br><span class="line">To github.com:michaelliao/learngit.git</span><br><span class="line"> * [new branch]      master -&gt; master</span><br><span class="line">Branch &#x27;master&#x27; set up to track remote branch &#x27;master&#x27; from &#x27;origin&#x27;.</span><br></pre></td></tr></table></figure>
<p>把本地库的内容推送到远程，用<code>git push</code>命令，实际上是把当前分支<code>master</code>推送到远程。</p>
<p>由于远程库是空的，我们第一次推送<code>master</code>分支时，加上了<code>-u</code>参数，Git不但会把本地的<code>master</code>分支内容推送的远程新的<code>master</code>分支，还会把本地的<code>master</code>分支和远程的<code>master</code>分支关联起来，在以后的推送或者拉取时就可以简化命令。</p>
<p>推送成功后，可以立刻在GitHub页面中看到远程库的内容已经和本地一模一样 。</p>
<p>从现在起，只要本地作了提交，就可以通过命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push origin master</span><br></pre></td></tr></table></figure>
<p>把本地<code>master</code>分支的最新修改推送至GitHub，现在，你就拥有了真正的分布式版本库！</p>
<h5 id="2-2-SSH警告"><a href="#2-2-SSH警告" class="headerlink" title="2.2 SSH警告"></a>2.2 SSH警告</h5><p>当你第一次使用Git的<code>clone</code>或者<code>push</code>命令连接GitHub时，会得到一个警告：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The authenticity of host &#x27;github.com (xx.xx.xx.xx)&#x27; can&#x27;t be established.</span><br><span class="line">RSA key fingerprint is xx.xx.xx.xx.xx.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)?</span><br></pre></td></tr></table></figure>
<p>这是因为Git使用SSH连接，而SSH连接在第一次验证GitHub服务器的Key时，需要你确认GitHub的Key的指纹信息是否真的来自GitHub的服务器，输入<code>yes</code>回车即可。</p>
<p>Git会输出一个警告，告诉你已经把GitHub的Key添加到本机的一个信任列表里了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Warning: Permanently added &#x27;github.com&#x27; (RSA) to the list of known hosts.</span><br></pre></td></tr></table></figure>
<p>这个警告只会出现一次，后面的操作就不会有任何警告了。</p>
<p>如果你实在担心有人冒充GitHub服务器，输入<code>yes</code>前可以对照<a href="https://help.github.com/articles/what-are-github-s-ssh-key-fingerprints/">GitHub的RSA Key的指纹信息</a>是否与SSH连接给出的一致。</p>
<h5 id="2-3删除远程库"><a href="#2-3删除远程库" class="headerlink" title="2.3删除远程库"></a>2.3删除远程库</h5><p>如果添加的时候地址写错了，或者就是想删除远程库，可以用<code>git remote rm &lt;name&gt;</code>命令。使用前，建议先用<code>git remote -v</code>查看远程库信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git remote -v</span><br><span class="line">origin  git@github.com:michaelliao/learn-git.git (fetch)</span><br><span class="line">origin  git@github.com:michaelliao/learn-git.git (push)</span><br></pre></td></tr></table></figure>
<p>然后，根据名字删除，比如删除<code>origin</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git remote rm origin</span><br></pre></td></tr></table></figure>
<p>此处的“删除”其实是解除了本地和远程的绑定关系，并不是物理上删除了远程库。远程库本身并没有任何改动。要真正删除远程库，需要登录到GitHub，在后台页面找到删除按钮再删除。</p>
<p><strong>小结</strong></p>
<p>要关联一个远程库，使用命令<code>git remote add origin git@server-name:path/repo-name.git</code>；</p>
<p>关联一个远程库时必须给远程库指定一个名字，<code>origin</code>是默认习惯命名；</p>
<p>关联后，使用命令<code>git push -u origin master</code>第一次推送master分支的所有内容；</p>
<p>此后，每次本地提交后，只要有必要，就可以使用命令<code>git push origin master</code>推送最新修改；</p>
<p>分布式版本系统的最大好处之一是在本地工作完全不需要考虑远程库的存在，也就是有没有联网都可以正常工作，而SVN在没有联网的时候是拒绝干活的！当有网络的时候，再把本地提交推送一下就完成了同步，真是太方便了！</p>
<h4 id="3-远程库克隆"><a href="#3-远程库克隆" class="headerlink" title="3. 远程库克隆"></a>3. 远程库克隆</h4><p>上次我们讲了先有本地库，后有远程库的时候，如何关联远程库。</p>
<p>现在，假设<font color="red"><strong>我们从零开发，那么最好的方式是先创建远程库，然后，从远程库克隆</strong>。</font></p>
<p>首先，登陆GitHub，创建一个新的仓库，名字叫<code>gitskills</code> 。</p>
<p>我们勾选<code>Initialize this repository with a README</code>，这样GitHub会自动为我们创建一个<code>README.md</code>文件。创建完毕后，可以看到<code>README.md</code>文件。</p>
<p>现在，远程库已经准备好了，下一步是用命令<code>git clone</code>克隆一个本地库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git clone git@github.com:michaelliao/gitskills.git</span><br><span class="line">Cloning into &#x27;gitskills&#x27;...</span><br><span class="line">remote: Counting objects: 3, done.</span><br><span class="line">remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3</span><br><span class="line">Receiving objects: 100% (3/3), done.</span><br></pre></td></tr></table></figure>
<p>注意把Git库的地址换成你自己的，然后进入<code>gitskills</code>目录看看，已经有<code>README.md</code>文件了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cd gitskills</span><br><span class="line">$ ls</span><br><span class="line">README.md</span><br></pre></td></tr></table></figure>
<p>如果有多个人协作开发，那么每个人各自从远程克隆一份就可以了。</p>
<p>你也许还注意到，GitHub给出的地址不止一个，还可以用<code>https://github.com/michaelliao/gitskills.git</code>这样的地址。实际上，Git支持多种协议，默认的<code>git://</code>使用ssh，但也可以使用<code>https</code>等其他协议。</p>
<p>使用<code>https</code>除了速度慢以外，还有个最大的麻烦是每次推送都必须输入口令，但是在某些只开放http端口的公司内部就无法使用<code>ssh</code>协议而只能用<code>https</code>。</p>
<p><strong>小结</strong></p>
<p>要克隆一个仓库，首先必须知道仓库的地址，然后使用<code>git clone</code>命令克隆。</p>
<p>Git支持多种协议，包括<code>https</code>，但<code>ssh</code>协议速度最快。</p>
<h3 id="五、分支管理"><a href="#五、分支管理" class="headerlink" title="五、分支管理"></a>五、分支管理</h3><h4 id="1-分支管理的概念"><a href="#1-分支管理的概念" class="headerlink" title="1. 分支管理的概念"></a>1. 分支管理的概念</h4><p>分支就是科幻电影里的平行宇宙，当你正在电脑前努力学习Git的时候，另一个你正在另一个平行宇宙里努力学习SVN。</p>
<p>如果两个平行宇宙互不干扰，那对现在的你也没啥影响。不过，在某个时间点，两个平行宇宙合并了，结果，你既学会了Git又学会了SVN！</p>
<p>分支在实际中有什么用呢？假设你准备开发一个新功能，但是需要两周才能完成，第一周你写了50%的代码，如果立刻提交，由于代码还没写完，不完整的代码库会导致别人不能干活了。如果等代码全部写完再一次提交，又存在丢失每天进度的巨大风险。</p>
<p>现在有了分支，就不用怕了。你创建了一个属于你自己的分支，别人看不到，还继续在原来的分支上正常工作，而你在自己的分支上干活，想提交就提交，直到开发完毕后，再一次性合并到原来的分支上，这样，既安全，又不影响别人工作。</p>
<p>其他版本控制系统如SVN等都有分支管理，但是用过之后你会发现，这些版本控制系统创建和切换分支比蜗牛还慢，简直让人无法忍受，结果分支功能成了摆设，大家都不去用。</p>
<p>但Git的分支是与众不同的，无论创建、切换和删除分支，Git在1秒钟之内就能完成！无论你的版本库是1个文件还是1万个文件。</p>
<h4 id="2-创建与合并分支"><a href="#2-创建与合并分支" class="headerlink" title="2. 创建与合并分支"></a>2. 创建与合并分支</h4><h5 id="2-1-分支概念与实践"><a href="#2-1-分支概念与实践" class="headerlink" title="2.1 分支概念与实践"></a>2.1 分支概念与实践</h5><p>在<a href="https://www.liaoxuefeng.com/wiki/896043488029600/897013573512192">版本回退</a>里，你已经知道，每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫主分支，即<code>master</code>分支。<code>HEAD</code>严格来说不是指向提交，而是指向<code>master</code>，<code>master</code>才是指向提交的，所以，<code>HEAD</code>指向的就是当前分支。</p>
<p>一开始的时候，<code>master</code>分支是一条线，Git用<code>master</code>指向最新的提交，再用<code>HEAD</code>指向<code>master</code>，就能确定当前分支，以及当前分支的提交点：</p>
<p><img src="https://pic.imgdb.cn/item/61e696dd2ab3f51d91240513.jpg" alt></p>
<p>每次提交，<code>master</code>分支都会向前移动一步，这样，随着你不断提交，<code>master</code>分支的线也越来越长。</p>
<p>当我们创建新的分支，例如<code>dev</code>时，Git新建了一个指针叫<code>dev</code>，指向<code>master</code>相同的提交，再把<code>HEAD</code>指向<code>dev</code>，就表示当前分支在<code>dev</code>上：</p>
<p><img src="https://pic.imgdb.cn/item/61e696fd2ab3f51d91242093.jpg" alt></p>
<p>你看，Git创建一个分支很快，因为除了增加一个<code>dev</code>指针，改改<code>HEAD</code>的指向，工作区的文件都没有任何变化！</p>
<p>不过，从现在开始，对工作区的修改和提交就是针对<code>dev</code>分支了，比如新提交一次后，<code>dev</code>指针往前移动一步，而<code>master</code>指针不变：</p>
<p><img src="https://pic.imgdb.cn/item/61e697152ab3f51d912435f0.jpg" alt></p>
<p>假如我们在<code>dev</code>上的工作完成了，就可以把<code>dev</code>合并到<code>master</code>上。Git怎么合并呢？最简单的方法，就是直接把<code>master</code>指向<code>dev</code>的当前提交，就完成了合并： </p>
<p><img src="https://pic.imgdb.cn/item/61e697332ab3f51d91245144.jpg" alt></p>
<p>所以Git合并分支也很快！就改改指针，工作区内容也不变！</p>
<p>合并完分支后，甚至可以删除<code>dev</code>分支。删除<code>dev</code>分支就是把<code>dev</code>指针给删掉，删掉后，我们就剩下了一条<code>master</code>分支：</p>
<p><img src="https://pic.imgdb.cn/item/61e697492ab3f51d912464db.jpg" alt></p>
<p>真是太神奇了，你看得出来有些提交是通过分支完成的吗？</p>
<p>下面开始实战。</p>
<p>首先，我们创建<code>dev</code>分支，然后切换到<code>dev</code>分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout -b dev</span><br><span class="line">Switched to a new branch &#x27;dev&#x27;</span><br></pre></td></tr></table></figure>
<p><code>git checkout</code>命令加上<code>-b</code>参数表示创建并切换，相当于以下两条命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch dev</span><br><span class="line">$ git checkout dev</span><br><span class="line">Switched to branch &#x27;dev&#x27;</span><br></pre></td></tr></table></figure>
<p>然后，用<code>git branch</code>命令查看当前分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch</span><br><span class="line">* dev</span><br><span class="line">  master</span><br></pre></td></tr></table></figure>
<p><code>git branch</code>命令会列出所有分支，当前分支前面会标一个<code>*</code>号。</p>
<p>然后，我们就可以在<code>dev</code>分支上正常提交，比如对<code>readme.txt</code>做个修改，加上一行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Creating a new branch is quick.</span><br></pre></td></tr></table></figure>
<p>然后提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt </span><br><span class="line">$ git commit -m &quot;branch test&quot;</span><br><span class="line">[dev b17d20e] branch test</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>
<p>现在，<code>dev</code>分支的工作完成，我们就可以切换回<code>master</code>分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout master</span><br><span class="line">Switched to branch &#x27;master&#x27;</span><br></pre></td></tr></table></figure>
<p>切换回<code>master</code>分支后，再查看一个<code>readme.txt</code>文件，刚才添加的内容不见了！因为那个提交是在<code>dev</code>分支上，而<code>master</code>分支此刻的提交点并没有变：</p>
<p><img src="https://pic.imgdb.cn/item/61e697672ab3f51d91248608.jpg" alt></p>
<p>现在，我们把<code>dev</code>分支的工作成果合并到<code>master</code>分支上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git merge dev</span><br><span class="line">Updating d46f35e..b17d20e</span><br><span class="line">Fast-forward</span><br><span class="line"> readme.txt | 1 +</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>
<p><code>git merge</code>命令用于合并指定分支到当前分支。合并后，再查看<code>readme.txt</code>的内容，就可以看到，和<code>dev</code>分支的最新提交是完全一样的。</p>
<p>注意到上面的<code>Fast-forward</code>信息，Git告诉我们，这次合并是“快进模式”，也就是直接把<code>master</code>指向<code>dev</code>的当前提交，所以合并速度非常快。</p>
<p>当然，也不是每次合并都能<code>Fast-forward</code>，我们后面会讲其他方式的合并。</p>
<p>合并完成后，就可以放心地删除<code>dev</code>分支了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch -d dev</span><br><span class="line">Deleted branch dev (was b17d20e).</span><br></pre></td></tr></table></figure>
<p>删除后，查看<code>branch</code>，就只剩下<code>master</code>分支了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch</span><br><span class="line">* master</span><br></pre></td></tr></table></figure>
<p>因为创建、合并和删除分支非常快，所以Git鼓励你使用分支完成某个任务，合并后再删掉分支，这和直接在<code>master</code>分支上工作效果是一样的，但过程更安全。</p>
<h5 id="2-2-switch"><a href="#2-2-switch" class="headerlink" title="2.2 switch"></a>2.2 switch</h5><p>我们注意到切换分支使用<code>git checkout &lt;branch&gt;</code>，而前面讲过的撤销修改则是<code>git checkout -- &lt;file&gt;</code>，同一个命令，有两种作用，确实有点令人迷惑。</p>
<p>实际上，切换分支这个动作，用<code>switch</code>更科学。因此，最新版本的Git提供了新的<code>git switch</code>命令来切换分支：</p>
<p>创建并切换到新的<code>dev</code>分支，可以使用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch -c dev</span><br></pre></td></tr></table></figure>
<p>直接切换到已有的<code>master</code>分支，可以使用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch master</span><br></pre></td></tr></table></figure>
<p>使用新的<code>git switch</code>命令，比<code>git checkout</code>要更容易理解。</p>
<p><strong>小结</strong></p>
<p>Git鼓励大量使用分支：</p>
<p>查看分支：<code>git branch</code></p>
<p>创建分支：<code>git branch &lt;name&gt;</code></p>
<p>切换分支：<code>git checkout &lt;name&gt;</code>或者<code>git switch &lt;name&gt;</code></p>
<p>创建+切换分支：<code>git checkout -b &lt;name&gt;</code>或者<code>git switch -c &lt;name&gt;</code></p>
<p>合并某分支到当前分支：<code>git merge &lt;name&gt;</code></p>
<p>删除分支：<code>git branch -d &lt;name&gt;</code></p>
<h4 id="3-解决冲突"><a href="#3-解决冲突" class="headerlink" title="3. 解决冲突"></a>3. 解决冲突</h4><p>人生不如意之事十之八九，合并分支往往也不是一帆风顺的。</p>
<p>准备新的<code>feature1</code>分支，继续我们的新分支开发：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch -c feature1</span><br><span class="line">Switched to a new branch &#x27;feature1&#x27;</span><br></pre></td></tr></table></figure>
<p>修改<code>readme.txt</code>最后一行，改为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Creating a new branch is quick AND simple.</span><br></pre></td></tr></table></figure>
<p>在<code>feature1</code>分支上提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt</span><br><span class="line"></span><br><span class="line">$ git commit -m &quot;AND simple&quot;</span><br><span class="line">[feature1 14096d0] AND simple</span><br><span class="line"> 1 file changed, 1 insertion(+), 1 deletion(-)</span><br></pre></td></tr></table></figure>
<p>切换到<code>master</code>分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch master</span><br><span class="line">Switched to branch &#x27;master&#x27;</span><br><span class="line">Your branch is ahead of &#x27;origin/master&#x27; by 1 commit.</span><br><span class="line">  (use &quot;git push&quot; to publish your local commits)</span><br></pre></td></tr></table></figure>
<p>Git还会自动提示我们当前<code>master</code>分支比远程的<code>master</code>分支要超前1个提交。</p>
<p>在<code>master</code>分支上把<code>readme.txt</code>文件的最后一行改为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Creating a new branch is quick &amp; simple.</span><br></pre></td></tr></table></figure>
<p>提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt </span><br><span class="line">$ git commit -m &quot;&amp; simple&quot;</span><br><span class="line">[master 5dc6824] &amp; simple</span><br><span class="line"> 1 file changed, 1 insertion(+), 1 deletion(-)</span><br></pre></td></tr></table></figure>
<p>现在，<code>master</code>分支和<code>feature1</code>分支各自都分别有新的提交，变成了这样：</p>
<p><img src="https://pic.imgdb.cn/item/61e69e7a2ab3f51d912a6360.jpg" alt></p>
<p>这种情况下，Git无法执行“快速合并”，只能试图把各自的修改合并起来，但这种合并就可能会有冲突，我们试试看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git merge feature1</span><br><span class="line">Auto-merging readme.txt</span><br><span class="line">CONFLICT (content): Merge conflict in readme.txt</span><br><span class="line">Automatic merge failed; fix conflicts and then commit the result.</span><br></pre></td></tr></table></figure>
<p>果然冲突了！Git告诉我们，<code>readme.txt</code>文件存在冲突，必须手动解决冲突后再提交。<code>git status</code>也可以告诉我们冲突的文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Your branch is ahead of &#x27;origin/master&#x27; by 2 commits.</span><br><span class="line">  (use &quot;git push&quot; to publish your local commits)</span><br><span class="line"></span><br><span class="line">You have unmerged paths.</span><br><span class="line">  (fix conflicts and run &quot;git commit&quot;)</span><br><span class="line">  (use &quot;git merge --abort&quot; to abort the merge)</span><br><span class="line"></span><br><span class="line">Unmerged paths:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to mark resolution)</span><br><span class="line"></span><br><span class="line">	both modified:   readme.txt</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br></pre></td></tr></table></figure>
<p>我们可以直接查看readme.txt的内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Git is a distributed version control system.</span><br><span class="line">Git is free software distributed under the GPL.</span><br><span class="line">Git has a mutable index called stage.</span><br><span class="line">Git tracks changes of files.</span><br><span class="line">&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</span><br><span class="line">Creating a new branch is quick &amp; simple.</span><br><span class="line">=======</span><br><span class="line">Creating a new branch is quick AND simple.</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1</span><br></pre></td></tr></table></figure>
<p>Git用<code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code>，<code>=======</code>，<code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code>标记出不同分支的内容，我们修改如下后保存：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Creating a new branch is quick and simple.</span><br></pre></td></tr></table></figure>
<p>再提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt </span><br><span class="line">$ git commit -m &quot;conflict fixed&quot;</span><br><span class="line">[master cf810e4] conflict fixed</span><br></pre></td></tr></table></figure>
<p>现在，<code>master</code>分支和<code>feature1</code>分支变成了下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/61e69ea22ab3f51d912a838f.jpg" alt></p>
<p>用带参数的<code>git log</code>也可以看到分支的合并情况：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --graph --pretty=oneline --abbrev-commit</span><br><span class="line">*   cf810e4 (HEAD -&gt; master) conflict fixed</span><br><span class="line">|\  </span><br><span class="line">| * 14096d0 (feature1) AND simple</span><br><span class="line">* | 5dc6824 &amp; simple</span><br><span class="line">|/  </span><br><span class="line">* b17d20e branch test</span><br><span class="line">* d46f35e (origin/master) remove test.txt</span><br><span class="line">* b84166e add test.txt</span><br><span class="line">* 519219b git tracks changes</span><br><span class="line">* e43a48b understand how stage works</span><br><span class="line">* 1094adb append GPL</span><br><span class="line">* e475afc add distributed</span><br><span class="line">* eaadf4e wrote a readme file</span><br></pre></td></tr></table></figure>
<p>最后，删除<code>feature1</code>分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch -d feature1</span><br><span class="line">Deleted branch feature1 (was 14096d0).</span><br></pre></td></tr></table></figure>
<p>工作完成。</p>
<p><strong>小结</strong></p>
<p>当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。</p>
<p>解决冲突就是把Git合并失败的文件手动编辑为我们希望的内容，再提交。</p>
<p>用<code>git log --graph</code>命令可以看到分支合并图。</p>
<h4 id="4-分支管理策略"><a href="#4-分支管理策略" class="headerlink" title="4. 分支管理策略"></a>4. 分支管理策略</h4><h5 id="4-1-分支管理"><a href="#4-1-分支管理" class="headerlink" title="4.1 分支管理"></a>4.1 分支管理</h5><p>通常，合并分支时，如果可能，Git会用<code>Fast forward</code>模式，但这种模式下，删除分支后，会丢掉分支信息。</p>
<p>如果要强制禁用<code>Fast forward</code>模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。</p>
<p>下面我们实战一下<code>--no-ff</code>方式的<code>git merge</code>：</p>
<p>首先，仍然创建并切换<code>dev</code>分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch -c dev</span><br><span class="line">Switched to a new branch &#x27;dev&#x27;</span><br></pre></td></tr></table></figure>
<p>修改readme.txt文件，并提交一个新的commit：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt </span><br><span class="line">$ git commit -m &quot;add merge&quot;</span><br><span class="line">[dev f52c633] add merge</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>
<p>现在，我们切换回<code>master</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch master</span><br><span class="line">Switched to branch &#x27;master&#x27;</span><br></pre></td></tr></table></figure>
<p>准备合并<code>dev</code>分支，请注意<code>--no-ff</code>参数，表示禁用<code>Fast forward</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git merge --no-ff -m &quot;merge with no-ff&quot; dev</span><br><span class="line">Merge made by the &#x27;recursive&#x27; strategy.</span><br><span class="line"> readme.txt | 1 +</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>
<p>因为本次合并要创建一个新的commit，所以加上<code>-m</code>参数，把commit描述写进去。</p>
<p>合并后，我们用<code>git log</code>看看分支历史：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --graph --pretty=oneline --abbrev-commit</span><br><span class="line">*   e1e9c68 (HEAD -&gt; master) merge with no-ff</span><br><span class="line">|\  </span><br><span class="line">| * f52c633 (dev) add merge</span><br><span class="line">|/  </span><br><span class="line">*   cf810e4 conflict fixed</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>可以看到，不使用<code>Fast forward</code>模式，merge后就像这样：</p>
<p><img src="https://pic.imgdb.cn/item/61e69fbd2ab3f51d912b56e2.jpg" alt></p>
<h5 id="4-2-分支策略"><a href="#4-2-分支策略" class="headerlink" title="4.2 分支策略"></a>4.2 分支策略</h5><p>在实际开发中，我们应该按照几个基本原则进行分支管理：</p>
<p>首先，<code>master</code>分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；</p>
<p>那在哪干活呢？干活都在<code>dev</code>分支上，也就是说，<code>dev</code>分支是不稳定的，到某个时候，比如1.0版本发布时，再把<code>dev</code>分支合并到<code>master</code>上，在<code>master</code>分支发布1.0版本；</p>
<p>你和你的小伙伴们每个人都在<code>dev</code>分支上干活，每个人都有自己的分支，时不时地往<code>dev</code>分支上合并就可以了。</p>
<p>所以，团队合作的分支看起来就像这样：</p>
<p><img src="https://pic.imgdb.cn/item/61e69fd92ab3f51d912b6b96.jpg" alt></p>
<p><strong>小结</strong></p>
<p>Git分支十分强大，在团队开发中应该充分应用。</p>
<p>合并分支时，加上<code>--no-ff</code>参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而<code>fast forward</code>合并就看不出来曾经做过合并。</p>
<h4 id="5-Bug分支"><a href="#5-Bug分支" class="headerlink" title="5. Bug分支"></a>5. Bug分支</h4><p>软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。</p>
<p>当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支<code>issue-101</code>来修复它，但是，等等，当前正在<code>dev</code>上进行的工作还没有提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch dev</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">	new file:   hello.py</span><br><span class="line"></span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	modified:   readme.txt</span><br></pre></td></tr></table></figure>
<p>并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？</p>
<p>幸好，Git还提供了一个<code>stash</code>功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git stash</span><br><span class="line">Saved working directory and index state WIP on dev: f52c633 add merge</span><br></pre></td></tr></table></figure>
<p>现在，用<code>git status</code>查看工作区，就是干净的（除非有没有被Git管理的文件），因此可以放心地创建分支来修复bug。</p>
<p>首先确定要在哪个分支上修复bug，假定需要在<code>master</code>分支上修复，就从<code>master</code>创建临时分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout master</span><br><span class="line">Switched to branch &#x27;master&#x27;</span><br><span class="line">Your branch is ahead of &#x27;origin/master&#x27; by 6 commits.</span><br><span class="line">  (use &quot;git push&quot; to publish your local commits)</span><br><span class="line"></span><br><span class="line">$ git checkout -b issue-101</span><br><span class="line">Switched to a new branch &#x27;issue-101&#x27;</span><br></pre></td></tr></table></figure>
<p>现在修复bug，需要把“Git is free software …”改为“Git is a free software …”，然后提交：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add readme.txt </span><br><span class="line">$ git commit -m &quot;fix bug 101&quot;</span><br><span class="line">[issue-101 4c805e2] fix bug 101</span><br><span class="line"> 1 file changed, 1 insertion(+), 1 deletion(-)</span><br></pre></td></tr></table></figure>
<p>修复完成后，切换到<code>master</code>分支，并完成合并，最后删除<code>issue-101</code>分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch master</span><br><span class="line">Switched to branch &#x27;master&#x27;</span><br><span class="line">Your branch is ahead of &#x27;origin/master&#x27; by 6 commits.</span><br><span class="line">  (use &quot;git push&quot; to publish your local commits)</span><br><span class="line"></span><br><span class="line">$ git merge --no-ff -m &quot;merged bug fix 101&quot; issue-101</span><br><span class="line">Merge made by the &#x27;recursive&#x27; strategy.</span><br><span class="line"> readme.txt | 2 +-</span><br><span class="line"> 1 file changed, 1 insertion(+), 1 deletion(-)</span><br></pre></td></tr></table></figure>
<p>太棒了，原计划两个小时的bug修复只花了5分钟！现在，是时候接着回到<code>dev</code>分支干活了！</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch dev</span><br><span class="line">Switched to branch &#x27;dev&#x27;</span><br><span class="line"></span><br><span class="line">$ git status</span><br><span class="line">On branch dev</span><br><span class="line">nothing to commit, working tree clean</span><br></pre></td></tr></table></figure>
<p>工作区是干净的，刚才的工作现场存到哪去了？用<code>git stash list</code>命令看看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git stash list</span><br><span class="line">stash@&#123;0&#125;: WIP on dev: f52c633 add merge</span><br></pre></td></tr></table></figure>
<p>工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，有两个办法：</p>
<p>一是用<code>git stash apply</code>恢复，但是恢复后，stash内容并不删除，你需要用<code>git stash drop</code>来删除；</p>
<p>另一种方式是用<code>git stash pop</code>，恢复的同时把stash内容也删了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git stash pop</span><br><span class="line">On branch dev</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">	new file:   hello.py</span><br><span class="line"></span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	modified:   readme.txt</span><br><span class="line"></span><br><span class="line">Dropped refs/stash@&#123;0&#125; (5d677e2ee266f39ea296182fb2354265b91b3b2a)</span><br></pre></td></tr></table></figure>
<p>再用<code>git stash list</code>查看，就看不到任何stash内容了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git stash list</span><br></pre></td></tr></table></figure>
<p>你可以多次stash，恢复的时候，先用<code>git stash list</code>查看，然后恢复指定的stash，用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git stash apply stash@&#123;0&#125;</span><br></pre></td></tr></table></figure>
<p>在master分支上修复了bug后，我们要想一想，dev分支是早期从master分支分出来的，所以，这个bug其实在当前dev分支上也存在。</p>
<p>那怎么在dev分支上修复同样的bug？重复操作一次，提交不就行了？</p>
<p>有木有更简单的方法？</p>
<p>有！</p>
<p>同样的bug，要在dev上修复，我们只需要把<code>4c805e2 fix bug 101</code>这个提交所做的修改“复制”到dev分支。注意：我们只想复制<code>4c805e2 fix bug 101</code>这个提交所做的修改，并不是把整个master分支merge过来。</p>
<p>为了方便操作，Git专门提供了一个<code>cherry-pick</code>命令，让我们能复制一个特定的提交到当前分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch</span><br><span class="line">* dev</span><br><span class="line">  master</span><br><span class="line">$ git cherry-pick 4c805e2</span><br><span class="line">[master 1d4b803] fix bug 101</span><br><span class="line"> 1 file changed, 1 insertion(+), 1 deletion(-)</span><br></pre></td></tr></table></figure>
<p>Git自动给dev分支做了一次提交，注意这次提交的commit是<code>1d4b803</code>，它并不同于master的<code>4c805e2</code>，因为这两个commit只是改动相同，但确实是两个不同的commit。用<code>git cherry-pick</code>，我们就不需要在dev分支上手动再把修bug的过程重复一遍。</p>
<p>有些聪明的童鞋会想了，既然可以在master分支上修复bug后，在dev分支上可以“重放”这个修复过程，那么直接在dev分支上修复bug，然后在master分支上“重放”行不行？当然可以，不过你仍然需要<code>git stash</code>命令保存现场，才能从dev分支切换到master分支。</p>
<p><strong>小结</strong></p>
<p>修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除；</p>
<p>当手头工作没有完成时，先把工作现场<code>git stash</code>一下，然后去修复bug，修复后，再<code>git stash pop</code>，回到工作现场；</p>
<p>在master分支上修复的bug，想要合并到当前dev分支，可以用<code>git cherry-pick &lt;commit&gt;</code>命令，把bug提交的修改“复制”到当前分支，避免重复劳动。</p>
<h4 id="6-Feature分支"><a href="#6-Feature分支" class="headerlink" title="6. Feature分支"></a>6. Feature分支</h4><p>软件开发中，总有无穷无尽的新的功能要不断添加进来。</p>
<p>添加一个新功能时，你肯定不希望因为一些实验性质的代码，把主分支搞乱了，所以，每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，最后，删除该feature分支。</p>
<p>现在，你终于接到了一个新任务：开发代号为Vulcan的新功能，该功能计划用于下一代星际飞船。</p>
<p>于是准备开发：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch -c feature-vulcan</span><br><span class="line">Switched to a new branch &#x27;feature-vulcan&#x27;</span><br></pre></td></tr></table></figure>
<p>5分钟后，开发完毕：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add vulcan.py</span><br><span class="line"></span><br><span class="line">$ git status</span><br><span class="line">On branch feature-vulcan</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">	new file:   vulcan.py</span><br><span class="line"></span><br><span class="line">$ git commit -m &quot;add feature vulcan&quot;</span><br><span class="line">[feature-vulcan 287773e] add feature vulcan</span><br><span class="line"> 1 file changed, 2 insertions(+)</span><br><span class="line"> create mode 100644 vulcan.py</span><br></pre></td></tr></table></figure>
<p>切回<code>dev</code>，准备合并：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git switch dev</span><br></pre></td></tr></table></figure>
<p>一切顺利的话，feature分支和bug分支是类似的，合并，然后删除。</p>
<p>但是！</p>
<p>就在此时，接到上级命令，因经费不足，新功能必须取消！</p>
<p>虽然白干了，但是这个包含机密资料的分支还是必须就地销毁：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch -d feature-vulcan</span><br><span class="line">error: The branch &#x27;feature-vulcan&#x27; is not fully merged.</span><br><span class="line">If you are sure you want to delete it, run &#x27;git branch -D feature-vulcan&#x27;.</span><br></pre></td></tr></table></figure>
<p>销毁失败。Git友情提醒，<code>feature-vulcan</code>分支还没有被合并，如果删除，将丢失掉修改，如果要强行删除，需要使用大写的<code>-D</code>参数。。</p>
<p>现在我们强行删除：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch -D feature-vulcan</span><br><span class="line">Deleted branch feature-vulcan (was 287773e).</span><br></pre></td></tr></table></figure>
<p>终于删除成功！</p>
<p><strong>小结</strong></p>
<p>开发一个新feature，最好新建一个分支；</p>
<p>如果要丢弃一个没有被合并过的分支，可以通过<code>git branch -D &lt;name&gt;</code>强行删除。</p>
<h3 id="六、多人协作"><a href="#六、多人协作" class="headerlink" title="六、多人协作"></a>六、多人协作</h3><p>当你从远程仓库克隆时，实际上Git自动把本地的<code>master</code>分支和远程的<code>master</code>分支对应起来了，并且，远程仓库的默认名称是<code>origin</code>。</p>
<p>要查看远程库的信息，用<code>git remote</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git remote</span><br><span class="line">origin</span><br></pre></td></tr></table></figure>
<p>或者，用<code>git remote -v</code>显示更详细的信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git remote -v</span><br><span class="line">origin  git@github.com:michaelliao/learngit.git (fetch)</span><br><span class="line">origin  git@github.com:michaelliao/learngit.git (push)</span><br></pre></td></tr></table></figure>
<p>上面显示了可以抓取和推送的<code>origin</code>的地址。如果没有推送权限，就看不到push的地址。</p>
<h4 id="1-推送分支"><a href="#1-推送分支" class="headerlink" title="1. 推送分支"></a>1. 推送分支</h4><p>推送分支，就是把该分支上的所有本地提交推送到远程库。推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push origin master</span><br></pre></td></tr></table></figure>
<p>如果要推送其他分支，比如<code>dev</code>，就改成：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push origin dev</span><br></pre></td></tr></table></figure>
<p>但是，并不是一定要把本地分支往远程推送，那么，哪些分支需要推送，哪些不需要呢？</p>
<ul>
<li><code>master</code>分支是主分支，因此要时刻与远程同步；</li>
<li><code>dev</code>分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步；</li>
<li>bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug；</li>
<li>feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。</li>
</ul>
<p>总之，就是在Git中，分支完全可以在本地自己藏着玩，是否推送，视你的心情而定！</p>
<h4 id="2-抓取分支"><a href="#2-抓取分支" class="headerlink" title="2. 抓取分支"></a>2. 抓取分支</h4><p>多人协作时，大家都会往<code>master</code>和<code>dev</code>分支上推送各自的修改。</p>
<p>现在，模拟一个你的小伙伴，可以在另一台电脑（注意要把SSH Key添加到GitHub）或者同一台电脑的另一个目录下克隆：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git clone git@github.com:michaelliao/learngit.git</span><br><span class="line">Cloning into &#x27;learngit&#x27;...</span><br><span class="line">remote: Counting objects: 40, done.</span><br><span class="line">remote: Compressing objects: 100% (21/21), done.</span><br><span class="line">remote: Total 40 (delta 14), reused 40 (delta 14), pack-reused 0</span><br><span class="line">Receiving objects: 100% (40/40), done.</span><br><span class="line">Resolving deltas: 100% (14/14), done.</span><br></pre></td></tr></table></figure>
<p>当你的小伙伴从远程库clone时，默认情况下，你的小伙伴只能看到本地的<code>master</code>分支。不信可以用<code>git branch</code>命令看看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch</span><br><span class="line">* master</span><br></pre></td></tr></table></figure>
<p>现在，你的小伙伴要在<code>dev</code>分支上开发，就必须创建远程<code>origin</code>的<code>dev</code>分支到本地，于是他用这个命令创建本地<code>dev</code>分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git checkout -b dev origin/dev</span><br></pre></td></tr></table></figure>
<p>现在，他就可以在<code>dev</code>上继续修改，然后，时不时地把<code>dev</code>分支<code>push</code>到远程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git add env.txt</span><br><span class="line"></span><br><span class="line">$ git commit -m &quot;add env&quot;</span><br><span class="line">[dev 7a5e5dd] add env</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line"> create mode 100644 env.txt</span><br><span class="line"></span><br><span class="line">$ git push origin dev</span><br><span class="line">Counting objects: 3, done.</span><br><span class="line">Delta compression using up to 4 threads.</span><br><span class="line">Compressing objects: 100% (2/2), done.</span><br><span class="line">Writing objects: 100% (3/3), 308 bytes | 308.00 KiB/s, done.</span><br><span class="line">Total 3 (delta 0), reused 0 (delta 0)</span><br><span class="line">To github.com:michaelliao/learngit.git</span><br><span class="line">   f52c633..7a5e5dd  dev -&gt; dev</span><br></pre></td></tr></table></figure>
<p>你的小伙伴已经向<code>origin/dev</code>分支推送了他的提交，而碰巧你也对同样的文件作了修改，并试图推送：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat env.txt</span><br><span class="line">env</span><br><span class="line"></span><br><span class="line">$ git add env.txt</span><br><span class="line"></span><br><span class="line">$ git commit -m &quot;add new env&quot;</span><br><span class="line">[dev 7bd91f1] add new env</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line"> create mode 100644 env.txt</span><br><span class="line"></span><br><span class="line">$ git push origin dev</span><br><span class="line">To github.com:michaelliao/learngit.git</span><br><span class="line"> ! [rejected]        dev -&gt; dev (non-fast-forward)</span><br><span class="line">error: failed to push some refs to &#x27;git@github.com:michaelliao/learngit.git&#x27;</span><br><span class="line">hint: Updates were rejected because the tip of your current branch is behind</span><br><span class="line">hint: its remote counterpart. Integrate the remote changes (e.g.</span><br><span class="line">hint: &#x27;git pull ...&#x27;) before pushing again.</span><br><span class="line">hint: See the &#x27;Note about fast-forwards&#x27; in &#x27;git push --help&#x27; for details.</span><br></pre></td></tr></table></figure>
<p>推送失败，因为你的小伙伴的最新提交和你试图推送的提交有冲突，解决办法也很简单，Git已经提示我们，先用<code>git pull</code>把最新的提交从<code>origin/dev</code>抓下来，然后，在本地合并，解决冲突，再推送：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git pull</span><br><span class="line">There is no tracking information for the current branch.</span><br><span class="line">Please specify which branch you want to merge with.</span><br><span class="line">See git-pull(1) for details.</span><br><span class="line"></span><br><span class="line">    git pull &lt;remote&gt; &lt;branch&gt;</span><br><span class="line"></span><br><span class="line">If you wish to set tracking information for this branch you can do so with:</span><br><span class="line"></span><br><span class="line">    git branch --set-upstream-to=origin/&lt;branch&gt; dev</span><br></pre></td></tr></table></figure>
<p><code>git pull</code>也失败了，原因是没有指定本地<code>dev</code>分支与远程<code>origin/dev</code>分支的链接，根据提示，设置<code>dev</code>和<code>origin/dev</code>的链接：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch --set-upstream-to=origin/dev dev</span><br><span class="line">Branch &#x27;dev&#x27; set up to track remote branch &#x27;dev&#x27; from &#x27;origin&#x27;.</span><br></pre></td></tr></table></figure>
<p>再pull：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git pull</span><br><span class="line">Auto-merging env.txt</span><br><span class="line">CONFLICT (add/add): Merge conflict in env.txt</span><br><span class="line">Automatic merge failed; fix conflicts and then commit the result.</span><br></pre></td></tr></table></figure>
<p>这回<code>git pull</code>成功，但是合并有冲突，需要手动解决，解决的方法和分支管理中的<a href="http://www.liaoxuefeng.com/wiki/896043488029600/900004111093344">解决冲突</a>完全一样。解决后，提交，再push：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git commit -m &quot;fix env conflict&quot;</span><br><span class="line">[dev 57c53ab] fix env conflict</span><br><span class="line"></span><br><span class="line">$ git push origin dev</span><br><span class="line">Counting objects: 6, done.</span><br><span class="line">Delta compression using up to 4 threads.</span><br><span class="line">Compressing objects: 100% (4/4), done.</span><br><span class="line">Writing objects: 100% (6/6), 621 bytes | 621.00 KiB/s, done.</span><br><span class="line">Total 6 (delta 0), reused 0 (delta 0)</span><br><span class="line">To github.com:michaelliao/learngit.git</span><br><span class="line">   7a5e5dd..57c53ab  dev -&gt; dev</span><br></pre></td></tr></table></figure>
<p>因此，多人协作的工作模式通常是这样：</p>
<ol>
<li>首先，可以试图用<code>git push origin &lt;branch-name&gt;</code>推送自己的修改；</li>
<li>如果推送失败，则因为远程分支比你的本地更新，需要先用<code>git pull</code>试图合并；</li>
<li>如果合并有冲突，则解决冲突，并在本地提交；</li>
<li>没有冲突或者解决掉冲突后，再用<code>git push origin &lt;branch-name&gt;</code>推送就能成功！</li>
</ol>
<p>如果<code>git pull</code>提示<code>no tracking information</code>，则说明本地分支和远程分支的链接关系没有创建，用命令<code>git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;</code>。</p>
<p>这就是多人协作的工作模式，一旦熟悉了，就非常简单。</p>
<p><strong>小结</strong></p>
<ul>
<li>查看远程库信息，使用<code>git remote -v</code>；</li>
<li>本地新建的分支如果不推送到远程，对其他人就是不可见的；</li>
<li>从本地推送分支，使用<code>git push origin branch-name</code>，如果推送失败，先用<code>git pull</code>抓取远程的新提交；</li>
<li>在本地创建和远程分支对应的分支，使用<code>git checkout -b branch-name origin/branch-name</code>，本地和远程分支的名称最好一致；</li>
<li>建立本地分支和远程分支的关联，使用<code>git branch --set-upstream branch-name origin/branch-name</code>；</li>
<li>从远程抓取分支，使用<code>git pull</code>，如果有冲突，要先处理冲突。</li>
</ul>
<h3 id="七、Rebase"><a href="#七、Rebase" class="headerlink" title="七、Rebase"></a>七、Rebase</h3><p>在上一节我们看到了，多人在同一个分支上协作时，很容易出现冲突。即使没有冲突，后push的童鞋不得不先pull，在本地合并，然后才能push成功。</p>
<p>每次合并再push后，分支变成了这样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --graph --pretty=oneline --abbrev-commit</span><br><span class="line">* d1be385 (HEAD -&gt; master, origin/master) init hello</span><br><span class="line">*   e5e69f1 Merge branch &#x27;dev&#x27;</span><br><span class="line">|\  </span><br><span class="line">| *   57c53ab (origin/dev, dev) fix env conflict</span><br><span class="line">| |\  </span><br><span class="line">| | * 7a5e5dd add env</span><br><span class="line">| * | 7bd91f1 add new env</span><br><span class="line">| |/  </span><br><span class="line">* |   12a631b merged bug fix 101</span><br><span class="line">|\ \  </span><br><span class="line">| * | 4c805e2 fix bug 101</span><br><span class="line">|/ /  </span><br><span class="line">* |   e1e9c68 merge with no-ff</span><br><span class="line">|\ \  </span><br><span class="line">| |/  </span><br><span class="line">| * f52c633 add merge</span><br><span class="line">|/  </span><br><span class="line">*   cf810e4 conflict fixed</span><br></pre></td></tr></table></figure>
<p>总之看上去很乱，有强迫症的童鞋会问：为什么Git的提交历史不能是一条干净的直线？</p>
<p>其实是可以做到的！</p>
<p>Git有一种称为rebase的操作，有人把它翻译成“变基”。</p>
<p>先不要随意展开想象。我们还是从实际问题出发，看看怎么把分叉的提交变成直线。</p>
<p>在和远程分支同步后，我们对<code>hello.py</code>这个文件做了两次提交。用<code>git log</code>命令看看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --graph --pretty=oneline --abbrev-commit</span><br><span class="line">* 582d922 (HEAD -&gt; master) add author</span><br><span class="line">* 8875536 add comment</span><br><span class="line">* d1be385 (origin/master) init hello</span><br><span class="line">*   e5e69f1 Merge branch &#x27;dev&#x27;</span><br><span class="line">|\  </span><br><span class="line">| *   57c53ab (origin/dev, dev) fix env conflict</span><br><span class="line">| |\  </span><br><span class="line">| | * 7a5e5dd add env</span><br><span class="line">| * | 7bd91f1 add new env</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>注意到Git用<code>(HEAD -&gt; master)</code>和<code>(origin/master)</code>标识出当前分支的HEAD和远程origin的位置分别是<code>582d922 add author</code>和<code>d1be385 init hello</code>，本地分支比远程分支快两个提交。</p>
<p>现在我们尝试推送本地分支：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push origin master</span><br><span class="line">To github.com:michaelliao/learngit.git</span><br><span class="line"> ! [rejected]        master -&gt; master (fetch first)</span><br><span class="line">error: failed to push some refs to &#x27;git@github.com:michaelliao/learngit.git&#x27;</span><br><span class="line">hint: Updates were rejected because the remote contains work that you do</span><br><span class="line">hint: not have locally. This is usually caused by another repository pushing</span><br><span class="line">hint: to the same ref. You may want to first integrate the remote changes</span><br><span class="line">hint: (e.g., &#x27;git pull ...&#x27;) before pushing again.</span><br><span class="line">hint: See the &#x27;Note about fast-forwards&#x27; in &#x27;git push --help&#x27; for details.</span><br></pre></td></tr></table></figure>
<p>很不幸，失败了，这说明有人先于我们推送了远程分支。按照经验，先pull一下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git pull</span><br><span class="line">remote: Counting objects: 3, done.</span><br><span class="line">remote: Compressing objects: 100% (1/1), done.</span><br><span class="line">remote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0</span><br><span class="line">Unpacking objects: 100% (3/3), done.</span><br><span class="line">From github.com:michaelliao/learngit</span><br><span class="line">   d1be385..f005ed4  master     -&gt; origin/master</span><br><span class="line"> * [new tag]         v1.0       -&gt; v1.0</span><br><span class="line">Auto-merging hello.py</span><br><span class="line">Merge made by the &#x27;recursive&#x27; strategy.</span><br><span class="line"> hello.py | 1 +</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>
<p>再用<code>git status</code>看看状态：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Your branch is ahead of &#x27;origin/master&#x27; by 3 commits.</span><br><span class="line">  (use &quot;git push&quot; to publish your local commits)</span><br><span class="line"></span><br><span class="line">nothing to commit, working tree clean</span><br></pre></td></tr></table></figure>
<p>加上刚才合并的提交，现在我们本地分支比远程分支超前3个提交。</p>
<p>用<code>git log</code>看看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --graph --pretty=oneline --abbrev-commit</span><br><span class="line">*   e0ea545 (HEAD -&gt; master) Merge branch &#x27;master&#x27; of github.com:michaelliao/learngit</span><br><span class="line">|\  </span><br><span class="line">| * f005ed4 (origin/master) set exit=1</span><br><span class="line">* | 582d922 add author</span><br><span class="line">* | 8875536 add comment</span><br><span class="line">|/  </span><br><span class="line">* d1be385 init hello</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>对强迫症童鞋来说，现在事情有点不对头，提交历史分叉了。如果现在把本地分支push到远程，有没有问题？</p>
<p>有！</p>
<p>什么问题？</p>
<p>不好看！</p>
<p>有没有解决方法？</p>
<p>有！</p>
<p>这个时候，rebase就派上了用场。我们输入命令<code>git rebase</code>试试：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git rebase</span><br><span class="line">First, rewinding head to replay your work on top of it...</span><br><span class="line">Applying: add comment</span><br><span class="line">Using index info to reconstruct a base tree...</span><br><span class="line">M	hello.py</span><br><span class="line">Falling back to patching base and 3-way merge...</span><br><span class="line">Auto-merging hello.py</span><br><span class="line">Applying: add author</span><br><span class="line">Using index info to reconstruct a base tree...</span><br><span class="line">M	hello.py</span><br><span class="line">Falling back to patching base and 3-way merge...</span><br><span class="line">Auto-merging hello.py</span><br></pre></td></tr></table></figure>
<p>输出了一大堆操作，到底是啥效果？再用<code>git log</code>看看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --graph --pretty=oneline --abbrev-commit</span><br><span class="line">* 7e61ed4 (HEAD -&gt; master) add author</span><br><span class="line">* 3611cfe add comment</span><br><span class="line">* f005ed4 (origin/master) set exit=1</span><br><span class="line">* d1be385 init hello</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>原本分叉的提交现在变成一条直线了！这种神奇的操作是怎么实现的？其实原理非常简单。我们注意观察，发现Git把我们本地的提交“挪动”了位置，放到了<code>f005ed4 (origin/master) set exit=1</code>之后，这样，整个提交历史就成了一条直线。rebase操作前后，最终的提交内容是一致的，但是，我们本地的commit修改内容已经变化了，它们的修改不再基于<code>d1be385 init hello</code>，而是基于<code>f005ed4 (origin/master) set exit=1</code>，但最后的提交<code>7e61ed4</code>内容是一致的。</p>
<p>这就是rebase操作的特点：把分叉的提交历史“整理”成一条直线，看上去更直观。缺点是本地的分叉提交已经被修改过了。</p>
<p>最后，通过push操作把本地分支推送到远程：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Mac:~/learngit michael$ git push origin master</span><br><span class="line">Counting objects: 6, done.</span><br><span class="line">Delta compression using up to 4 threads.</span><br><span class="line">Compressing objects: 100% (5/5), done.</span><br><span class="line">Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done.</span><br><span class="line">Total 6 (delta 2), reused 0 (delta 0)</span><br><span class="line">remote: Resolving deltas: 100% (2/2), completed with 1 local object.</span><br><span class="line">To github.com:michaelliao/learngit.git</span><br><span class="line">   f005ed4..7e61ed4  master -&gt; master</span><br></pre></td></tr></table></figure>
<p>再用<code>git log</code>看看效果：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --graph --pretty=oneline --abbrev-commit</span><br><span class="line">* 7e61ed4 (HEAD -&gt; master, origin/master) add author</span><br><span class="line">* 3611cfe add comment</span><br><span class="line">* f005ed4 set exit=1</span><br><span class="line">* d1be385 init hello</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>远程分支的提交历史也是一条直线。</p>
<p><strong>小结</strong></p>
<ul>
<li>rebase操作可以把本地未push的分叉提交历史整理成直线；</li>
<li>rebase的目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比。</li>
</ul>
<h3 id="八、标签管理"><a href="#八、标签管理" class="headerlink" title="八、标签管理"></a>八、标签管理</h3><p>发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。</p>
<p>Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。</p>
<p>Git有commit，为什么还要引入tag？</p>
<p>“请把上周一的那个版本打包发布，commit号是6a5819e…”</p>
<p>“一串乱七八糟的数字不好找！”</p>
<p>如果换一个办法：</p>
<p>“请把上周一的那个版本打包发布，版本号是v1.2”</p>
<p>“好的，按照tag v1.2查找commit就行！”</p>
<p>所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。</p>
<h4 id="1-创建标签"><a href="#1-创建标签" class="headerlink" title="1. 创建标签"></a>1. 创建标签</h4><p>在Git中打标签非常简单，首先，切换到需要打标签的分支上：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git branch</span><br><span class="line">* dev</span><br><span class="line">  master</span><br><span class="line">$ git checkout master</span><br><span class="line">Switched to branch &#x27;master&#x27;</span><br></pre></td></tr></table></figure>
<p>然后，敲命令<code>git tag &lt;name&gt;</code>就可以打一个新标签：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git tag v1.0</span><br></pre></td></tr></table></figure>
<p>可以用命令<code>git tag</code>查看所有标签：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git tag</span><br><span class="line">v1.0</span><br></pre></td></tr></table></figure>
<p>默认标签是打在最新提交的commit上的。有时候，如果忘了打标签，比如，现在已经是周五了，但应该在周一打的标签没有打，怎么办？</p>
<p>方法是找到历史提交的commit id，然后打上就可以了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git log --pretty=oneline --abbrev-commit</span><br><span class="line">12a631b (HEAD -&gt; master, tag: v1.0, origin/master) merged bug fix 101</span><br><span class="line">4c805e2 fix bug 101</span><br><span class="line">e1e9c68 merge with no-ff</span><br><span class="line">f52c633 add merge</span><br><span class="line">cf810e4 conflict fixed</span><br><span class="line">5dc6824 &amp; simple</span><br><span class="line">14096d0 AND simple</span><br><span class="line">b17d20e branch test</span><br><span class="line">d46f35e remove test.txt</span><br><span class="line">b84166e add test.txt</span><br><span class="line">519219b git tracks changes</span><br><span class="line">e43a48b understand how stage works</span><br><span class="line">1094adb append GPL</span><br><span class="line">e475afc add distributed</span><br><span class="line">eaadf4e wrote a readme file</span><br></pre></td></tr></table></figure>
<p>比方说要对<code>add merge</code>这次提交打标签，它对应的commit id是<code>f52c633</code>，敲入命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git tag v0.9 f52c633</span><br></pre></td></tr></table></figure>
<p>再用命令<code>git tag</code>查看标签：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git tag</span><br><span class="line">v0.9</span><br><span class="line">v1.0</span><br></pre></td></tr></table></figure>
<p>注意，标签不是按时间顺序列出，而是按字母排序的。可以用<code>git show &lt;tagname&gt;</code>查看标签信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git show v0.9</span><br><span class="line">commit f52c63349bc3c1593499807e5c8e972b82c8f286 (tag: v0.9)</span><br><span class="line">Author: Michael Liao &lt;askxuefeng@gmail.com&gt;</span><br><span class="line">Date:   Fri May 18 21:56:54 2018 +0800</span><br><span class="line"></span><br><span class="line">    add merge</span><br><span class="line"></span><br><span class="line">diff --git a/readme.txt b/readme.txt</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>可以看到，<code>v0.9</code>确实打在<code>add merge</code>这次提交上。</p>
<p>还可以创建带有说明的标签，用<code>-a</code>指定标签名，<code>-m</code>指定说明文字：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git tag -a v0.1 -m &quot;version 0.1 released&quot; 1094adb</span><br></pre></td></tr></table></figure>
<p>用命令<code>git show &lt;tagname&gt;</code>可以看到说明文字：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git show v0.1</span><br><span class="line">tag v0.1</span><br><span class="line">Tagger: Michael Liao &lt;askxuefeng@gmail.com&gt;</span><br><span class="line">Date:   Fri May 18 22:48:43 2018 +0800</span><br><span class="line"></span><br><span class="line">version 0.1 released</span><br><span class="line"></span><br><span class="line">commit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (tag: v0.1)</span><br><span class="line">Author: Michael Liao &lt;askxuefeng@gmail.com&gt;</span><br><span class="line">Date:   Fri May 18 21:06:15 2018 +0800</span><br><span class="line"></span><br><span class="line">    append GPL</span><br><span class="line"></span><br><span class="line">diff --git a/readme.txt b/readme.txt</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p> 注意：标签总是和某个commit挂钩。如果这个commit既出现在master分支，又出现在dev分支，那么在这两个分支上都可以看到这个标签。 </p>
<p><strong>小结</strong></p>
<ul>
<li>命令<code>git tag &lt;tagname&gt;</code>用于新建一个标签，默认为<code>HEAD</code>，也可以指定一个commit id；</li>
<li>命令<code>git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;</code>可以指定标签信息；</li>
<li>命令<code>git tag</code>可以查看所有标签。</li>
</ul>
<h4 id="2-操作标签"><a href="#2-操作标签" class="headerlink" title="2. 操作标签"></a>2. 操作标签</h4><p>如果标签打错了，也可以删除：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git tag -d v0.1</span><br><span class="line">Deleted tag &#x27;v0.1&#x27; (was f15b0dd)</span><br></pre></td></tr></table></figure>
<p>因为创建的标签都只存储在本地，不会自动推送到远程。所以，打错的标签可以在本地安全删除。</p>
<p>如果要推送某个标签到远程，使用命令<code>git push origin &lt;tagname&gt;</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push origin v1.0</span><br><span class="line">Total 0 (delta 0), reused 0 (delta 0)</span><br><span class="line">To github.com:michaelliao/learngit.git</span><br><span class="line"> * [new tag]         v1.0 -&gt; v1.0</span><br></pre></td></tr></table></figure>
<p>或者，一次性推送全部尚未推送到远程的本地标签：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push origin --tags</span><br><span class="line">Total 0 (delta 0), reused 0 (delta 0)</span><br><span class="line">To github.com:michaelliao/learngit.git</span><br><span class="line"> * [new tag]         v0.9 -&gt; v0.9</span><br></pre></td></tr></table></figure>
<p>如果标签已经推送到远程，要删除远程标签就麻烦一点，先从本地删除：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git tag -d v0.9</span><br><span class="line">Deleted tag &#x27;v0.9&#x27; (was f52c633)</span><br></pre></td></tr></table></figure>
<p>然后，从远程删除。删除命令也是push，但是格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git push origin :refs/tags/v0.9</span><br><span class="line">To github.com:michaelliao/learngit.git</span><br><span class="line"> - [deleted]         v0.9</span><br></pre></td></tr></table></figure>
<p>要看看是否真的从远程库删除了标签，可以登陆GitHub查看。</p>
<p><strong>小结</strong></p>
<ul>
<li>命令<code>git push origin &lt;tagname&gt;</code>可以推送一个本地标签；</li>
<li>命令<code>git push origin --tags</code>可以推送全部未推送过的本地标签；</li>
<li>命令<code>git tag -d &lt;tagname&gt;</code>可以删除一个本地标签；</li>
<li>命令<code>git push origin :refs/tags/&lt;tagname&gt;</code>可以删除一个远程标签。</li>
</ul>
<h3 id="九、Github"><a href="#九、Github" class="headerlink" title="九、Github"></a>九、Github</h3><p>我们一直用GitHub作为免费的远程仓库，如果是个人的开源项目，放到GitHub上是完全没有问题的。其实GitHub还是一个开源协作社区，通过GitHub，既可以让别人参与你的开源项目，也可以参与别人的开源项目。</p>
<p>在GitHub出现以前，开源项目开源容易，但让广大人民群众参与进来比较困难，因为要参与，就要提交代码，而给每个想提交代码的群众都开一个账号那是不现实的，因此，群众也仅限于报个bug，即使能改掉bug，也只能把diff文件用邮件发过去，很不方便。</p>
<p>但是在GitHub上，利用Git极其强大的克隆和分支功能，广大人民群众真正可以第一次自由参与各种开源项目了。</p>
<p>如何参与一个开源项目呢？比如人气极高的bootstrap项目，这是一个非常强大的CSS框架，你可以访问它的项目主页<a href="https://github.com/twbs/bootstrap">https://github.com/twbs/bootstrap</a>，点“Fork”就在自己的账号下克隆了一个bootstrap仓库，然后，从自己的账号下clone：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone git@github.com:michaelliao/bootstrap.git</span><br></pre></td></tr></table></figure>
<p>一定要从自己的账号下clone仓库，这样你才能推送修改。如果从bootstrap的作者的仓库地址<code>git@github.com:twbs/bootstrap.git</code>克隆，因为没有权限，你将不能推送修改。</p>
<p>Bootstrap的官方仓库<code>twbs/bootstrap</code>、你在GitHub上克隆的仓库<code>my/bootstrap</code>，以及你自己克隆到本地电脑的仓库，他们的关系就像下图显示的那样：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">┌─ GitHub ────────────────────────────────────┐</span><br><span class="line">│                                             │</span><br><span class="line">│ ┌─────────────────┐     ┌─────────────────┐ │</span><br><span class="line">│ │ twbs/bootstrap  │────&gt;│  my/bootstrap   │ │</span><br><span class="line">│ └─────────────────┘     └─────────────────┘ │</span><br><span class="line">│                                  ▲          │</span><br><span class="line">└──────────────────────────────────┼──────────┘</span><br><span class="line">                                   ▼</span><br><span class="line">                          ┌─────────────────┐</span><br><span class="line">                          │ local/bootstrap │</span><br><span class="line">                          └─────────────────┘</span><br></pre></td></tr></table></figure>
<p>如果你想修复bootstrap的一个bug，或者新增一个功能，立刻就可以开始干活，干完后，往自己的仓库推送。</p>
<p>如果你希望bootstrap的官方库能接受你的修改，你就可以在GitHub上发起一个pull request。当然，对方是否接受你的pull request就不一定了。</p>
<p>如果你没能力修改bootstrap，但又想要试一把pull request，那就Fork一下我的仓库：<a href="https://github.com/michaelliao/learngit">https://github.com/michaelliao/learngit</a>，创建一个<code>your-github-id.txt</code>的文本文件，写点自己学习Git的心得，然后推送一个pull request给我，我会视心情而定是否接受。</p>
<p><strong>小结</strong></p>
<ul>
<li>在GitHub上，可以任意Fork开源仓库；</li>
<li>自己拥有Fork后的仓库的读写权限；</li>
<li>可以推送pull request给官方仓库来贡献代码。</li>
</ul>
<h3 id="十、使用Gitee"><a href="#十、使用Gitee" class="headerlink" title="十、使用Gitee"></a>十、使用Gitee</h3><p>使用GitHub时，国内的用户经常遇到的问题是访问速度太慢，有时候还会出现无法连接的情况（原因你懂的）。</p>
<p>如果我们希望体验Git飞一般的速度，可以使用国内的Git托管服务——<a href="https://gitee.com?utm_source=blog_lxf">Gitee</a>（<a href="https://gitee.com?utm_source=blog_lxf">gitee.com</a>）。</p>
<p>和GitHub相比，Gitee也提供免费的Git仓库。此外，还集成了代码质量检测、项目演示等功能。对于团队协作开发，Gitee还提供了项目管理、代码托管、文档管理的服务，5人以下小团队免费。</p>
<p> Gitee的免费版本也提供私有库功能，只是有5人的成员上限。 </p>
<p>使用Gitee和使用GitHub类似，我们在Gitee上注册账号并登录后，需要先上传自己的SSH公钥。选择右上角用户头像 -&gt; 菜单“修改资料”，然后选择“SSH公钥”，填写一个便于识别的标题，然后把用户主目录下的<code>.ssh/id_rsa.pub</code>文件的内容粘贴进去，点击“确定”即可完成并看到刚才添加的Key。</p>
<p>如果我们已经有了一个本地的git仓库（例如，一个名为learngit的本地库），如何把它关联到Gitee的远程库上呢？</p>
<p>首先，我们在Gitee上创建一个新的项目，选择右上角用户头像 -&gt; 菜单“控制面板”，然后点击“创建项目”。</p>
<p>项目名称最好与本地库保持一致：</p>
<p>然后，我们在本地库上使用命令<code>git remote add</code>把它和Gitee的远程库关联：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote add origin git@gitee.com:liaoxuefeng/learngit.git</span><br></pre></td></tr></table></figure>
<p>之后，就可以正常地用<code>git push</code>和<code>git pull</code>推送了！</p>
<p>如果在使用命令<code>git remote add</code>时报错：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote add origin git@gitee.com:liaoxuefeng/learngit.git</span><br><span class="line">fatal: remote origin already exists.</span><br></pre></td></tr></table></figure>
<p>这说明本地库已经关联了一个名叫<code>origin</code>的远程库，此时，可以先用<code>git remote -v</code>查看远程库信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br><span class="line">origin	git@github.com:michaelliao/learngit.git (fetch)</span><br><span class="line">origin	git@github.com:michaelliao/learngit.git (push)</span><br></pre></td></tr></table></figure>
<p>可以看到，本地库已经关联了<code>origin</code>的远程库，并且，该远程库指向GitHub。</p>
<p>我们可以删除已有的GitHub远程库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote rm origin</span><br></pre></td></tr></table></figure>
<p>再关联Gitee的远程库（注意路径中需要填写正确的用户名）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote add origin git@gitee.com:liaoxuefeng/learngit.git</span><br></pre></td></tr></table></figure>
<p>此时，我们再查看远程库信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br><span class="line">origin	git@gitee.com:liaoxuefeng/learngit.git (fetch)</span><br><span class="line">origin	git@gitee.com:liaoxuefeng/learngit.git (push)</span><br></pre></td></tr></table></figure>
<p>现在可以看到，origin已经被关联到Gitee的远程库了。通过<code>git push</code>命令就可以把本地库推送到Gitee上。</p>
<p>有的小伙伴又要问了，一个本地库能不能既关联GitHub，又关联Gitee呢？</p>
<p>答案是肯定的，因为git本身是分布式版本控制系统，可以同步到另外一个远程库，当然也可以同步到另外两个远程库。</p>
<p>使用多个远程库时，我们要注意，git给远程库起的默认名称是<code>origin</code>，如果有多个远程库，我们需要用不同的名称来标识不同的远程库。</p>
<p>仍然以<code>learngit</code>本地库为例，我们先删除已关联的名为<code>origin</code>的远程库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote rm origin</span><br></pre></td></tr></table></figure>
<p>然后，先关联GitHub的远程库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote add github git@github.com:michaelliao/learngit.git</span><br></pre></td></tr></table></figure>
<p>注意，远程库的名称叫<code>github</code>，不叫<code>origin</code>了。</p>
<p>接着，再关联Gitee的远程库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote add gitee git@gitee.com:liaoxuefeng/learngit.git</span><br></pre></td></tr></table></figure>
<p>同样注意，远程库的名称叫<code>gitee</code>，不叫<code>origin</code>。</p>
<p>现在，我们用<code>git remote -v</code>查看远程库信息，可以看到两个远程库：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git remote -v</span><br><span class="line">gitee	git@gitee.com:liaoxuefeng/learngit.git (fetch)</span><br><span class="line">gitee	git@gitee.com:liaoxuefeng/learngit.git (push)</span><br><span class="line">github	git@github.com:michaelliao/learngit.git (fetch)</span><br><span class="line">github	git@github.com:michaelliao/learngit.git (push)</span><br></pre></td></tr></table></figure>
<p>如果要推送到GitHub，使用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git push github master</span><br></pre></td></tr></table></figure>
<p>如果要推送到Gitee，使用命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git push gitee master</span><br></pre></td></tr></table></figure>
<p>这样一来，我们的本地库就可以同时与多个远程库互相同步：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">┌─────────┐ ┌─────────┐</span><br><span class="line">│ GitHub  │ │  Gitee  │</span><br><span class="line">└─────────┘ └─────────┘</span><br><span class="line">     ▲           ▲</span><br><span class="line">     └─────┬─────┘</span><br><span class="line">           │</span><br><span class="line">    ┌─────────────┐</span><br><span class="line">    │ Local Repo  │</span><br><span class="line">    └─────────────┘</span><br></pre></td></tr></table></figure>
<p>Gitee也同样提供了Pull request功能，可以让其他小伙伴参与到开源项目中来。你可以通过Fork我的仓库：<a href="https://gitee.com/liaoxuefeng/learngit?utm_source=blog_lxf">https://gitee.com/liaoxuefeng/learngit</a>，创建一个<code>your-gitee-id.txt</code>的文本文件， 写点自己学习Git的心得，然后推送一个pull request给我，这个仓库会在Gitee和GitHub做双向同步。</p>
<h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><p>本博客主要内容转自廖雪峰大神的Git学习版块，原文链接：<a href="https://www.liaoxuefeng.com/wiki/896043488029600">廖雪峰Git学习</a><br>如果涉嫌侵权，著作权人可以联系我，我会立即删除。 </p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>Git学习</category>
      </categories>
      <tags>
        <tag>学习</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>休闲事情</title>
    <url>/2022/06/11/leisure-activities/</url>
    <content><![CDATA[<h4 id="1-学习提升"><a href="#1-学习提升" class="headerlink" title="1 学习提升"></a>1 学习提升</h4><h5 id="1-1-读书"><a href="#1-1-读书" class="headerlink" title="1.1 读书"></a>1.1 读书</h5><p>总说“要么读书，要么旅行，身体和心灵总有一个在路上”。作为自我认识深刻的人呀 ，闲来无事就会去看书。是打发，也是在别人的文字里找到有共鸣的我。</p>
<h5 id="1-2-图书馆"><a href="#1-2-图书馆" class="headerlink" title="1.2 图书馆"></a>1.2 图书馆</h5><p>一定要远离宿舍，图书馆能做的事情不要太多啊！看书上网画画写字，我还会逛书架，就像逛超市一样，发现一些奇葩的书，一些别人记在边角的小情绪，或者从一个奇怪的方向看书等等，意外得像一个礼物不是么?</p>
<h5 id="1-3-练字"><a href="#1-3-练字" class="headerlink" title="1.3 练字"></a>1.3 练字</h5><p>练字的好处太多了就不列举了，一个人就应该多做做有（zhuang）益(zhuang)身(bi)心(ge)的事呀</p>
<h5 id="1-4-口才"><a href="#1-4-口才" class="headerlink" title="1.4 口才"></a>1.4 口才</h5><p>每天花1个小时读新闻，在读的过程中不仅能够帮助我们练习口才，还能够梳理逻辑。因为新闻和文章不一样，有着严谨的逻辑。每次读完以后，尝试着用自己的话表达出来，坚持下去，你的表达能力会越来越好。</p>
<h5 id="1-5-了解行业资讯"><a href="#1-5-了解行业资讯" class="headerlink" title="1.5 了解行业资讯"></a>1.5 了解行业资讯</h5><p>曾听过一句话：“在了解事实的过程中，我们会筛选出许多资讯，这些资讯会引导我们做出决定”。每天花十分钟了解资讯，观察行业的最新情况，以便进行深度评价。判断所处行业是朝阳还是夕阳产业，或转行或深耕，规划职业生涯，定位日后的发展方向。</p>
<h5 id="1-6-参加活动，见识牛人"><a href="#1-6-参加活动，见识牛人" class="headerlink" title="1.6 参加活动，见识牛人"></a>1.6 参加活动，见识牛人</h5><p>在一个熟悉的圈子待久了，我们常常受限于自己的视野。<br>想要扩展自己的眼界，先要认识到自己圈子的不足。<br>多去参加活动，接触更多的牛人，把他们的经验运用在工作、生活上，能够规避很多坑。<br>另外，接触新鲜的人和物，就能看到自己跟别人的差距，从而促使你自我成长。</p>
<h5 id="1-7-学会理财"><a href="#1-7-学会理财" class="headerlink" title="1.7 学会理财"></a>1.7 学会理财</h5><p>理财的目的就是钱生钱，也就说所说的资产增值。要合理安排自己现有的资产并进行分配。然后财富才能进入一个不断积累的过程，财富积累后自然就可以做一些自己想做的事情，财富增值并不是最终的理财目标，而是我们达到人生目标的必经之路。 　</p>
<h5 id="1-8-学习PS、PR"><a href="#1-8-学习PS、PR" class="headerlink" title="1.8 学习PS、PR"></a>1.8 学习PS、PR</h5><p>随着自媒体的发展，越来越多人开始做博主，视频博主最紧要的事情九四剪辑视频，你学会可以学习商单，一单十几或七八百，不过对技术要求很多，随着佣金越高做出的视频效果自然非同寻常了。当然当有一天，你的其他知识学习很好时，你可以随时离开不喜欢的公司，随时成为一位自由工作者。</p>
<h4 id="2-梳理总结与反思"><a href="#2-梳理总结与反思" class="headerlink" title="2 梳理总结与反思"></a>2 梳理总结与反思</h4><h5 id="2-1-梳理总结"><a href="#2-1-梳理总结" class="headerlink" title="2.1 梳理总结"></a>2.1 梳理总结</h5><p>整理自己电脑、网盘已经各种在线网站(博客)中的文件夹/文档，定时整理邮箱邮件，不然囤积太多。</p>
<h5 id="2-2-规划时间"><a href="#2-2-规划时间" class="headerlink" title="2.2 规划时间"></a>2.2 规划时间</h5><p>管理自己的时间，花时间去复盘，做做计划，让自己更加清楚，做做总结呗，看看实际和梦想的差距，再进行调整，还可以写点字。此外，还要好好想想最近有什么事情需要做，及时去做，不拖延。</p>
<h5 id="2-3-调整心态，摒弃杂念"><a href="#2-3-调整心态，摒弃杂念" class="headerlink" title="2.3 调整心态，摒弃杂念"></a>2.3 调整心态，摒弃杂念</h5><p>人生在世，难免面对许多纷纷扰扰。心态烦躁，眼前的小事也就成了大事；心静下来，许多大事也就化成了小事。<br>面对问题时，保持从容镇定，心静则清，心清则明，这是最好的生活态度，也是人生至高的境界。<br>人生短暂，也许有些事我们无法改变，但如何提升自己却可以由我们自己决定。<br>请相信，那些你流过的汗、读过的书、走过的路、行过的善，最终都会回馈到你自己的身上。</p>
<h5 id="2-4-反省自己"><a href="#2-4-反省自己" class="headerlink" title="2.4 反省自己"></a>2.4 反省自己</h5><p>学习学习心理学，定时反省自己，找找自己的优缺点，针对缺点重点改进。</p>
<h5 id="2-5-输出内容"><a href="#2-5-输出内容" class="headerlink" title="2.5 输出内容"></a>2.5 输出内容</h5><p>我自己会定期在知乎上面写一些回答，把自己学到的知识，用相对通俗易懂的话去表达出来。即费曼学习法:当你将一些专业知识讲给别人听，别人能听得懂，就说明你已经对知识的理解非常扎实了。<br>写篇博客文章。一点点的闲暇时间是更新博客文章的绝佳时机，也不完也可以分几次，累积。</p>
<h4 id="3-生活娱乐"><a href="#3-生活娱乐" class="headerlink" title="3 生活娱乐"></a>3 生活娱乐</h4><h5 id="3-1-电影刷剧"><a href="#3-1-电影刷剧" class="headerlink" title="3.1 电影刷剧"></a>3.1 电影刷剧</h5><h5 id="3-2-收纳清理"><a href="#3-2-收纳清理" class="headerlink" title="3.2 收纳清理"></a>3.2 收纳清理</h5><p>要时常清理一下自己的生活环境，定时收纳平时乱放的一些东西，此外还要时常打扫卫生、清理自己的办公桌，一个干净卫生的环境、一张整洁的桌子使你更有效率，你只需要把办公桌上或周围地上积累的各式各样垃圾都简单清理一下就完事，同样的简单、不花多少时间。</p>
<h5 id="3-3-打电话。"><a href="#3-3-打电话。" class="headerlink" title="3.3 打电话。"></a>3.3 打电话。</h5><p>列一份你需要打电话清单，记下号码并且随身带上。不管在桌前还是在路上，你都可以在很短时间内解决一些通话。</p>
<h4 id="4-休闲探索"><a href="#4-休闲探索" class="headerlink" title="4 休闲探索"></a>4 休闲探索</h4><h5 id="4-1-城市徒步"><a href="#4-1-城市徒步" class="headerlink" title="4.1 城市徒步"></a>4.1 城市徒步</h5><p>不管是不是从小长到大的城市，这个城市一定有你还没到过的角落，拿着地图就像在游戏中开拓新领域一样的去探索吧，想象每走过一个地方那个地方的黑色就没有了，感觉很有趣啊!尤其是在上海这种地方，一不小心就会发现什么故居或者优秀历史建筑，比如我到过的老舍故居张爱玲楼下的咖啡厅丰子恺故居林微因住过的小楼等等，会有很多意外收获哦~</p>
<h5 id="4-2-夜游"><a href="#4-2-夜游" class="headerlink" title="4.2 夜游"></a>4.2 夜游</h5><p>城市的夜晚和白天完全是两个样子你造嘛!!很多有格调的店子可能只有五六点以后才开，很多露天的小演奏，不一样的夜景，还有各种跑车。。。一个人觉得不安全的话可以豆瓣同城啊，有很多夜游团伙(团伙? )，一起看凌晨的城市，安静或者喧嚣的，多么有趣!</p>
<h5 id="4-3-看海"><a href="#4-3-看海" class="headerlink" title="4.3 看海"></a>4.3 看海</h5><p>我承认大家一起在海边玩真的很高兴，可偶然自己一个人去的时候，是完全不一样的feel~还能独自感受大海浩浩荡荡，涛声阵阵</p>
<h4 id="5-运动健身"><a href="#5-运动健身" class="headerlink" title="5 运动健身"></a>5 运动健身</h4><p>跑步、游泳、健身，运动的时候一个人也很开心而且更自在啊!!!不用担心有人看到自己面红耳赤的样子，也不用担心形象不佳，爱怎么动怎么动！</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>生活休闲</category>
        <category>闲暇时光</category>
      </categories>
      <tags>
        <tag>休闲</tag>
      </tags>
  </entry>
  <entry>
    <title>个人博客搭建历程</title>
    <url>/2022/01/18/my-first-bolg/</url>
    <content><![CDATA[<h3 id="〇-简要介绍"><a href="#〇-简要介绍" class="headerlink" title="〇. 简要介绍"></a>〇. 简要介绍</h3><p>本博客的搭建不需要什么技术，而且自己截止到当前时间对Git和Github的使用也刚学，所以只是参考网上博客和B站的教学视频来做的。</p>
<h3 id="一-部署Github-Pages"><a href="#一-部署Github-Pages" class="headerlink" title="一. 部署Github Pages"></a>一. 部署Github Pages</h3><p>在自己的Github上创建一个新的<strong>Repositories</strong>，注意命名规则是——Github账号名.github.io。</p>
<h3 id="二-安装Hexo"><a href="#二-安装Hexo" class="headerlink" title="二. 安装Hexo"></a>二. 安装Hexo</h3><h4 id="1-插入注解"><a href="#1-插入注解" class="headerlink" title="1. 插入注解"></a>1. 插入注解</h4><h5 id="1-常见的博客搭建方法"><a href="#1-常见的博客搭建方法" class="headerlink" title="(1) 常见的博客搭建方法"></a>(1) 常见的博客搭建方法</h5><p>据我目前所知，搭建博客总体上分有两种方法：</p>
<ul>
<li>第三方平台：在现有的博客网站、论坛或社区上注册个人主页。如知乎，掘金，简书，segmentFault，csdn，博客园等。特点：无技术门槛，可专注于内容。</li>
<li>静态网站生成技术：在终端执行命令快速生成静态网站，如 Jekyll、Hugo、Hexo等。然后通过 Github Pages、Coding 等展示。特点：通过插件可增加评论、搜索、分析等功能。有多种主题可选。在本地以特定格式书写文章，放置在指定的文件夹。每次发布或修改文章都需要重新生成和部署网站。</li>
<li>其他：<a href="https://juejin.cn/post/6951679886557708302">参考链接-如何搭建个人博客 ：几种方案比较 + 个人实现</a> </li>
</ul>
<h5 id="2-静态网站生成技术比较"><a href="#2-静态网站生成技术比较" class="headerlink" title="(2) 静态网站生成技术比较"></a>(2) 静态网站生成技术比较</h5><p>在该方法当中，也与许多可供选择的方案，像 Jekyll、Hugo、Hexo、Typecho、Wordpress等等，那么它们各有什么优缺点呢，这里我就不说了(不是我的重点)，请参考链接：<a href="https://blog.laoda.de/archives/blog-choosing">Hexo还是Hugo？Typecho还是Wordpress？读完这篇或许你就有答案了！</a></p>
<h4 id="2-安装相应软件"><a href="#2-安装相应软件" class="headerlink" title="2. 安装相应软件"></a>2. 安装相应软件</h4><h5 id="1-安装Node-js"><a href="#1-安装Node-js" class="headerlink" title="(1) 安装Node.js"></a>(1) 安装Node.js</h5><p><a href="https://nodejs.org/zh-cn/">Node.js的官方网站</a>，下载好了之后直接安装，安装过程中会出现</p>
<p><img src="https://pic.imgdb.cn/item/61e64e022ab3f51d91dc2599.png" alt="&lt;font color=red&gt;图1&lt;/font&gt;  Node安装"></p>
<p>要点第四个，添加到Path(环境变量)。</p>
<h5 id="2-Git安装"><a href="#2-Git安装" class="headerlink" title="(2) Git安装"></a>(2) Git安装</h5><p>Git的安装则是傻瓜式安装即可，虽然步骤多，但是按照默认的来就行；</p>
<p>好的，现在已经全部准备完毕，下面简单测试一下看看是否安装成功。</p>
<h5 id="3-测试"><a href="#3-测试" class="headerlink" title="(3) 测试"></a>(3) 测试</h5><p> 为了验证我们的Node.js和Git是否安装完毕，可以在CMD（win+R，输入“cmd”即可打开）中输入两条命令查看： </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">node -v</span><br><span class="line">git version</span><br></pre></td></tr></table></figure>
<p>若两条命令没有报错，而且显示了版本号，则安装成功！</p>
<h4 id="3-Hexo本地化"><a href="#3-Hexo本地化" class="headerlink" title="3. Hexo本地化"></a>3. Hexo本地化</h4><h5 id="1-创建新文件夹"><a href="#1-创建新文件夹" class="headerlink" title="(1) 创建新文件夹"></a>(1) 创建新文件夹</h5><p>这个新文件夹在之后会保存Hexo的文件以及你的博客相关内容，十分重要，我是建立在E盘下新文件夹I.myBlog。</p>
<h5 id="2-安装Hexo"><a href="#2-安装Hexo" class="headerlink" title="(2) 安装Hexo"></a>(2) 安装Hexo</h5><p>打开Windows中使用cmd命令打开命令行，然后更改命令行目录到上一步创建的新文件夹中，然后执行下面的命令安装Hexo：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure>
<p>注意安装过程中网络不要断开，安装完成后文件夹还是一个空的状态，下一步要初始化。</p>
<h5 id="3-初始化Hexo"><a href="#3-初始化Hexo" class="headerlink" title="(3) 初始化Hexo"></a>(3) 初始化Hexo</h5><p> 执行以下代码完成初始化</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure>
<p>这时候文件夹就出现了新文件，不再是空的，文件主要有以下几种：</p>
<ul>
<li>.deploty_git：上传到Github的最终文件，如果不使用Git，则不会生成</li>
<li>.github：一个神秘文件夹</li>
<li>node_modules：一个神秘文件夹</li>
<li>public：上传到Github的最终文件，不安装Git时可以使用这种方法直接部署</li>
<li>scaffolds：存放创建新文章时的模板文件</li>
<li>theme：主题存放文件夹</li>
<li>post：博客文章存放的地方</li>
</ul>
<h3 id="三-基础配置与指令"><a href="#三-基础配置与指令" class="headerlink" title="三. 基础配置与指令"></a>三. 基础配置与指令</h3><h4 id="1-主题配置-美化"><a href="#1-主题配置-美化" class="headerlink" title="1. 主题配置(美化)"></a>1. 主题配置(美化)</h4><p>进入博客文件夹根目录，打开<strong>_config.yml</strong>文件，然后自行进行修改。</p>
<p><img src="https://pic.imgdb.cn/item/61e64e022ab3f51d91dc258c.png" alt="&lt;font color=red&gt; 图2&lt;/font&gt;  _config文件截图示意"></p>
<p>这里是一些基本的，比如说你的网站名（第六行）、描述（第八行）、作者（第十行）、语言（第十一行）等都可以进行自主修改。</p>
<p><img src="https://pic.imgdb.cn/item/61e64e022ab3f51d91dc25a2.png" alt="&lt;font color=red&gt; 图3&lt;/font&gt;  url修改"></p>
<p>如图3所示，这里url的修改十分重要，一定要记得修改。要改成你将来要部署的网站！！！比如你要用GitHub Pages部署，就必须是那一节你部署过的网站链接！！！ </p>
<p><img src="https://pic.imgdb.cn/item/61e64e022ab3f51d91dc2594.png" alt="&lt;font color=red&gt; 图4&lt;/font&gt;  git链接修改"></p>
<p><strong><font color="red">注意，下面这几处修改十分重要！！！</font></strong></p>
<ul>
<li>第100行的主题名改为之前让你复制的那个文件夹名(记得空格，在你没有下载新主题之前不用管，默认即可)；</li>
<li>第105行“type”后面填写“git”；</li>
<li>第106行加上“repo”，然后加上链接——不是GitHub Pages的链接，是GitHub Pages的库的链接！！</li>
</ul>
<blockquote>
<p>注意：这里由于Github在2021年进行了一次更新，应该是不能再使用https协议了，转而应该示意SSH，但是我这里使用了https好像也没什么问题，其实我自己现在对Git、GitHub的使用也还是模模糊糊，对一些操作、概念(如：密钥、公钥、令牌等)不明白，所以也就是稀里糊涂，后面我还要仔细学习学习Git的知识，如果你的使用https不行，建议参考链接进行设置——<a href="https://vabzjaby.github.io/2021/10/05/pwh/#%E7%AC%AC%E5%9B%9B%E6%AD%A5-%E7%BC%96%E8%BE%91">如何在手机端搭建并部署Hexo - 听话的便当 - 个人博客</a></p>
</blockquote>
<ul>
<li>第107行加上“branch”，名字起main。注意，这里与Github在2021年进行了一次更新有关，之前应该是填写master，现在填写main即可；</li>
</ul>
<p>完成这些后，可以再次用<code>hexo s</code>这个指令查看预览，发现已经更新主题了。</p>
<h4 id="2-Hexo的常用命令"><a href="#2-Hexo的常用命令" class="headerlink" title="2. Hexo的常用命令"></a>2. Hexo的常用命令</h4><ul>
<li><code>hexo s</code>：在本地演示网站</li>
<li><code>hexo g</code>：在本地渲染静态页面(就是将md转换为html)</li>
<li><code>hexo d</code>：把静态页面一键上传到Github Pages</li>
<li><code>hexo new</code>：<ul>
<li><code>标题</code>(直接加标题则是在source/_posts文件夹生成—个”标题.md”文件作为文章，建议生成新文章和页面时最好用阿拉伯数字或者英文，这样链接不会很长)</li>
<li><code>page+标题</code>(标题前加page则是生成一个新页面，hexo刚开始只有默认的首页和归档页，创建其他页面，例如标签、关于等页面都需要此指令生成)</li>
<li><code>post+标题</code>(在_config.yml中将post_asset_folder设为true之后hexo new post “first-page”之后新建了一篇文章，就会同时在hexo/source/_post文件夹下生成一个.md文件和一个同名文件夹，文件夹存放文章中的图片)</li>
</ul>
</li>
<li><code>hexo clean</code>：清理已有的静态页面(不是删干净重来)</li>
</ul>
<h4 id="3-示例配置过程"><a href="#3-示例配置过程" class="headerlink" title="3. 示例配置过程"></a>3. 示例配置过程</h4><p>在执行<code>hexo init</code>之后，可以再执行<code>hexg g</code> 在本地渲染页面，完成后文件夹中会出现<strong>public</strong>文件夹。然后再执行<code>hexo new ts</code> 这时在source文件夹下会生成_post文件夹，里面有hello-world.md(默认自带)和ts.md(代码新生成的)，然后我们可以在ts.md中写一些东西，就是日后的博客文章。还可以使用命令<code>hexo new page about</code> 生成一个新的about文件夹，里面有index.md，可以写一些介绍自己的内容。</p>
<p>弄好上面的之后，我们可以执行命令<code>hexo s</code> 在本地演示网站，此时cmd窗口会生成一个<a href="http://localhost:4000">http://localhost:4000</a> 的链接，我们在浏览器打开就可以看到渲染的博客页面了。</p>
<h4 id="4-主题的美化"><a href="#4-主题的美化" class="headerlink" title="4. 主题的美化"></a>4. 主题的美化</h4><p>主要是利用<strong>根目录下的_config.yml文件</strong>和<strong>themes文件夹下某一主题文件夹下的_config.yml</strong>进行修改美化页面，可以自己进行探索。</p>
<h5 id="1-关于主题的设置"><a href="#1-关于主题的设置" class="headerlink" title="(1) 关于主题的设置"></a>(1) 关于主题的设置</h5><ul>
<li>存放位置：/themes/主题名/_config.yml(注意：有别于根目录下的_config.yml文件)<ul>
<li>这里需要说明的是：Hexo在渲染静态页面时，themes文件夹的_config.yml文件优先度比根目录的_config.yml高</li>
</ul>
</li>
<li>修改方式：大部分为布尔值(只需要修改true或者false即可)，支持中文的主题大部分会有注释<ul>
<li>这里推荐Hexo的Next与Ayer主题，两者有很清晰明了的中文，页面也很清爽(我的主题是Ayer)</li>
</ul>
</li>
<li>添加自定义js的方式：在/themes/主题名/layout/layout.ejs中添加<ul>
<li>该方法是我在用Next主题时发现的，当时成功了，是后来Ayer主题失败了，还是要按照不同的主题来</li>
</ul>
</li>
</ul>
<p>完成配置后，可以使用<code>hexok s</code> 在本地演示网站看一下配置情况。</p>
<h5 id="2-Markdown语法学习"><a href="#2-Markdown语法学习" class="headerlink" title="(2) Markdown语法学习"></a>(2) Markdown语法学习</h5><p>自己学习，这里不讲。</p>
<h3 id="四-部署与上传"><a href="#四-部署与上传" class="headerlink" title="四. 部署与上传"></a>四. 部署与上传</h3><p>在进行完这一步之后，就可以真正意义上有一个可以在浏览器打开的自己的博客网站！！！</p>
<p>正常情况下，在cmd输入指令<code>npm install hexo-deployer-git --save</code> 之后指令<code>hexo g -d</code> 理论上就可以成功将博客文件上传到Github上了，然后在GitHub网页的settings页面下找到pages，就可以找到对应的网址，打开即可看到自己的博客。<br>但是，由于Github服务器在国外，所以经常出现网络问题，需要搭梯子，下面是失败情况下的一些处理方法。</p>
<ul>
<li>失败则输入指令<code>git config --global credential.helper wincred</code></li>
<li>再次失败，错误原因是spawn的话就是网络有些问题，重启cmd试试</li>
<li>还是失败的话可能是电脑上装了两个hexo</li>
<li>最终还是失败的话可以使用指令<code>hexo g</code>，然后直接在网页端的GitHub上传public文件夹。</li>
</ul>
<h3 id="五-相关链接"><a href="#五-相关链接" class="headerlink" title="五. 相关链接"></a>五. 相关链接</h3><h4 id="1-帮助文档链接"><a href="#1-帮助文档链接" class="headerlink" title="1. 帮助文档链接"></a>1. 帮助文档链接</h4><p>在自己搭建第一个博客中出现了很多问题，有许多博客、视频帮助了我，贴在下面可以可看一下。</p>
<p>听话的便当的B站视频：<a href="https://space.bilibili.com/435364951/channel/seriesdetail?sid=812995">听话的_便当的个人空间_哔哩哔哩_bilibili</a> </p>
<p>听话的便当的博客：<a href="https://byn6.github.io/2021/08/29/gp-h/">https://byn6.github.io/2021/08/29/gp-h/</a></p>
<p>Ayer中文官方文档： <a href="https://shen-yu.gitee.io/2019/ayer">https://shen-yu.gitee.io/2019/ayer</a></p>
<p>Hexo官方文档：<a href="https://hexo.io/zh-cn/docs/asset-folders.html">https://hexo.io/zh-cn/docs/asset-folders.html</a></p>
<p>Hexo常用命令详解及使用方法：<a href="https://www.mabiji.com/hexo/hexocommands.html">https://www.mabiji.com/hexo/hexocommands.html</a></p>
<p>Hexo为文章设置目录与标签的方法：<a href="https://blog.csdn.net/weixin_44543463/article/details/119738094">https://blog.csdn.net/weixin_44543463/article/details/119738094</a></p>
<p>[hexo]如何更换主题、删除文章：<a href="https://www.cnblogs.com/mrwuzs/p/7942689.html">https://www.cnblogs.com/mrwuzs/p/7942689.html</a></p>
<p>hexo搭建博客后图片无法显示的完美解决：<a href="https://blog.csdn.net/weixin_43702620/article/details/118655308">https://blog.csdn.net/weixin_43702620/article/details/118655308</a></p>
<p>不会git就学不会github吗？：<a href="https://www.zhihu.com/question/29566172">https://www.zhihu.com/question/29566172</a></p>
<p>git是干什么的？ sourcetree是干什么的？：<a href="https://www.zhihu.com/question/48322958">https://www.zhihu.com/question/48322958</a></p>
<p>SourceTree的基本使用教程：<a href="https://blog.csdn.net/qq_34129814/article/details/82847753">https://blog.csdn.net/qq_34129814/article/details/82847753</a></p>
<p>Hexo server报错TypeError: Cannot read property ‘utcOffset’ of null解决方法：<a href="http://t.zoukankan.com/mmzuo-798-p-10510225.html">http://t.zoukankan.com/mmzuo-798-p-10510225.html</a></p>
<p>免费图床，看这一篇就够了！：<a href="https://www.duangvps.com/archives/1737">https://www.duangvps.com/archives/1737</a></p>
<h4 id="2-数学公式解决方法"><a href="#2-数学公式解决方法" class="headerlink" title="2. 数学公式解决方法"></a>2. 数学公式解决方法</h4><p>如何在Hexo中插入数学公式 <a href="https://qingstudios.com/2020/03/01/Hexo%E4%B8%AD%E6%8F%92%E5%85%A5%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/">https://qingstudios.com/2020/03/01/Hexo%E4%B8%AD%E6%8F%92%E5%85%A5%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</a></p>
<p>MathJax在Hexo博客中插入数学公式：<a href="https://blog.csdn.net/weixin_39789979/article/details/110806006">https://blog.csdn.net/weixin_39789979/article/details/110806006</a></p>
<p>markdown斜体和数学公式的矛盾：<a href="https://blog.moper.net/2640.html">https://blog.moper.net/2640.html</a></p>
<p>hexo博客MathJax公式渲染问题：<a href="https://www.cnblogs.com/Ai-heng/p/7282110.htm">https://www.cnblogs.com/Ai-heng/p/7282110.htm</a></p>
<p>hexo 如何解决markdown下划线要加反斜杠，以及如何使用其他markdown的渲染引擎？：<a href="https://segmentfault.com/q/1010000003987383">https://segmentfault.com/q/1010000003987383</a></p>
<h4 id="3-优秀博客示例"><a href="#3-优秀博客示例" class="headerlink" title="3. 优秀博客示例"></a>3. 优秀博客示例</h4><p>二十五画生 <a href="https://blog.laoda.de">https://blog.laoda.de</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>生活休闲</category>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>博客</tag>
        <tag>Hexo</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title>信号与系统概念本质理解</title>
    <url>/2022/01/27/signal-and-system-essence/</url>
    <content><![CDATA[<h3 id="一、模拟频率、模拟角频率、数字频率"><a href="#一、模拟频率、模拟角频率、数字频率" class="headerlink" title="一、模拟频率、模拟角频率、数字频率"></a>一、模拟频率、模拟角频率、数字频率</h3><h4 id="1-模拟频率-f"><a href="#1-模拟频率-f" class="headerlink" title="1. 模拟频率$f$"></a>1. 模拟频率$f$</h4><p>包括在模拟电路、高频电路以及传感器课程上，都是以$f$作为频率响应函数的横坐标。使用f的好处是其真实反映了实际系统的工作情况，从$0$到$\infty$，反映了实际模拟信号振荡速度的快慢。  </p>
<h4 id="2-模拟角频率-Omega"><a href="#2-模拟角频率-Omega" class="headerlink" title="2. 模拟角频率$\Omega$"></a>2. 模拟角频率$\Omega$</h4><p>$\Omega=2πf$，写成$\cos(\Omega t)$来描述模拟余弦函数。此时$\Omega$的取值也是从$0$到$\infty$，这体现出模拟(角)频率没有周期性的特点。</p>
<h4 id="3-数字角频率-omega"><a href="#3-数字角频率-omega" class="headerlink" title="3. 数字角频率$\omega$"></a>3. 数字角频率$\omega$</h4><p>$\omega$完全颠覆了我们过往对于频率的认识，首先要明确的是数字信号的获得是通过对模拟信号采样的方式。</p>
<p>它的引入可从$\cos(\Omega t)$开始：$\cos(\Omega t)$中相位变化一周期($2π$)所需时间为$T$，则模拟角频率定义成$\Omega = \dfrac{2π}{T}=2πf$。对于该余弦信号，采样后变成了一个离散数字序列，此时再谈过了多少时间走完一个周期已经没有意义，而是过了间隔$N$相位刚好变化一个周期。故数字角频率推导出为$\omega = \dfrac{2\pi}{N}$，余弦信号则为$cos(\omega n)$。既然$N$是由对应一段时间$T$采样而来，那么$N = T*f_s = \dfrac{T}{t_s}$($f_s$为采样率，$t_s$为对应的采样间隔)，自然而然，$\omega = \dfrac{\Omega}{f_s} = \dfrac{\Omega}{t_s} = \dfrac{2πf}{f_s}$。</p>
<p><strong><font color="red">简单来说，数字角频率$\omega$是模拟角频率$\Omega$对于采样率$f_s$的归一化，这是数字角频率$\omega$的核心要义。</font></strong></p>
<blockquote>
<p>注意：$Ω$和$ω$具体哪个表示模拟角频率和数字角频率看具体的书籍，理解这个意思就行。</p>
</blockquote>
<h4 id="4-一些涉及本质的问题"><a href="#4-一些涉及本质的问题" class="headerlink" title="4.  一些涉及本质的问题"></a>4.  一些涉及本质的问题</h4><h5 id="1-如何理解数字信号处理中傅里叶变换的周期为-2π-？"><a href="#1-如何理解数字信号处理中傅里叶变换的周期为-2π-？" class="headerlink" title="1-如何理解数字信号处理中傅里叶变换的周期为$2π$ ？"></a>1-如何理解数字信号处理中傅里叶变换的周期为$2π$ ？</h5><blockquote>
<p>分析信号的频谱特性时，经常要对信号进行傅里叶变换，但傅里叶变换是以$2π$为周期，而时域里的信号角频率的范围是很宽的，为什么傅里叶变换中的$0 \sim 2π$就可以代表信号整个频率范围呢？</p>
</blockquote>
<p>1、模拟信号的频率：模拟频率越大，信号变化越快。拿构成模拟信号的频率分量来说，比如$\cos(\Omega t)$。</p>
<p>2、数字信号是对模拟信号[等间隔]抽样得到的，即$\cos(\Omega t_s n)=\cos(\omega n)$，$ω=Ω t_s$[称为数字频率]，由于<strong>离散[数字]信号的自变量<font color="red">n是整数</font></strong>，因此数字频率$\omega$与$ω+2πM$是同一个数字频率，即：</p>
<script type="math/tex; mode=display">
\cos(\omega n) = \cos[(\omega+2πM)n]</script><p>因此对离散信号作傅里叶变换，实际上是将离散信号[量化后就是数字信号]分解为$e^{jωn}$的线性组合，其频谱就具有周期性，频率为$\omega$的频谱等于频率为$\omega+2πM$的频谱。</p>
<p>3、再来看$\cos(\omega n)$是构成实数离散信号的基本信号，它最大的频率是多少呢？周期最小$N=1$，故变化最快的是$ω=π$；变化最慢的是直流$ω=0$。故$ω=0$频率最小，$w=π$最高频率，对应模拟信号频率为：</p>
<script type="math/tex; mode=display">
\Omega=\dfrac{ω}{T}=\dfrac{π}{T}=\dfrac{f_s}{2}</script><p>即为采样频率的一半。对实数离散信号来说，$0 \sim 2π$的频谱图是以$\omega=\pi$对称的。</p>
<p>4、由时域采样定理，采样频率$f_s$最小为被采模拟信号最高频率的2倍；故可认为被采模拟信号最高频率为$\dfrac{f_s}{2}$，这个频率对应数字频率的$π$。</p>
<p>5、实际中即使模拟信号的最高频率是无穷大，但是可以通过滤波，滤去无用的高频分量，再对他抽样以避免频谱混叠。</p>
<p><strong>e.g.</strong> $f_s=1 \mathrm{Hz}$，$\Omega$分别等于$\dfrac{π}{8}$和$\dfrac{17\pi}{8}$，得到如下两幅图。可以看出虽然模拟角频率$\Omega$增加了$2π$，但由于采样点数和采样值都相同，所以实际的离散信号是一回事。 </p>
<p><img src="https://pic1.zhimg.com/80/v2-f23a15fb4a8a85a2c9885597f2d54a70_720w.jpg" alt> </p>
<p>正是因为数字信号对于$\omega$具有周期性，DSP才增加了额外的很多考虑：</p>
<p>1）DTFT、DFT是将数字信号从时间域$n$转为频域$\omega$，因此我们只转为$\omega$在$[-π,π]$区间内复指数信号的叠加(也可以考虑$[0,2π]$，不过由于$ω=0$和$2π$是低频信号，$ω=π$是高频信号，考虑$[-π,π]$更接近模拟信号的频谱分布)。</p>
<p>2）我们根据$\omega=\dfrac{Ω}{f_s}$可知，从模拟角频率到数字角频率不只会落在$[-π,π]$，若转为数字频谱后其频带占用超过了$[-π,π]$，则由于具有周期性，相互之间会产生混叠。我们要把频谱$\omega$限定在$[-π,π]$，则由奈奎斯特采样定理可以知道，$f_s$必须≥信号最高频率的2倍才不会发生信号混叠，因此$f_s$能采样到的信号最高频率为$\dfrac{fs}{2}$。</p>
<p>3）在运用频域采样法设计IIR时，我们基于的AD/DA转换就是上述的$ω=\dfrac{\Omega}{f_s}$(双线性变化法则不是)。上述说到模拟角频率$\Omega$是没有周期性可言的，但是由于采样率的限制(离散化)，导致说数字角频率$\omega$具有周期性。从映射角度理解，数字角频率$[-π,π]$在模拟角频率上的映射是一对多的。采样间隔$T=\dfrac{1}{f_s}$，在给定$T$时，数字角频率$\omega$受到的影响来源于以$\dfrac{2π}{T}$为单位的模拟角频率$Ω = \dfrac{ω}{T}$）。为了使数字频域不发生混叠，我们需要将模拟角频率$\Omega$框在$[-\dfrac{π}{T}, \dfrac{π}{T}]$，这在设计高通/带阻数字滤波器时是复杂的，如下图展示的用频域采样法设计高通FIR时所需要增加的裁剪步骤。 </p>
<h5 id="2-如何理解数字频率的归一化？"><a href="#2-如何理解数字频率的归一化？" class="headerlink" title="2-如何理解数字频率的归一化？"></a>2-如何理解数字频率的归一化？</h5><p>频率的归一化与幅度的归一化有不同的出发点，主要是在数字信号处理中，针对采样频率进行归一化，归一化后就可以在数字角频率$0 \sim 2\pi$下进行分析和讨论，这样做的原因有两点：一是类似于最大值的归一化(此时的$2\pi$对应采样率)，二是采样后的频谱是以$2\pi$为周期的。 </p>
<p>数字角频率是模拟角频率对采样频率的归一化，很好理解，本来模拟信号一秒钟变化5弧度，抽样率为$5 \mathrm{Hz}$，采样后信号变成每抽样点变化1弧度，这里“归一”成“抽样点”了或者说“基准”变成“抽样点”了。 </p>
<h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><p>参考链接1.1：<a href="https://zhuanlan.zhihu.com/p/53664104">数字角频率ω与模拟角频率Ω的理解 - 知乎</a></p>
<p>参考链接1.2：<a href="https://blog.csdn.net/jyfu2_12/article/details/78650858">模拟角频率和数字角频率的关系 - CSDN</a></p>
<p>参考链接1.3：<a href="http://www.360doc.com/content/18/1008/20/38894361_793066968.shtml">如果理解数字信号处理中傅里叶变换的周期为2pi？- 360文档</a></p>
<p>参考链接1.4：<a href="https://www.zhihu.com/question/65441510/answer/298234811">所有的傅里叶变换之后的频域信号周期都是2π吗？ - 尔尔的回答 - 知乎</a></p>
<hr>
<h3 id="二、傅里叶级数-傅里叶变换推导"><a href="#二、傅里叶级数-傅里叶变换推导" class="headerlink" title="二、傅里叶级数-傅里叶变换推导"></a>二、傅里叶级数-傅里叶变换推导</h3><h4 id="1-傅里叶级数"><a href="#1-傅里叶级数" class="headerlink" title="1. 傅里叶级数"></a>1. 傅里叶级数</h4><h5 id="1-傅里叶级数的普通形式"><a href="#1-傅里叶级数的普通形式" class="headerlink" title="(1) 傅里叶级数的普通形式"></a>(1) 傅里叶级数的普通形式</h5><p>设信号的周期为$T_1$，对应的$\omega_1 = \dfrac{2\pi}{T_1}$</p>
<p>周期函数可以分解为三角函数系的组合：</p>
<script type="math/tex; mode=display">
f(t) = \sum_{n=0}^{\infty}a_k\cos n \omega_1 t + \sum_{n = 0}^{\infty}b_n\sin n \omega_1 t \tag{1.1}</script><p>但是我们见到课本上一般是下面这种形式：</p>
<script type="math/tex; mode=display">
f(t) = \frac{a_0}{2}+\sum_{n=1}^{\infty}[a_n\cos n \omega_1 t + b_n\sin n \omega_1 t]</script><p>将$n = 0$单独分出来化简就可以得到下面的形式，我们就以课本常见的格式求解系数：$a_0, a_n, b_n$</p>
<script type="math/tex; mode=display">
\frac{a_0}{2} = \frac{1}{T_1}\int_{0}^{T_1}{f(t)dt}</script><script type="math/tex; mode=display">
a_n = \frac{2}{T_1}\int_{0}^{T_1}f(t)\cos n \omega_1 tdt</script><script type="math/tex; mode=display">
b_n = \frac{2}{T_1}\int_{0}^{T_1}f(t)\sin n \omega_1 tdt</script><h5 id="2-傅里叶级数的复数形式"><a href="#2-傅里叶级数的复数形式" class="headerlink" title="(2) 傅里叶级数的复数形式"></a>(2) 傅里叶级数的复数形式</h5><p>利用欧拉公式：$e^{j\theta} = \cos\theta + j\sin\theta$ 可以将其转换为复指数形式：</p>
<script type="math/tex; mode=display">
f(t) = \sum_{n = -\infty}^{\infty}C_ne^{jn\omega_0 t}</script><p>其系数为：</p>
<script type="math/tex; mode=display">
C_n = \frac{1}{T}\int_0^Tf(t)e^{-jn\omega_0 t}dt</script><p>其中：$\omega_0 = \dfrac{2\pi}{T}$</p>
<h4 id="2-傅里叶变换"><a href="#2-傅里叶变换" class="headerlink" title="2. 傅里叶变换"></a>2. 傅里叶变换</h4><p>对于非周期信号，我们可以将其视为一种特殊的周期函数，只不过该周期函数的周期$T \rightarrow \infty$，据此，我们做以下变换，将傅里叶级数转变为傅里叶变换：</p>
<script type="math/tex; mode=display">
f(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty} F(\omega)e^{j\omega t}d\omega</script><p>其中，$F(\omega)$就是$f(t)$的傅里叶变换：</p>
<script type="math/tex; mode=display">
F(\omega) = \int_{-\infty}^{\infty}f(t)e^{-j\omega t}dt</script><h4 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a>参考链接</h4><p>参考链接2.1：<a href="https://www.bilibili.com/video/BV1Et411R78v?share_source=copy_web">纯干货数学推导_傅里叶级数与傅里叶变换 - Bilibili</a></p>
<p>参考链接2.2：<a href="https://zhuanlan.zhihu.com/p/96521358">傅里叶变换系列学习（1）——从线性变换说起 - 技术派到了中年的文章 - 知乎</a></p>
<hr>
<h3 id="三、拉普拉斯变换、Z变换"><a href="#三、拉普拉斯变换、Z变换" class="headerlink" title="三、拉普拉斯变换、Z变换"></a>三、拉普拉斯变换、Z变换</h3><h4 id="1-拉普拉斯变换LT"><a href="#1-拉普拉斯变换LT" class="headerlink" title="1. 拉普拉斯变换LT"></a>1. 拉普拉斯变换LT</h4><p>不是所有的信号都能从时域经过傅里叶变换到频域。必须是满足<strong>狄利赫里条件</strong>的信号才能进行傅里叶变换。</p>
<ul>
<li><p>狄利赫里条件</p>
<p>函数在任意有限区间内连续，或只有有限个第一类间断点；</p>
<p>在一个周期内，函数有有限个极大值或极小值；</p>
<p>函数绝对可积，即$\int_0^\infty |f(t)|&lt; \infty$；</p>
</li>
</ul>
<p>为了将绝对不可积函数可以进行傅里叶变换，我们引入一个衰减因子$e^{-\sigma t}$，将傅里叶变换推广到拉普拉斯变换：</p>
<script type="math/tex; mode=display">
F(\omega) = \int_{-\infty}^{\infty}f(t)e^{-\sigma t}e^{-j\omega t}dt =  \int_{-\infty}^{\infty}f(t)e^{-(\sigma+j\omega) t}dt</script><script type="math/tex; mode=display">
f(t) = \frac{1}{2\pi} \int_{-\infty}^{\infty}F(\omega)e^{(\sigma+j\omega) t}d\omega</script><h4 id="2-Z变换ZT"><a href="#2-Z变换ZT" class="headerlink" title="2. Z变换ZT"></a>2. Z变换ZT</h4><p>离散数字信号域的ZT相当于连续时间信号的FT，其也是为了处理不满足绝对可和的数字信号(e.g. $ f(k) = k^2$)，因此我们给它乘以一个衰减因子$e^{-\sigma n}$(另一种理解方法，让离散复指数乘以一个放大因子，不断变大，就能跟上信号的变化，从而降服信号)</p>
<script type="math/tex; mode=display">
X(z) = \sum_{n = -\infty}^{\infty}x(n)z^{-n} = \sum_{n = -\infty}^{\infty}x(n)e^{-\sigma n}e^{-j\omega n}</script><script type="math/tex; mode=display">
x(n) = \frac{1}{2\pi j} \oint X(z)z^{n-1}dz</script><p>和拉普拉斯变换一样，z变换也有<strong>收敛域</strong>的概念。 拉普拉斯$s = \sigma +j \omega$ 对应的是直角坐标是一个平面。 Z变换$z = e^{\sigma + j\omega}$对应的是一个极坐标系是一个圆。 </p>
<p><img src="https://pic2.zhimg.com/80/v2-b0771ba027e7d23fed45975f267bb407_720w.jpg?source=1940ef5c" alt> </p>
<h4 id="参考链接-2"><a href="#参考链接-2" class="headerlink" title="参考链接"></a>参考链接</h4><p>参考链接3.1：<a href="https://www.zhihu.com/question/22085329">傅里叶变换、拉普拉斯变换、Z 变换的联系是什么？为什么要进行这些变换？ - 知乎</a></p>
<p>参考链接3.2：<a href="https://www.cnblogs.com/taigacon/p/8313482.html">离散时间信号处理学习笔记 - 8. z逆变换 - 博客园 - TaigaComplex</a></p>
<hr>
<h3 id="四-、DFS、DTFT、DFT、FFT"><a href="#四-、DFS、DTFT、DFT、FFT" class="headerlink" title="四 、DFS、DTFT、DFT、FFT"></a>四 、DFS、DTFT、DFT、FFT</h3><p>根据前面的学习，对一些信号采样然后进行DTFT之后，得到的信号在频域上还是连续的（也就是对于我们归一化之后的角频率ω还是连续的，是以2π为周期的，以π为对称中心），这样对于计算机来说还是没有办法处理的，所以再在频域内对其采样，采样之后的得到的就是DFT。<br>DFS是对于时域是离散的周期信号来说的，DFS与DFT的区别就是相对于我们之前时域连续信号中的傅里叶级数与傅里叶变换。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">变换方法</th>
<th style="text-align:center">时域</th>
<th style="text-align:center">频域</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$\rm{FS}$</td>
<td style="text-align:center">连续周期信号</td>
<td style="text-align:center">离散非周期频谱</td>
</tr>
<tr>
<td style="text-align:center">$\rm{FT}$</td>
<td style="text-align:center">连续非周期信号</td>
<td style="text-align:center">连续非周期频谱</td>
</tr>
<tr>
<td style="text-align:center">$\rm{DFS}$</td>
<td style="text-align:center">离散周期信号</td>
<td style="text-align:center">离散周期频谱</td>
</tr>
<tr>
<td style="text-align:center">$\rm{DTFT}$</td>
<td style="text-align:center">离散非周期信号</td>
<td style="text-align:center">连续周期频谱</td>
</tr>
<tr>
<td style="text-align:center">$\rm{DFT}$</td>
<td style="text-align:center">离散非周期信号</td>
<td style="text-align:center">离散非周期频谱</td>
</tr>
<tr>
<td style="text-align:center">$\rm{FFT}$</td>
<td style="text-align:center">离散非周期信号</td>
<td style="text-align:center">离散非周期频谱</td>
</tr>
</tbody>
</table>
</div>
<h4 id="1-离散时间傅里叶变换DTFT"><a href="#1-离散时间傅里叶变换DTFT" class="headerlink" title="1. 离散时间傅里叶变换DTFT"></a>1. 离散时间傅里叶变换DTFT</h4><p>严格来说，<strong>DTFT</strong>处理的是离散时间序列信号，这种信号在时间上是离散的，但是信号的幅度是可以任意值，也就是说没有量化，还不是数字信号。非严格的场合，可以认为它是离散信号，并且是非周期信号。</p>
<script type="math/tex; mode=display">
X(\omega) = \sum_{-\infty}^{\infty}x[n]e^{-j\omega n}</script><script type="math/tex; mode=display">
x[n] = \frac{1}{2\pi}\int_{2\pi} X(\omega)e^{-j\omega n}</script><h4 id="2-离散傅里叶级数DFS"><a href="#2-离散傅里叶级数DFS" class="headerlink" title="2. 离散傅里叶级数DFS"></a>2. 离散傅里叶级数DFS</h4><p>一个周期为$N$的周期序列，即$x(n)=x(n+kN)$，$k$为任意整数，$N$为周期<br><strong>周期序列不能进行Z变换，因为其在负无穷到正无穷都周而复始永不衰减，即Z平面上没有收敛域。</strong>但是正像连续时间周期信号可用傅氏级数表示，周期序列也可用离散的傅里叶级数来表示，也即用周期为$N$的正弦序列及其谐波来表示。</p>
<p>在连续域，我们的基是$\displaystyle{e^{jk\omega_0 t}}$是周期，其中$k$是整数，一直变化对应到不同的基，$\omega_0 = \dfrac{2\pi}{T}$是周期，t是连续的时间变量。这是我们第一次处理离散信号，离散信号的周期变成了$N$，连续的时间信号$t$换成离散的序列$n$，这样我们的基就变成了$ e^{j\frac{2\pi}{N}kn}$</p>
<p>离散域通常用大写字母序列表示正变换结果，小写字母序列表示原信号。 </p>
<script type="math/tex; mode=display">
X[k] = \frac{1}{N}\sum_{n=0}^{N-1}x[n]e^{-j\frac{2\pi}{N}kn}</script><script type="math/tex; mode=display">
x[n] = \sum_{k=0}^{N-1}X[k]e^{j\frac{2\pi}{N}kn}</script><p>由上知， <strong>DFS是DTFT在单位圆上的N等分</strong>，其傅里叶级数不仅是离散的，而且还是周期性的(以N为周期) </p>
<h4 id="3-离散傅里叶变换DFT"><a href="#3-离散傅里叶变换DFT" class="headerlink" title="3. 离散傅里叶变换DFT"></a>3. 离散傅里叶变换DFT</h4><p><strong>DFT</strong>才是我们耳熟能详的变换，<strong>DFT</strong>处理的是非周期离散信号，这样的信号计算机比较容易处理，我们再看看<strong>DTFT</strong>，正变换产生了连续信号，这个不利于计算机处理。为了方便计算机处理，必须让正变换的结果也是离散值，这一点<strong>DFS</strong>正满足。<strong>DFS</strong>应对的是周期离散信号，正变换后的序列也是周期离散的。前面我们提到，对于非周期信号，数学家总是先假设其周期为T，然后取极限，让T趋向于无穷大，这样得到了非周期信号的变换结果。细心的你，可能已经发现，非周期信号的变换结果，都是连续的，连续信号对于CPU来说，不是一个好东西。</p>
<p>所以，数学家在这次处理非周期信号的时候，采用了不同的方法。这次的方法是对非周期信号做周期延拓，这样不就变成<strong>DFS</strong>了吗，还要<strong>DFT</strong>做什么呢。别急，DFS变换后是周期离散信号，但是<strong>DFT</strong>只取<strong>DFS</strong>变换后的一个周期。<strong>DFT</strong>在处理的时候投机取巧，方便了计算机处理，所以我们通常听得比较多的就是<strong>DFT</strong>。</p>
<p>另外，<strong>DFT</strong>也可以从<strong>DTFT</strong>抽样过来，<strong>DTFT</strong>作为连续信号，均匀采样$N$个点，就变成<strong>DFT</strong>了。</p>
<script type="math/tex; mode=display">
X[k] = \sum_{n = 0}^{N-1} x[n]e^{-j\frac{2\pi}{N}nk}</script><script type="math/tex; mode=display">
x[n] = \frac{1}{N} \sum_{k = 0}^{N-1} X[k]e^{j\frac{2\pi}{N}nk}</script><p><strong><font color="red">[6]</font></strong>可以发现，DFT与离散傅里叶级数(DFS)仅有一点点区别，那就是DFT在逆变换时，即在合成源信号时，会乘上1/N，而DFS相反，除开这一点并无区别，所以这就表示是，<strong><font color="red">DFT实际上是，以信号的N点为周期的信号的DFS</font></strong>，那么DFT和DTFT是什么关系呢？从上述角度上来讲，只能说当DFT所取点数趋于无穷时，那么DFT就可以趋近于DTFT，这也是傅里叶变换的来源，那么书上不是有DFT是DTFT的采样的结论么？该结论是怎样来的？有条件么？</p>
<p>在$\omega$轴上$[0 ,2\pi]$范围内，对DTFT的变换结果$X(e^{j\omega})$以$N$个等间隔频率$\omega_k = \dfrac{2\pi k}{N}，(0 \leq k \leq N-1)$均匀抽样，可得：</p>
<script type="math/tex; mode=display">
X(e^{j\omega})|_{\omega = \frac{2\pi k}{N}}=\sum_{ n= 0}^{N-1}x[n]e^{\frac{j2\pi kn}{N}}，0 \leq k \leq N-1</script><p>个人认为，这也解答了为何DFS与DFT有个系数上的差异，这就是为了在DTFT与DFT的满足采样特点的吻合。</p>
<p>在许多书上，离散时间傅里叶变换都是由是连续信号采样得到的离散信号而得到，这样来介绍，由于有采样频率等的影响，这样会对连续、离散信号产生一定的混淆，个人认为，离散傅里叶变换时离散时间信号的频域特性，并不一定非得和采样、连续信号等产生关系，离散信号可以是由连续信号采样，归一化产生，也可以是其本身就是离散的，如年月日，像素坐标等，如果离散信号是由连续信号采样得到的，那么，当用给定的采样频率，对连续信号采样，归一化后，那么就产生了一个新的离散信号，至于其傅里叶变换和原来的傅里叶变换有何关系，这就是另外一个话题了。</p>
<p>傅里叶变换的核心思想便是能让任意一个信号都可以由一种基本信号来合成，这个基本信号对于连续时间信号便是：$\exp(j\omega t)$，离散时间信号便是：$\exp(j\omega n)$。</p>
<p>对于DTFT的定义式，可以发现其积分是由$[-\pi, \pi]$，而对于FT的定义式积分是从$(-\infty, \infty)$，<strong><font color="red">其根本原因就是——基本信号存在差异</font></strong>，对于$\exp(j\omega t)$，由于$t$可以取任意值因此其不存在周期，当$\omega$取任意值，$\exp(j\omega t)$都会变为一个独一无二的信号，故FT需要从$(-\infty, \infty)$，让每一个$\exp(j\omega t)$都能成为被合成信号的成分（当然这还得取决于前面系数，即傅里叶系数），而对于DTFT而言，由于其基本信号$\exp(j\omega n)$中$n$只能取正整数故周期为$2\pi$，故当要合成新信号时，只需要取任何一个$2\pi$区段就可获得合成信号的所有成分，故上式取$[-\pi, \pi]$即可，当然，这样也很容易得出$F(\exp(jw))$也是周期的。<strong><font color="red">[6]</font></strong></p>
<h4 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h4><p>参考链接4.1：<a href="https://blog.csdn.net/qq_44554964/article/details/108025590">数字信号处理（DTFT与DFT、DFS的详细讲解以及FFT算法）- CSDN</a></p>
<p>参考链接4.2：<a href="https://zhuanlan.zhihu.com/p/97835957">傅里叶变换系列学习（3)——FS,FT,DTFT,DFS,DFT,FFT - 技术派到了中年的文章 - 知乎</a></p>
<p>参考链接4.3：<a href="https://www.cnblogs.com/BitArt/archive/2012/11/24/2786390.html">一幅图弄清DFT与DTFT,DFS的关系 - 博客园</a></p>
<p>参考链接4.4：<a href="https://www.codenong.com/cs106440896">五种傅里叶变换的分析（FT、FS、DTFT、DFT、DFS）</a></p>
<p>参考链接4.5：<a href="https://blog.51cto.com/rainlee/1629264">关于对傅里叶变换的一些理解 - 51博客</a></p>
<p>参考链接4.6：<a href="http://www.360doc.com/content/19/0611/18/908538_841808337.shtml">数字信号处理系列串讲第6篇（离散信号的频域分析之三）——离散傅里叶变换DFT（1）- 360文档</a></p>
<hr>
<h3 id="五、采样频率、频率分辨率"><a href="#五、采样频率、频率分辨率" class="headerlink" title="五、采样频率、频率分辨率"></a>五、采样频率、频率分辨率</h3><h4 id="1-采样定理"><a href="#1-采样定理" class="headerlink" title="1. 采样定理"></a>1. 采样定理</h4><ul>
<li>时域采样定理的一种表述方式是：当时间信号函数$f(t)$的最高频率分量为$f_M$时，$f(t)$的值可由一系列采样间隔$t_s \leq \dfrac{1}{2×f_M}$的采样值来确定，即采样点的重复频率$f ≥ 2×f_M$。时域采样定理是采样误差理论、随机变量采样理论和多变量采样理论的基础。  </li>
<li>频域采样定理：对于时间上受限制的连续信号$f(t)$ (即当$│t│&gt;t_m $时，$f(t)=0$，这里信号的持续时间是$(-t_m,t_m)$，若其频谱为$F(ω)$，则可在频域上用一系列离散的采样值来表示，只要这些采样点的频率间隔$\omega \leq \dfrac{\pi}{t_m}$</li>
</ul>
<h4 id="2-FFT频率分辨率"><a href="#2-FFT频率分辨率" class="headerlink" title="2. FFT频率分辨率"></a>2. FFT频率分辨率</h4><p><strong><font color="red">[1]</font></strong>这里涉及到两种意义下的分辨率问题，一种叫“波形频率分辨率(Waveform frequency resolution)”，另一种则叫做“FFT分辨率”。虽然，这个分类和命名不一定是很专业的术语，但却有助于对“频率分辨率”概念的理解。在没有补零的情况下，这两个概念通常容易被混淆，因为它们是等价的。</p>
<p>波形频率分辨率是指可以被分辨的2个频率的最小间隔（Spacing）；而FFT 分辨率则是频谱中的数据点数(The number of points in the spectrum)，它是与做FFT的点数直接相关的。</p>
<p><strong>(1) 波形分辨率/物理分辨率</strong></p>
<script type="math/tex; mode=display">
\Delta R_w = \frac{1}{T}</script><p>其中，$T$是实际信号的时间长度。</p>
<p><strong>(2) FFT分辨率</strong></p>
<script type="math/tex; mode=display">
\Delta R_f = \frac{f_s}{N_{\rm{FFT}}}</script><p>其中，$f_s$为采样频率(the sampling frequency)，$N_{\rm{FFT}}$为FFT的点数。$\Delta R_f $代表了FFT频率轴上的频率取值的间隔(Spacing)。 </p>
<p>值得注意的是，可能有很好的FFT分辨率，但却不一定能够很好的把2个频率成分简单的分开。同样，可能有很高的波形分辨率，但波形的能量峰值会通过整个频谱而分散开(这是因为FFT的频率泄漏现象)。</p>
<p>我们知道，信号的离散傅里叶变换(DFT)或快速傅里叶变换(FFT)是对波形的任何一边补零形成的无限序列进行计算的。这就是，为什么FFT的每个频率单元(bin)都具有明显的$\mathrm{sinc}$波的形状。</p>
<p>波形频率分辨率$1/T$与一个$\mathrm{sinc}$函数空值间隔(the space between nulls)是一样的。<strong><font color="red">[1]</font></strong></p>
<p><strong>(3) 区别辨析</strong></p>
<p><strong><font color="red">[2]</font></strong>用MATLAB做FFT并不要求数据点个数必须为以2为基数的整数次方。之所以很多资料上说控制数据点数为以2为基数的整数次方，是因为这样就能采用以2为基的FFT算法，提升运算性能。</p>
<p>如果数据点数不是以2为基数的整数次方，处理方法有两种，一种是在原始数据开头或末尾补零，即将数据补到以2为基数的整数次方，<strong>这是“补零”的一个用处</strong>；第二种是采用以任意数为基数的FFT算法。</p>
<p>而MATLAB的$\rm{fft}(x, N)$函数参数$N$正好就是数据$x$的长度，但又不是以2为基数的整数次方时，并不会采用补零的方法，而是采用以任意数为基数的FFT算法，这样也能得到很好的结果，只不过速度要稍稍慢了一些，但一般的计算量是体现不出来的。 </p>
<p>比如，现在我有一个信号，这个信号中仅包含两个正（余）弦波，一个是$1MHz$，一个是$1.05MHz$，即：</p>
<script type="math/tex; mode=display">
x = \cos(2\pi × 1000000t) + \cos(2\pi × 1050000t)</script><p>设采样频率为$f_s = 100\mathrm{MHz}$，如果采1000个点，那么对应的时域信号时长为$\rm T = 10\mu s$。</p>
<p><img src="https://pic.imgdb.cn/item/6412b628ebf10e5d5376b4b3.jpg"></p>
<p>直接对这1000个数据点其做快速傅里叶变换，将得到频谱图： </p>
<p><img src="https://pic.imgdb.cn/item/6412b64debf10e5d53770c1c.jpg"></p>
<p>可以发现，频谱点稀疏，在1MHz附近根本无法将$1 \mathrm{MHz}$和$1.05 \mathrm{MHz}$的两个频率分开。 发现频率成分无法被区分开来，第一反应应该就是：<strong>频率分辨率不够</strong>。那么如何提高频率分辨率呢？这里就涉及两种分辨率的区别了。之所以要区分，是因为后面要进行“补零”操作。<strong>如果不补零，直接对原始数据做FFT，那么这两种分辨率是相等的。</strong></p>
<script type="math/tex; mode=display">
\Delta R_w = \frac{1}{10\mu s} = \Delta R_{\rm{fft}} = \frac{100MHz}{1000} = 100kHz</script><p>那么，如果现在在原始数据点后补零会有什么效果呢？假设在这 1000个原始数据点后面再补充零达到7000个点，那么数据变成了： </p>
<p><img src="https://pic.imgdb.cn/item/6412b712ebf10e5d5378c59b.jpg"></p>
<p>此时对其做快速傅里叶变换，结果如下： </p>
<p><img src="https://pic.imgdb.cn/item/6412b75eebf10e5d5379a933.jpg"></p>
<p>可以发现，频谱点密集了不少，但是在$1MHz$ 附近依然无法将$1MHz$ 和$1.05MHz$  的两个频率成分分开。这是因为从波形分辨率公式可以看出，<strong>波形分辨率只与原始数据的时长 $\rm T$ 有关</strong>，而与参与FFT的数据点数无关。所以，虽然补了很多零，但波形分辨率依然为：$\Delta R_w = \dfrac{1}{10 \mu s} = 100kHz$，该分辨率大于$1MHz$ 和$1.05MHz$ 这两个频率成分之间的距离$50kHz$  。这就好比用筛子分黄豆和大米，分辨率就好像是筛子上孔的大小，如果筛子的孔太大了，就没有办法把这两者分开。</p>
<p>而“时域补零相当于频域插值”，也就是说，补零操作增加了频域的插值点数，让频域曲线看起来更加光滑，也就是增加了FFT频率分辨率， <strong>这是“补零”的另一个原因</strong>。</p>
<p>显然，根据上面的分析可知，在采样频率不变的情况下，要想将$1MHz$ 和$1.05MHz$这两个频率成分分析出来，光靠“补零”是不够的，必须要改变波形分辨率，也就是要延长原始数据的时长。现在以相同的采样频率对信号采 7000个点作为原始信号： </p>
<p><img src="https://pic.imgdb.cn/item/6412b78debf10e5d537a3bb2.jpg"></p>
<p>对其做快速傅里叶变换，结果如下： </p>
<p><img src="https://pic.imgdb.cn/item/6412b7baebf10e5d537ac60f.jpg"></p>
<p>因为此时的波形分辨率为：$\Delta R_w = \dfrac{1}{70 \mu s} \approx  14kHz$ ，小于两个频率成分之间的距离 $50kHz$ ，所以可以看出有两个明显的峰值。</p>
<p>但是会发现$1MHz$对应的幅值为1，与原始信号中该频率成分的幅值一致，但$1.05MHz$ 对应的幅值明显低于1，但是其周边的点上却都有不小的幅值，这就是所谓的<strong>频谱泄露</strong>，因为数据点的个数影响，使得在$1MHz$ 处有谱线存在，但在$1.05MHz$处没有谱线存在，使测量结果偏离实际值 ,同时在实际频率点的能量分散到两侧的其它频率点上，并出现一些幅值较小的假谱。</p>
<p>为了解决这个问题，可以设法使得谱线同时经过$1MHz$ 和$1.05MHz$ 这两个频率点，找到他们的公约数。</p>
<p>如果原始数据不变，在后面再补充1000个零点：</p>
<p><img src="https://pic.imgdb.cn/item/6412b7d3ebf10e5d537b05d1.jpg"></p>
<p>那么FFT分辨率就是$12.5kHz$，是这两个频率的公约数，$1MHz = 80×12.5kHz，1.05MHz = 84×12.5kHz$ ，所以谱线同时经过 $1MHz$ 和 $1.05MHz$ 这两个频率点。</p>
<p>对其做快速傅里叶变换，结果如下：</p>
<p><img src="https://pic.imgdb.cn/item/6412b81bebf10e5d537bc164.jpg"></p>
<p>会发现两个频率对应的幅值均为1，与原始信号一致。</p>
<p><strong>这也是一种补零操作带来的影响???(这里有点问题，频谱泄露与补零没有关系，后续修改补充)</strong>。 </p>
<p>上图会有一些旁瓣出现，这是因为补零影响了原始信号，如果，直接采8000个点作为原始数据，那么有： </p>
<p><img src="https://pic.imgdb.cn/item/6412b851ebf10e5d537c506e.jpg"></p>
<p>并对其做FFT，结果如下 </p>
<p><img src="https://pic.imgdb.cn/item/6412b870ebf10e5d537cba2e.jpg"></p>
<p>这样就不存在补零带来的误差了。 <strong><font color="red">[2]</font></strong></p>
<h4 id="参考链接-3"><a href="#参考链接-3" class="headerlink" title="参考链接"></a>参考链接</h4><p>[1]傅里叶变换的波形分辨率与频率分辨率 <a href="https://wap.sciencenet.cn/blog-425437-1043431.html?mobile=1">https://wap.sciencenet.cn/blog-425437-1043431.html?mobile=1</a></p>
<p>[2]快速傅里叶变换(FFT)中为什么要“补零”？ - 李狗嗨的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/85863024">https://zhuanlan.zhihu.com/p/85863024</a> </p>
<p>[3]观察频谱的窗口——物理与计算分辨率 <a href="http://www.360doc.com/content/20/0402/23/32196507_903494495.shtml">http://www.360doc.com/content/20/0402/23/32196507_903494495.shtml</a></p>
<p>[4]傅里叶变换补零与能否提高频率分辨率 <a href="https://blog.csdn.net/chichuhe/article/details/90369651">https://blog.csdn.net/chichuhe/article/details/90369651</a></p>
<p>[5]DFT中人为地补零加长序列对频谱的影响？ - 杨树下的狐狸的回答 - 知乎 <a href="https://www.zhihu.com/question/30312644/answer/47615752">https://www.zhihu.com/question/30312644/answer/47615752</a> </p>
<hr>
<h3 id="六、能量信号、功率信号"><a href="#六、能量信号、功率信号" class="headerlink" title="六、能量信号、功率信号"></a>六、能量信号、功率信号</h3><hr>
<h3 id="七、信号的频谱，频谱密度，能量谱"><a href="#七、信号的频谱，频谱密度，能量谱" class="headerlink" title="七、信号的频谱，频谱密度，能量谱"></a>七、信号的频谱，频谱密度，能量谱</h3><h3 id="八、-其他问题"><a href="#八、-其他问题" class="headerlink" title="八、 其他问题"></a>八、 其他问题</h3><p><strong>如何理解奈奎斯特采样定理</strong></p>
<p>参考链接8.1：<a href="https://www.zhihu.com/question/28792456/answer/491794439">怎样理解Nyquist采样定理？ - 甜草莓的回答 - 知乎</a></p>
<p>参考链接8.2：<a href="https://www.zhihu.com/question/28792456/answer/2174692083">怎样理解Nyquist采样定理？ - 吃货彪彪的回答 - 知乎</a></p>
<p><strong>卷积与插值(内插)</strong></p>
<p>在CFA去马赛克和图像旋转放大时都需要进行插值运算。在空间域内，<font color="red"><strong>插值是卷积计算</strong></font>。</p>
<p>当已知数据点位于整数格点上时，插值函数h(x)应该有：h(0)=1;h(n)=0; n是非零整数。</p>
<p>在频率域，以步长1采样的图像中如果含有高于1/2的频率分量将会产生混叠。空间域内的卷积对应在频率域内的相乘运算，可以从中判断插值算法的特性</p>
<p>这是几类插值函数在空间域的曲线：</p>
<p><img src="http://www.baas1995.org/digitalprocessing/interp/interp01.jpg" alt></p>
<p>对应频率域的变换： </p>
<p><img src="http://www.baas1995.org/digitalprocessing/interp/interp02.jpg" alt></p>
<p>下面比较一下几类插值算法的特点。</p>
<p>1）邻域插值。即找离插值点最近的格点的值作为插值点的值。<br>插值函数是一个宽度为1的矩形脉冲。在频率域则是一个sinc函数。在-1/2和1/2的频率附近，sinc函数对原频谱中的高频分量进行衰减，使得插值图像变得模糊；而sinc函数在频率轴上向两端无限延伸，减弱缓慢，加入了新的高频分量，对应在插值图像中形成锯齿。</p>
<p>2）线性插值。权重由离插值点的距离线性决定。<br>插值函数是半宽为1的三角脉冲，即两个宽度为1的矩形脉冲的卷积，所以在频率域是一个sinc^2函数。与邻域插值相比，混入的高频有所减少，同时图像也变得模糊。</p>
<p>3）立方插值。<br>插值函数为：((A+2)x-(A+3))x^2+1 (0&lt;x&lt;1) ((Ax-5A)x+8A)x-4A (1&lt;x&lt;2)<br>（选自<a href="http://www.all-in-one.ee/~dersch/interpolator/interpolator.html）">http://www.all-in-one.ee/~dersch/interpolator/interpolator.html）</a><br>插值函数的特点：第一个节点位于x=1，此处曲线的斜率是A。第二个节点位于x=2，斜率是0。</p>
<p><img src="http://www.baas1995.org/digitalprocessing/interp/interp04.jpg" alt></p>
<p>在频率域上可以看出，参数A越接近0，原图像中的高频分量衰减得越厉害；A越接近-1，原图像中的高频分量衰减得越少，甚至中间波段还有提升。因此参数可以用来调节图像的锐度。根据这个网页上的数据，认为PS的算法接近于A=-0.75的效果。 </p>
<p><img src="http://www.baas1995.org/digitalprocessing/interp/interp05.jpg" alt></p>
<p>上面三种插值函数在+-1/2频率处的响应均降为0，从而抑制了混叠效应，同时模糊了图像。</p>
<p>4）sinc插值<br>sinc函数在频率域是一个矩形，看起来似乎最适合做内插。在+-1/2频点，矩形函数的值降低到1/2而非0，因此如果存在混叠则无法得到抑制。从实际来看，很难限制图像的带宽。因此使用sinc函数做插值在图像的边缘等地方会很敏感。另一方面，sinc函数做插值时收敛较慢，需要的采样点要多得多。在采样点较少时，在+-1/2频点会出现振荡，产生失真。</p>
<p><img src="http://www.baas1995.org/digitalprocessing/interp/interp03.jpg" alt></p>
<p>5）加窗sinc插值（选自<a href="http://www.all-in-one.ee/~dersch/interpolator/interpolator.html）">http://www.all-in-one.ee/~dersch/interpolator/interpolator.html）</a> 将两个不等宽矩形脉冲做卷积，得到梯形脉冲。使用这个函数在频域做乘积，则图像中的高频分量受到较少的衰减，图像细节被保留，而超出采样带宽较多的分量被阻断，不会产生类似振铃的效应。 在空间域，插值函数为sinc(x)*sinc(x/w)，w是窗口宽度。与立方或线性插值相比，所需要的采样点多很多。 </p>
<p>参考链接8.3：<a href="http://www.baas1995.org/digitalprocessing/interp/interp.htm">数字图像处理（九）插值算法之二</a></p>
<p>参考链接8.4：<a href="https://blog.csdn.net/shiyimin1/article/details/80141333">图像处理中的三次卷积插值(Cubic Convolution) - CSDN</a></p>
<p>参考链接8.5：<a href="https://blog.csdn.net/dx199771/article/details/111940754">图像插值算法——双立方（三次）卷积插值 - CSDN</a></p>
<p><strong>卷积与互相关函数的关系</strong></p>
<p>参考链接8.6：<a href="https://zhuanlan.zhihu.com/p/366472797">信号处理中的卷积、深度学习中的卷积和反卷积 - 初识CV的文章 - 知乎</a></p>
<p>参考链接8.7：<a href="https://www.zhihu.com/question/54677157/answer/1849389835">卷积(convolution)为什么叫「卷」积(「convolut」ion)？ - 初识CV的回答 - 知乎</a></p>
<p><strong>sinc函数是一组正交基？</strong></p>
<p>参考链接8.8：<a href="http://www.360doc.com/content/15/0321/23/202378_457041010.shtml">信号与系统（5）：采样定理和多分辨分析</a></p>
<p><strong>为什么Sinc是完美插值函数？</strong></p>
<p>参考链接8.9：<a href="https://zhuanlan.zhihu.com/p/150876542">为什么Sinc是完美插值函数 - starimpact的文章 - 知乎</a></p>
<p><strong>如何理解非周期的离散时间信号的傅里叶变换在频域上连续？</strong></p>
<p>题主问的是为什么频域上连续，其实频域的连续只和信号在时域上是否是周期信号有关，与信号是否离散还是连续没有关系。 如果题主明白连续非周期信号为什么在频域上连续，可以同理理解离散信号。由于周期的无穷大导致了频率幅度的无穷小，但也导致了频率的连续，但频率幅度之间的相对大小没有发生变化，所以序列傅立叶变换后的幅度值是一个相对值，叫频谱密度。 </p>
<p><strong>补零方式</strong></p>
<p>时域的末尾补零，提高频域计算分辨率，时域的中间补零，相当于是尺度变换，那么在频域，末尾补零，提高时域计算分辨率，中间补零，频域尺度变换，频域之间间隔越小，极限近似于连续，那么时域间断，那么相对的，频域之间间隔越大，时域平滑。 </p>
<p>时域上的f(t)变成f(2t)时，在频域上他的频谱就会压缩。这是时频之间的相反特性。 </p>
<p>参考链接8.10：<a href="https://www.zhihu.com/question/270235354">实现时域内插可以fft、频域补零，再ifft，那么频域补零为何要在中间部位补零呢？ - 知乎</a></p>
<p>参考链接8.11：<a href="http://zhidao.baidu.com/question/357985740?sharesource=weibo">信号与系统 时域与频域尺度变换的二个简单问题 - 百度知道</a></p>
<h4 id="参考链接：-1"><a href="#参考链接：-1" class="headerlink" title="参考链接："></a>参考链接：</h4><p>参考链接1：<a href="https://blog.csdn.net/weixin_45102840/article/details/105408476">概率密度函数及其在信号方面的简单理解（上）概率密度函数 - CSDN</a></p>
<p>参考链接2：<a href="https://www.zhihu.com/question/333121021/answer/738658182">信号的频谱，功率谱，能量谱，傅立叶级数，傅立叶展开，这几个有什么区别和联系吗，感觉很懵。？ - 张大侠的回答 - 知乎</a></p>
<p>参考链接3：<a href="https://zhuanlan.zhihu.com/p/417454806">功率谱密度（PSD） - 菜狗的文章 - 知乎</a></p>
<p>参考链接4：<a href="https://www.cnblogs.com/yongzhewudi/p/5911003.html">信号的频谱，频谱密度，能力谱区别 - 博客园</a></p>
<p>参考链接5：<a href="https://www.cnblogs.com/htj10/p/8638275.html">能量信号、功率信号、频谱、能量谱、功率谱、及一些定理 - 博客园</a></p>
<p>参考链接6：<a href="http://blog.sina.com.cn/s/blog_65d374720100kzms.html">功率谱密度（功率信号）、能量谱密度（能量信号）</a></p>
<p>参考链接7：<a href="https://zhuanlan.zhihu.com/p/449608778">离散时间傅立叶变换（DTFT） - 知乎 (zhihu.com)</a> </p>
<p>参考链接8：<a href="https://www.zhihu.com/question/39196097">做DFT时频率分辨能力的定义到底是什么？（程佩青《数字信号处理》） - 知乎</a></p>
<p>参考链接9：<a href="https://www.zhihu.com/question/489681533">在离散傅里叶变换（DFT）中如何理解采样频率、模拟频率和数字频率三者之间的关系？ - 知乎</a> </p>
<p>参考链接10：<a href="https://blog.csdn.net/u010237785/article/details/73881889">关于采样与DFT的一点点思考 - u010237785 - CSDN - dft 采样定理</a> </p>
<p>参考链接11：<a href="https://blog.csdn.net/qq_36756600/article/details/117700771">复信号处理（1）——复信号理解 - CSDN</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
        <category>信号处理与雷达</category>
        <category>信号与系统</category>
      </categories>
      <tags>
        <tag>信号与系统</tag>
      </tags>
  </entry>
  <entry>
    <title>thinking and record</title>
    <url>/2022/11/14/thinking-and-record/</url>
    <content><![CDATA[<h3 id="1-统计信号估计理论：滤波、预测、平滑-参数估计、状态估计"><a href="#1-统计信号估计理论：滤波、预测、平滑-参数估计、状态估计" class="headerlink" title="1 统计信号估计理论：滤波、预测、平滑 + 参数估计、状态估计"></a>1 统计信号估计理论：滤波、预测、平滑 + 参数估计、状态估计</h3><h4 id="1-1-什么是估计理论（Estimation）"><a href="#1-1-什么是估计理论（Estimation）" class="headerlink" title="1.1 什么是估计理论（Estimation）"></a>1.1 什么是估计理论（Estimation）</h4><p>估计理论是从被噪声污染后的量测数据中恢复无法直接观测的随机变量、随机过程或者系统某些特征的统计信号处理方法。</p>
<h4 id="1-2-估计的三种基本形式"><a href="#1-2-估计的三种基本形式" class="headerlink" title="1.2 估计的三种基本形式"></a>1.2 估计的三种基本形式</h4><h5 id="1-2-1-解释1"><a href="#1-2-1-解释1" class="headerlink" title="1.2.1 解释1"></a>1.2.1 解释1</h5><p>(1) 滤波——Filtering</p>
<p>滤波是用当前时刻及以前的数据来估计当前时刻感兴趣信息的一种运算过程。</p>
<p>(2) 预测——Prediction</p>
<p>预测是用当前时刻及以前的数据来估计未来某时刻感兴趣的信息。</p>
<p>(3) 平滑——Smoothing</p>
<p>平滑是利用感兴趣时刻之后的所有观测数据估计感兴趣时刻的信息，因此它是一种后验形式的估计。</p>
<h5 id="1-2-2-解释2"><a href="#1-2-2-解释2" class="headerlink" title="1.2.2 解释2"></a>1.2.2 解释2</h5><p>自适应滤波器的三种状态：滤波，预测，平滑</p>
<p>平滑只是滤波器的一种工作状态，如果感兴趣的信号部分在过去，现在，未来，那么滤波器是在分别做平滑，滤波，预测。根据时间序号$k$和信号样本集$N$，通过统计规则实现的自适应滤波器有三种效果：</p>
<p>(1) 预测：当$N&lt;k$，这是在预测未来的状态，我们在做预测估计(predicted estimate)，也就是预测；</p>
<p>(2) 滤波：当$N=k$，这是在用所有过去的观测值和当前的观测值估计当前的状态，这是在做滤波估计(filtered estimate)，也就是滤波；</p>
<p>(3) 平滑：当$N&gt;k$，这是在估计过去的状态，这是在做平滑估计(smoothed estimate)，也就是平滑。</p>
<blockquote>
<p>解释2来自：<a href="https://www.zhihu.com/question/35073143/answer/531133735">在信号处理和图像处理领域，滤波，平滑和去噪几个词的区别和联系？ - 甜草莓的回答 - 知乎</a></p>
</blockquote>
<h5 id="1-2-3-解释3"><a href="#1-2-3-解释3" class="headerlink" title="1.2.3 解释3"></a>1.2.3 解释3</h5><p>一个系统不可避免的受到外界的干扰，因而系统的状态一般是不能精确量测的。这样就存在着基于系统量测(输出)对系统状态进行估计(estimation)的问题</p>
<p>首先需要了解参数估计的一般理论。对于一个未知参数向量，量测是一个随机向量，一组量测构成了样本，对样本的统计量就称为对参数的一个估计量。利用样本对参数的估计量本质上是随机的，而当样本值给定时所得到的参数估计值一般与真值并不相同，因而需要用某些准则进行评价：</p>
<ul>
<li><p>如果所得估计量的数学期望等于真值，则称这个估计是对参数的一个无偏估计；</p>
</li>
<li><p>如果所得估计量依概率收敛于真值，称其是对参数的一个一致估计量。</p>
</li>
</ul>
<p>最常用的参数估计方法有最小二乘估计算法、极大似然估计算法等。近年来还发展有期望极大化算法等。</p>
<p>对于控制系统而言，状态估计问题是一个动态估计问题，分为三种不同的类型：</p>
<p>(1) 滤波问题：利用直到当前时刻的实时信息对当前的状态进行估计；</p>
<p>(2) 预测问题：利用直到当前时刻的实时信息对未来的状态进行估计；</p>
<p>(3) 平滑问题：利用直到当前时刻的实时信息对过去的状态进行估计。</p>
<p>仅仅就滤波问题而言，如果系统是线性的，一般可采用所谓卡尔曼(Kalman)滤波算法进行状态估计。由于相当多的系统不满足线性随机系统的条件，所以标准卡尔曼滤波算法并不适用。为此，针对非线性情况，早年就发展了所谓扩展卡尔曼滤波算法，即利用在线的线性化方法，把非线性系统在一个名义值附近进行线性近似，然后利用标准卡尔曼滤波算法进行一步滤波处理，然后递推实现在线滤波。</p>
<blockquote>
<p>解释3来自：<a href="https://www.zhihu.com/question/24877642/answer/263267371">滤波、预测和平滑之间的关系是什么？ - 繁星丶落幕的回答 - 知乎</a></p>
</blockquote>
<h5 id="1-2-4-解释4"><a href="#1-2-4-解释4" class="headerlink" title="1.2.4 解释4"></a>1.2.4 解释4</h5><p>这里讨论的是对信号波形进行估计的问题，或者是对函数的估计问题。一般是用某一个时间点$t$以前测量得到的信息，对某一时刻$t +\Delta t$上面的信号值进行估计。随着估计时间的不断推移，就可以得到全部信号。</p>
<p>(1) 平滑或内插：如果时间差$\Delta t &lt; 0$，这时的估计工作称为平滑或内插。其作用是一致已经得到观测点上的信号值，是对过去观测值的处理。</p>
<p>(2) 滤波：如果时间差$\Delta t = 0$，这时的估计工作称为滤波。它是对目前时间点上的信号值进行估计，一般用于抑制当前时间点上的观测噪声。</p>
<p>(3) 预测：如果时间差$\Delta t &gt; 0$，这时的估计工作是对将来时间点上的信号值进行估计，是一种预测。<br>在实际应用中，滤波应用得比较多。</p>
<blockquote>
<p>解释4来自：<a href="https://wenku.baidu.com/view/0288465b01768e9951e79b89680203d8ce2f6afc.html?_wkts_=1668424284360&amp;bdQuery=%E6%BB%A4%E6%B3%A2%E3%80%81%E9%A2%84%E6%B5%8B%E5%92%8C%E5%B9%B3%E6%BB%91">第五章 统计滤波 - 百度文库</a></p>
</blockquote>
<h4 id="1-3-滤波、预测和平滑的区别"><a href="#1-3-滤波、预测和平滑的区别" class="headerlink" title="1.3 滤波、预测和平滑的区别"></a>1.3 滤波、预测和平滑的区别</h4><p>滤波和预测是能够根据当前获得的数据实现对所期望时刻的数据估计，因此它们是一类实时(Online)运算。而平滑因需要所有观测数据进行处理，所以它是一类离线(Offline)运算或者批处理运算。</p>
<h4 id="1-4-估计的两种形式"><a href="#1-4-估计的两种形式" class="headerlink" title="1.4 估计的两种形式"></a>1.4 估计的两种形式</h4><p>(1) 参数估计：根据量测数据来估计系统中随时间不变或缓慢变化的参数，其中被估计的参数包括随机变量和非随机变量。</p>
<p>(2) 状态估计：基于状态空间模型，根据量测数据来估计系统中随时间连续变化的状态，其中包括离散时间状态和连续时间状态。</p>
<h4 id="1-5-参数估计和状态估计区别"><a href="#1-5-参数估计和状态估计区别" class="headerlink" title="1.5 参数估计和状态估计区别"></a>1.5 参数估计和状态估计区别</h4><p>参数估计本质上属于概率论和数理统计理论的一个分支。而状态估计是控制理论、计算机技术及概率论与数理统计理论相结合的产物，是现代控制理论的一个重要分支。</p>
<blockquote>
<p>以上内容转自：<a href="http://www.suanfajun.com/%e7%bb%9f%e8%ae%a1%e4%bf%a1%e5%8f%b7%e4%bc%b0%e8%ae%a1%e7%90%86%e8%ae%ba%e6%bb%a4%e6%b3%a2%e9%a2%84%e6%b5%8b%e5%b9%b3%e6%bb%91%e5%8f%82%e6%95%b0%e4%bc%b0%e8%ae%a1%e7%8a%b6%e6%80%81%e4%bc%b0%e8%ae%a1.html">统计信号估计理论：滤波、预测、平滑 + 参数估计、状态估计</a></p>
</blockquote>
<h3 id="2-维纳滤波"><a href="#2-维纳滤波" class="headerlink" title="2 维纳滤波"></a>2 维纳滤波</h3><h4 id="2-1-维纳滤波基本原理和概述"><a href="#2-1-维纳滤波基本原理和概述" class="headerlink" title="2.1 维纳滤波基本原理和概述"></a>2.1 维纳滤波基本原理和概述</h4><p>维纳(Wiener)是用来解决从噪声中提取信号的一种过滤(或滤波)方法。这种线性滤波问题，可以看做是一种估计问题或一种线性估计问题。</p>
<p>一个线性系统，如果它的单位样本响应为$h(n)$，当输入一个随机信号$x(n)$，且：</p>
<script type="math/tex; mode=display">
x(n) = s(n) + v(n) \tag{2.1.1}</script><p>其中，$x(n)$表示信号，$v(n)$表示噪声，则输出$y(n)$为：</p>
<script type="math/tex; mode=display">
y(n) = \sum_m h(m)x(n-m) \tag{2.1.2}</script><p>我们希望$x(n)$通过线性系统$h(n)$后得到的$y(n)$尽量接近于$s(n)$，因此称$y(n)$为$s(n)$的<br>估计值，用$\hat s(n)$表示，即：</p>
<script type="math/tex; mode=display">
y(n) = \hat s(n) \tag{2.1.3}</script><p>则维纳滤波器的输入—输出关系可用下面图表示。</p>
<p><img src="https://pic.imgdb.cn/item/637238eb16f2c2beb1dad908.jpg" style="zoom:80%"></p>
<p>实际上，式$(2.1.2)$所示的卷积可理解为从当前和过去的观察值$x(n), x(n-1), \cdots, x(n-m), \cdots$ 来估计信号的当前值$s(n)$。因此，用$h(n)$进行过滤问题实际上是一种统计估计问题。</p>
<p>一般地滤波、预测、平滑可以从如下划分：</p>
<ul>
<li>从当前的和过去的观察值$x(n), x(n-1), x(n-2), \cdots$估计当前的信号值$y(n)=\hat s(n)$称为过滤或滤波；</li>
<li>从过去的观察值，估计当前的或者将来的信号值$y(n)=\hat s(n +N), (N≥0)$称为外推或预测；</li>
<li>从过去的观察值，估计过去的信号值$y(n)=\hat s(n-N), (N&gt;1)$称为平滑或内插。</li>
</ul>
<p>因此维纳滤波器又常常被称为最佳线性过滤与预测或线性最优估计。这里所谓的最佳与最优是以最小均方误差为准则的。</p>
<p>如果分别以$s(n)$与$\hat s(n)$表示信号的真实值与估计值，而用$e(n)$表示它们之间的误差，即：</p>
<script type="math/tex; mode=display">
e(n)= s(n)-s(n) \tag{2.1.4}</script><p>显然$e(n)$可能是正值，也可能是负值，并且它是一个随机变量。因此，用它的均方误差来表达误差是合理的，所谓均方误差最小即它的平方的统计期望最小：</p>
<script type="math/tex; mode=display">
\xi (n) = \min E[e^2(n)] \tag{2.1.5}</script><p>用最小均方误差准则作为最佳过滤准则的原因还在于其理论分析比较简单，不要求对概率的描述。</p>
<p><strong><font color="red">维纳-霍夫方程的求解</font></strong></p>
<p>为了按$(2.1.5)$式所示的最小均方误差准则来确定维纳滤波器的冲激响应$h(n)$，令$\xi(n)$对$h(j)$的导数等于零，即可得：</p>
<script type="math/tex; mode=display">
R_{xs}(m) = \sum_i h(i)R_{xx}(m-i) \tag{2.1.6}</script><p>式中，$R_{xs}(m)$是$s(n)$与$x(n)$的互相关函数，$R_{xx}(m)$是$x(n)$的自相关函数，分别定义为：</p>
<script type="math/tex; mode=display">
R_{xx} = E[x(n)s(n+m)] \qquad R_{xx} = E[x(n)x(n+m)]</script><p>式$(2.1.6)$称为维纳滤波器的标准方程或维纳-霍夫(Wiener-Hopf)方程。若已知$R_{xs}(m)$和$R_{xx}(m)$，那么解此方程即可求的维纳滤波器的冲激响应。</p>
<p>式$(2.1.6)$所示标准方程右端的求和范围即$i$的取值范围没有具体标明，实际上有三种情况：</p>
<ul>
<li>有限冲激响应(FIR)维纳滤波器，$i$从$0$到$N-1$取得有限个整数值；</li>
<li>非因果无限冲激响应(非因果IIR)维纳滤波器，$i$从$-\infty$到$+\infty$所有整数值;</li>
<li>因果无限冲激响应(因果IIR)维纳滤波器，$i$从$0$到$+\infty$取正整数值。</li>
</ul>
<p>上述三种情况下标准方程的解法不同，本文只描述FIR维纳滤波器的求解。设滤波器冲激响应序列的长度为$N$，冲激响应矢量为：</p>
<script type="math/tex; mode=display">
h = [h(0), h(1), \cdots,h(N-1) ]^{\mathrm T} \tag{2.1.7}</script><p>滤波器的输入数据矢量：</p>
<script type="math/tex; mode=display">
x = [x(n), x(n-1), \cdots, x(n-N+1)]^{\mathrm T} \tag{2.1.8}</script><p>则滤波器的输出为：</p>
<script type="math/tex; mode=display">
y(n) = \hat s(n) = x^{\mathrm T}h = h^{\mathrm T}x \tag{2.1.9}</script><p>这样，式$(2.1.6)$所示的维纳-霍夫方程可写成：</p>
<script type="math/tex; mode=display">
P^{\mathrm T} = h^{\mathrm T}R \quad {\mathrm{OR}} \quad P =  R^{\mathrm T}h \tag{2.1.10}</script><p>其中，$P = E[x(n)s(n)]$是$s(n)$与$x(n)$的互相关函数，它是一个$N$维列矢量；$R$是$x(n)$的自相关函数，是$N$阶方阵，$R = E[x(n)x^{\mathrm T}(x)]$</p>
<p>利用求逆矩阵的方法直接求解式$(2.1.10)$，得：</p>
<script type="math/tex; mode=display">
h_{\mathrm{opt}} = R^{-1}P</script><p>这里opt表示“最佳”，这就是FIR维纳滤波器的冲激响应。</p>
<blockquote>
<p>以上内容主要转自：<a href="https://www.renrendoc.com/paper/179876044.html">维纳滤波器</a></p>
</blockquote>
<h3 id="3-卡尔曼滤波"><a href="#3-卡尔曼滤波" class="headerlink" title="3 卡尔曼滤波"></a>3 卡尔曼滤波</h3><p> <a href="https://www.cnblogs.com/laozhu1234/p/14932627.html">关于卡尔曼滤波中协方差矩阵Q,R的一些思考,卡尔曼原理讲解</a>：这个人写的感觉挺不错，但没有细看。</p>
<p>卡尔曼滤波算法从名称上来看落脚点是一个滤波算法，一般的滤波算法都是频域滤波，而卡尔曼滤波算法是一个时域滤波，时域就是它的强大之处。卡尔曼滤波也是一种最优估计算法，常见的最优估计算法有“最小二乘法”等，卡尔曼滤波也是一个迭代器，根据以知的先验值，预测下一时刻的估计值。<br>再说说滤波这个名词，本质上就是给加权。既然是加权，那么卡尔曼滤波算法本质就是数据融合的操作(Data fusion)，卡尔曼增益就是融合过程中的权重，融合的内容有两个：分别是满足一定分布的先验状态估计值和满足一定分布的观测值之间的融合（也就是你们可能在其他资料上看到的两个椭圆相乘，其实就是数据的融合），融合后得到后验状态估计值。</p>
<p>其实深度学习中的卷积操作也是加权求和，本质上也是滤波，本质上也是求导，比如边缘检测的离散微分算子sobel的计算过程，也是分别在x和y两个方向上求导，提取出边缘特征。（下面是我个人的思考）由点及面，我们就可以想像到一个大尺寸的卷积核，其实就相当于“变形求导”，通过卷积核内部权重数值大小给定求导曲线，卷积就是求解在这条曲线上的导数。 一个卷积核是提取特征，那么多个卷积核组合起来就有无限可能，提取到的就是高维特征（高维特征对应图片的纹理等深层次的语义信息），组合起来非常之强大，就能够实现深度学习中的分类、边框回归等功能。</p>
<blockquote>
<p>以上内容转自：<a href="https://blog.csdn.net/qq_39523365/article/details/109094713">深入理解卡尔曼滤波算法</a></p>
</blockquote>
<p>维纳滤波与卡尔曼滤波比较：</p>
<p>解释1：</p>
<p>维纳滤波和卡尔曼滤波都是解决线性滤波和预测问题的方法，并且都是以均方误差最小为准则的，在平稳条件下两者的稳态结果是一致的。但是它们解决问题的方法有很大区别。维纳滤波是根据全部过去观测值和当前观测值来估计信号的当前值，因此它的解形式是系统的传递函数$S(k) = A(k)S(k-1)+w_1(k-1)$</p>
<p>卡尔曼滤波是用当前一个估计值和最近一个观测值来估计信号的当前值,它的解形式是状态变量值。维纳滤波只适用于平稳随机过程,卡尔曼滤波就没有这个限制。设计维纳滤波器要求已知信号与噪声的相关函数,设计卡尔曼滤波要求已知状态方程和量测方程,当然两者之间也有联系。 </p>
<blockquote>
<p>以上内容转自：<a href="https://easylearn.baidu.com/edu-page/tiangong/questiondetail?id=1721316694860560840&amp;fr=search">比较维纳滤波和卡尔曼滤波方法的区别和联系</a></p>
</blockquote>
<p>解释2：</p>
<ul>
<li><p>共同点：都解决最佳线性滤波和预测问题，都以均方误差最小为最优准则，平稳条件下它们得到的稳态结果一致。</p>
</li>
<li><p>不同点:</p>
<ul>
<li><p>维纳滤波根据$x(n),x(n-1),…,x(1)$估计信号的当前值，它的解以系统的系统函数$H(z)$或单位脉冲响应$h(n)$形式给出。这种系统常称为最佳线性滤波器。</p>
<p>卡尔曼滤波用前一个估计值和最近一个观察数据来估计信号当前值，它用状态方程和递推的方法进行估计，它的解以估计值（常是状态变量值）形式给出。系统常称为线性最优估计器。</p>
</li>
<li><p>维纳滤波只适用于平稳随机过程；卡尔曼滤波适用于平稳和非平稳随机过程。</p>
</li>
<li><p>维纳滤波设计时要已知信号与噪声的统计分布规律。卡尔曼滤波设计时要求已知状态方程和量测方程。</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>以上内容转自：<a href="https://www.docin.com/p-1404646080.html">维纳滤波和卡尔曼滤波 - 豆丁网 (docin.com)</a> </p>
</blockquote>
<p><a href="http://t.zoukankan.com/dushikang-p-8310821.html">卡尔曼滤波的原理与思想 - 走看看</a></p>
<p><a href="https://blog.csdn.net/xiaoma_bk/article/details/82491083">卡尔曼滤波 kalman  理解</a></p>
<p><a href="https://bbs.21ic.com/icview-292853-1-1.html">授之以渔： 卡尔曼滤波器 ….大泻蜜</a></p>
<p><a href="https://blog.csdn.net/miraito_wa/article/details/125968274">卡尔曼滤波算法总结</a></p>
<p><a href="https://blog.csdn.net/shanzsz/article/details/87282192">其他-卡尔曼滤波</a></p>
<p><a href="https://blog.csdn.net/m0_64007201/article/details/127410885">卡尔曼滤波介绍</a></p>
<p><a href="https://blog.csdn.net/Mr_sticker/article/details/106918752">卡尔曼滤波的个人理解</a></p>
<p><a href="https://www.bilibili.com/read/cv17659049">世界上应用最广泛的算法之一的卡尔曼滤波算法原理-从放弃到精通-无人机/机器人应用 - 哔哩哔哩 (bilibili.com)</a> ：卡尔曼滤波器的通俗举例 - 好了，现在对于某一分钟我们有两个有关于该房间的温度值。</p>
<p><a href="https://blog.csdn.net/weixin_43096365/article/details/122447840">卡尔曼滤波算法 - 生命不止，学习不息的博客-CSDN博客</a> </p>
<p><a href="https://blog.csdn.net/sillykog/article/details/78535767">卡尔曼滤波和维纳滤波 - sillykog的博客-CSDN博客_卡尔曼滤波和维纳滤波的区别</a> </p>
<p><a href="https://blog.csdn.net/weixin_42570192/article/details/122265199">维纳滤波和卡尔曼滤波 - 西岸贤的博客-CSDN博客_维纳滤波和卡尔曼滤波</a> </p>
<p><a href="https://blog.csdn.net/victor_zy/article/details/82862904?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromBaidu-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromBaidu-1.control">卡尔曼滤波算法详细推导 - victorzy的博客-CSDN博客_卡尔曼滤波推导</a> </p>
<h3 id="4-协方差"><a href="#4-协方差" class="headerlink" title="4 协方差"></a>4 协方差</h3><p><a href="https://blog.csdn.net/weixin_46021869/article/details/117334362">初学）从协方差到协方差矩阵，再到特征向量和特征值</a></p>
<p><a href="http://www.360doc.com/content/17/0810/21/37752273_678258912.shtml">协方差矩阵的特征向量指的是什么</a></p>
<p><a href="https://www.lianxh.cn/news/9a0b602dbe8fb.html">主成分分析-交互固定效应基础：协方差矩阵的几何意义</a>：好像写的很好！！！</p>
<p><a href="https://blog.csdn.net/qq_38148024/article/details/97269169">协方差的几何意义</a></p>
<p><a href="https://blog.csdn.net/goodshot/article/details/50961720">协方差的意义和计算公式</a></p>
<h3 id="5-距离分辨率与带宽"><a href="#5-距离分辨率与带宽" class="headerlink" title="5 距离分辨率与带宽"></a>5 距离分辨率与带宽</h3><h4 id="5-1-距离分辨力"><a href="#5-1-距离分辨力" class="headerlink" title="5.1 距离分辨力"></a>5.1 距离分辨力</h4><p>距离分辨力本质上是和脉冲宽度有关，脉宽越窄，距离分辨力越高，但是由于在发射机峰值功率一定的情况下，脉宽越窄，对应的平均功率越小，这会导致雷达最大可测距离减小，这就是俗称的距离分辨力与最大可测距离之间的矛盾。</p>
<p>为了解决这个矛盾，雷达接收机中频信号处理引入匹配滤波技术(时宽近似为带宽分之一)，可以解决这个矛盾。chirp信号发射的一般是大时宽带宽积的信号，大时宽保证最大可测距离，大带宽(MF之后对应小时宽)保证高的距离分辨力。</p>
<ul>
<li>距离分辨率本质上到底是看脉宽还是看带宽？ <ul>
<li>早期的确是时宽，因为考虑到最大可测距离与距离分辨力的矛盾，引入脉压技术，所以现代雷达说的都是带宽。  本质是时(脉)宽，但一般现代雷达中脉宽和带宽存在一个倒数关系，所以经常用那个带宽的公式。</li>
</ul>
</li>
</ul>
<h4 id="5-2-速度分辨力"><a href="#5-2-速度分辨力" class="headerlink" title="5.2 速度分辨力"></a>5.2 速度分辨力</h4><p>至于速度分辨力(多普勒分辨力)和雷达对目标回波做的相参积累时间T有关，近似等于$\dfrac{1}{T}$。</p>
<blockquote>
<p>以上内容转自：<a href="https://www.zhihu.com/question/318858527/answer/643253088">线性调频脉冲信号的距离分辨率和速度分辨率与什么因素有关？ - fever wong的回答 - 知乎</a></p>
</blockquote>
<h3 id="6-随机信号处理笔记之白噪声"><a href="#6-随机信号处理笔记之白噪声" class="headerlink" title="6 随机信号处理笔记之白噪声"></a>6 随机信号处理笔记之白噪声</h3><h4 id="6-1-白噪声的概念"><a href="#6-1-白噪声的概念" class="headerlink" title="6.1 白噪声的概念"></a>6.1 白噪声的概念</h4><p>“白噪声”，Additive White Gaussian Noise(AWGN)。“白”的概念来自于光学，和白光的“白”是同一个意思，指的是包含所有频率分量的噪声，且这所有的频率分量是等值的。</p>
<p>白噪声，就是说功率谱为一常数；即其协方差函数在时延$\tau = 0$时不为0，在$\tau \neq 0$时值为0； 换句话说，样本点互不相关。 所以“白”与“不白”是和分布没有关系的。 当随机地从高斯分布中获取采样值时，采样点所组成的随机过程就是<strong>“高斯白噪声”</strong>； 同理，随机从均匀分布中获取采样值时，采样点所组成的随机过程就是<strong>“均匀白噪声”</strong>。 </p>
<h4 id="6-2-白噪声的统计学定义"><a href="#6-2-白噪声的统计学定义" class="headerlink" title="6.2 白噪声的统计学定义"></a>6.2 白噪声的统计学定义</h4><p>如果白噪声的功率谱密度在所有频率上都是一个常数(功率谱密度只与白噪声与否有关，即与自相关函数有关，与高斯与否无关。 )：</p>
<script type="math/tex; mode=display">
P(\omega) = \frac{N_0}{2}</script><p>其中，$N_0&gt;0, \omega \in (-\infty, \infty)$，则称该噪声为白噪声。 白噪声的单边功率谱密度： </p>
<script type="math/tex; mode=display">
P_{单}(\omega) = N_0</script><p>其中，$N_0&gt;0, \omega \in (0, \infty)$。</p>
<h4 id="6-3-白噪声的自相关函数"><a href="#6-3-白噪声的自相关函数" class="headerlink" title="6.3 白噪声的自相关函数"></a>6.3 白噪声的自相关函数</h4><p>根据维纳-辛钦定理，平稳随机过程的功率谱密度函数和自相关函数是傅里叶变换对。</p>
<p>白噪声的自相关函数：</p>
<script type="math/tex; mode=display">
R(\tau) = \frac{N_0}{2}\delta(\tau)</script><p>对于所有的$\tau \neq 0$，都有$R(\tau) = 0$，说明白噪声仅在$\tau=0$时刻才是相关的，而在其他时刻（$\tau \neq 0$)的随机变量都是不相关的。</p>
<p>白噪声的平均功率： </p>
<script type="math/tex; mode=display">
R(0) = \frac{N_0}{2}\delta(0) = \infty</script><p>因此真正“白”的噪声是不存在的。实际工程应用中，只要噪声的功率谱密度均匀分布的频率范围远大于通信系统的工作频带($3 \text{ dB}$带宽)，就可将其视作白噪声。 </p>
<blockquote>
<p>以上内容主要转自：<a href="https://blog.csdn.net/qq_43045275/article/details/106472372">随机信号处理笔记之白噪声</a></p>
</blockquote>
<h3 id="7-窄带噪声、高斯噪声、白噪声"><a href="#7-窄带噪声、高斯噪声、白噪声" class="headerlink" title="7 窄带噪声、高斯噪声、白噪声"></a>7 窄带噪声、高斯噪声、白噪声</h3><h4 id="7-1-概念解析"><a href="#7-1-概念解析" class="headerlink" title="7.1 概念解析"></a>7.1 概念解析</h4><ul>
<li>高斯噪声是指它的概率密度函数服从高斯分布(即正态分布)的一类噪声；</li>
<li>窄带噪声是指频带范围较窄的一类噪声，系统的频带宽度远远小于其中心频率的系统；</li>
<li>白噪声是指它的功率谱密度函数在整个频域内是常数。</li>
</ul>
<p>可以看出他们描述的属于不同的领域，<strong>高斯噪声是从概率方面描述</strong>，<strong>窄带是从带宽方面描述</strong>，<strong>白噪声是从功率方面描述</strong>。</p>
<blockquote>
<p>以上内容转自：<a href="https://zhuanlan.zhihu.com/p/131593352">窄带噪声、高斯噪声、白噪声 - 嵌入式随笔的文章 - 知乎</a></p>
</blockquote>
<h4 id="7-2-高斯白噪声"><a href="#7-2-高斯白噪声" class="headerlink" title="7.2 高斯白噪声"></a>7.2 高斯白噪声</h4><p>高斯型白噪声也称高斯白噪声，是指噪声的概率密度函数满足正态分布统计特性，同时它的功率谱密度函数是常数的一类噪声。高斯型白噪声同时涉及到噪声的两个不同方面，即概率密度函数的正态分布性和功率谱密度函数均匀性，二者缺一不可。 </p>
<p>假设高斯白噪声服从$(0, \sigma^2)$正态分布，则其自相关函数为：</p>
<script type="math/tex; mode=display">
R(\tau) = \sigma^2 \delta(\tau)</script><p>高斯白噪声的功率谱密度$P(\omega)$是其自相关函数$\R(\tau)$的傅里叶变换，有：</p>
<script type="math/tex; mode=display">
P(\omega) = \frac{N_0}{2} = \sigma^2</script><p>也就是在通信信道中，一般噪声的均值$\mu = 0$，那么当噪声的均值是零的时候，噪声的平均功率等于其方差。 </p>
<blockquote>
<p>以上内容转自：<a href="https://www.zhihu.com/question/35577235/answer/1368652133">高斯白噪声的自相关函数是什么？ - 高宏的回答 - 知乎</a></p>
</blockquote>
<ul>
<li><strong>高斯色噪声</strong></li>
</ul>
<p>那么，是否有“非白的高斯”噪声呢？答案是肯定的，这就是”高斯色噪声“。</p>
<p>这种噪声其分布是高斯的，但是它的频谱不是一个常数，或者说，对高斯信号采样的时候不是随机采样的，而是按照某种规律来采样的。</p>
<blockquote>
<p>以上内容转自：<a href="https://www.cnblogs.com/angel/articles/459650.html">噪声的讨论 - 博客园</a></p>
</blockquote>
<h4 id="7-3-几个问题的辨析解释"><a href="#7-3-几个问题的辨析解释" class="headerlink" title="7.3 几个问题的辨析解释"></a>7.3 几个问题的辨析解释</h4><h5 id="7-3-1-白噪声的方差等于什么"><a href="#7-3-1-白噪声的方差等于什么" class="headerlink" title="7.3.1 白噪声的方差等于什么"></a>7.3.1 白噪声的方差等于什么</h5><p>当均值为零时，高斯白噪声的功率谱密度等于方差，此时的功率谱密度为双边功率谱密度，数字信号处理上面有关于这些的推导，$\dfrac{n_0}{2}$是双边功率谱密度，$n_0$为单边功率谱密度，一般都是考虑双边的。</p>
<p>通常文献所说的噪声的方差，是指加性高斯白噪声(AWGN)的经过采样后的采样值的方差，具体说，就是数字通信原理里说的在<strong>相关接收/或匹配滤波</strong>后的输出，此时的方差是$\dfrac{N_0}{2}$（见Proakis的digital communcations的第五章第一、二节）。</p>
<p>若说某个白高斯过程的方差是$\dfrac{N_0}{2}$，这个提法本身就是不严格的，因为对于均值为0的高斯白噪声，方差就是总功率，因为：</p>
<script type="math/tex; mode=display">
总功率=直流功率+交流功率=0+交流功率=0+方差</script><p>因为均值为0，所以直流功率为0，方差就是交流功率，亦即总功率。</p>
<p>根据白噪声的定义，其功率谱在整个频段上都是一个常数，此时总功率是对功率谱在整个频段上的积分，这个总功率自然就是无穷大的了。 实际上，即使输入的噪声真的是AWGN，经过相关接收/或匹配滤波后，噪声项就不再是白的了，因为相关接收/匹配滤波器本身就可以被看作是一个滤波器(积分器的系统函数为1,即是个低通)，<strong>wgn经过滤波器后，功率自然就不再是无穷大了</strong>，虽然它的相关接收/匹配滤波器输出采样值分布仍然是高斯的。</p>
<p><strong>如果有作者说“高斯白噪声服从的是均值为0，方差为$\dfrac{N_0}{2}$的高斯分布”，那一定是指白噪声经过相关接收/或匹配滤波后并采样获得的离散随机过程是均值为0，方差为$\dfrac{N_0}{2}$的高斯分。</strong> </p>
<blockquote>
<p>以上内容转自：<a href="https://zhidao.baidu.com/question/369362806302624204.html">白噪声的方差等于什么_百度知道 (baidu.com)</a></p>
</blockquote>
<h5 id="7-3-2-为什么高斯白噪声的平均功率等于方差"><a href="#7-3-2-为什么高斯白噪声的平均功率等于方差" class="headerlink" title="7.3.2 为什么高斯白噪声的平均功率等于方差"></a>7.3.2 为什么高斯白噪声的平均功率等于方差</h5><p>高斯白噪声$\xi(t)$是一种平稳的随机过程，假设该过程为<em>ξ</em>(<em>t</em>)，那么其自相关函数的定义如下： </p>
<script type="math/tex; mode=display">
R(t) =E(\xi(t) \xi(t+\tau))</script><p>随机过程的自相关函数非常重要，它有两条非常重要的性质： </p>
<ul>
<li>$R(0) = E(\xi^2(t)]$，表示平均功率</li>
<li>$R(\infty) = E^2(\xi(t))$，表示直流功率</li>
</ul>
<p>为什么$R(0)$表示平均功率，$R(\infty)$表示直流功率呢？</p>
<p>其实$R(0)$表示平均功率相对好理解一些，输入的信号是$ξ(t)$，信号的平方就是功率，对功率取个$E(\cdot)$，就是取平均，那就是平均功率了。</p>
<p>当$τ$为无穷大时，$ξ(t)$和$ξ(t+\tau)$相当于独立同分布的两个随机变量了，因此：</p>
<script type="math/tex; mode=display">
\begin{aligned}
R(\infty) &=E[\xi(t) \xi(t+\tau)] \\
&=E[\xi(t)] \cdot E[\xi(t+\tau)] \\
&=E[\xi(t)] \cdot E[\xi(t)] \\
&=E^2[\xi(t)]
\end{aligned}</script><p>这个就是信号先求平均(即直流分量)，再平方结果自然就是直流的功率了。 </p>
<p><strong>自相关函数$R(\tau)$也叫二阶原点矩，而自协方差函数$C(\tau)$是二阶中心矩</strong>，它的定义为： </p>
<script type="math/tex; mode=display">
\begin{aligned}
C(\tau) &=E[[\xi(t)-m(t)][\xi(t+\tau)-m(t+\tau)]] \\
&=E[\xi(t) \xi(t+\tau)]-m(t)) \cdot m(t+\tau)
\end{aligned}</script><p>其中，$m(t)$表示$t$时刻的平均值。当$\tau = 0$时，$C(0) = E[\xi^2 (t)]−m^2 (t) = R(0) − m^2 (t)$，即平均功率减去均值平方，表示方差。所以，对于高斯白噪声来说，它的均值为0，即$m(t)$为0，因此平均功率等于方差。</p>
<blockquote>
<p>以上内容转自：<a href="https://zhuanlan.zhihu.com/p/345016734">为什么高斯白噪声的平均功率等于方差？ - 张大侠的文章 - 知乎</a></p>
</blockquote>
<h5 id="7-3-3-待定。。。之后根据下面的链接进行补充"><a href="#7-3-3-待定。。。之后根据下面的链接进行补充" class="headerlink" title="7.3.3 待定。。。之后根据下面的链接进行补充"></a>7.3.3 待定。。。之后根据下面的链接进行补充</h5><p><a href="https://www.zhihu.com/question/52360072">为何连续高斯白噪声的方差定义是无穷大？ - 知乎</a>；<a href="https://blog.csdn.net/bananashoes/article/details/83029099">高斯白噪声解惑_hupipi96的博客-CSDN博客</a> ；<a href="https://blog.csdn.net/gllnupt/article/details/41149671?spm=1001.2101.3001.6650.4&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-4-41149671-blog-83029099.pc_relevant_3mothn_strategy_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-4-41149671-blog-83029099.pc_relevant_3mothn_strategy_recovery&amp;utm_relevant_index=5">学了3,4年，终于明白了高斯白噪声的那些东西。_gllnupt的博客-CSDN博客</a> ；<a href="https://blog.csdn.net/lsg32/article/details/8646407?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-8646407-blog-83029099.pc_relevant_3mothn_strategy_recovery&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-8646407-blog-83029099.pc_relevant_3mothn_strategy_recovery&amp;utm_relevant_index=1">高斯噪声和高斯白噪声相关介绍_咆哮的狼的博客-CSDN博客</a> ；<a href="https://blog.csdn.net/hltt3838/article/details/125206849">高斯白噪声（white Gaussian noise，WGN）_他人是一面镜子，保持谦虚的态度的博客-CSDN博客</a> ；<a href="https://blog.csdn.net/From0_to1_/article/details/120947023">高斯白噪声与信噪比 - CSDN</a>；<a href="http://www.360doc.com/content/20/0119/23/13328254_887072728.shtml">噪声相关笔记 - 360个人图书馆</a>；<a href="http://staff.ustc.edu.cn/~wyzhou/chapter2%20basic.pdf">通信基本概念与基础知识</a>；<a href="https://www.sohu.com/a/155330773_464087">白噪声高斯噪声高斯白噪声的区别 - 搜狐</a>；<a href="https://blog.csdn.net/u011650143/article/details/69047456">高斯白噪声 - CSDN</a>；<a href="https://xk.qust.edu.cn/__local/5/48/53/C8018705C781490F59C28EDBA88_6B7849BC_7F781.pdf?e=.pdf">3.4 信道的加性噪声 一、噪声的分类 </a></p>
<h3 id="8-自相关矩阵与协方差矩阵"><a href="#8-自相关矩阵与协方差矩阵" class="headerlink" title="8 自相关矩阵与协方差矩阵"></a>8 自相关矩阵与协方差矩阵</h3><p><a href="https://blog.csdn.net/weixin_39910711/article/details/112726698?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E8%87%AA%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-112726698.nonecase&amp;spm=1018.2226.3001.4187">自相关函数与互相关函数 - CSDN</a></p>
<p><a href="https://blog.csdn.net/shulianghan/article/details/123248041?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166878585816782414963883%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=166878585816782414963883&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-4-123248041-null-null.142^v65^pc_rank_34_queryrelevant25,201^v3^control_2,213^v2^t3_esquery_v2&amp;utm_term=%E8%87%AA%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0&amp;spm=1018.2226.3001.4187">【数字信号处理】相关函数应用 ( 高斯白噪声 的 自相关函数 分析 )</a></p>
<p><a href="https://blog.csdn.net/feng__shuai/article/details/53609543?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166878585816782414963883%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=166878585816782414963883&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-53609543-null-null.142^v65^pc_rank_34_queryrelevant25,201^v3^control_2,213^v2^t3_esquery_v2&amp;utm_term=%E8%87%AA%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0&amp;spm=1018.2226.3001.4187">自相关函数的理解 - CSDN</a></p>
<p><a href="https://blog.csdn.net/qq_41691212/article/details/121870083">【什么是自相关矩阵，自协方差矩阵，互相关矩阵，互协方差矩阵？】 - CSDN</a></p>
<h3 id="9-卷积和相关"><a href="#9-卷积和相关" class="headerlink" title="9 卷积和相关"></a>9 卷积和相关</h3><p><a href="https://www.zhihu.com/question/22298352">如何通俗易懂地解释卷积？ - 知乎</a></p>
<p><a href="https://www.eet-china.com/mp/a80624.html">信号处理绕不过去的坎：相关与卷积 - 面包板</a></p>
<h3 id="10-条件期望"><a href="#10-条件期望" class="headerlink" title="10 条件期望"></a>10 条件期望</h3><h4 id="10-1-条件期望的定义-摘自Wiki"><a href="#10-1-条件期望的定义-摘自Wiki" class="headerlink" title="10.1 条件期望的定义(摘自Wiki)"></a>10.1 条件期望的定义(摘自Wiki)</h4><p>(1) 设$X$和$Y$都是离散型随机变量，则$X$在给定事件$Y=y_k$条件时的条件期望： </p>
<script type="math/tex; mode=display">
E(X \mid Y=y) = \sum_i x_i P(X=x_i \mid Y=y_k)</script><p>(2) 设$X$是连续型随机变量，$Y$是离散型随机变量，$X$在给定事件$Y=y_k$条件时的条件期望： </p>
<script type="math/tex; mode=display">
E(X \mid Y=y) = \int_{-\infty}^{\infty} x f_{X \mid Y}(x \mid Y=y_k) \text{ d}x</script><p> (3) 设$X$和$Y$都是连续型随机变量，联合概率密度为$f_{X,Y}(x,y)$，$Y$的密度函数为$f_Y(y)$，则$X$的条件概率密度函数：</p>
<script type="math/tex; mode=display">
f_{X \mid Y}(x \mid y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}</script><p>在给定事件$Y=y$条件时的条件期望：</p>
<script type="math/tex; mode=display">
E(X \mid Y=y) = \int_{-\infty}^{\infty} x f_{X \mid Y}(x \mid y) \text{ d}x = \frac{1}{f_Y(y)}\int_{-\infty}^{\infty} x f_{X,Y}(x,y) \text{ d}x</script><h4 id="10-2-区分-E-X-、-E-X-mid-Y-、-E-X-mid-Y-y"><a href="#10-2-区分-E-X-、-E-X-mid-Y-、-E-X-mid-Y-y" class="headerlink" title="10.2 区分$E(X)$、$E(X \mid Y)$、$E(X \mid Y=y)$"></a>10.2 区分$E(X)$、$E(X \mid Y)$、$E(X \mid Y=y)$</h4><ul>
<li>$E(X)$：一个数；</li>
<li>$E(X \mid Y)$：随机变量，关于$Y$的函数，没有固定的$y$值；</li>
<li>$E(X \mid Y=y)$ ：不是随机变量，而是一个关于$y$的函数$f(y)$，对于给定的$y$，有唯一确定值与之对应。</li>
</ul>
<p>$E(X)$是对所有$\omega \in \Omega$， $X(\omega)$取值<strong>全体的加权平均</strong>；而$E(X \mid Y=y)$是局限在$\omega \in \begin{Bmatrix} \omega \mid Y(\omega)=y \end{Bmatrix}$时，$X(\omega)$ 取值<strong>局部的加权平均</strong>。</p>
<h4 id="10-4-全期望公式-Law-of-total-expectation"><a href="#10-4-全期望公式-Law-of-total-expectation" class="headerlink" title="10.4 全期望公式(Law of total expectation)"></a>10.4 全期望公式(Law of total expectation)</h4><p>设$X,Y$为随机变量，下列期望和条件期望均存在，则：</p>
<script type="math/tex; mode=display">
E(X) = E(E(X \mid Y))</script><ul>
<li>若$Y$为连续型随机变量，则：</li>
</ul>
<script type="math/tex; mode=display">
E(X) = E(E(X \mid Y)) = \int_{-\infty}^{\infty} E(X \mid Y = y) f_Y(y) \text{ d}y</script><ul>
<li>若$Y$为离散型随机变量，则：</li>
</ul>
<script type="math/tex; mode=display">
E(X) = E(E(X \mid Y)) = \sum_i E(X \mid Y = y_i) P(Y = y_i)</script><blockquote>
<p>以上内容转自：<a href="https://zhuanlan.zhihu.com/p/417592820">条件期望与全期望公式 - Ryan的文章 - 知乎</a></p>
</blockquote>
<h4 id="10-5-其他几点"><a href="#10-5-其他几点" class="headerlink" title="10.5 其他几点"></a>10.5 其他几点</h4><h5 id="10-5-1-条件期望是否能去掉"><a href="#10-5-1-条件期望是否能去掉" class="headerlink" title="10.5.1 条件期望是否能去掉"></a>10.5.1 条件期望是否能去掉</h5><p>(1) 设$h(\cdot)$是一个函数，则：</p>
<script type="math/tex; mode=display">
\begin{aligned}
E(X \cdot h(Y))  &=  E[E(X \cdot h(Y) \mid Y)] \\
&=  \int_{-\infty}^{\infty} E(X \cdot h(Y) \mid Y = y) f_Y(y) \text{ d}y \\
&= \int_{-\infty}^{\infty} E(X \cdot h(y) \mid Y = y) f_Y(y) \text{ d}y \\
&=  \int_{-\infty}^{\infty} h(y)E(X \mid Y = y) f_Y(y) \text{ d}y \\
&= \int_{-\infty}^{\infty} h(y) [E(X)](Y)_{Y = y} f_Y(y) \text{ d}y \\
&= \int_{-\infty}^{\infty} h(y) G(Y)_{Y = y} f_Y(y) \text{ d}y
\end{aligned}</script><p>e.g. $X \sim \text{Exp}(Y)$，则：</p>
<script type="math/tex; mode=display">
\begin{aligned}
E(X)  &= E(E(X \mid Y)) \\
&=  \int_{-\infty}^{\infty} E(X \mid Y = y) f_Y(y) \text{ d}y \\
&= \int_{-\infty}^{\infty} G(Y)_{Y = y} f_Y(y) \text{ d}y \\
&= \int_{-\infty}^{\infty} \frac{1}{Y} \mid_{Y = y} f_Y(y) \text{ d}y \\
&= \int_{-\infty}^{\infty} \frac{1}{y} f_Y(y) \text{ d}y 
\end{aligned}</script><h3 id="11-群延时与相位延时"><a href="#11-群延时与相位延时" class="headerlink" title="11 群延时与相位延时"></a>11 群延时与相位延时</h3><p><a href="https://blog.csdn.net/qq_37335890/article/details/83043204?spm=1001.2101.3001.6650.19&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-19-83043204-blog-79203927.pc_relevant_3mothn_strategy_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-19-83043204-blog-79203927.pc_relevant_3mothn_strategy_recovery&amp;utm_relevant_index=21">信号与系统学习难点（一）群时延与相频特性</a></p>
<p><a href="https://blog.csdn.net/zhoupian/article/details/117935161">一文读懂群延时(Group Delay)：非常简单易懂</a></p>
<h3 id="12-幅频特性和相频特性"><a href="#12-幅频特性和相频特性" class="headerlink" title="12 幅频特性和相频特性"></a>12 幅频特性和相频特性</h3><p><a href="https://blog.csdn.net/christiedong10/article/details/122115486?spm=1001.2101.3001.6650.12&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-12-122115486-blog-83043204.pc_relevant_3mothn_strategy_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-12-122115486-blog-83043204.pc_relevant_3mothn_strategy_recovery&amp;utm_relevant_index=19">幅频特性和相频特性的物理意义</a></p>
<h3 id="13-维纳滤波器和平时所说的低通频域滤波器的区别"><a href="#13-维纳滤波器和平时所说的低通频域滤波器的区别" class="headerlink" title="13 维纳滤波器和平时所说的低通频域滤波器的区别"></a>13 维纳滤波器和平时所说的低通频域滤波器的区别</h3><p><a href="https://web.xidian.edu.cn/kywang/files/20171213_190142.pdf">第五章 数字滤波器的基本概念及一些特殊滤波器</a></p>
<h3 id="14-线性代数中各种矩阵的简介及其性质"><a href="#14-线性代数中各种矩阵的简介及其性质" class="headerlink" title="14 线性代数中各种矩阵的简介及其性质"></a>14 线性代数中各种矩阵的简介及其性质</h3><p><a href="https://blog.csdn.net/wnma3mz/article/details/90182652?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-4-90182652-blog-647511.pc_relevant_recovery_v2&amp;spm=1001.2101.3001.4242.3&amp;utm_relevant_index=7">线性代数中各种矩阵的简介</a></p>
<p><a href="https://blog.csdn.net/myan/article/details/647511?spm=1001.2101.3001.6650.6&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-6-647511-blog-105778485.pc_relevant_recovery_v2&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-6-647511-blog-105778485.pc_relevant_recovery_v2&amp;utm_relevant_index=10">理解矩阵（一）</a></p>
<p><a href="https://blog.csdn.net/I_canjnu/article/details/105778485?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3-105778485-blog-104575987.pc_relevant_3mothn_strategy_and_data_recovery&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-3-105778485-blog-104575987.pc_relevant_3mothn_strategy_and_data_recovery&amp;utm_relevant_index=6">矩阵相关定义性质全总结</a></p>
<p><a href="https://blog.csdn.net/qq_41157212/article/details/104575987?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-104575987-blog-74169484.pc_relevant_default&amp;spm=1001.2101.3001.4242.1&amp;utm_relevant_index=3">矩阵的分类以及性质</a></p>
<h3 id="15-向量的模和范数"><a href="#15-向量的模和范数" class="headerlink" title="15 向量的模和范数"></a>15 向量的模和范数</h3><h4 id="15-1-向量的模"><a href="#15-1-向量的模" class="headerlink" title="15.1 向量的模"></a>15.1 向量的模</h4><p>向量的模，表示向量的长度；向量$\boldsymbol x = (x_1, x_2, \cdots, x_n)^{\mathrm T}$它的模的计算公式：</p>
<script type="math/tex; mode=display">
|\boldsymbol x| = \sqrt{x_1^2+x_2^2+ \cdots + x_n^2}</script><h4 id="15-2-向量的常见范数"><a href="#15-2-向量的常见范数" class="headerlink" title="15.2 向量的常见范数"></a>15.2 向量的常见范数</h4><h5 id="15-2-0-向量的0范数"><a href="#15-2-0-向量的0范数" class="headerlink" title="15.2.0 向量的0范数"></a>15.2.0 向量的0范数</h5><p>向量中非零元素的个数 ；</p>
<h5 id="15-2-1-向量的1范数-L1范数、曼哈顿范数"><a href="#15-2-1-向量的1范数-L1范数、曼哈顿范数" class="headerlink" title="15.2.1 向量的1范数(L1范数、曼哈顿范数)"></a>15.2.1 向量的1范数(L1范数、曼哈顿范数)</h5><script type="math/tex; mode=display">
\|\boldsymbol x \|_1 = \sum_{i = 1}^{n} |x_i|</script><h5 id="15-2-2-向量的2范数-L2范数、欧式范数"><a href="#15-2-2-向量的2范数-L2范数、欧式范数" class="headerlink" title="15.2.2 向量的2范数(L2范数、欧式范数)"></a>15.2.2 向量的2范数(L2范数、欧式范数)</h5><script type="math/tex; mode=display">
\|\boldsymbol x \|_2 = \sqrt{\sum_{i = 1}^{n} x_i^2}</script><p><strong><font color="red">向量的2范数等价于向量的模长</font></strong>。</p>
<h5 id="15-2-3-向量的无穷范数-infty-范数、最大值范数"><a href="#15-2-3-向量的无穷范数-infty-范数、最大值范数" class="headerlink" title="15.2.3 向量的无穷范数($\infty$范数、最大值范数)"></a>15.2.3 向量的无穷范数($\infty$范数、最大值范数)</h5><script type="math/tex; mode=display">
\| \boldsymbol x \|_{\infty} = \max_{1 \leq i \leq n} |x_i|</script><h5 id="15-2-4-向量的p范数"><a href="#15-2-4-向量的p范数" class="headerlink" title="15.2.4 向量的p范数"></a>15.2.4 向量的p范数</h5><script type="math/tex; mode=display">
\| \boldsymbol x \|_p = \left( \sum _{i = 1}^{n} |x_i|^p \right)^{\frac{1}{p}}</script><blockquote>
<p>临时补充：复数的模</p>
<p>$z_1 = a + bi$、$z_2 = c + di$</p>
<p>$|z_1 \times z_2| = |z_1| \times |z_2|$</p>
<p>$\left|\dfrac{z_1}{z_2} \right| = \dfrac{|z_1|}{|z_2|}$</p>
</blockquote>
<p><a href="https://blog.csdn.net/geter_CS/article/details/85100045?spm=1001.2101.3001.6650.13&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-13-85100045-blog-87282642.pc_relevant_multi_platform_whitelistv4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-13-85100045-blog-87282642.pc_relevant_multi_platform_whitelistv4&amp;utm_relevant_index=21">熟悉陌生的2-范数（向量的模）</a></p>
<p><a href="https://blog.csdn.net/linkequa/article/details/87282642?spm=1001.2101.3001.6650.4&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EESLANDING%7Edefault-4-87282642-blog-81698878.pc_relevant_landingrelevant&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EESLANDING%7Edefault-4-87282642-blog-81698878.pc_relevant_landingrelevant&amp;utm_relevant_index=9">向量 模（module） 范数（norm）</a></p>
<h3 id="16-充分性，必要性，充分条件，必要条件的区别"><a href="#16-充分性，必要性，充分条件，必要条件的区别" class="headerlink" title="16 充分性，必要性，充分条件，必要条件的区别"></a>16 充分性，必要性，充分条件，必要条件的区别</h3><h4 id="16-1-概述"><a href="#16-1-概述" class="headerlink" title="16.1 概述"></a>16.1 概述</h4><p>令：$A$是命题，$B$是结论，则有：</p>
<ul>
<li>$A \longrightarrow B$：$A$是$B$的充分条件；<ul>
<li>$A$成立$B$一定成立，$A$不成立$B$不一定不成立。</li>
</ul>
</li>
<li>$B \longrightarrow A$：$A$是$B$的必要条件；<ul>
<li>$A$成立$B$不一定成立，$A$不成立$B$一定不成立。</li>
</ul>
</li>
<li>$A \longleftrightarrow B$：$A$和$B$互为充要条件（充分必要），即$B$成立当且仅当$A$成立。</li>
</ul>
<blockquote>
<p>E.g. 说$B$的充要条件是$A$，是把后者作为条件：</p>
<p>即证$B$的充要条件是$A$，是：证充分性（$A \longrightarrow B$），证必要性（$B \longrightarrow A$）</p>
</blockquote>
<p>转载于：<a href="https://www.cnblogs.com/acha/p/6263887.html">充分性，必要性，充分条件，必要条件的区别 - 博客园 - 用户_zwl </a></p>
<h4 id="16-2-充分条件、必要条件、充要条件"><a href="#16-2-充分条件、必要条件、充要条件" class="headerlink" title="16.2 充分条件、必要条件、充要条件"></a>16.2 充分条件、必要条件、充要条件</h4><p>以初中平面几何中的全等三角形的判定定理1：若两个三角形的三条对应边均相等，则这两个三角形全等为例，首先引入充分条件与必要条件。</p>
<ul>
<li><strong>充分条件：</strong> 若已知两三角形三边对应相等，则两三角形全等 ，也就是说：两个三角形三边对应相等是这两个三角形全等成立的充分条件 。其中充分条件的含义是，若两三角形三边对应相等，则有充分理由断定“这两个三角形全等”成立。</li>
<li><strong>必要条件：</strong> 若已知两个三角形全等，则这两个三角形三条边对应相等 ，也就是说：两个三角形三边对应相等是这两个三角形全等成立的必要条件 。其中必要条件的含义是，若两个三角形全等，则必定需要 满足一个条件：“两三角形三边对应相等”，即只有满足了这个条件，这两个三角形全等才成立。</li>
<li><strong>充分必要条件：</strong> 结合上述两部分论断，我们可以得到如下的命题：两三角形全等的充要条件是这两个三角形的三边对应相等。</li>
</ul>
<h4 id="16-3-充分性、必要性"><a href="#16-3-充分性、必要性" class="headerlink" title="16.3 充分性、必要性"></a>16.3 充分性、必要性</h4><ul>
<li><strong>充分性：</strong>常用箭头$\Longleftarrow$表示，即由条件推出结论（假设两三角形三边对应相等成立，来证明两三角形全等）。</li>
<li><strong>必要性：</strong>常用箭头$\Longrightarrow$表示，即由结论推条件（假设两三角形全等成立，来证明三角形三边对应相等）。</li>
</ul>
<h4 id="16-4-定义"><a href="#16-4-定义" class="headerlink" title="16.4 定义"></a>16.4 定义</h4><p>有了上面的例子，很容易得到充分条件、必要条件等的定义：</p>
<p>设$A$、$B$是两个命题，并且有：$A$成立的充要条件是$B$成立，则：</p>
<ul>
<li><strong>充分条件：</strong>若$B$成立则$A$成立，即$B$成立是$A$成立的充分条件。</li>
<li><strong>必要条件：</strong>若$A$成立则$B$成立，即$B$成立是$A$成立的必要条件。</li>
<li><strong>充分性：</strong> 用 $\Longleftarrow$代替，即“假定$B$成立，去证明$A$成立”(条件推结论)。</li>
<li><strong>必要性：</strong> 用$\Longrightarrow$代替，即“假定$A$成立，去证明$B$成立”(结论推条件)。</li>
</ul>
<h4 id="16-5-“当且仅当”（if-and-only-if）"><a href="#16-5-“当且仅当”（if-and-only-if）" class="headerlink" title="16.5 “当且仅当”（if and only if）"></a>16.5 “当且仅当”（if and only if）</h4><p>有时候也会用到<strong>“当且仅当”</strong>一词来表示<strong>充要条件</strong> 。仍用上面所举三角形全等的例子，就可以说：两三角形全等当且仅当两三角形三边对应相等。也可以一般化为：$A$成立当且仅当$B$成立，这时：</p>
<ul>
<li><strong>充分条件：</strong>当$B$成立时$A$成立（$B$成立能充分说明$A$成立）。</li>
<li><strong>必要条件：</strong>仅当$B$成立时$A$才成立（$B$成立是$A$成立所必定需要的）。</li>
</ul>
<h4 id="16-6-补充：充分必要条件的意义"><a href="#16-6-补充：充分必要条件的意义" class="headerlink" title="16.6 补充：充分必要条件的意义"></a>16.6 补充：充分必要条件的意义</h4><p>$A$成立的充要条件是$B$成立，说明命题$A$与$B$是相互等价的，是同一现象（事物）的不同表现形式。<br>也即：若$A$成立的充要条件是$B$成立，则得到$A$与$B$互为充要条件。</p>
<h4 id="16-7-找准哪个命题是“充要条件”"><a href="#16-7-找准哪个命题是“充要条件”" class="headerlink" title="16.7 找准哪个命题是“充要条件”"></a>16.7 找准哪个命题是“充要条件”</h4><p>实际运用中，常会出现两种十分相似的表述，但是二者仍有很大区别，即</p>
<ul>
<li>$A$成立的充要条件是$B$。</li>
<li>$A$成立是$B$成立的充要条件。</li>
</ul>
<p>在第一种表述中，可以认为$A$是“结论”，$B$是“条件”；</p>
<p>而第二种表述恰恰相反。如果在实际分析中出现了第二种表述，就不能简单地认为$A\Longrightarrow B$是必要性了，还需要找到哪个命题是“条件”，哪个命题是“结论”，然后进行进一步的分析。</p>
<p>转载于：<a href="https://blog.csdn.net/qq_41437512/article/details/108066877">理解充分条件与必要条件 - CSDN - CSDN用户zorchp </a></p>
<h3 id="17-概率论——总体、样本、统计量-顺序统计量-的基本概念"><a href="#17-概率论——总体、样本、统计量-顺序统计量-的基本概念" class="headerlink" title="17 概率论——总体、样本、统计量(顺序统计量)的基本概念"></a>17 概率论——总体、样本、统计量(顺序统计量)的基本概念</h3><h4 id="17-1-总体"><a href="#17-1-总体" class="headerlink" title="17.1 总体"></a>17.1 总体</h4><p>总体是与我们所研究的问题有关的所有个体组成，而样本是总体中抽取的一部分个体。若总体中个体的数目为有限个，则称为有限总体, 否则称为无限总体。 </p>
<p><strong>关于总体的一个争论：总体是不是一个随机变量？</strong></p>
<p>具体可以参考下面这个链接：<a href="https://www.zhihu.com/question/301826927">为什么总体和样本都是随机变量？ - 知乎</a></p>
<h4 id="17-2-样本"><a href="#17-2-样本" class="headerlink" title="17.2 样本"></a>17.2 样本</h4><p><strong><font color="red">样本具有两重性</font></strong>，意思是样本<strong>既可看成具体的数，又可以看成随机变量(或随机向量)</strong>。在完成抽样后，它是具体的数；在实施抽样前，它被看成随机变量。因为在实施具体抽样之前无法预料抽样的结果，只 能预料它可能取值的范围，故可把它看成一个随机变量，因此才有概率分布可言。为区别起见，今后用大写的英文字母表示随机变量或随机向量，用小写字母表示具体的观察值。</p>
<h4 id="17-3-统计量"><a href="#17-3-统计量" class="headerlink" title="17.3 统计量"></a>17.3 统计量</h4><p><strong>数理统计的任务是通过样本去推断总体</strong>。而样本自身是一些杂乱无章的数字，要对这些数字进行加工整理，计算出一些有用的量。可以这样理解：这种由样本算出来的量，把样本中与所要解决的问题有关的信息集中起来了我们把这种量称为统计量，其定义如下：</p>
<p><strong>由样本算出的量是统计量，或者是统计量是样本的函数</strong>。对这一定义我们作如下几点说明: </p>
<ul>
<li><p>统计量只与样本有关，不能与未知参数有关。</p>
<ul>
<li>例如，$X \sim N(\mu, \sigma^2)$，当$\mu$和$\sigma^2$皆为未知参数时，$\sum _ {i=1}^{n}(X_i-\mu)^2$和$\sum _ {i=1}^{n}\dfrac{X_i^2}{\sigma^2}$都不是统计量，而$\sum _ {i=1}^{n}X_i ^2$和$\sum_{i=1}^{n}X_i^2$均是统计量</li>
</ul>
</li>
<li>由于样本具有两重性，同时统计量是样本的函数，因此统计量也具有两重性。正因为统计量可视为随机变量(或随机向量)，因此才有概率分布可言，这是利用统计量进行统计推断的依据。 </li>
<li>在什么问题中选用什么统计量，要看问题的性质。一般说来，所提出的统计量应是最好的集中了样本中与所讨论问题有关的信息，这不是容易做到的。</li>
</ul>
<h4 id="17-4-顺序统计量"><a href="#17-4-顺序统计量" class="headerlink" title="17.4 顺序统计量"></a>17.4 顺序统计量</h4><p>首先直接说明几点前提和结论：</p>
<ul>
<li>随机变量不可以比较大小；</li>
<li>顺序统计量属于随机变量。</li>
</ul>
<h5 id="17-4-1-顺序统计量的定义"><a href="#17-4-1-顺序统计量的定义" class="headerlink" title="17.4.1 顺序统计量的定义"></a>17.4.1 顺序统计量的定义</h5><p>顺序统计量：</p>
<script type="math/tex; mode=display">
X_{(1)}\le X_{(2)}\le \dots\le X_{(n)}</script><p>其中，$X _ {(1)}=\min\begin{Bmatrix}X_1,\dots,X_n\end{Bmatrix}$，$X_{(n)}=\max\begin{Bmatrix}X_1, \cdots, X_n\end{Bmatrix}$，$X_{(k)}$为$X_1, \cdots, X_n$的递增排序的第$k$位。$X _ {(n)}-X_{(1)}$样本极差。</p>
<h5 id="17-4-2-关于顺序统计量的理解"><a href="#17-4-2-关于顺序统计量的理解" class="headerlink" title="17.4.2 关于顺序统计量的理解"></a>17.4.2 关于顺序统计量的理解</h5><p>其实到写到这为止感觉还是没有透彻理解顺序统计量，我自己关于顺序统计量主要有两个疑点：</p>
<ul>
<li>从表面上看，顺序统计量就是对随机变量大小排序，但是我们知道随机变量是不能比较大小的。</li>
<li>其次，为什么顺序统计量是一个随机变量，感觉随机变量排序后只是一个大小的比较结果，咋就能视为一个随机变量了呢？</li>
</ul>
<p>首先，先说一下我的简单理解：</p>
<p>从总体中抽出$n$个样本$X_1, \cdots, X_n$，我们想要了解这些样本中大小排第$k$的样本(随机变量)的分布情况$F_{X_{(k)}}(x_k) = P(X _ {(k)} &lt; x_k)$，由于$k$的取值可以从$1 \sim n$，即我们先研究排序中每一位置的分布情况，所以何不就直接从小到大排个序，这也就导出了顺序统计量。</p>
<p>其次，我们看一下ChatGPT的回答：</p>
<ul>
<li>问：随机变量能比较大小吗？<ul>
<li>答：不可以，随机变量是数值不确定的变量，它们本身并不具有可比性，不能比较大小。但是我们可以对随机变量的概率分布进行比较，即可以比较两个随机变量所代表的概率分布的差异程度。 </li>
</ul>
</li>
<li>问：随机变量不能比较大小为什么会出现顺序统计量？<ul>
<li>答1：顺序统计量并不是指对随机变量进行大小比较，而是指对样本数据进行排序，将数据按照其大小排列，并编号。顺序统计量是样本中某一数值的排名，通常用于研究样本分布的性质，如判断样本的正态性等。因此，顺序统计量是由样本数据而不是随机变量生成的。 </li>
<li>答2：随机变量是描述随机现象的数学概念，不能直接比较大小。但是，我们可以通过研究其特定分布情况，对其进行统计分析，以得到有关其大小关系的统计结论。因此，出现了顺序统计量的概念，即对随机变量的样本数据进行排序，从而对其分布情况进行研究。 </li>
</ul>
</li>
<li>问：顺序统计量是随机变量但随机变量不是不能比较大小吗？<ul>
<li>答：顺序统计量和随机变量是两个不同的概念，顺序统计量是对随机变量的一种排列（即大小顺序）的统计描述。顺序统计量是随机变量，因为其值取决于随机变量的值，并且随机变量值不同时顺序统计量值也不同。然而，随机变量本身不能比较大小，因为它是一个概率分布，只能描述数据的可能性，而不能对数据的大小进行比较。 </li>
</ul>
</li>
<li>问：如何理解你说的“顺序统计量是对随机变量的一种排列（即大小顺序）的统计描述”，你的意思不就是对随机变量比较大小然后排序吗 ？<ul>
<li>答：是的，顺序统计量通过比较随机变量的大小并将其进行排序，得出其大小顺序，以此作为对随机变量的统计描述。但是这种比较大小和排序行为仅仅是在计算上的表现，并不能说明随机变量本身具有大小关系，因为随机变量本身并不具有确定的值。 </li>
</ul>
</li>
<li>问：请问“但是这种比较大小和排序行为仅仅是在计算上的表现，并不能说明随机变量本身具有大小关系”这句话能不能详细解释一下？<ul>
<li>答：所谓“比较大小和排序行为仅仅是在计算上的表现”，是指我们在研究统计问题时，常常需要对一组随机变量的取值进行比较和排序，以得到更直观、客观的描述。然而，这种比较大小和排序的行为，只是计算的一种方法，并不能真实反映出随机变量本身的大小关系，因为随机变量的大小关系是不确定的，只能通过概率的方式来表示。 </li>
</ul>
</li>
</ul>
<p>关于这一部分，主要参考下面的链接，特别注意第一个链接特别好：</p>
<p><a href="http://staff.ustc.edu.cn/~zwp/teach/Prob-Stat/Lec12_slides.pdf">概率论与数理统计课件 - 张伟平 - USTC</a>；<a href="http://staff.ustc.edu.cn/~zwp/index.html">张伟平个人主页 - 统计与金融系教授</a></p>
<p><a href="https://bookdown.org/hezhijian/book/intro.html#section-8">数理统计讲义</a></p>
<p><a href="https://www.cnblogs.com/jy333/p/14345941.html">数理统计1：数理统计的概念，总体与样本，统计量 - 江景平个人</a></p>
<h3 id="18-条件概率和事件的交的概率有什么区别"><a href="#18-条件概率和事件的交的概率有什么区别" class="headerlink" title="18 条件概率和事件的交的概率有什么区别"></a>18 条件概率和事件的交的概率有什么区别</h3><p>简单来说：</p>
<ul>
<li><p>条件概率是已知前提条件中描述的事件发生了。</p>
</li>
<li><p>事件的交则是涉及到的事件均还没有发生，求这几个事件一块发生的概率。</p>
</li>
</ul>
<blockquote>
<p>本质上来说，条件概率减小了事件的空间，而一般事件的交仍然是整个样本空间$\Omega$</p>
</blockquote>
<p>常见题目中的字眼：</p>
<ul>
<li>若出现“在已知。。。”，则一般为条件概率；</li>
<li>若一般出现“并且、且”等字眼，则为事件的交。</li>
</ul>
<p>参考链接：<a href="https://zhidao.baidu.com/question/988259011390372139.html">概率论中，条件概率和事件的交的概率有什么区别</a></p>
<h3 id="19-随机向量"><a href="#19-随机向量" class="headerlink" title="19 随机向量"></a>19 随机向量</h3><ul>
<li>根据参考链接[19.1]的评论区：<ul>
<li>总体研究的是一项数量指标还是多个维度的数量指标？比如一个班上的学生是一个总体，$X_1$是身高，$X_2$是体重，样本是$(X_1, X_2)$是个二维随机变量。还是总体只研究一个指标，比如身高$X$，$(X_1, X_2)$是样本，代表第一个学生和第二个学生的身高值。知乎博主给出的回答是：都可以，看你怎么定义研究的内容。 </li>
</ul>
</li>
<li>根据参考链接[19.2]给的例子：<ul>
<li>当个体上的数量指标不止一项时，我们用随机向量来表示总体。例如研究某地区小学生的发育状况时，人们关心的是其身高$X$和体重$Y$这两个数量指标，此时总体就可以用二维随机向量$ (X，Y)$或其联合分布$F(x, y)$表示。</li>
</ul>
</li>
</ul>
<blockquote>
<p>此外，我还从今天(2023.02.08)发现的最强大的智能对话ChatGPT上，通过问答获得以下结果：</p>
<p><img src="https://pic.imgdb.cn/item/63e3b44c4757feff33bd3f3b.jpg" style="zoom:70%"></p>
</blockquote>
<p>参考链接：</p>
<p>【19.1】<a href="https://www.zhihu.com/question/301826927/answer/528085629">为什么总体和样本都是随机变量？ - 杜克清风侠的回答 - 知乎</a></p>
<p>【19.2】<a href="http://staff.ustc.edu.cn/~zwp/teach/Prob-Stat/Lec12_slides.pdf">概率论与数理统计课件 - 张伟平 - USTC</a></p>
<h3 id="20-矩阵相关定义性质全总结"><a href="#20-矩阵相关定义性质全总结" class="headerlink" title="20 矩阵相关定义性质全总结"></a>20 矩阵相关定义性质全总结</h3><h4 id="20-1-基本概念"><a href="#20-1-基本概念" class="headerlink" title="20.1 基本概念"></a>20.1 基本概念</h4><ul>
<li><strong>矩阵定义：</strong>$m \times n$矩阵为$m \times n$个数排成的$m$行$n$列的表格，当$m=n$时，矩阵$\boldsymbol A$称为$n$阶方阵。</li>
<li><strong>零矩阵：</strong>矩阵所有元素都为0，在下文中用$\boldsymbol O$表示。</li>
<li><strong>单位阵：</strong>对角线全为1，其他全为0的方阵，一般用$\boldsymbol I$或$\boldsymbol E$表示，本文用$\boldsymbol I$，用$\boldsymbol E$表示初等矩阵。</li>
<li><strong>同型矩阵：</strong>$\boldsymbol A$矩阵为$m \times n$矩阵，$\boldsymbol B$矩阵为$s \times t$矩阵，若$m=s,n=t$，$\boldsymbol A$和$\boldsymbol B$为同型矩阵。</li>
<li><strong>矩阵相等：</strong>$\boldsymbol A$和$\boldsymbol B$相等意味着两个同型矩阵对应的元素都相等。</li>
<li><strong>矩阵行列式：</strong>$n$阶方阵$\boldsymbol A$构成的行列式，一般记为：$\det \boldsymbol A$或者$|\boldsymbol A|$。</li>
</ul>
<blockquote>
<p>注意：<br>只有方阵才有行列式；矩阵$\boldsymbol A$是表格，而行列式$|\boldsymbol A|$是数。</p>
</blockquote>
<h4 id="20-2-基本运算"><a href="#20-2-基本运算" class="headerlink" title="20.2 基本运算"></a>20.2 基本运算</h4><ul>
<li><strong>乘法：</strong>设$\boldsymbol A$是一个$m \times s$矩阵，$\boldsymbol B$是一个$s \times t$矩阵(即左矩阵的列数=右矩阵的行数)，则乘积$\boldsymbol{AB}$是一个$m \times t$矩阵，记为$\boldsymbol C$。</li>
<li><strong>矩阵多项式：</strong> 设$\boldsymbol A$是$n$阶矩阵，$f(x)=a_m x^m+ \cdots +a_1 x+a_0$是$x$的多项式，则称$a_m\boldsymbol A^m+a_{m-1}\boldsymbol A^{m-1}+ \cdots +a_1\boldsymbol A+a_0 \boldsymbol I$为矩阵多项式，记为$f(\boldsymbol A)$。</li>
</ul>
<p>注意一下几点：</p>
<ol>
<li>一般情况$\boldsymbol{AB} \neq \boldsymbol{BA}$；</li>
<li>$\boldsymbol A \neq \boldsymbol O$，$\boldsymbol B \neq \boldsymbol O$，但有可能$\boldsymbol{AB} = \boldsymbol O$；</li>
<li>$\boldsymbol{AB} = \boldsymbol{AC}, \boldsymbol A \neq \boldsymbol O$不能推出$\boldsymbol B = \boldsymbol C$；</li>
<li>$\boldsymbol{AB} = \boldsymbol O$可推出$\boldsymbol B$的列向量是$\boldsymbol{Ax}= \boldsymbol 0$的解；</li>
</ol>
<h4 id="20-3-矩阵基础操作"><a href="#20-3-矩阵基础操作" class="headerlink" title="20.3 矩阵基础操作"></a>20.3 矩阵基础操作</h4><h5 id="20-3-1-矩阵转置"><a href="#20-3-1-矩阵转置" class="headerlink" title="20.3.1 矩阵转置"></a>20.3.1 矩阵转置</h5><p><strong>(一) 定义</strong></p>
<p>将$m \times n$型矩阵$\boldsymbol A=(a_{ij})_{m \times n}$的行列互换的到的$n \times m$矩阵$(a_{ji})_{n \times m}$，称为$\boldsymbol A$的转置矩阵。 </p>
<p><strong>(二) 性质</strong></p>
<ol>
<li>$(\boldsymbol A + \boldsymbol B)^{\mathrm T} = \boldsymbol A^{\mathrm{T}} + \boldsymbol B^{\mathrm{T}}$</li>
<li>$(k\boldsymbol A)^{\mathrm T} = k\boldsymbol A^{\mathrm T}$</li>
<li>$(\boldsymbol A \boldsymbol B)^{\mathrm T} = \boldsymbol B^{\mathrm T}\boldsymbol A^{\mathrm T}$</li>
</ol>
<h5 id="20-3-2-伴随矩阵"><a href="#20-3-2-伴随矩阵" class="headerlink" title="20.3.2 伴随矩阵"></a>20.3.2 伴随矩阵</h5><p><strong>(一) 余子式和代数余子式</strong></p>
<ul>
<li><strong>余子式：</strong>是指在$n$阶行列式中，把元素$a_{ij}$(其中$i$和$j$分别表示行和列)所在的第$i$行和第$j$列划去后，留下来的$n-1$阶行列式叫做元素$a_{ij}$的余子式，记做$M_{ij}$。</li>
<li><strong>代数余子式：</strong>在余子式的基础上，记$A_{ij}=(-1)^{i+j}M_{ij}$，叫做元素$a_{ij}$的代数余子式。</li>
</ul>
<blockquote>
<p>注意：<br>只有方阵才有余子式，余子式和代数余子式都是一个行列式，其结果是一个数而不是矩阵。 </p>
</blockquote>
<p>只讲概念比较枯燥，通过例题计算更直观，例如计算下面这个行列式的$a_{32}$元素的余子式和代数余子式分别是多少： </p>
<script type="math/tex; mode=display">
\left|\begin{array}{lll}
a_ {11}& a_ {12} & a_ {13} \\
a_ {21} & a_ {22} & a_ {23} \\
a_ {31} & a_ {32} & a_ {33}
\end{array}\right|</script><p>根据概念，所谓$a_{32}$的余子式就是把$a_{32}$所在的行和列划去，留下来的行列式，那么$a_{32}$的余子式就是：</p>
<script type="math/tex; mode=display">
M_{32}= \left|\begin{array}{ll}
a_ {11}& a_ {13} \\
a_ {21} & a_ {23}
\end{array}\right|</script><p>那么，顺势而推，$a_{32}$的代数余子式就是：</p>
<script type="math/tex; mode=display">
A_{32} = (-1)^{3+2}\left|\begin{array}{ll}
a_ {11}& a_ {13} \\
a_ {21} & a_ {23}
\end{array}\right|</script><p><strong>(二) 伴随矩阵</strong></p>
<p>矩阵$\boldsymbol A​$的伴随矩阵一般记为$\boldsymbol A^*​$。由矩阵$\boldsymbol A​$的所有代数余子式构成，列对应行。</p>
<p>方阵$\boldsymbol A = (a_{ij})_{n \times n}$的各个元素的代数余子式转置构成伴随矩阵$\boldsymbol A^*$：</p>
<script type="math/tex; mode=display">
\boldsymbol A^* = \left(\begin{array}{cccc}
A_{11} & A_{21} & \cdots & A_{n1} \\
A_{12} & A_{22} & \cdots & A_{n2} \\
\vdots & \vdots & \ddots & \vdots\\
A_{1n} & A_{2n} & \cdots & A_{nn}
\end{array}\right)</script><p><strong>(三) 相关性质</strong></p>
<ol>
<li>$\boldsymbol A \boldsymbol A^ <em> =$ $\boldsymbol A^ </em> \boldsymbol A = |\boldsymbol A|\boldsymbol I$</li>
<li>$(\boldsymbol A^ <em> )^{-1} =$ $(\boldsymbol A^{-1})^ </em> $1</li>
<li>$(k \boldsymbol A)^ <em>  =$ $k^{n-1}\boldsymbol A^ </em> $</li>
<li>$(\boldsymbol A^ <em> )^{\mathrm{T}} =$ $(\boldsymbol A^{\mathrm{T}})^</em>$</li>
<li>$|\boldsymbol A^ * | = |\boldsymbol A|^{n-1}$</li>
<li>$\boldsymbol A^{-1} = \dfrac{1}{|\boldsymbol A|}\boldsymbol A^ * $</li>
<li>$(\boldsymbol{AB})^<em> =$ $\boldsymbol B^ </em> \boldsymbol A^ * $</li>
<li><p>伴随矩阵的秩：</p>
<ul>
<li>$\mathrm{rank}(\boldsymbol A^ * ) = n, \mathrm{rank}(\boldsymbol A) = n$</li>
<li>$\mathrm{rank}(\boldsymbol A^ * ) = 1, \mathrm{rank}(\boldsymbol A) = n-1$</li>
<li>$\mathrm{rank}(\boldsymbol A^ * ) = 0, \mathrm{rank}(\boldsymbol A) &lt; n-1$</li>
</ul>
<blockquote>
<p>矩阵的秩等于0的充分必要条件是这个矩阵是零矩阵。 </p>
</blockquote>
</li>
</ol>
<h5 id="20-3-3-逆矩阵"><a href="#20-3-3-逆矩阵" class="headerlink" title="20.3.3 逆矩阵"></a>20.3.3 逆矩阵</h5><p><strong>(一) 定义</strong></p>
<p>只有方阵才有逆矩阵，非方阵可以定义伪逆。</p>
<p>若$\boldsymbol A$、$\boldsymbol B$为$n$阶方阵，且满足：$\boldsymbol{AB} = \boldsymbol{BA} = \boldsymbol I$，则$\boldsymbol A$和$\boldsymbol B$互为逆矩阵，即：</p>
<script type="math/tex; mode=display">
\boldsymbol A^{-1} = \boldsymbol B; \quad \boldsymbol B^{-1} = \boldsymbol A</script><p><strong>(二) $n$阶矩阵$\boldsymbol A$可逆的充分必要条件</strong></p>
<ol>
<li>存在$n$阶矩阵$\boldsymbol A$，使$\boldsymbol{AB} = \boldsymbol I$或者$\boldsymbol{BA} = \boldsymbol I$；</li>
<li>$|\boldsymbol A|≠ 0​$，或者$\boldsymbol A​$满秩，或者$\boldsymbol A​$的列(行)向量线性无关；</li>
<li>齐次方程组$\boldsymbol{Ax = 0}$只有零解；</li>
<li>任意$\boldsymbol b$，非齐次线性方程组$\boldsymbol{Ax = b}$总有唯一解；</li>
<li>矩阵$\boldsymbol A$的所有特征值均不等于0；</li>
<li>能表示成一些初等矩阵的乘积：PN…P2P1A=E</li>
</ol>
<p><strong>(三) 性质</strong></p>
<ol>
<li>$k≠0$，$(k\boldsymbol A)^{-1}=\dfrac{1}{k} \boldsymbol A^{-1}$</li>
<li>$(\boldsymbol A \boldsymbol B)^{-1} = \boldsymbol B^{-1} \boldsymbol A^{-1}$</li>
<li>$(\boldsymbol A^{\mathrm T})^{-1} = (\boldsymbol A^{-1})^{\mathrm T}$</li>
<li>$|\boldsymbol A^{-1}| = \dfrac{1}{|\boldsymbol A|}$</li>
</ol>
<blockquote>
<p>注意：即使$\boldsymbol A$、$\boldsymbol B$和$\boldsymbol A + \boldsymbol B$都可逆，一般的$(\boldsymbol A + \boldsymbol B)^{-1} \neq  \boldsymbol A^{-1}+\boldsymbol B^{-1}$。</p>
</blockquote>
<p><strong>(四) 矩阵求逆方法</strong></p>
<ul>
<li><strong>公式法：</strong>$|\boldsymbol A| \neq 0$，则$\boldsymbol A^{-1} = \dfrac{1}{|\boldsymbol A|}\boldsymbol A^*$</li>
<li><strong>初等变换：</strong>$(\boldsymbol A \mid \boldsymbol I) \longrightarrow (\boldsymbol I \mid \boldsymbol A^{-1})$</li>
</ul>
<h5 id="20-3-4-初等矩阵、行阶梯矩阵和行最简矩阵"><a href="#20-3-4-初等矩阵、行阶梯矩阵和行最简矩阵" class="headerlink" title="20.3.4 初等矩阵、行阶梯矩阵和行最简矩阵"></a>20.3.4 初等矩阵、行阶梯矩阵和行最简矩阵</h5><p><strong>(一) 初等变换</strong></p>
<p>设$\boldsymbol A$是$m \times n$矩阵，进行初等倍乘、互换、倍加行(列)变换，统称为初等变换。</p>
<ol>
<li><strong>倍乘：</strong>用某个非零常数$k(k≠0)$乘$\boldsymbol A$的某行(列)的每个元素，此时行列式的值乘以$k$。</li>
<li><strong>互换</strong>：互换$\boldsymbol A$的某两行(列)的位置，此时行列式变为相反数，即行列式的值会乘以-1。</li>
<li><strong>倍加行(列)：</strong>将$\boldsymbol A$的某行(列)元素的$k$倍加到另一行(列)，此时行列式不变。</li>
</ol>
<p><strong>(二) 初等矩阵</strong></p>
<p>单位矩阵经<strong>一次</strong>初等变换得到的矩阵为初等矩阵。 常见初等矩阵的表示可用$\boldsymbol E$、$\boldsymbol P$或者$\boldsymbol Q$这3个字母。</p>
<p><strong>(三) 等价矩阵</strong></p>
<p>在线性代数和矩阵论中，有两个$m \times n$阶矩阵$\boldsymbol A$和$\boldsymbol B$，如果这两个矩阵满足：</p>
<script type="math/tex; mode=display">
\boldsymbol P \boldsymbol A \boldsymbol Q = \boldsymbol B</script><p>其中，$\boldsymbol P$是$m \times m$阶可逆矩阵，$\boldsymbol Q$是$n \times n$阶可逆矩阵），那么这两个矩阵之间是等价关系。也就是说，存在可逆矩阵$\boldsymbol P、\boldsymbol Q$，使得$\boldsymbol A$经过有限次的初等变换得到$\boldsymbol B$。 </p>
<blockquote>
<p>注意：<br><strong><font color="green">左乘矩阵</font></strong>相当于对原矩阵进行了初等行变换，<strong><font color="green">右乘矩阵</font></strong>相当于对原矩阵进行了初等列变换。 </p>
</blockquote>
<p><strong>(四) 性质</strong></p>
<ol>
<li>初等矩阵的<strong>转置</strong>仍然是初等矩阵；</li>
<li>初等矩阵均是可逆矩阵($\boldsymbol E$，满秩)，且其逆矩阵仍是初等矩阵；</li>
<li>用初等矩阵$\boldsymbol E$左乘(右乘)$\boldsymbol A$，其结果$\boldsymbol{EA}$($\boldsymbol{AE}$)相当于对$\boldsymbol A$作相应的初等行(列)变换。</li>
</ol>
<p><strong>(五) 行阶梯矩阵、行最简矩阵</strong></p>
<ul>
<li><strong>行阶梯矩阵</strong><ul>
<li>如果矩阵有零行(即这一行元素全是0)，则零行在最底部；</li>
<li>每个非零元素的主元(即该行的最左边的第一个非零元)，它们的列指标随着行指标的递增而严格增大。</li>
</ul>
</li>
<li><strong>行最简矩阵</strong>：<ul>
<li>是行阶梯矩阵；</li>
<li>非零行的主元都是1；</li>
<li>主元所在的列的其他元素都是0。</li>
</ul>
</li>
</ul>
<h4 id="20-4-矩阵的几种属性"><a href="#20-4-矩阵的几种属性" class="headerlink" title="20.4 矩阵的几种属性"></a>20.4 矩阵的几种属性</h4><h5 id="20-4-1-方阵的行列式"><a href="#20-4-1-方阵的行列式" class="headerlink" title="20.4.1 方阵的行列式"></a>20.4.1 方阵的行列式</h5><p><strong>(一) 概念</strong></p>
<p>矩阵行列式是指矩阵的全部元素构成的行列式，$\boldsymbol A$是数域$\mathbb{R}$上的一个$n$阶矩阵，则所有$\boldsymbol A$中的元素组成的行列式称为矩阵$\boldsymbol A$的行列式，记为$|\boldsymbol A|$或$\det(\boldsymbol A)$。行列式的意义是变换后，<strong>空间的膨胀系数</strong>。</p>
<p><strong>(二) 性质</strong></p>
<ol>
<li>$|\boldsymbol A^{\mathrm T}| = |\boldsymbol A| = \lambda_1 \times \lambda_2 \times \cdots \times \lambda_n$</li>
<li>$|k\boldsymbol A| = k^n |\boldsymbol A|$</li>
<li>$|\boldsymbol{AB}| = |\boldsymbol A| |\boldsymbol B|$</li>
<li>$|\boldsymbol A^*| = |\boldsymbol A|^{n-1}$</li>
<li>$|\boldsymbol A^{-1}| = |\boldsymbol A|^{-1}$</li>
</ol>
<h5 id="20-4-2-矩阵的秩"><a href="#20-4-2-矩阵的秩" class="headerlink" title="20.4.2 矩阵的秩"></a>20.4.2 矩阵的秩</h5><p><strong>(一) k阶子式(数)</strong></p>
<p>在$m \times n​$矩阵$\boldsymbol A​$中，任取$k​$行与$k​$列($k \leq m,k \leq n​$)，位于这些行与列的交叉点上的$k^2​$个元素按其在原来矩阵$\boldsymbol A​$中的次序可构成一个$k​$阶<strong>行列式</strong>，称其为矩阵$\boldsymbol A​$的一个$k​$阶子式。 </p>
<ul>
<li><strong>顺序主子式</strong></li>
</ul>
<p>顺序主子式是$n$阶方阵的$n$个行列式的序列，第$k$个行列式由方阵的前$k$行和前$k$列组成。 </p>
<p><img src="https://pic.imgdb.cn/item/63e4f0394757feff33a046a4.jpg"></p>
<p><strong>(二) 秩的概念</strong></p>
<p>设$\boldsymbol A$为$m \times n$矩阵，若$\boldsymbol A$中存在$r$阶子式不等于0，$r$阶以上子式均等于0，则称矩阵$\boldsymbol A$的秩为$r$，记为$r(\boldsymbol A)=\mathrm{rank}(\boldsymbol A)$。零矩阵的秩规定为0。</p>
<p><strong>(三) 性质</strong></p>
<ol>
<li>$r(\boldsymbol A)=0$等价于$\boldsymbol A=\boldsymbol O$，同时也有$\boldsymbol A\neq \boldsymbol O$等价于$r(\boldsymbol A) \geq 1$；</li>
<li>$\boldsymbol A$是$n$阶矩阵，$r(\boldsymbol A) = n$等价于$|\boldsymbol A| \neq 0$等价于$\boldsymbol A$可逆，$r(A)&lt;n$等价于$|\boldsymbol A| = 0$等价于$\boldsymbol A$不可逆;</li>
<li>若$\boldsymbol A$是$m \times n$矩阵，则$r(\boldsymbol A) \leq \min(m,n)$；</li>
<li>经过初等变换矩阵的秩不变；</li>
</ol>
<p><strong>(四) 拓展性质(公式)</strong></p>
<ol>
<li>$r(\boldsymbol A)=r(\boldsymbol A^{\mathrm T})$；$r(\boldsymbol A \boldsymbol A^{\mathrm T})=r(\boldsymbol A)$</li>
<li>$r(\boldsymbol A+\boldsymbol B)&lt;=r(\boldsymbol A)+r(\boldsymbol B)$</li>
<li>$r(\boldsymbol{AB}) \leq \min(r(\boldsymbol A), r(\boldsymbol B)) \leq \max(r(\boldsymbol A), r(\boldsymbol B)) \leq r(\boldsymbol A + \boldsymbol B) \leq r(\boldsymbol A)+r(\boldsymbol B)$</li>
<li>若$\boldsymbol A$可逆，则$r(\boldsymbol{AB})=r(\boldsymbol B)$，$r(\boldsymbol{BA})=r(\boldsymbol B)$</li>
<li>若$\boldsymbol A$是$m \times n$矩阵，$\boldsymbol B$是$n×s$矩阵且$\boldsymbol{AB=O}$，则$r(\boldsymbol A)+r(\boldsymbol B) \leq n$</li>
</ol>
<h5 id="20-4-3-矩阵的迹"><a href="#20-4-3-矩阵的迹" class="headerlink" title="20.4.3 矩阵的迹"></a>20.4.3 矩阵的迹</h5><h5 id="20-4-4-矩阵的特征值"><a href="#20-4-4-矩阵的特征值" class="headerlink" title="20.4.4 矩阵的特征值"></a>20.4.4 矩阵的特征值</h5><p><strong>n阶矩阵一定有n个特征值。</strong>因为特征值是特征多项式的根,n阶方阵的特征多项式是个n次多项式,根据代数基本定理,n次多项式有且只有n个根(重根按重数计算),这些根可能是实数,也可能是复数。</p>
<p>更加详细的说法为：一个n阶矩阵一定有n个特征值（包括重根），也可能是复根。一个n阶实对称矩阵一定有n个实特征值（包括重根）。每一个特征值至少有一个特征向量（不止一个）。不同特征值对应特征向量线性无关。</p>
<p><strong>特征根是特征多项式的根的概念，对特征根t当有特征向量a满足Aa-ta等于0成立，称特征根为特征值。当方阵不能对角化时，不是所有特征根都能称为特征值。</strong> </p>
<h4 id="20-5-几种特殊矩阵"><a href="#20-5-几种特殊矩阵" class="headerlink" title="20.5 几种特殊矩阵"></a>20.5 几种特殊矩阵</h4><h5 id="20-5-1-奇异矩阵"><a href="#20-5-1-奇异矩阵" class="headerlink" title="20.5.1 奇异矩阵"></a>20.5.1 奇异矩阵</h5><p>行列式为零的方阵称为奇异矩阵。</p>
<p>该定义蕴含着奇异矩阵是方阵，因为行列式是对方阵而言的。行列式恰好为零，是不是就很“奇异”呢？换个问题，行列式恰好为1的矩阵奇异不奇异呢？行列式恰好为2呢？3呢？素数呢？从某种意义上说，这些矩阵确实都很奇异。但为何只说行列式为零的矩阵才奇异呢？</p>
<p>这很可能是由线性方程组的解的个数引出的名词。对于系数行列式非零的情况，方程组的解是唯一的；否则，就有无穷多解。换句话说，系数行列式可能取各种值，但不管是什么值，只要不为零，相应的方程组的解一定是唯一的。但是，如果系数行列式恰巧为零，方程组的解就可以有无穷多。这样，行列式为零的矩阵就显得很“突出”、很“不一样”、很“另类”、很“奇怪”，等等。而“奇异”包含了奇怪和异端两种意思，正好用于描述这种矩阵。 </p>
<p>参考链接：<br>本文来自李毅伟科学网博客。 链接地址：<a href="https://blog.sciencenet.cn/blog-315774-889594.html">https://blog.sciencenet.cn/blog-315774-889594.html </a>   </p>
<h5 id="20-5-2-正交矩阵"><a href="#20-5-2-正交矩阵" class="headerlink" title="20.5.2 正交矩阵"></a>20.5.2 正交矩阵</h5><p><strong>(一) 定义</strong></p>
<p>设$\boldsymbol Q$为$n$阶方阵，若$\boldsymbol {QQ}^{\mathrm T} = \boldsymbol Q^{\mathrm T} \boldsymbol Q=\boldsymbol I$，则称$\boldsymbol Q$为正交矩阵。 </p>
<p><strong>(二) 性质</strong></p>
<ol>
<li>$\boldsymbol Q^{\mathrm T} = \boldsymbol Q^{-1}$；</li>
<li>$\boldsymbol Q$的行(列)向量都是单位向量且两两正交；</li>
<li>$|\boldsymbol Q| = \pm 1$；</li>
<li>对任意一向量$\boldsymbol x$，有$|\boldsymbol{Qx}| = |\boldsymbol x|$，即正交阵不改变向量模长；</li>
<li>正交阵的特征值一定是1或-1；</li>
<li>正交阵的每一个行(列)向量都是模为1的向量；</li>
<li>正交阵每个元素绝对值都$\leq1$，若有一个元素为1，则该元素所在的行和列的其余元素均为0。 </li>
</ol>
<h5 id="20-5-3-相似矩阵"><a href="#20-5-3-相似矩阵" class="headerlink" title="20.5.3 相似矩阵"></a>20.5.3 相似矩阵</h5><p><strong>(一) 定义</strong></p>
<p>设$\boldsymbol A$、$\boldsymbol B$都是$n$阶矩阵，若存在<strong>可逆矩阵</strong>$\boldsymbol P$，使得$\boldsymbol P^{-1}\boldsymbol{AP}= \boldsymbol B$，则称$\boldsymbol B$是$\boldsymbol A$的相似矩阵，或$\boldsymbol A$相似于$\boldsymbol B$，记为$\boldsymbol A \sim \boldsymbol B$。</p>
<p><strong>(二) 性质</strong></p>
<ol>
<li>$\boldsymbol A \sim \boldsymbol A$；</li>
<li>$\boldsymbol A \sim \boldsymbol B \Longleftrightarrow \boldsymbol B \sim \boldsymbol A$；</li>
<li>若$\boldsymbol A \sim \boldsymbol B$，$\boldsymbol B \sim \boldsymbol C$ 则可以推导出$\boldsymbol A \sim \boldsymbol C$；</li>
<li>$n$阶<strong>方阵</strong>$\boldsymbol A$可对角化的<strong><font color="blue">充分必要条件</font></strong>是$\boldsymbol A$有$n$个线性无关的特征向量。(可得若$n$阶矩阵$\boldsymbol A$有$n$个不同的特征值$\lambda_1, \lambda_2, \cdots, \lambda_n$，则$\boldsymbol A$可相似对角化，且对角矩阵元素一一对应特征值。)</li>
<li>$n$阶矩阵$\boldsymbol A$可相似对角化的充分必要条件是$\boldsymbol A$的每个特征值中，线性无关的特征向量的个数恰好等于该特征值的重数。</li>
<li>若$\boldsymbol A \sim \boldsymbol \Lambda$，其中$\boldsymbol \Lambda$为<strong>对角阵</strong>，则称$\boldsymbol A$可相似对角化，$\boldsymbol \Lambda$是$\boldsymbol A$的相似标准形。</li>
</ol>
<p><strong>(三) 相似的必要条件</strong></p>
<ol>
<li>特征多项式相同：$|\lambda\boldsymbol I - \boldsymbol A|=|\lambda \boldsymbol I-\boldsymbol B|$</li>
<li>$r(\boldsymbol A)=r(\boldsymbol A)$</li>
<li>$\boldsymbol A$和$\boldsymbol B$有相同的特征值</li>
<li>$|\boldsymbol A|=|\boldsymbol B|=特征值之积$</li>
<li>$\mathrm{tr}(\boldsymbol A) = \mathrm{tr}(\boldsymbol B) = 特征值之和$</li>
<li>$\boldsymbol A^2 \sim \boldsymbol B^2(\boldsymbol A^n \sim \boldsymbol B^n)$</li>
<li>$\boldsymbol A+k\boldsymbol I \sim \boldsymbol B+k\boldsymbol I$</li>
<li>如果$\boldsymbol A$可逆，$\boldsymbol A^{-1} \sim \boldsymbol B^{-1}$</li>
</ol>
<h5 id="20-5-4-实对称矩阵"><a href="#20-5-4-实对称矩阵" class="headerlink" title="20.5.4 实对称矩阵"></a>20.5.4 实对称矩阵</h5><p><strong>(一) 定义</strong></p>
<p>除了主对角线，两侧相对应的数相同的矩阵，也即满足：</p>
<script type="math/tex; mode=display">
\boldsymbol A = \boldsymbol A^{\mathrm T}</script><p><strong>(二) 性质</strong></p>
<ol>
<li>实对称矩阵<strong>&lt;/font color=red&gt;必可相似对角化&lt;/font&gt;</strong>；</li>
<li>实对称矩阵的属于不同特征值对应的特征向量相互<strong><font color="red">正交</font></strong>；</li>
<li>设$\boldsymbol A$为$n$阶实对称矩阵，则必存在正交阵$\boldsymbol Q$，使得$\boldsymbol Q^{-1}\boldsymbol A\boldsymbol Q = \boldsymbol Q^{T}\boldsymbol A\boldsymbol Q = \boldsymbol \Lambda$</li>
</ol>
<h5 id="20-5-5-酉矩阵-幺正矩阵"><a href="#20-5-5-酉矩阵-幺正矩阵" class="headerlink" title="20.5.5 酉矩阵(幺正矩阵 )"></a>20.5.5 酉矩阵(幺正矩阵 )</h5><p><strong>(一) 定义</strong></p>
<p><strong>酉矩阵（unitary matrix）</strong>也叫<strong>幺正矩阵</strong>。当矩阵元为实数时也叫<strong>正交矩阵</strong>，是正交矩阵的复数拓展，即矩阵元可以是复数。酉矩阵$\boldsymbol U$的定义同样为：</p>
<script type="math/tex; mode=display">
\boldsymbol U^{\mathrm H} \boldsymbol U = \boldsymbol U \boldsymbol U^{\mathrm H} = \boldsymbol I</script><p>其中，$\boldsymbol U^{\mathrm H}$表示共轭转置。酉矩阵一定是方阵。</p>
<p><strong>(二) 性质</strong></p>
<ol>
<li>$\boldsymbol U^{-1} = \boldsymbol U^{\mathrm H}$，同样就有$\boldsymbol U^{-1}$和$\boldsymbol U^{\mathrm T}$也是酉矩阵；</li>
<li>$|\boldsymbol U| = 1$；</li>
</ol>
<p><strong>(三) 充要条件</strong></p>
<p>复矩阵$\boldsymbol A \in \mathbb{C}_{n \times n}$是酉矩阵的充要条件是$\boldsymbol A$的$n$个列向量是标准的正交向量。 可见，快速判断一个复矩阵是不是酉矩阵可以看它任意两个列向量是不是<strong>正交</strong>且<strong>单位</strong>的。</p>
<h5 id="20-5-6-正定矩阵"><a href="#20-5-6-正定矩阵" class="headerlink" title="20.5.6 正定矩阵"></a>20.5.6 正定矩阵</h5><p><strong>(一) 定义</strong><br>对于$n$阶方阵$\boldsymbol A$，若对于任何非零向量$\boldsymbol x$，都有：</p>
<script type="math/tex; mode=display">
\boldsymbol x^{\mathrm T}\boldsymbol {Ax} > 0</script><p>则$\boldsymbol A$为正定矩阵。在实数域上，正定矩阵是<strong>实对称矩阵</strong>，在复数域上，正定矩阵是<strong>Hermit矩阵</strong>。</p>
<p><strong>(二) 性质</strong></p>
<ol>
<li>正定阵行列式恒为正，即正定矩阵一定可逆；</li>
<li>实对称矩阵$\boldsymbol A$正定当且仅当$\boldsymbol A$与单位矩阵<strong>合同</strong>；</li>
<li>若$\boldsymbol A$是正定矩阵，则$\boldsymbol A$的逆矩阵也是正定矩阵；</li>
<li>两个正定矩阵的<strong>和</strong>是正定矩阵；</li>
<li>在实数域中，正定矩阵的主对角线元素均大于0。</li>
</ol>
<p><strong>(三) 一个矩阵是正定矩阵的充要条件</strong></p>
<ol>
<li>$\boldsymbol A$的特征值均为正；</li>
<li>存在可逆矩阵$\boldsymbol P$，使得$\boldsymbol A = \boldsymbol P^{\mathrm T} \boldsymbol P$，即$\boldsymbol A$与$\boldsymbol I$合同；</li>
<li>$\boldsymbol A$的前$k(k = 1,2,\cdots,n)$阶顺序主子式均大于零；</li>
</ol>
<h4 id="20-6-小结：常见几种矩阵变换——合同、相似、等价"><a href="#20-6-小结：常见几种矩阵变换——合同、相似、等价" class="headerlink" title="20.6 小结：常见几种矩阵变换——合同、相似、等价"></a>20.6 小结：常见几种矩阵变换——合同、相似、等价</h4><h5 id="20-6-1-矩阵合同"><a href="#20-6-1-矩阵合同" class="headerlink" title="20.6.1 矩阵合同"></a>20.6.1 矩阵合同</h5><p><strong>(一) 定义</strong></p>
<p>设$\boldsymbol A$和$\boldsymbol B$是两个$n$阶方阵，若存在可逆阵$\boldsymbol C$，使得$\boldsymbol C^{\mathrm T}\boldsymbol A\boldsymbol C=\boldsymbol B$，则称$\boldsymbol A$合同于$\boldsymbol B$，记成$\boldsymbol A \simeq \boldsymbol B$.</p>
<p>在线性代数，特别是二次型理论中，常常用到矩阵间的合同关系。 </p>
<p><strong>(二) 性质</strong></p>
<ol>
<li><strong>反身性：</strong>任意矩阵都与其自身合同；</li>
<li><strong>对称性：</strong>矩阵$\boldsymbol A$合同于矩阵$\boldsymbol A$，则可以推出矩阵$\boldsymbol A$合同于矩阵$\boldsymbol B$；</li>
<li><strong>传递性：</strong>矩阵$\boldsymbol A$合同于矩阵$\boldsymbol B$，矩阵$\boldsymbol B$合同于矩阵$\boldsymbol C$，则可以推出矩阵$\boldsymbol A$合同于矩阵$\boldsymbol C$。</li>
</ol>
<p><strong>(三) 矩阵合同的判别/充要条件</strong></p>
<ul>
<li>$r(\boldsymbol A) = r(\boldsymbol B)$<ul>
<li>设$\boldsymbol A$和$\boldsymbol B$均为复数域上的$n$阶对称矩阵，则$\boldsymbol A$与$\boldsymbol B$在复数域上合同等价于$\boldsymbol A$与$\boldsymbol B$的秩相同。</li>
<li>设$\boldsymbol A​$与$\boldsymbol B​$均为实数域上的$n​$阶对称矩阵，则$\boldsymbol A​$与$\boldsymbol B​$在实数域上合同等价于$\boldsymbol A​$与$\boldsymbol B​$有相同的正、负惯性指数(即正、负特征值的个数相等)。</li>
</ul>
</li>
</ul>
<h5 id="20-6-2-小结：相似、合同、等价"><a href="#20-6-2-小结：相似、合同、等价" class="headerlink" title="20.6.2 小结：相似、合同、等价"></a>20.6.2 小结：相似、合同、等价</h5><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>合同</th>
<th>相似</th>
<th>等价</th>
</tr>
</thead>
<tbody>
<tr>
<td>定义</td>
<td>$\boldsymbol C^{\mathrm T}\boldsymbol A\boldsymbol C=\boldsymbol B$</td>
<td>$\boldsymbol P^{-1}\boldsymbol A\boldsymbol P=\boldsymbol B$</td>
<td>$\boldsymbol P\boldsymbol A\boldsymbol Q=\boldsymbol B$</td>
</tr>
<tr>
<td>记作</td>
<td>$\boldsymbol A \simeq \boldsymbol B$</td>
<td>$\boldsymbol A \sim \boldsymbol B$</td>
<td>$\boldsymbol A \cong \boldsymbol B$</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>相似矩阵必为等价矩阵，等价矩阵未必为相似矩阵，满足$\boldsymbol{PQ=I}$的等价矩阵是相似矩阵。</li>
<li>合同矩阵必为等价矩阵，等价矩阵未必为合同矩阵，满足 pA=pB,qA=qB的等价矩阵是合同矩阵。</li>
<li>相似矩阵未必合同，合同矩阵未必相似。</li>
<li>正交相似矩阵必合同，正交合同矩阵必相似。</li>
<li>实对称矩阵相似必合同，实对称矩阵合同未必相似。</li>
</ul>
<p>矩阵合同、矩阵相似、矩阵等价是矩阵理论中常见的三个概念，它们的定义如下：</p>
<ol>
<li>矩阵合同：设$\boldsymbol A, \boldsymbol B$是 $n$ 阶实（或复）矩阵，若存在一个$n$阶可逆实（或复）矩阵$\boldsymbol P$，使得$\boldsymbol B = \boldsymbol{P^\mathrm T A P}$，则称矩阵$\boldsymbol A$和$\boldsymbol B$合同。记为$\boldsymbol A \simeq \boldsymbol B$。</li>
<li>矩阵相似：设$\boldsymbol A, \boldsymbol B$是$n$阶实（或复）矩阵，若存在一个$n$阶可逆实（或复）矩阵$\boldsymbol P$，使得$\boldsymbol B=\boldsymbol{P^{-1}AP}$，则称矩阵$\boldsymbol A$和$\boldsymbol B$相似。记为$\boldsymbol A \sim \boldsymbol B$。</li>
<li>矩阵等价：设$\boldsymbol A, \boldsymbol B$是$n$阶实（或复）矩阵，若存在$n$阶可逆实（或复）矩阵$\boldsymbol P, \boldsymbol Q$，使得$\boldsymbol B = \boldsymbol{P^{-1}AQ}$，则称矩阵$\boldsymbol A$和$\boldsymbol B$等价。记为$\boldsymbol A \approx \boldsymbol B$。</li>
</ol>
<p>三者的联系与区别如下：</p>
<ol>
<li>矩阵合同是一种特殊的矩阵等价关系，即如果矩阵$\boldsymbol A$ 和$\boldsymbol B$合同，则它们等价，即$\boldsymbol A \approx \boldsymbol B$。但反之不成立，即等价的矩阵不一定合同。</li>
<li>矩阵相似是一种更加严格的关系，如果矩阵$\boldsymbol A$和$\boldsymbol B$相似，则它们一定等价，即$\boldsymbol A \sim \boldsymbol B$蕴含$\boldsymbol A \approx \boldsymbol B$。但反之不成立，即等价的矩阵不一定相似。</li>
<li>矩阵等价是最一般的关系，如果矩阵$\boldsymbol A$和$\boldsymbol B$等价，则它们具有一些共同的性质，例如它们的秩、行列式、迹等都相同，但它们不一定相似或合同。</li>
</ol>
<p>总的来说，矩阵等价是最一般的关系，矩阵相似是一种较为特殊的等价关系，而矩阵合同是在实对称矩阵和复Hermitian矩阵中的等价关系。</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css">]]></content>
      <categories>
        <category>学习提升</category>
      </categories>
      <tags>
        <tag>问题</tag>
      </tags>
  </entry>
</search>
